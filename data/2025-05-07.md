<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.AI](#cs.AI) [Total: 25]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [math.OC](#math.OC) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 27]
- [cs.IR](#cs.IR) [Total: 5]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [math.AP](#math.AP) [Total: 1]
- [cs.RO](#cs.RO) [Total: 10]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.CR](#cs.CR) [Total: 4]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [stat.ML](#stat.ML) [Total: 10]
- [eess.IV](#eess.IV) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.CY](#cs.CY) [Total: 7]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models](https://arxiv.org/abs/2505.02847)
*Bang Zhang,Ruotian Ma,Qingxuan Jiang,Peisong Wang,Jiaqi Chen,Zheng Xie,Xingyu Chen,Yue Wang,Fanghua Ye,Jian Li,Yifan Yang,Zhaopeng Tu,Xiaolong Li*

Main category: cs.CL

TL;DR: SAGE是一种自动化评估框架，用于衡量大型语言模型（LLM）的高阶社会认知能力，通过模拟人类情感变化和内心思维，提供更真实的对话评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法难以衡量LLM对人类情感的理解，SAGE旨在填补这一空白，通过情感轨迹和内心思维提供更真实的评估。

Method: SAGE引入了一个“感知智能体”，在对话中模拟人类情感变化（情绪评分）和内心思维（可解释的思考过程）。

Result: 实验表明，SAGE的情感评分与心理学评估标准（如BLRI）高度相关，并揭示了前沿模型（如GPT-4o）与早期基线之间的显著差距。

Conclusion: SAGE为开发真正具备共情和社交能力的语言模型提供了可扩展、可解释的评估工具。

Abstract: Assessing how well a large language model (LLM) understands human, rather
than merely text, remains an open challenge. To bridge the gap, we introduce
Sentient Agent as a Judge (SAGE), an automated evaluation framework that
measures an LLM's higher-order social cognition. SAGE instantiates a Sentient
Agent that simulates human-like emotional changes and inner thoughts during
interaction, providing a more realistic evaluation of the tested model in
multi-turn conversations. At every turn, the agent reasons about (i) how its
emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a
numerical emotion trajectory and interpretable inner thoughts. Experiments on
100 supportive-dialogue scenarios show that the final Sentient emotion score
correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings
and utterance-level empathy metrics, validating psychological fidelity. We also
build a public Sentient Leaderboard covering 18 commercial and open-source
models that uncovers substantial gaps (up to 4x) between frontier systems
(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in
conventional leaderboards (e.g., Arena). SAGE thus provides a principled,
scalable and interpretable tool for tracking progress toward genuinely
empathetic and socially adept language agents.

</details>


### [2] [Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors](https://arxiv.org/abs/2505.02850)
*Nicy Scaria,Silvester John Joseph Kennedy,Diksha Seth,Ananya Thakur,Deepak Subramani*

Main category: cs.CL

TL;DR: 论文提出了一种基于分层概念图的框架，利用LLM生成高质量的多选题，针对多样化认知水平和常见误解设计干扰项，显著优于现有基线方法。专家评估和学生测试均验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统手动生成高质量多选题耗时且依赖专家知识，现有自动化方法无法满足高认知水平和领域特定误解的需求。该研究旨在通过结构化知识引导LLM解决这一问题。

Method: 采用分层概念图构建物理知识框架，自动检索相关概念作为LLM的上下文，生成题目和干扰项，并通过自动验证确保质量。与基线方法（基础LLM和RAG生成）对比。

Result: 专家评估显示成功率达75.20%（基线约37%）；学生测试中猜测正确率降至28.05%（基线37.10%），证明其能更有效评估概念理解。

Conclusion: 概念图驱动的方法能跨认知水平稳健评估，快速识别概念缺口，实现规模化针对性反馈。

Abstract: Generating high-quality MCQs, especially those targeting diverse cognitive
levels and incorporating common misconceptions into distractor design, is
time-consuming and expertise-intensive, making manual creation impractical at
scale. Current automated approaches typically generate questions at lower
cognitive levels and fail to incorporate domain-specific misconceptions. This
paper presents a hierarchical concept map-based framework that provides
structured knowledge to guide LLMs in generating MCQs with distractors. We
chose high-school physics as our test domain and began by developing a
hierarchical concept map covering major Physics topics and their
interconnections with an efficient database design. Next, through an automated
pipeline, topic-relevant sections of these concept maps are retrieved to serve
as a structured context for the LLM to generate questions and distractors that
specifically target common misconceptions. Lastly, an automated validation is
completed to ensure that the generated MCQs meet the requirements provided. We
evaluate our framework against two baseline approaches: a base LLM and a
RAG-based generation. We conducted expert evaluations and student assessments
of the generated MCQs. Expert evaluation shows that our method significantly
outperforms the baseline approaches, achieving a success rate of 75.20% in
meeting all quality criteria compared to approximately 37% for both baseline
methods. Student assessment data reveal that our concept map-driven approach
achieved a significantly lower guess success rate of 28.05% compared to 37.10%
for the baselines, indicating a more effective assessment of conceptual
understanding. The results demonstrate that our concept map-based approach
enables robust assessment across cognitive levels and instant identification of
conceptual gaps, facilitating faster feedback loops and targeted interventions
at scale.

</details>


### [3] [30DayGen: Leveraging LLMs to Create a Content Corpus for Habit Formation](https://arxiv.org/abs/2505.02851)
*Franklin Zhang,Sonya Zhang,Alon Halevy*

Main category: cs.CL

TL;DR: 30 Day Me是一款利用大型语言模型（LLM）帮助用户分解目标并跟踪进展的习惯养成应用，其核心功能包括生成独特的30天挑战和实时搜索与用户目标匹配的挑战。


<details>
  <summary>Details</summary>
Motivation: 研究旨在展示如何利用LLM快速构建特定领域的内容库，支持行为和教育活动，并提升内容生成和语义去重的效率。

Method: 通过30DAYGEN系统从15K+网页中生成3,531个独特的30天挑战，结合LLM优化内容生成和语义去重流程。

Result: 应用成功实现了挑战的动态生成和实时搜索，验证了LLM在内容构建和行为干预中的实用性。

Conclusion: 研究表明LLM能高效支持习惯养成工具的开发，为行为和教育活动提供了一种可扩展的解决方案。

Abstract: In this paper, we present 30 Day Me, a habit formation application that
leverages Large Language Models (LLMs) to help users break down their goals
into manageable, actionable steps and track their progress. Central to the app
is the 30DAYGEN system, which generates 3,531 unique 30-day challenges sourced
from over 15K webpages, and enables runtime search of challenge ideas aligned
with user-defined goals. We showcase how LLMs can be harnessed to rapidly
construct domain specific content corpora for behavioral and educational
purposes, and propose a practical pipeline that incorporates effective LLM
enhanced approaches for content generation and semantic deduplication.

</details>


### [4] [Ensuring Reproducibility in Generative AI Systems for General Use Cases: A Framework for Regression Testing and Open Datasets](https://arxiv.org/abs/2505.02854)
*Masumi Morishige,Ryo Koshihara*

Main category: cs.CL

TL;DR: GPR-bench是一个轻量级、可扩展的基准测试工具，用于评估生成式AI系统的可重复性和可靠性，包含双语数据集和自动评估流程。实验显示，新模型在正确性上略有提升，但提示工程对简洁性的提升更显著。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI系统因模型更新或提示调整导致的行为漂移问题，提供一种可操作的回归测试方法。

Method: GPR-bench结合双语数据集（覆盖8类任务）和自动化评估流程（使用LLM-as-a-Judge评分），测试不同模型版本和提示配置的效果。

Result: 新模型在正确性上提升有限，而简洁性提示显著提升输出简洁性（+12.37 pp），准确性仅略微下降（-1.7 pp）。

Conclusion: GPR-bench为可重复性监测提供了低门槛工具，但需优化基准设计以适应快速演进的模型。

Abstract: Reproducibility and reliability remain pressing challenges for generative AI
systems whose behavior can drift with each model update or prompt revision. We
introduce GPR-bench, a lightweight, extensible benchmark that operationalizes
regression testing for general purpose use cases. GPR-bench couples an open,
bilingual (English and Japanese) dataset covering eight task categories (e.g.,
text generation, code generation, and information retrieval) and 10 scenarios
in each task categories (80 total test cases for each language) with an
automated evaluation pipeline that employs "LLM-as-a-Judge" scoring of
correctness and conciseness. Experiments across three recent model versions -
gpt-4o-mini, o3-mini, and o4-mini - and two prompt configurations (default
versus concise-writing instruction) reveal heterogeneous quality. Our results
show that newer models generally improve correctness, but the differences are
modest and not statistically significant, suggesting that GPR-bench may not be
sufficiently challenging to differentiate between recent model versions. In
contrast, the concise-writing instruction significantly enhances conciseness
(+12.37 pp, Mann-Whitney U test: p < 0.001, effect size r = 0.2995) with
minimal degradations on accuracy (-1.7 pp), demonstrating the effectiveness of
prompt engineering. Released under the MIT License, GPR- bench lowers the
barrier to initiating reproducibility monitoring and provides a foundation for
community-driven extensions, while also raising important considerations about
benchmark design for rapidly evolving language models.

</details>


### [5] [Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models](https://arxiv.org/abs/2505.02858)
*Henry Tari,Nojus Sereiva,Rishabh Kaushal,Thales Bertaglia,Adriana Iamnitchi*

Main category: cs.CL

TL;DR: 该论文探讨了利用大型语言模型生成跨平台社交媒体合成数据的潜力，通过多平台主题提示和不同模型测试，评估了合成数据的词法和语义质量，并提出了改进高保真数据集的方法。


<details>
  <summary>Details</summary>
Motivation: 由于获取真实的跨平台社交媒体数据集既昂贵又受限，研究旨在探索大型语言模型生成合成数据以替代真实数据的可行性。

Method: 采用了多平台主题提示方法，利用多种大型语言模型从两个真实数据集生成合成数据，并对比了真实数据与合成数据的词法和语义特性。

Result: 实验表明，大型语言模型生成跨平台合成数据具有潜力，但不同模型的表现各异，可能需要后处理来提高数据保真度。

Conclusion: 论文证实了合成数据的可行性，提出了新的多平台数据集保真度指标，并建议进一步优化语言模型和后处理方法以提升数据质量。

Abstract: Social media datasets are essential for research on a variety of topics, such
as disinformation, influence operations, hate speech detection, or influencer
marketing practices. However, access to social media datasets is often
constrained due to costs and platform restrictions. Acquiring datasets that
span multiple platforms, which is crucial for understanding the digital
ecosystem, is particularly challenging. This paper explores the potential of
large language models to create lexically and semantically relevant social
media datasets across multiple platforms, aiming to match the quality of real
data. We propose multi-platform topic-based prompting and employ various
language models to generate synthetic data from two real datasets, each
consisting of posts from three different social media platforms. We assess the
lexical and semantic properties of the synthetic data and compare them with
those of the real data. Our empirical findings show that using large language
models to generate synthetic multi-platform social media data is promising,
different language models perform differently in terms of fidelity, and a
post-processing approach might be needed for generating high-fidelity synthetic
datasets for research. In addition to the empirical evaluation of three state
of the art large language models, our contributions include new fidelity
metrics specific to multi-platform social media datasets.

</details>


### [6] [Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI](https://arxiv.org/abs/2505.02859)
*Jonas Bokstaller,Julia Altheimer,Julian Dormehl,Alina Buss,Jasper Wiltfang,Johannes Schneider,Maximilian Röglinger*

Main category: cs.CL

TL;DR: 论文提出了一种结合XAI和LLM的新型交互式聊天机器人架构，用于电池健康状态预测，提升了非专业用户对ML模型的理解。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型的黑箱特性日益明显，XAI的应用需求增加；同时，LLM在理解人类语言和复杂模式方面取得显著进展，两者的结合有望提升模型解释性。

Method: 通过微调LLM构建交互式聊天机器人，将其作为XAI的解释接口，并在电池健康预测场景中实例化和验证该架构。

Result: 原型测试表明，该架构显著提升了用户（尤其是XAI经验不足者）对ML模型的理解能力。

Conclusion: 结合XAI与LLM的交互式架构有效增强了模型的可解释性，未来可扩展至更多应用场景。

Abstract: Across various sectors applications of eXplainableAI (XAI) gained momentum as
the increasing black-boxedness of prevailing Machine Learning (ML) models
became apparent. In parallel, Large Language Models (LLMs) significantly
developed in their abilities to understand human language and complex patterns.
By combining both, this paper presents a novel reference architecture for the
interpretation of XAI through an interactive chatbot powered by a fine-tuned
LLM. We instantiate the reference architecture in the context of
State-of-Health (SoH) prediction for batteries and validate its design in
multiple evaluation and demonstration rounds. The evaluation indicates that the
implemented prototype enhances the human interpretability of ML, especially for
users with less experience with XAI.

</details>


### [7] [Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](https://arxiv.org/abs/2505.02862)
*Haoming Yang,Ke Ma,Xiaojun Jia,Yingfei Sun,Qianqian Xu,Qingming Huang*

Main category: cs.CL

TL;DR: 论文提出了一种名为ICRT的新型越狱攻击框架，利用人类认知的启发式和偏见，通过认知分解和相关性偏置优化恶意提示，有效绕过主流大语言模型的安全机制。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型性能出色，但其安全机制仍易受越狱攻击威胁。现有研究多基于暴力优化或手动设计，难以揭示实际风险。

Method: 采用认知启发式（如简单效应和相关性偏置）分解和重组恶意提示；提出基于排序的危害性评估指标（如Elo、HodgeRank）。

Result: 实验表明ICRT能稳定绕过主流模型的安全机制，生成高风险内容。

Conclusion: 研究揭示了越狱攻击风险，为防御策略设计提供了新视角。

Abstract: Despite the remarkable performance of Large Language Models (LLMs), they
remain vulnerable to jailbreak attacks, which can compromise their safety
mechanisms. Existing studies often rely on brute-force optimization or manual
design, failing to uncover potential risks in real-world scenarios. To address
this, we propose a novel jailbreak attack framework, ICRT, inspired by
heuristics and biases in human cognition. Leveraging the simplicity effect, we
employ cognitive decomposition to reduce the complexity of malicious prompts.
Simultaneously, relevance bias is utilized to reorganize prompts, enhancing
semantic alignment and inducing harmful outputs effectively. Furthermore, we
introduce a ranking-based harmfulness evaluation metric that surpasses the
traditional binary success-or-failure paradigm by employing ranking aggregation
methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify
the harmfulness of generated content. Experimental results show that our
approach consistently bypasses mainstream LLMs' safety mechanisms and generates
high-risk content, providing insights into jailbreak attack risks and
contributing to stronger defense strategies.

</details>


### [8] [Accelerating Large Language Model Reasoning via Speculative Search](https://arxiv.org/abs/2505.02865)
*Zhihai Wang,Jie Wang,Jilai Pan,Xilin Xia,Huiling Zhen,Mingxuan Yuan,Jianye Hao,Feng Wu*

Main category: cs.CL

TL;DR: 提出了一种名为Speculative Search（SpecSearch）的新框架，通过小模型与大模型的协作，优化思维生成，显著加速大语言模型的推理速度，同时保持推理质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于树搜索的推理方法虽然提升了大语言模型的推理能力，但由于生成大量中间推理步骤（即思维），导致推理延迟高，限制了模型的应用。

Method: 提出SpecSearch框架，通过小模型与大模型在思维和标记层面的协作，优化思维生成，并结合质量保留拒绝机制过滤低质量思维。

Result: 在Qwen和Llama模型上的实验表明，SpecSearch在保持推理质量的同时，实现了最高2.12倍的加速效果，优于现有方法。

Conclusion: SpecSearch在显著提升推理速度的同时，保持了与大型模型相当的推理质量，解决了现有方法的延迟问题。

Abstract: Tree-search-based reasoning methods have significantly enhanced the reasoning
capability of large language models (LLMs) by facilitating the exploration of
multiple intermediate reasoning steps, i.e., thoughts. However, these methods
suffer from substantial inference latency, as they have to generate numerous
reasoning thoughts, severely limiting LLM applicability. To address this
challenge, we propose a novel Speculative Search (SpecSearch) framework that
significantly accelerates LLM reasoning by optimizing thought generation.
Specifically, SpecSearch utilizes a small model to strategically collaborate
with a large model at both thought and token levels, efficiently generating
high-quality reasoning thoughts. The major pillar of SpecSearch is a novel
quality-preserving rejection mechanism, which effectively filters out thoughts
whose quality falls below that of the large model's outputs. Moreover, we show
that SpecSearch preserves comparable reasoning quality to the large model.
Experiments on both the Qwen and Llama models demonstrate that SpecSearch
significantly outperforms state-of-the-art approaches, achieving up to
2.12$\times$ speedup with comparable reasoning quality.

</details>


### [9] [Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading](https://arxiv.org/abs/2505.02872)
*Cfir Avraham Hadar,Omer Shubi,Yoav Meiri,Yevgeni Berzak*

Main category: cs.CL

TL;DR: 论文研究了是否可以通过眼动数据自动解码读者的开放型阅读目标，通过目标分类和重构任务，结合多模态LLM模型，实验证明LLM能从眼动中有效提取阅读目标信息。


<details>
  <summary>Details</summary>
Motivation: 探讨日常阅读中，读者常因特定目标而调整阅读行为，研究是否能通过眼动数据自动识别这些目标，填补该领域空白。

Method: 提出目标分类和重构任务框架，利用大规模英语阅读眼动数据，开发并比较多种结合眼动和文本的多模态LLM模型（判别式和生成式）。

Result: 实验表明，模型在两类任务中均表现良好，验证了LLM可从眼动数据中有效推断读者的文本相关目标。

Conclusion: 眼动数据携带了阅读目标的关键信息，多模态LLM能成功解码这些信息，为阅读行为分析提供了新方向。

Abstract: When reading, we often have specific information that interests us in a text.
For example, you might be reading this paper because you are curious about LLMs
for eye movements in reading, the experimental design, or perhaps you only care
about the question ``but does it work?''. More broadly, in daily life, people
approach texts with any number of text-specific goals that guide their reading
behavior. In this work, we ask, for the first time, whether open-ended reading
goals can be automatically decoded from eye movements in reading. To address
this question, we introduce goal classification and goal reconstruction tasks
and evaluation frameworks, and use large-scale eye tracking for reading data in
English with hundreds of text-specific information seeking tasks. We develop
and compare several discriminative and generative multimodal LLMs that combine
eye movements and text for goal classification and goal reconstruction. Our
experiments show considerable success on both tasks, suggesting that LLMs can
extract valuable information about the readers' text-specific goals from eye
movements.

</details>


### [10] [Logits-Constrained Framework with RoBERTa for Ancient Chinese NER](https://arxiv.org/abs/2505.02983)
*Wenjie Hua,Shenghan Xu*

Main category: cs.CL

TL;DR: 该论文提出了一种Logits-Constrained (LC)框架用于古汉语命名实体识别，采用GujiRoBERTa和可微分解码机制，在EvaHan 2025基准测试中表现优于传统CRF和BiLSTM方法。


<details>
  <summary>Details</summary>
Motivation: 解决古汉语命名实体识别中高标签或大数据场景下传统方法的性能不足问题。

Method: 两阶段模型，结合GujiRoBERTa进行上下文编码和可微分解码机制以确保有效的BMES标签转换。

Result: LC框架在性能上优于CRF和BiLSTM方法，尤其在高标签或大数据设置中表现更佳。

Conclusion: 提出的模型选择标准为实际古汉语NLP任务提供了实用指导，平衡标签复杂性和数据集大小。

Abstract: This paper presents a Logits-Constrained (LC) framework for Ancient Chinese
Named Entity Recognition (NER), evaluated on the EvaHan 2025 benchmark. Our
two-stage model integrates GujiRoBERTa for contextual encoding and a
differentiable decoding mechanism to enforce valid BMES label transitions.
Experiments demonstrate that LC improves performance over traditional CRF and
BiLSTM-based approaches, especially in high-label or large-data settings. We
also propose a model selection criterion balancing label complexity and dataset
size, providing practical guidance for real-world Ancient Chinese NLP tasks.

</details>


### [11] [RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale](https://arxiv.org/abs/2505.03005)
*Daniel Goldstein,Eric Alcaide,Janna Lu,Eugene Cheah*

Main category: cs.CL

TL;DR: RADLADS是一种快速将softmax注意力Transformer转换为线性注意力解码器模型的协议，并提出两种新的RWKV变体架构。转换过程仅需350-700M tokens，成本低于2000美元，且推理质量接近原始Transformer。这些模型在标准测试中表现优异，模型已开源。


<details>
  <summary>Details</summary>
Motivation: 旨在高效且低成本地将现有softmax注意力Transformer模型转换为线性注意力解码器模型，同时保持高质量推理性能。

Method: 提出RADLADS协议，通过少量数据（350-700M tokens）进行转换，并设计两种新的RWKV架构。支持从Qwen2.5开源模型转换7B、32B和72B规模的模型。

Result: 转换后的72B线性注意力模型成本低于2000美元，推理质量接近原始Transformer，并在标准测试中达到同类模型的最优性能。

Conclusion: RADLADS提供了一种高效、低成本的模型转换方案，同时保持了高性能，模型已开源以促进进一步研究与应用。

Abstract: We present Rapid Attention Distillation to Linear Attention Decoders at Scale
(RADLADS), a protocol for rapidly converting softmax attention transformers
into linear attention decoder models, along with two new RWKV-variant
architectures, and models converted from popular Qwen2.5 open source models in
7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,
less than 0.005% of the token count used to train the original teacher models.
Converting to our 72B linear attention model costs less than \$2,000 USD at
today's prices, yet quality at inference remains close to the original
transformer. These models achieve state-of-the-art downstream performance
across a set of standard benchmarks for linear attention models of their size.
We release all our models on HuggingFace under the Apache 2.0 license, with the
exception of our 72B models which are also governed by the Qwen License
Agreement.
  Models at
https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102
Training Code at https://github.com/recursal/RADLADS-paper

</details>


### [12] [Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis](https://arxiv.org/abs/2505.03019)
*Albérick Euraste Djiré,Abdoul Kader Kaboré,Earl T. Barr,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: 摘要


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在训练时可能直接记忆而非泛化数据，这引发了隐私、知识产权和模型评估可靠性的担忧。

Method: 提出PEARL方法，通过输入扰动检测LLMs的输出一致性，以区分记忆和泛化行为。

Result: 在Pythia和GPT-4o模型上测试显示，PEARL能有效识别记忆行为，并提供训练数据来源的证据。

Conclusion: PEARL为检测LLMs的记忆行为提供了无侵入且有效的框架。

Abstract: While Large Language Models (LLMs) achieve remarkable performance through
training on massive datasets, they can exhibit concerning behaviors such as
verbatim reproduction of training data rather than true generalization. This
memorization phenomenon raises significant concerns about data privacy,
intellectual property rights, and the reliability of model evaluations. This
paper introduces PEARL, a novel approach for detecting memorization in LLMs.
PEARL assesses how sensitive an LLM's performance is to input perturbations,
enabling memorization detection without requiring access to the model's
internals. We investigate how input perturbations affect the consistency of
outputs, enabling us to distinguish between true generalization and
memorization. Our findings, following extensive experiments on the Pythia open
model, provide a robust framework for identifying when the model simply
regurgitates learned information. Applied on the GPT 4o models, the PEARL
framework not only identified cases of memorization of classic texts from the
Bible or common code from HumanEval but also demonstrated that it can provide
supporting evidence that some data, such as from the New York Times news
articles, were likely part of the training data of a given model.

</details>


### [13] [A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts](https://arxiv.org/abs/2505.03025)
*Steven Bedrick,A. Seza Doğruöz,Sergiu Nisioi*

Main category: cs.CL

TL;DR: 该论文探讨了临床对话领域合成数据集的创建、评估和使用方法，并提出了一种新的分类法来区分数据的合成类型和程度。


<details>
  <summary>Details</summary>
Motivation: 由于真实临床对话数据的隐私和治理问题难以获取，合成数据集在此领域显得尤为重要。然而，当前缺乏理论指导如何最佳利用和推广这些数据。

Method: 论文回顾了合成数据集的创建和评估方法，并提出了一个新颖的分类法，用于比较和评估不同合成数据的类型和程度。

Result: 研究概述了临床对话合成数据集的现状，并通过分类法为数据合成提供了系统化的评估框架。

Conclusion: 论文强调了合成数据集在临床对话领域的重要性，提出的分类法有助于未来研究的数据比较和应用推广。

Abstract: Synthetic data sets are used across linguistic domains and NLP tasks,
particularly in scenarios where authentic data is limited (or even
non-existent). One such domain is that of clinical (healthcare) contexts, where
there exist significant and long-standing challenges (e.g., privacy,
anonymization, and data governance) which have led to the development of an
increasing number of synthetic datasets. One increasingly important category of
clinical dataset is that of clinical dialogues which are especially sensitive
and difficult to collect, and as such are commonly synthesized.
  While such synthetic datasets have been shown to be sufficient in some
situations, little theory exists to inform how they may be best used and
generalized to new applications. In this paper, we provide an overview of how
synthetic datasets are created, evaluated and being used for dialogue related
tasks in the medical domain. Additionally, we propose a novel typology for use
in classifying types and degrees of data synthesis, to facilitate comparison
and evaluation.

</details>


### [14] [UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output](https://arxiv.org/abs/2505.03030)
*Sicong Huang,Jincheng He,Shiyuan Huang,Karthik Raja Anandan,Arkajyoti Chakraborty,Ian Lane*

Main category: cs.CL

TL;DR: 论文提出了一个框架，用于在LLM输出中检测幻觉并定位其位置，最终在Mu-SHROOM任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在处理知识密集型查询时产生的幻觉问题，并精确定位幻觉发生的位置。

Method: 引入一个框架，依次检索相关上下文、识别答案中的错误内容，并将其映射回LLM输出中的具体部分，同时通过自动优化提示增强效果。

Result: 系统在Mu-SHROOM任务中取得了最高综合性能，在所有语言中平均排名第一。

Conclusion: 该框架有效提升了幻觉检测和定位的准确性，代码和实验结果已公开。

Abstract: Hallucinations pose a significant challenge for large language models when
answering knowledge-intensive queries. As LLMs become more widely adopted, it
is crucial not only to detect if hallucinations occur but also to pinpoint
exactly where in the LLM output they occur. SemEval 2025 Task 3, Mu-SHROOM:
Multilingual Shared-task on Hallucinations and Related Observable
Overgeneration Mistakes, is a recent effort in this direction. This paper
describes the UCSC system submission to the shared Mu-SHROOM task. We introduce
a framework that first retrieves relevant context, next identifies false
content from the answer, and finally maps them back to spans in the LLM output.
The process is further enhanced by automatically optimizing prompts. Our system
achieves the highest overall performance, ranking #1 in average position across
all languages. We release our code and experiment results.

</details>


### [15] [Teaching Models to Understand (but not Generate) High-risk Data](https://arxiv.org/abs/2505.03052)
*Ryan Wang,Matthew Finlayson,Luca Soldaini,Swabha Swayamdipta,Robin Jia*

Main category: cs.CL

TL;DR: SLUNG是一种新的预训练范式，通过选择性损失机制，让模型能够理解高风险内容但不学习生成它们。实验表明，该方法提升了模型对高风险内容的理解能力，同时不增加其生成此类内容的可能。


<details>
  <summary>Details</summary>
Motivation: 为了防止模型生成高风险内容（如有毒或受版权保护的文本），开发者通常会在预训练数据中过滤掉这些内容。然而，完全移除这些数据会削弱模型识别和恰当应对有害或敏感内容的能力。

Method: 提出了SLUNG（选择性损失机制），在预训练中避免对高风险token的预测损失，但保留其上下文信息，迫使模型理解这类内容而不生成它们。

Result: 实验表明，SLUNG提升了模型对高风险内容的理解（如识别有毒内容的能力），同时未增加模型生成高风险内容的概率（如回复的毒性）。

Conclusion: SLUNG使模型能够从未被过滤的高风险文本中获益，达到理解和应对此类内容的目标。

Abstract: Language model developers typically filter out high-risk content -- such as
toxic or copyrighted text -- from their pre-training data to prevent models
from generating similar outputs. However, removing such data altogether limits
models' ability to recognize and appropriately respond to harmful or sensitive
content. In this paper, we introduce Selective Loss to Understand but Not
Generate (SLUNG), a pre-training paradigm through which models learn to
understand high-risk data without learning to generate it. Instead of uniformly
applying the next-token prediction loss, SLUNG selectively avoids incentivizing
the generation of high-risk tokens while ensuring they remain within the
model's context window. As the model learns to predict low-risk tokens that
follow high-risk ones, it is forced to understand the high-risk content.
Through our experiments, we show that SLUNG consistently improves models'
understanding of high-risk data (e.g., ability to recognize toxic content)
without increasing its generation (e.g., toxicity of model responses). Overall,
our SLUNG paradigm enables models to benefit from high-risk text that would
otherwise be filtered out.

</details>


### [16] [Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text](https://arxiv.org/abs/2505.03053)
*Jennifer Healey,Laurie Byrum,Md Nadeem Akhtar,Surabhi Bhargava,Moumita Sinha*

Main category: cs.CL

TL;DR: 论文提出一个半自动化的偏见评估框架，用于自由文本回答，核心是结合人类洞察力，以解决LLM在真实部署中的偏见评估问题。


<details>
  <summary>Details</summary>
Motivation: LLM评估在真实部署中因任务特定提示和实验情境的交互而复杂化，现有的基于短上下文、固定选择的基准测试在部署情境不同时可能失效，而大规模人类评估又成本高。

Method: 开发了一个半自动化的偏见评估框架，通过操作性的偏见定义自动化流程，并提出了超越多项选择的偏见分类方法，结合人类评估识别偏见基准中的问题模板。

Result: 框架结合人类洞察力，有效识别和分类偏见，成功发现偏见基准中的问题模板。

Conclusion: 半自动化框架结合人类评估，为LLM的自由文本偏见评估提供了可行且高效的方法。

Abstract: LLM evaluation is challenging even the case of base models. In real world
deployments, evaluation is further complicated by the interplay of task
specific prompts and experiential context. At scale, bias evaluation is often
based on short context, fixed choice benchmarks that can be rapidly evaluated,
however, these can lose validity when the LLMs' deployed context differs. Large
scale human evaluation is often seen as too intractable and costly. Here we
present our journey towards developing a semi-automated bias evaluation
framework for free text responses that has human insights at its core. We
discuss how we developed an operational definition of bias that helped us
automate our pipeline and a methodology for classifying bias beyond multiple
choice. We additionally comment on how human evaluation helped us uncover
problematic templates in a bias benchmark.

</details>


### [17] [Improving Model Alignment Through Collective Intelligence of Open-Source LLMS](https://arxiv.org/abs/2505.03059)
*Junlin Wang,Roy Xie,Shang Zhu,Jue Wang,Ben Athiwaratkun,Bhuwan Dhingra,Shuaiwen Leon Song,Ce Zhang,James Zou*

Main category: cs.CL

TL;DR: 论文提出了一种名为MoAA的方法，通过整合多种语言模型的能力生成高质量的对齐数据，从而提升模型在监督微调和偏好优化上的表现，显著提高了LLaMA-3.1-8B-Instruct的胜率。


<details>
  <summary>Details</summary>
Motivation: 构建基于人类指令和反馈的高质量对齐数据通常成本高且难以扩展，且可能在多样性和泛化性上受限。MoAA旨在利用多模型协作解决这些问题。

Method: MoAA结合多种语言模型的优势生成对齐数据，用于监督微调和偏好优化，避免了对单一模型（如GPT-4o）的依赖。

Result: 实验显示，MoAA将LLaMA-3.1-8B-Instruct在Arena-Hard和AlpacaEval2上的胜率分别从19.5提升至48.3和从22.33提升至57.23。

Conclusion: MoAA提供了一种可扩展且多样化的合成数据方法，能自我改进模型性能，推动了开源大模型的发展，减少对外部强监督的依赖。

Abstract: Building helpful and harmless large language models (LLMs) requires effective
model alignment approach based on human instructions and feedback, which
necessitates high-quality human-labeled data. Constructing such datasets is
often expensive and hard to scale, and may face potential limitations on
diversity and generalization. To address these challenges, we introduce Mixture
of Agents Alignment (MoAA), that leverages the collective strengths of various
language models to provide high-quality data for model alignment. By employing
MoAA, we enhance both supervised fine-tuning and preference optimization,
leading to improved performance compared to using a single model alone to
generate alignment data (e.g. using GPT-4o alone). Evaluation results show that
our approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on
Arena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising
direction for model alignment through this new scalable and diverse synthetic
data recipe. Furthermore, we demonstrate that MoAA enables a self-improvement
pipeline, where models finetuned on MoA-generated data surpass their own
initial capabilities, providing evidence that our approach can push the
frontier of open-source LLMs without reliance on stronger external supervision.
Data and code will be released.

</details>


### [18] [Survey of Abstract Meaning Representation: Then, Now, Future](https://arxiv.org/abs/2505.03229)
*Behrooz Mansouri*

Main category: cs.CL

TL;DR: 本文对抽象意义表示（AMR）进行了综述，探讨了其图结构表示句意的能力，解析与生成任务的方法，以及AMR在文本生成、分类等领域的应用。


<details>
  <summary>Details</summary>
Motivation: AMR作为一种图结构的语义表示框架，能够有效编码复杂句子的含义，研究其发展和应用有助于提升机器对人类语言的理解能力。

Method: 通过调查AMR及其扩展，分析文本到AMR解析和AMR到文本生成的传统、当前及未来可能的方法，并回顾AMR在多个领域的应用。

Result: 综述揭示了AMR的最新进展和挑战，展示了其在文本处理和信息提取中的潜力。

Conclusion: AMR研究为未来提升机器语言理解提供了方向，其广泛应用潜力值得进一步探索。

Abstract: This paper presents a survey of Abstract Meaning Representation (AMR), a
semantic representation framework that captures the meaning of sentences
through a graph-based structure. AMR represents sentences as rooted, directed
acyclic graphs, where nodes correspond to concepts and edges denote
relationships, effectively encoding the meaning of complex sentences. This
survey investigates AMR and its extensions, focusing on AMR capabilities. It
then explores the parsing (text-to-AMR) and generation (AMR-to-text) tasks by
showing traditional, current, and possible futures approaches. It also reviews
various applications of AMR including text generation, text classification, and
information extraction and information seeking. By analyzing recent
developments and challenges in the field, this survey provides insights into
future directions for research and the potential impact of AMR on enhancing
machine understanding of human language.

</details>


### [19] [Ψ-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback](https://arxiv.org/abs/2505.03293)
*Shijing Zhu,Zhuang Chen,Guanqun Bi,Binghang Li,Yaxi Deng,Dazhen Wan,Libiao Peng,Xiyao Xiao,Rongsheng Zhang,Tangjie Lv,Zhipeng Hu,FangFang Li,Minlie Huang*

Main category: cs.CL

TL;DR: 本文提出 Ψ-Arena，一种互动框架，用于评估和优化基于大语言模型（LLM）的心理咨询师，通过多视角评估和闭环优化提升性能。实验表明，该方法显著提升了模型的咨询能力。


<details>
  <summary>Details</summary>
Motivation: 现有对LLM心理咨询能力的评估局限静态测试、单一视角和开放式框架，缺乏实际可行反馈。

Method: 提出Ψ-Arena框架，包含多阶段模拟对话、三方评估（客户、咨询师和主管）及基于反馈的闭环优化。

Result: 实验显示不同LLM在真实场景下表现差异显著，优化后咨询性能提升高达141%。

Conclusion: Ψ-Arena为推进心理健康领域可靠且人性化的LLM应用提供了基础资源。

Abstract: Large language models (LLMs) have shown promise in providing scalable mental
health support, while evaluating their counseling capability remains crucial to
ensure both efficacy and safety. Existing evaluations are limited by the static
assessment that focuses on knowledge tests, the single perspective that centers
on user experience, and the open-loop framework that lacks actionable feedback.
To address these issues, we propose {\Psi}-Arena, an interactive framework for
comprehensive assessment and optimization of LLM-based counselors, featuring
three key characteristics: (1) Realistic arena interactions that simulate
real-world counseling through multi-stage dialogues with psychologically
profiled NPC clients, (2) Tripartite evaluation that integrates assessments
from the client, counselor, and supervisor perspectives, and (3) Closed-loop
optimization that iteratively improves LLM counselors using diagnostic
feedback. Experiments across eight state-of-the-art LLMs show significant
performance variations in different real-world scenarios and evaluation
perspectives. Moreover, reflection-based optimization results in up to a 141%
improvement in counseling performance. We hope PsychoArena provides a
foundational resource for advancing reliable and human-aligned LLM applications
in mental healthcare.

</details>


### [20] [Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation](https://arxiv.org/abs/2505.03320)
*Junyu Ma,Tianqing Fang,Zhisong Zhang,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 通过『回忆与推理』（RwR）方法，利用教师模型的思维链摘要，提升Mamba模型的长上下文记忆能力，无需架构改动即可超越Transformer/混合模型基线。


<details>
  <summary>Details</summary>
Motivation: 解决Mamba模型在实际应用中长序列处理能力的限制，通过简单的微调方法提升其长上下文记忆能力。

Method: 在微调阶段，通过在输入前添加教师模型生成的思维链摘要作为提示，教导Mamba主动回忆和推理长上下文。

Result: 在LONGMEMEVAL和HELMET基准测试中，RwR显著提升了Mamba的长上下文性能，同时保持短上下文能力，且未改变模型架构。

Conclusion: RwR是一种简单有效的方法，能够在不改动模型架构的情况下显著提升Mamba的长上下文处理能力，为类似模型提供了实用改进方向。

Abstract: Mamba's theoretical infinite-context potential is limited in practice when
sequences far exceed training lengths. This work explores unlocking Mamba's
long-context memory ability by a simple-yet-effective method, Recall with
Reasoning (RwR), by distilling chain-of-thought (CoT) summarization from a
teacher model. Specifically, RwR prepends these summarization as CoT prompts
during fine-tuning, teaching Mamba to actively recall and reason over long
contexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's
long-context performance against comparable Transformer/hybrid baselines under
similar pretraining conditions, while preserving short-context capabilities,
all without architectural changes.

</details>


### [21] [Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation](https://arxiv.org/abs/2505.03406)
*Mohammad Shoaib Ansari,Mohd Sohail Ali Khan,Shubham Revankar,Aditya Varma,Anil S. Mokhade*

Main category: cs.CL

TL;DR: 该论文研究大型语言模型（LLMs）在医疗领域的应用，重点是使用RAG和QLoRA技术增强医疗决策支持，并通过量化低秩适配优化模型效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs如何整合医院特定数据以提升医疗决策支持的准确性和实用性，同时解决参数效率和存储优化的挑战。

Method: 基于Llama 3.2-3B-Instruct模型，结合RAG和QLoRA技术，嵌入和检索医疗信息，并通过量化技术优化模型效率。

Result: 模型在多个医疗基准测试中表现良好，能提供基本医疗建议，并在疾病预测、治疗建议和报告摘要等应用中显著提升准确性。

Conclusion: 研究表明LLMs在医疗领域具有潜力，但仍需解决伦理和技术挑战，未来可进一步优化和部署到实际临床环境中。

Abstract: This research paper investigates the application of Large Language Models
(LLMs) in healthcare, specifically focusing on enhancing medical decision
support through Retrieval-Augmented Generation (RAG) integrated with
hospital-specific data and fine-tuning using Quantized Low-Rank Adaptation
(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By
embedding and retrieving context-relevant healthcare information, the system
significantly improves response accuracy. QLoRA facilitates notable parameter
efficiency and memory optimization, preserving the integrity of medical
information through specialized quantization techniques. Our research also
shows that our model performs relatively well on various medical benchmarks,
indicating that it can be used to make basic medical suggestions. This paper
details the system's technical components, including its architecture,
quantization methods, and key healthcare applications such as enhanced disease
prediction from patient symptoms and medical history, treatment suggestions,
and efficient summarization of complex medical reports. We touch on the ethical
considerations-patient privacy, data security, and the need for rigorous
clinical validation-as well as the practical challenges of integrating such
systems into real-world healthcare workflows. Furthermore, the lightweight
quantized weights ensure scalability and ease of deployment even in
low-resource hospital environments. Finally, the paper concludes with an
analysis of the broader impact of LLMs on healthcare and outlines future
directions for LLMs in medical settings.

</details>


### [22] [MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks](https://arxiv.org/abs/2505.03427)
*Mouath Abu Daoud,Chaimae Abouzahir,Leen Kharouf,Walid Al-Eisawi,Nizar Habash,Farah E. Shamout*

Main category: cs.CL

TL;DR: 该论文介绍了MedArabiQ——一个阿拉伯语医疗领域的基准数据集，用于评估大语言模型（LLMs）在多种医疗任务中的表现，填补了该领域高质量数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在医疗领域展现出潜力，但阿拉伯语医疗领域缺乏相关数据集和基准，阻碍了LLMs的公平部署和多语言扩展。该研究旨在填补这一空白。

Method: 研究通过整合医学考试和公开数据集构建了包含七种医疗任务的MedArabiQ，并引入多种修改以评估LLMs性能与偏差缓解。对五种前沿LLMs（如GPT-4o）进行了全面评估。

Result: 评估揭示了LLMs在阿拉伯语医疗任务中的性能差异，强调了开发多语言高质量基准的必要性，以确保LLMs在医疗领域的公平应用。

Conclusion: MedArabiQ的发布为增强LLMs多语言能力提供了基础，推动了生成式AI在医疗领域的公平使用与研究进展。

Abstract: Large Language Models (LLMs) have demonstrated significant promise for
various applications in healthcare. However, their efficacy in the Arabic
medical domain remains unexplored due to the lack of high-quality
domain-specific datasets and benchmarks. This study introduces MedArabiQ, a
novel benchmark dataset consisting of seven Arabic medical tasks, covering
multiple specialties and including multiple choice questions,
fill-in-the-blank, and patient-doctor question answering. We first constructed
the dataset using past medical exams and publicly available datasets. We then
introduced different modifications to evaluate various LLM capabilities,
including bias mitigation. We conducted an extensive evaluation with five
state-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude
3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of
new high-quality benchmarks that span different languages to ensure fair
deployment and scalability of LLMs in healthcare. By establishing this
benchmark and releasing the dataset, we provide a foundation for future
research aimed at evaluating and enhancing the multilingual capabilities of
LLMs for the equitable use of generative AI in healthcare.

</details>


### [23] [An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation](https://arxiv.org/abs/2505.03452)
*Matan Orbach,Ohad Eytan,Benjamin Sznajder,Ariel Gera,Odellia Boni,Yoav Kantor,Gal Bloch,Omri Levy,Hadas Abraham,Nitzan Barzilay,Eyal Shnarch,Michael E. Factor,Shila Ofek-Koifman,Paula Ta-Shma,Assaf Toledo*

Main category: cs.CL

TL;DR: 研究表明，通过贪婪或迭代随机搜索方法可以高效优化RAG配置，显著提升性能，建议优先优化模型而非按RAG流程顺序优化。


<details>
  <summary>Details</summary>
Motivation: 当前为特定用例找到最优的RAG配置既复杂又昂贵，虽有HPO框架但缺乏严格评估，因此需系统研究其有效性。

Method: 使用5种HPO算法在5个多样化数据集（包括新收集的真实产品文档数据集）上进行了全面研究，探索了迄今最大的HPO搜索空间并优化了两个评价指标。

Result: RAG的HPO可以通过贪婪或迭代随机搜索高效完成，且对所有数据集均显著提升性能；贪婪方法中优先优化模型优于按RAG流程顺序优化。

Conclusion: 研究表明RAG配置优化具有高效性和广泛提升作用，推荐在HPO时优先优化模型而非遵循传统流程顺序。

Abstract: Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a
given use case can be complex and expensive. Motivated by this challenge,
frameworks for RAG hyper-parameter optimization (HPO) have recently emerged,
yet their effectiveness has not been rigorously benchmarked. To address this
gap, we present a comprehensive study involving 5 HPO algorithms over 5
datasets from diverse domains, including a new one collected for this work on
real-world product documentation. Our study explores the largest HPO search
space considered to date, with two optimized evaluation metrics. Analysis of
the results shows that RAG HPO can be done efficiently, either greedily or with
iterative random search, and that it significantly boosts RAG performance for
all datasets. For greedy HPO approaches, we show that optimizing models first
is preferable to the prevalent practice of optimizing sequentially according to
the RAG pipeline order.

</details>


### [24] [Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis](https://arxiv.org/abs/2505.03467)
*Shuang Zhou,Jiashuo Wang,Zidu Xu,Song Wang,David Brauer,Lindsay Welton,Jacob Cogan,Yuen-Hei Chung,Lei Tian,Zaifu Zhan,Yu Hou,Mingquan Lin,Genevieve B. Melton,Rui Zhang*

Main category: cs.CL

TL;DR: ConfiDx 是一个基于大语言模型（LLM）的系统，专门针对诊断不确定性进行识别和解释，通过微调开源 LLM 并结合诊断标准，显著提升了自动诊断系统的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 诊断过程中，临床笔记信息不足会导致不确定性增加，从而引发误诊风险。当前系统在识别和解释这种不确定性方面研究不足，因此需要开发新方法来提升诊断系统的可信度。

Method: 研究团队通过微调开源 LLM，结合诊断标准，构建了 ConfiDx 模型，并整理了包含不同诊断模糊程度的标注数据集，以评估模型效果。

Result: ConfiDx 在真实数据集上表现出色，能有效识别诊断不确定性，同时提升了诊断准确性和解释的可信度。

Conclusion: ConfiDx 是首个同时解决诊断不确定性识别和解释的研究，显著提高了自动诊断系统的可靠性，为临床实践提供了更可信的工具。

Abstract: Explainable disease diagnosis, which leverages patient information (e.g.,
signs and symptoms) and computational models to generate probable diagnoses and
reasonings, offers clear clinical values. However, when clinical notes
encompass insufficient evidence for a definite diagnosis, such as the absence
of definitive symptoms, diagnostic uncertainty usually arises, increasing the
risk of misdiagnosis and adverse outcomes. Although explicitly identifying and
explaining diagnostic uncertainties is essential for trustworthy diagnostic
systems, it remains under-explored. To fill this gap, we introduce ConfiDx, an
uncertainty-aware large language model (LLM) created by fine-tuning open-source
LLMs with diagnostic criteria. We formalized the task and assembled richly
annotated datasets that capture varying degrees of diagnostic ambiguity.
Evaluating ConfiDx on real-world datasets demonstrated that it excelled in
identifying diagnostic uncertainties, achieving superior diagnostic
performance, and generating trustworthy explanations for diagnoses and
uncertainties. To our knowledge, this is the first study to jointly address
diagnostic uncertainty recognition and explanation, substantially enhancing the
reliability of automatic diagnostic systems.

</details>


### [25] [Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models](https://arxiv.org/abs/2505.03469)
*Bin Yu,Hang Yuan,Yuliang Wei,Bailing Wang,Weizhen Qi,Kai Chen*

Main category: cs.CL

TL;DR: 论文提出了LS-Mixture SFT方法，通过结合长短链式思维数据微调非推理模型，显著提升推理能力并减少冗余输出。


<details>
  <summary>Details</summary>
Motivation: 解决传统监督微调（SFT）从大型推理模型蒸馏时继承的“过度思考”问题，即生成冗长的推理链。

Method: 提出LS-Mixture SFT，混合长链式思维数据与结构保留重写的短链数据用于模型微调。

Result: 相比直接SFT，平均准确率提升2.3%，响应长度减少47.61%。

Conclusion: LS-Mixture SFT能在避免过度思考的同时高效赋予模型推理能力。

Abstract: Recent advances in large language models have demonstrated that Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from
large reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning
capabilities to non-reasoning models. However, models fine-tuned with this
approach inherit the "overthinking" problem from teacher models, producing
verbose and redundant reasoning chains during inference. To address this
challenge, we propose \textbf{L}ong-\textbf{S}hort Chain-of-Thought
\textbf{Mixture} \textbf{S}upervised \textbf{F}ine-\textbf{T}uning
(\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their
short counterparts obtained through structure-preserved rewriting. Our
experiments demonstrate that models trained using the LS-Mixture SFT method,
compared to those trained with direct SFT, achieved an average accuracy
improvement of 2.3\% across various benchmarks while substantially reducing
model response length by approximately 47.61\%. This work offers an approach to
endow non-reasoning models with reasoning capabilities through supervised
fine-tuning while avoiding the inherent overthinking problems inherited from
teacher models, thereby enabling efficient reasoning in the fine-tuned models.

</details>


### [26] [Evaluation of LLMs on Long-tail Entity Linking in Historical Documents](https://arxiv.org/abs/2505.03473)
*Marta Boscariol,Luana Bulla,Lia Draetta,Beatrice Fiumanò,Emanuele Lenzi,Leonardo Piano*

Main category: cs.CL

TL;DR: 论文评估了GPT和LLaMA3在长尾实体链接任务中的表现，发现在特定历史文本领域，LLMs表现良好，可作为传统方法的补充。


<details>
  <summary>Details</summary>
Motivation: 长尾实体在训练数据和知识库中代表性不足，使用LLMs解决长尾实体链接问题是一个未被充分研究的领域。

Method: 使用MHERCL v0.1基准数据集，比较GPT和LLaMA3与ReLiK框架在长尾实体链接任务上的性能。

Result: 初步实验表明，LLMs在长尾实体链接任务中表现良好，显示出填补头部与长尾实体链接差距的潜力。

Conclusion: LLMs可以作为长尾实体链接技术的有效补充，但仍需进一步研究以优化其在长尾场景中的应用。

Abstract: Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)
applications, enabling the disambiguation of entity mentions by linking them to
their corresponding entries in a reference knowledge base (KB). Thanks to their
deep contextual understanding capabilities, LLMs offer a new perspective to
tackle EL, promising better results than traditional methods. Despite the
impressive generalization capabilities of LLMs, linking less popular, long-tail
entities remains challenging as these entities are often underrepresented in
training data and knowledge bases. Furthermore, the long-tail EL task is an
understudied problem, and limited studies address it with LLMs. In the present
work, we assess the performance of two popular LLMs, GPT and LLama3, in a
long-tail entity linking scenario. Using MHERCL v0.1, a manually annotated
benchmark of sentences from domain-specific historical texts, we quantitatively
compare the performance of LLMs in identifying and linking entities to their
corresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity
Linking and Relation Extraction framework. Our preliminary experiments reveal
that LLMs perform encouragingly well in long-tail EL, indicating that this
technology can be a valuable adjunct in filling the gap between head and
long-tail EL.

</details>


### [27] [Sentence Embeddings as an intermediate target in end-to-end summarisation](https://arxiv.org/abs/2505.03481)
*Maciej Zembrzuski,Saad Mahamood*

Main category: cs.CL

TL;DR: 提出了一种结合抽取式和生成式方法的新模型，通过利用预训练的句子级嵌入来提升大规模用户评论摘要的质量。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络方法在处理大规模输入数据集时效果不佳，尤其在用户评论摘要任务中。

Method: 结合抽取式方法（利用预训练句子级嵌入）与生成式模型，改进内容选择。

Result: 新方法在大规模输入数据摘要任务中表现优于现有方法，且预测摘要句子级嵌入比传统句子选择概率分布更有效。

Conclusion: 联合使用抽取与生成方法及句子级嵌入能显著提升摘要系统的性能，尤其适用于松散对齐的源-目标语料。

Abstract: Current neural network-based methods to the problem of document summarisation
struggle when applied to datasets containing large inputs. In this paper we
propose a new approach to the challenge of content-selection when dealing with
end-to-end summarisation of user reviews of accommodations. We show that by
combining an extractive approach with externally pre-trained sentence level
embeddings in an addition to an abstractive summarisation model we can
outperform existing methods when this is applied to the task of summarising a
large input dataset. We also prove that predicting sentence level embedding of
a summary increases the quality of an end-to-end system for loosely aligned
source to target corpora, than compared to commonly predicting probability
distributions of sentence selection.

</details>


### [28] [Faster MoE LLM Inference for Extremely Large Models](https://arxiv.org/abs/2505.03531)
*Haoqi Yang,Luohe Shi,Qiwei Li,Zuchao Li,Ping Wang,Bo Du,Mengjia Shen,Hai Zhao*

Main category: cs.CL

TL;DR: 该论文探讨了细粒度稀疏混合专家（MoE）模型的效率动态，研究表明减少激活专家数量能在某些场景下显著提升效率且性能损失较小，而减少专家总数则效率提升有限但性能损失严重。方法可实现至少10%的吞吐量提升且无性能损失。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注粗粒度MoE架构，而对细粒度MoE的研究不足，尤其是服务负载下的效率动态及专家数量减少对效率与性能权衡的影响。

Method: 通过实验分析不同服务负载下细粒度MoE模型的效率动态，并探讨减少激活专家数量与专家总数对模型性能的影响。

Result: 减少激活专家数量可显著提升效率且性能损失较小；减少专家总数效率提升有限但性能损失严重。方法可实现至少10%的吞吐量提升且无性能损失。

Conclusion: MoE推理优化仍具巨大探索与改进潜力，细粒度MoE模型在效率与性能的权衡中表现出独特优势。

Abstract: Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually
becoming the mainstream approach for ultra-large-scale models. Existing
optimization efforts for MoE models have focused primarily on coarse-grained
MoE architectures. With the emergence of DeepSeek Models, fine-grained MoE
models are gaining popularity, yet research on them remains limited. Therefore,
we want to discuss the efficiency dynamic under different service loads.
Additionally, fine-grained models allow deployers to reduce the number of
routed experts, both activated counts and total counts, raising the question of
how this reduction affects the trade-off between MoE efficiency and
performance. Our findings indicate that while deploying MoE models presents
greater challenges, it also offers significant optimization opportunities.
Reducing the number of activated experts can lead to substantial efficiency
improvements in certain scenarios, with only minor performance degradation.
Reducing the total number of experts provides limited efficiency gains but
results in severe performance degradation. Our method can increase throughput
by at least 10\% without any performance degradation. Overall, we conclude that
MoE inference optimization remains an area with substantial potential for
exploration and improvement.

</details>


### [29] [Say It Another Way: A Framework for User-Grounded Paraphrasing](https://arxiv.org/abs/2505.03563)
*Cléa Chataigner,Rebecca Ma,Prakhar Ganesh,Afaf Taïk,Elliot Creager,Golnoosh Farnadi*

Main category: cs.CL

TL;DR: 研究提出了一种基于最小语言转换的分类框架，通过控制性改述生成自然的提示变体，验证LLMs在刻板印象评估中的响应差异，结果显示细微提示修改可显著改变模型行为。


<details>
  <summary>Details</summary>
Motivation: 探索提示措辞变化对大型语言模型行为的影响，尤其是自然语言变体下的稳定性与可靠性问题。

Method: 提出基于语言转换分类的框架，生成自然的提示变体，使用BBQ数据集并通过人工和自动验证。

Result: 研究发现即使细微的提示修改也会显著影响模型行为。

Conclusion: 强调需要开发对改述敏感的稳健评估协议。

Abstract: Small changes in how a prompt is worded can lead to meaningful differences in
the behavior of large language models (LLMs), raising concerns about the
stability and reliability of their evaluations. While prior work has explored
simple formatting changes, these rarely capture the kinds of natural variation
seen in real-world language use. We propose a controlled paraphrasing framework
based on a taxonomy of minimal linguistic transformations to systematically
generate natural prompt variations. Using the BBQ dataset, we validate our
method with both human annotations and automated checks, then use it to study
how LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our
analysis shows that even subtle prompt modifications can lead to substantial
changes in model behavior. These results highlight the need for robust,
paraphrase-aware evaluation protocols.

</details>


### [30] [Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure](https://arxiv.org/abs/2505.03675)
*Anuja Tayal,Devika Salunke,Barbara Di Eugenio,Paula G Allen-Meares,Eulalia P Abril,Olga Garcia-Bedoya,Carolyn A Dickens,Andrew D. Boyd*

Main category: cs.CL

TL;DR: 研究了ChatGPT为非洲裔美国心衰患者生成自我护理对话的潜力，发现有效提示设计关键，但模型仍缺乏共情和互动。


<details>
  <summary>Details</summary>
Motivation: 填补非洲裔美国心衰患者自我护理对话数据的空白，提升医疗对话质量。

Method: 使用四种提示策略（领域、AAVE、SDOH、SDOH推理），生成对话并评估其效果。

Result: SDOH和推理提示改善对话质量，但ChatGPT仍缺乏共情和互动能力。

Conclusion: 提示设计对生成医疗对话至关重要，但需进一步优化以增强共情和互动。

Abstract: We explore the potential of ChatGPT (3.5-turbo and 4) to generate
conversations focused on self-care strategies for African-American heart
failure patients -- a domain with limited specialized datasets. To simulate
patient-health educator dialogues, we employed four prompting strategies:
domain, African American Vernacular English (AAVE), Social Determinants of
Health (SDOH), and SDOH-informed reasoning. Conversations were generated across
key self-care domains of food, exercise, and fluid intake, with varying turn
lengths (5, 10, 15) and incorporated patient-specific SDOH attributes such as
age, gender, neighborhood, and socioeconomic status. Our findings show that
effective prompt design is essential. While incorporating SDOH and reasoning
improves dialogue quality, ChatGPT still lacks the empathy and engagement
needed for meaningful healthcare communication.

</details>


### [31] [IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages](https://arxiv.org/abs/2505.03688)
*Sharvi Endait,Ruturaj Ghatage,Aditya Kulkarni,Rajlaxmi Patil,Raviraj Joshi*

Main category: cs.CL

TL;DR: 论文介绍了IndicSQuAD，一个涵盖九种主要印度语言的多语言抽取式问答数据集，旨在解决印度语言在问答系统中的资源不足问题。


<details>
  <summary>Details</summary>
Motivation: 高资源语言在问答系统方面取得了快速进展，但印度语言由于其资源匮乏而代表性不足，尽管它们有大量的母语使用者。本论文旨在通过构建IndicSQuAD数据集来填补这一空白。

Method: 通过从SQuAD数据集系统性地翻译和扩展，构建了涵盖九种印度语言的QA数据集IndicSQuAD，并确保了语言忠实度和答案范围的准确性。使用语言特定的单语BERT模型和多语言MuRIL-BERT评估基线性能。

Result: 基线结果显示，在低资源环境下存在一些挑战。实验结果还指出了未来工作的潜在方向，包括扩展到更多语言、开发领域特定数据集和整合多模态数据。

Conclusion: IndicSQuAD为印度语言的问答系统提供了重要资源，并强调了在低资源环境下进一步研究的必要性。数据集和模型已公开分享。

Abstract: The rapid progress in question-answering (QA) systems has predominantly
benefited high-resource languages, leaving Indic languages largely
underrepresented despite their vast native speaker base. In this paper, we
present IndicSQuAD, a comprehensive multi-lingual extractive QA dataset
covering nine major Indic languages, systematically derived from the SQuAD
dataset. Building on previous work with MahaSQuAD for Marathi, our approach
adapts and extends translation techniques to maintain high linguistic fidelity
and accurate answer-span alignment across diverse languages. IndicSQuAD
comprises extensive training, validation, and test sets for each language,
providing a robust foundation for model development. We evaluate baseline
performances using language-specific monolingual BERT models and the
multilingual MuRIL-BERT. The results indicate some challenges inherent in
low-resource settings. Moreover, our experiments suggest potential directions
for future work, including expanding to additional languages, developing
domain-specific datasets, and incorporating multimodal data. The dataset and
models are publicly shared at https://github.com/l3cube-pune/indic-nlp

</details>


### [32] [NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation](https://arxiv.org/abs/2505.03711)
*Baharul Islam,Nasim Ahmad,Ferdous Ahmed Barbhuiya,Kuntal Dey*

Main category: cs.CL

TL;DR: 论文提出了一个针对SemEval 2025 Task 5的跨语言学科分类系统，通过双语数据训练、负采样和基于边缘的检索目标，使用维度标记自注意力机制来编码句子嵌入，在资源有限的情况下表现竞争性，但仍需改进。


<details>
  <summary>Details</summary>
Motivation: 解决跨语言学科分类问题，特别是在英语和德语学术领域中，资源有限的情况下高效捕获相关学科信息。

Method: 采用了双语数据训练，结合负采样和基于边缘的检索目标，设计了维度标记自注意力机制以减少内部维度并高效编码句子嵌入。

Result: 系统在一般定量设置中平均召回率为32.24%，在定性评估中分别为43.16%和31.53%，且GPU使用率低。

Conclusion: 该方法在资源有限条件下有效，但仍需进一步提升性能。

Abstract: We present our system submission for SemEval 2025 Task 5, which focuses on
cross-lingual subject classification in the English and German academic
domains. Our approach leverages bilingual data during training, employing
negative sampling and a margin-based retrieval objective. We demonstrate that a
dimension-as-token self-attention mechanism designed with significantly reduced
internal dimensions can effectively encode sentence embeddings for subject
retrieval. In quantitative evaluation, our system achieved an average recall
rate of 32.24% in the general quantitative setting (all subjects), 43.16% and
31.53% of the general qualitative evaluation methods with minimal GPU usage,
highlighting their competitive performance. Our results demonstrate that our
approach is effective in capturing relevant subject information under resource
constraints, although there is still room for improvement.

</details>


### [33] [WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch](https://arxiv.org/abs/2505.03733)
*Zimu Lu,Yunqiao Yang,Houxing Ren,Haotian Hou,Han Xiao,Ke Wang,Weikang Shi,Aojun Zhou,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 论文提出了WebGen-Bench，一个衡量LLM代理从零创建多文件网站代码能力的基准，包含多样化的网站生成指令和647个测试用例。评估中，最佳组合准确率仅为27.8%，突显了基准的挑战性。此外，通过训练数据集WebGen-Instruct，Qwen2.5-Coder-32B-Instruct的准确率提升至38.2%，优于其他模型。


<details>
  <summary>Details</summary>
Motivation: LLM代理在复杂代码库中生成和管理代码的能力潜力巨大，但目前缺乏衡量其从零创建多文件网站能力的基准。

Method: 提出WebGen-Bench基准，包含多样化的网站生成指令和647个测试用例，并利用自动化测试工具评估三种高性能代码代理框架的性能。

Result: 最佳组合（Bolt.diy+DeepSeek-R1）在测试用例中的准确率仅为27.8%，而通过训练数据集WebGen-Instruct训练的Qwen2.5-Coder-32B-Instruct准确率达到38.2%。

Conclusion: WebGen-Bench是一个具有挑战性的基准，揭示了LLM代理在网站生成任务中的局限性，同时展示了通过针对性训练提升性能的可能性。

Abstract: LLM-based agents have demonstrated great potential in generating and managing
code within complex codebases. In this paper, we introduce WebGen-Bench, a
novel benchmark designed to measure an LLM-based agent's ability to create
multi-file website codebases from scratch. It contains diverse instructions for
website generation, created through the combined efforts of human annotators
and GPT-4o. These instructions span three major categories and thirteen minor
categories, encompassing nearly all important types of web applications. To
assess the quality of the generated websites, we use GPT-4o to generate test
cases targeting each functionality described in the instructions, and then
manually filter, adjust, and organize them to ensure accuracy, resulting in 647
test cases. Each test case specifies an operation to be performed on the
website and the expected result after the operation. To automate testing and
improve reproducibility, we employ a powerful web-navigation agent to execute
tests on the generated websites and determine whether the observed responses
align with the expected results. We evaluate three high-performance code-agent
frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and
open-source LLMs as engines. The best-performing combination, Bolt.diy powered
by DeepSeek-R1, achieves only 27.8\% accuracy on the test cases, highlighting
the challenging nature of our benchmark. Additionally, we construct
WebGen-Instruct, a training set consisting of 6,667 website-generation
instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories
generated from a subset of this training set achieves an accuracy of 38.2\%,
surpassing the performance of the best proprietary model.

</details>


### [34] [VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model](https://arxiv.org/abs/2505.03739)
*Zuwei Long,Yunhang Shen,Chaoyou Fu,Heting Gao,Lijiang Li,Peixian Chen,Mengdan Zhang,Hang Shao,Jian Li,Jinlong Peng,Haoyu Cao,Ke Li,Rongrong Ji,Xing Sun*

Main category: cs.CL

TL;DR: VITA-Audio 是一种端到端的大型语音模型，通过引入轻量级多模态令牌预测模块和四阶段渐进式训练策略，显著降低了音频流式传输中的延迟，并加速了推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有语音模型在流式传输时生成第一个音频令牌的延迟较高，限制了其在实际部署中的应用。

Method: 引入轻量级多模态令牌预测模块（MCTP），在一次前向传递中生成多个音频令牌，并采用四阶段渐进式训练策略。

Result: 在7B参数规模下，VITA-Audio实现了3~5倍的推理加速，并在语音识别、文本转语音和口语问答任务上超越同类开源模型。

Conclusion: VITA-Audio 是首个能在首次前向传递中生成音频输出的多模态大语言模型，显著降低了流式传输的延迟。

Abstract: With the growing requirement for natural human-computer interaction,
speech-based systems receive increasing attention as speech is one of the most
common forms of daily communication. However, the existing speech models still
experience high latency when generating the first audio token during streaming,
which poses a significant bottleneck for deployment. To address this issue, we
propose VITA-Audio, an end-to-end large speech model with fast audio-text token
generation. Specifically, we introduce a lightweight Multiple Cross-modal Token
Prediction (MCTP) module that efficiently generates multiple audio tokens
within a single model forward pass, which not only accelerates the inference
but also significantly reduces the latency for generating the first audio in
streaming scenarios. In addition, a four-stage progressive training strategy is
explored to achieve model acceleration with minimal loss of speech quality. To
our knowledge, VITA-Audio is the first multi-modal large language model capable
of generating audio output during the first forward pass, enabling real-time
conversational capabilities with minimal latency. VITA-Audio is fully
reproducible and is trained on open-source data only. Experimental results
demonstrate that our model achieves an inference speedup of 3~5x at the 7B
parameter scale, but also significantly outperforms open-source models of
similar model size on multiple benchmarks for automatic speech recognition
(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [Uncertainty Quantification for Machine Learning in Healthcare: A Survey](https://arxiv.org/abs/2505.02874)
*L. Julián Lechuga López,Shaza Elsharief,Dhiyaa Al Jorf,Firas Darwish,Congbo Ma,Farah E. Shamout*

Main category: cs.LG

TL;DR: 该论文综述了医疗领域中机器学习（ML）不确定性量化（UQ）的现状，提出了一个框架，将不同UQ方法整合到ML流程中，并展望了未来的应用方向。


<details>
  <summary>Details</summary>
Motivation: 当前医疗领域的ML工具缺乏系统性的不确定性量化，限制了其可靠性和临床应用。

Method: 通过全面分析现有UQ方法，构建一个框架，展示如何将这些方法整合到ML流程的各个阶段（数据、训练、评估）。

Result: 总结了医疗领域常用的UQ方法，并提出了其他领域可能适用于医疗的新方法。

Conclusion: 该研究为医疗ML中的UQ提供了清晰的挑战与机遇概览，帮助研究者和从业者选择合适的技术以提升系统的可靠性与信任度。

Abstract: Uncertainty Quantification (UQ) is pivotal in enhancing the robustness,
reliability, and interpretability of Machine Learning (ML) systems for
healthcare, optimizing resources and improving patient care. Despite the
emergence of ML-based clinical decision support tools, the lack of principled
quantification of uncertainty in ML models remains a major challenge. Current
reviews have a narrow focus on analyzing the state-of-the-art UQ in specific
healthcare domains without systematically evaluating method efficacy across
different stages of model development, and despite a growing body of research,
its implementation in healthcare applications remains limited. Therefore, in
this survey, we provide a comprehensive analysis of current UQ in healthcare,
offering an informed framework that highlights how different methods can be
integrated into each stage of the ML pipeline including data processing,
training and evaluation. We also highlight the most popular methods used in
healthcare and novel approaches from other domains that hold potential for
future adoption in the medical context. We expect this study will provide a
clear overview of the challenges and opportunities of implementing UQ in the ML
pipeline for healthcare, guiding researchers and practitioners in selecting
suitable techniques to enhance the reliability, safety and trust from patients
and clinicians on ML-driven healthcare solutions.

</details>


### [36] [A Wireless Collaborated Inference Acceleration Framework for Plant Disease Recognition](https://arxiv.org/abs/2505.02877)
*Hele Zhu,Xinyi Huang,Haojia Gao,Mengfei Jiang,Haohua Que,Lei Mu*

Main category: cs.LG

TL;DR: 该论文提出了一种边缘设备与云服务器协同的植物病害识别推理框架，通过深度强化学习修剪模型并确定最优分裂点，显著提升了推理速度并降低了能耗，同时保持了可接受的识别精度。


<details>
  <summary>Details</summary>
Motivation: 传统手动识别方法准确率低、成本高且效率低下，而深度学习在资源受限设备上运行困难，云服务器又受带宽限制。因此，亟需一种高效、低耗的植物病害识别方案。

Method: 使用深度强化学习修剪DNN模型以减少计算量，基于贪婪策略确定最优分裂点，实现边缘与云的协同推理加速，并通过Gradio实现人机交互系统。

Result: 实验表明，该框架显著提升了推理速度，同时保持了可接受的识别精度，为植物病害快速诊断提供了新方案。

Conclusion: 提出的协同推理框架解决了资源受限设备的效率与能耗问题，为农业病害防治提供了实用且高效的技术支持。

Abstract: Plant disease is a critical factor affecting agricultural production.
Traditional manual recognition methods face significant drawbacks, including
low accuracy, high costs, and inefficiency. Deep learning techniques have
demonstrated significant benefits in identifying plant diseases, but they still
face challenges such as inference delays and high energy consumption. Deep
learning algorithms are difficult to run on resource-limited embedded devices.
Offloading these models to cloud servers is confronted with the restriction of
communication bandwidth, and all of these factors will influence the
inference's efficiency. We propose a collaborative inference framework for
recognizing plant diseases between edge devices and cloud servers to enhance
inference speed. The DNN model for plant disease recognition is pruned through
deep reinforcement learning to improve the inference speed and reduce energy
consumption. Then the optimal split point is determined by a greedy strategy to
achieve the best collaborated inference acceleration. Finally, the system for
collaborative inference acceleration in plant disease recognition has been
implemented using Gradio to facilitate friendly human-machine interaction.
Experiments indicate that the proposed collaborative inference framework
significantly increases inference speed while maintaining acceptable
recognition accuracy, offering a novel solution for rapidly diagnosing and
preventing plant diseases.

</details>


### [37] [LLM4FTS: Enhancing Large Language Models for Financial Time Series Prediction](https://arxiv.org/abs/2505.02880)
*Zian Liu,Renjun Jia*

Main category: cs.LG

TL;DR: 论文提出了LLM4FTS框架，通过可学习的分割和动态小波卷积模块增强LLM对金融时间序列的建模能力，实现了股票预测的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型在金融时间序列预测中受限于模型容量，而现有基于LLM的方法忽视市场数据的多尺度特征。

Method: 结合K-means++聚类和DTW距离识别尺度不变模式，提出自适应分割和动态小波卷积模块。

Result: 在真实金融数据集上验证了框架的优越性，实现了股票收益预测的最佳效果。

Conclusion: LLM4FTS框架显著提升了LLM在金融预测中的应用，具有实际交易系统的部署价值。

Abstract: Predicting financial time series presents significant challenges due to
inherent low signal-to-noise ratios and intricate temporal patterns.
Traditional machine learning models exhibit limitations in this forecasting
task constrained by their restricted model capacity. Recent advances in large
language models (LLMs), with their greatly expanded parameter spaces,
demonstrate promising potential for modeling complex dependencies in temporal
sequences. However, existing LLM-based approaches typically focus on
fixed-length patch analysis due to the Transformer architecture, ignoring
market data's multi-scale pattern characteristics. In this study, we propose
$LLM4FTS$, a novel framework that enhances LLM capabilities for temporal
sequence modeling through learnable patch segmentation and dynamic wavelet
convolution modules. Specifically,we first employ K-means++ clustering based on
DTW distance to identify scale-invariant patterns in market data. Building upon
pattern recognition results, we introduce adaptive patch segmentation that
partitions temporal sequences while preserving maximal pattern integrity. To
accommodate time-varying frequency characteristics, we devise a dynamic wavelet
convolution module that emulates discrete wavelet transformation with enhanced
flexibility in capturing time-frequency features. These three modules work
together to improve large language model's ability to handle scale-invariant
patterns in financial time series. Extensive experiments on real-world
financial datasets substantiate the framework's efficacy, demonstrating
superior performance in capturing complex market patterns and achieving
state-of-the-art results in stock return prediction. The successful deployment
in practical trading systems confirms its real-world applicability,
representing a significant advancement in LLM applications for financial
forecasting.

</details>


### [38] [Rewriting Pre-Training Data Boosts LLM Performance in Math and Code](https://arxiv.org/abs/2505.02881)
*Kazuki Fujii,Yukito Tajima,Sakae Mizuki,Hinari Shimada,Taihei Shiotani,Koshiro Saito,Masanari Ohi,Masaki Kawamura,Taishi Nakamura,Takumi Okamoto,Shigeki Ishida,Kakeru Hattori,Youmi Ma,Hiroya Takamura,Rio Yokota,Naoaki Okazaki*

Main category: cs.LG

TL;DR: 该论文介绍了两款新数据集SwallowCode和SwallowMath，通过优化公开数据显著提升了大语言模型在程序合成和数学推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在程序合成和数学推理任务中因预训练语料质量不足而表现受限的问题。

Method: 使用四阶段流水线（语法验证、风格过滤、两阶段LLM重写）优化代码片段，并将数学问题转化为简洁的逐步解释。

Result: 在固定50B token训练预算下，使用SwallowCode和SwallowMath分别显著提升了模型在HumanEval、GSM8K等基准测试中的表现。

Conclusion: 通过系统优化公开数据，显著提升了模型性能，并公开了所有数据集和工具以促进可重复研究和领域专用LLM的预训练。

Abstract: The performance of large language models (LLMs) in program synthesis and
mathematical reasoning is fundamentally limited by the quality of their
pre-training corpora. We introduce two openly licensed datasets, released under
the Llama 3.3 Community License, that significantly enhance LLM performance by
systematically rewriting public data. SwallowCode (approximately 16.1 billion
tokens) refines Python snippets from The-Stack-v2 through a novel four-stage
pipeline: syntax validation, pylint-based style filtering, and a two-stage LLM
rewriting process that enforces style conformity and transforms snippets into
self-contained, algorithmically efficient examples. Unlike prior methods that
rely on exclusionary filtering or limited transformations, our
transform-and-retain approach upgrades low-quality code, maximizing data
utility. SwallowMath (approximately 2.3 billion tokens) enhances Finemath-4+ by
removing boilerplate, restoring context, and reformatting solutions into
concise, step-by-step explanations. Within a fixed 50 billion token training
budget, continual pre-training of Llama-3.1-8B with SwallowCode boosts pass@1
by +17.0 on HumanEval and +17.7 on HumanEval+ compared to Stack-Edu, surpassing
the baseline model's code generation capabilities. Similarly, substituting
SwallowMath yields +12.4 accuracy on GSM8K and +7.6 on MATH. Ablation studies
confirm that each pipeline stage contributes incrementally, with rewriting
delivering the largest gains. All datasets, prompts, and checkpoints are
publicly available, enabling reproducible research and advancing LLM
pre-training for specialized domains.

</details>


### [39] [Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?](https://arxiv.org/abs/2505.02884)
*Guangzhi Sun,Potsawee Manakul,Xiao Zhan,Mark Gales*

Main category: cs.LG

TL;DR: 论文提出了DF-MCQ方法，通过KL散度在自动生成的多选题上扁平化模型预测分布，有效实现大语言模型的真正遗忘（unlearning），而非仅仅模糊化（obfuscation）。实验显示其拒绝率超90%，且不确定性显著高于模糊化方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决当前大语言模型遗忘技术（如模糊化）仅通过注入错误信息掩盖知识而非真正移除的问题，从而满足数据隐私、法规合规和伦理AI的需求。

Method: 提出了DF-MCQ方法，利用KL散度在自动生成的多选题上扁平化模型预测分布，有效移除目标知识并触发适当的拒绝行为。

Result: 实验结果表明，DF-MCQ在遗忘目标个体知识时拒绝率超过90%，且在探测问题上的不确定性显著高于模糊化方法，达到随机选择水平。

Conclusion: 论文结论是DF-MCQ方法真正实现了知识移除而非模糊化，为大语言模型的遗忘提供了更有效的解决方案。

Abstract: Unlearning has emerged as a critical capability for large language models
(LLMs) to support data privacy, regulatory compliance, and ethical AI
deployment. Recent techniques often rely on obfuscation by injecting incorrect
or irrelevant information to suppress knowledge. Such methods effectively
constitute knowledge addition rather than true removal, often leaving models
vulnerable to probing. In this paper, we formally distinguish unlearning from
obfuscation and introduce a probing-based evaluation framework to assess
whether existing approaches genuinely remove targeted information. Moreover, we
propose DF-MCQ, a novel unlearning method that flattens the model predictive
distribution over automatically generated multiple-choice questions using
KL-divergence, effectively removing knowledge about target individuals and
triggering appropriate refusal behaviour. Experimental results demonstrate that
DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level
uncertainty that is much higher than obfuscation on probing questions.

</details>


### [40] [When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger](https://arxiv.org/abs/2505.02888)
*Rintaro Ando*

Main category: cs.LG

TL;DR: N2M-RSI是一个形式化模型，展示AI代理在反馈输出作为输入并超过信息整合阈值后，内部复杂性会无限增长，统一了自提示LLM、哥德尔自引用和AutoML等概念。


<details>
  <summary>Details</summary>
Motivation: 探索AI代理通过自反馈实现无限复杂性的潜力，并揭示群体交互可能带来的超线性效应。

Method: 提出N2M-RSI模型，基于自反馈和明确的信息整合阈值，模型实现与具体实现无关。

Result: 模型显示AI代理内部复杂性将无限增长，群体交互可能产生超线性效应。

Conclusion: N2M-RSI为自改进AI提供了理论框架，但因安全考虑未公开具体实现细节。

Abstract: We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal
formal model showing that once an AI agent feeds its own outputs back as inputs
and crosses an explicit information-integration threshold, its internal
complexity will grow without bound under our assumptions. The framework unifies
earlier ideas on self-prompting large language models, G\"odelian
self-reference, and AutoML, yet remains implementation-agnostic. The model
furthermore scales naturally to interacting swarms of agents, hinting at
super-linear effects once communication among instances is permitted. For
safety reasons, we omit system-specific implementation details and release only
a brief, model-agnostic toy prototype in Appendix C.

</details>


### [41] [Early Prediction of Sepsis: Feature-Aligned Transfer Learning](https://arxiv.org/abs/2505.02889)
*Oyindolapo O. Komolafe,Zhimin Mei,David Morales Zarate,Gregory William Spangenberg*

Main category: cs.LG

TL;DR: 论文提出了一种名为FATL的机器学习方法，通过特征对齐迁移学习来早期预测败血症，解决现有模型特征不一致和人群偏见的问题。


<details>
  <summary>Details</summary>
Motivation: 败血症是一种危及生命的疾病，早期检测对挽救生命至关重要，但现有诊断方法往往在损伤发生后才识别，且模型特征不一致、存在人群偏见。

Method: 提出了FATL方法，通过识别和聚焦跨研究的重要且常见的特征，结合多样人群训练的模型知识，采用加权方法提高泛化能力。

Result: FATL提供了一种一致且临床相关的败血症早期预测方案，适用于不同患者人群和临床环境，尤其适用于资源有限的医院。

Conclusion: FATL是一个实用且可扩展的解决方案，有望改善患者预后、降低医疗成本并促进更公平的医疗服务。

Abstract: Sepsis is a life threatening medical condition that occurs when the body has
an extreme response to infection, leading to widespread inflammation, organ
failure, and potentially death. Because sepsis can worsen rapidly, early
detection is critical to saving lives. However, current diagnostic methods
often identify sepsis only after significant damage has already occurred. Our
project aims to address this challenge by developing a machine learning based
system to predict sepsis in its early stages, giving healthcare providers more
time to intervene.
  A major problem with existing models is the wide variability in the patient
information or features they use, such as heart rate, temperature, and lab
results. This inconsistency makes models difficult to compare and limits their
ability to work across different hospitals and settings. To solve this, we
propose a method called Feature Aligned Transfer Learning (FATL), which
identifies and focuses on the most important and commonly reported features
across multiple studies, ensuring the model remains consistent and clinically
relevant.
  Most existing models are trained on narrow patient groups, leading to
population bias. FATL addresses this by combining knowledge from models trained
on diverse populations, using a weighted approach that reflects each models
contribution. This makes the system more generalizable and effective across
different patient demographics and clinical environments. FATL offers a
practical and scalable solution for early sepsis detection, particularly in
hospitals with limited resources, and has the potential to improve patient
outcomes, reduce healthcare costs, and support more equitable healthcare
delivery.

</details>


### [42] [RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM Inference](https://arxiv.org/abs/2505.02922)
*Yaoqi Chen,Jinkai Zhang,Baotong Lu,Qianxi Zhang,Chengruidong Zhang,Jingjia Luo,Di Liu,Huiqiang Jiang,Qi Chen,Jing Liu,Bailu Ding,Xiao Yan,Jiawei Jiang,Chen Chen,Mingxing Zhang,Yuqing Yang,Fan Yang,Mao Yang*

Main category: cs.LG

TL;DR: RetroInfer is a system that optimizes long-context LLM inference by redefining the KV cache as a vector storage system, leveraging attention sparsity for speed and efficiency without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiencies in GPU memory and bandwidth usage during long-context LLM inference, which hinder performance.

Method: Utilizes a 'wave index' for efficient token retrieval via techniques like tripartite attention approximation and a 'wave buffer' for hardware coordination, improving throughput.

Result: Achieves up to 4.5X speedup over full attention and 10.5X over sparse baselines while maintaining full-attention accuracy.

Conclusion: RetroInfer offers a robust solution for long-context LLM inference, balancing speed, efficiency, and accuracy.

Abstract: The growing context lengths of large language models (LLMs) pose significant
challenges for efficient inference, primarily due to GPU memory and bandwidth
constraints. We present RetroInfer, a novel system that reconceptualizes the
key-value (KV) cache as a vector storage system which exploits the inherent
attention sparsity to accelerate long-context LLM inference. At its core is the
wave index, an Attention-aWare VEctor index that enables efficient and accurate
retrieval of critical tokens through techniques such as tripartite attention
approximation, accuracy-bounded attention estimation, and segmented clustering.
Complementing this is the wave buffer, which coordinates KV cache placement and
overlaps computation and data transfer across GPU and CPU to sustain high
throughput. Unlike prior sparsity-based methods that struggle with token
selection and hardware coordination, RetroInfer delivers robust performance
without compromising model accuracy. Experiments on long-context benchmarks
show up to 4.5X speedup over full attention within GPU memory limits and up to
10.5X over sparse attention baselines when KV cache is extended to CPU memory,
all while preserving full-attention-level accuracy.

</details>


### [43] [Smooth Quadratic Prediction Markets](https://arxiv.org/abs/2505.02959)
*Enrique Nueve,Bo Waggoner*

Main category: cs.LG

TL;DR: 该论文提出了一种新的预测市场设计——平滑二次预测市场，通过分解和修改基于对偶的成本函数市场定价机制，激励代理人集体实现广义最速梯度下降。相比DCFMM，该市场在AD证券上具有更优的最坏情况货币损失，同时保留了瞬时价格存在、信息整合、表达性、无套利和某种形式的激励兼容性等公理保证。


<details>
  <summary>Details</summary>
Motivation: 探讨是否可以通过其他学习算法来设计预测市场，突破现有基于对偶的成本函数预测市场的局限，提升市场效率并适应更实际的交易约束。

Method: 提出平滑二次预测市场，通过分解和修改DCFMM的定价机制，实现广义最速梯度下降。在预算有限和仅限买入证券的现实约束下分析了代理人交易行为。

Result: 平滑二次预测市场在AD证券上表现出更优的最坏情况货币损失，同时保持了核心公理保证。初步分析了自适应流动性的实现方法。

Conclusion: 该研究为预测市场设计提供了新方向，未来可探索价格更新规则与费用结构分离的设计，同时保留关键保证。

Abstract: When agents trade in a Duality-based Cost Function prediction market, they
collectively implement the learning algorithm Follow-The-Regularized-Leader. We
ask whether other learning algorithms could be used to inspire the design of
prediction markets. By decomposing and modifying the Duality-based Cost
Function Market Maker's (DCFMM) pricing mechanism, we propose a new prediction
market, called the Smooth Quadratic Prediction Market, the incentivizes agents
to collectively implement general steepest gradient descent. Relative to the
DCFMM, the Smooth Quadratic Prediction Market has a better worst-case monetary
loss for AD securities while preserving axiom guarantees such as the existence
of instantaneous price, information incorporation, expressiveness, no
arbitrage, and a form of incentive compatibility. To motivate the application
of the Smooth Quadratic Prediction Market, we independently examine agents'
trading behavior under two realistic constraints: bounded budgets and buy-only
securities. Finally, we provide an introductory analysis of an approach to
facilitate adaptive liquidity using the Smooth Quadratic AD Prediction Market.
Our results suggest future designs where the price update rule is separate from
the fee structure, yet guarantees are preserved.

</details>


### [44] [Physics-Learning AI Datamodel (PLAID) datasets: a collection of physics simulations for machine learning](https://arxiv.org/abs/2505.02974)
*Fabien Casenave,Xavier Roynard,Brian Staber,Nissrine Akkari,William Piat,Michele Alessandro Bucci,Abbas Kabalan,Xuan Minh Vuong Nguyen,Luca Saverio,Raphaël Carpintero Perez,Anthony Kalaydjian,Samy Fouché,Thierry Gonon,Ghassan Najjar,Emmanuel Menier,Matthieu Nastorg,Christian Rey*

Main category: cs.LG

TL;DR: PLAID框架提出以标准化和灵活性解决机器学习模型在物理模拟中数据集分散和局限性问题，发布了六个数据集并提供了基准测试工具。


<details>
  <summary>Details</summary>
Motivation: 现有物理模拟数据集存在领域局限、工具分散和简单数据模型难以推广的问题，需要统一标准以促进机器学习在科学模拟中的应用。

Method: 提出了PLAID框架，定义统一的数据描述标准，并配套工具库支持多领域物理模拟数据的创建与管理，发布六个跨领域数据集。

Result: PLAID框架实现了跨结构力学和计算流体动力学的数据集标准化，并提供基准测试工具，促进社区参与评估。

Conclusion: PLAID为物理模拟数据提供了通用解决方案，支持机器学习模型的高效开发和应用，推动了科学模拟领域的协作与创新。

Abstract: Machine learning-based surrogate models have emerged as a powerful tool to
accelerate simulation-driven scientific workflows. However, their widespread
adoption is hindered by the lack of large-scale, diverse, and standardized
datasets tailored to physics-based simulations. While existing initiatives
provide valuable contributions, many are limited in scope-focusing on specific
physics domains, relying on fragmented tooling, or adhering to overly
simplistic datamodels that restrict generalization. To address these
limitations, we introduce PLAID (Physics-Learning AI Datamodel), a flexible and
extensible framework for representing and sharing datasets of physics
simulations. PLAID defines a unified standard for describing simulation data
and is accompanied by a library for creating, reading, and manipulating complex
datasets across a wide range of physical use cases (gitlab.com/drti/plaid). We
release six carefully crafted datasets under the PLAID standard, covering
structural mechanics and computational fluid dynamics, and provide baseline
benchmarks using representative learning methods. Benchmarking tools are made
available on Hugging Face, enabling direct participation by the community and
contribution to ongoing evaluation efforts (huggingface.co/PLAIDcompetitions).

</details>


### [45] [More Optimal Fractional-Order Stochastic Gradient Descent for Non-Convex Optimization Problems](https://arxiv.org/abs/2505.02985)
*Mohammad Partohaghighi,Roummel Marcia,YangQuan Chen*

Main category: cs.LG

TL;DR: 提出了一种数据驱动的分数阶随机梯度下降方法2SEDFOSGD，通过结合双尺度有效维度算法动态调整分数指数，解决了传统方法中分数指数调优和稳定性问题，并在非凸优化问题中展示了更快的收敛速度和更鲁棒的参数估计。


<details>
  <summary>Details</summary>
Motivation: 传统分数阶随机梯度下降（FOSGD）在优化中存在长记忆效应，但其分数指数的调优和稳定性限制了实际应用。

Method: 提出2SEDFOSGD方法，通过跟踪模型敏感性和有效维度，动态调整分数指数，结合双尺度有效维度（2SED）算法。

Result: 在非凸优化问题中，该方法在Gaussian和α-stable噪声环境下表现出更快的收敛速度和更鲁棒的参数估计。

Conclusion: 2SEDFOSGD展示了在高级建模和估计任务中数据驱动的分数阶技术潜力。

Abstract: Fractional-order stochastic gradient descent (FOSGD) leverages fractional
exponents to capture long-memory effects in optimization. However, its utility
is often limited by the difficulty of tuning and stabilizing these exponents.
We propose 2SED Fractional-Order Stochastic Gradient Descent (2SEDFOSGD), which
integrates the Two-Scale Effective Dimension (2SED) algorithm with FOSGD to
adapt the fractional exponent in a data-driven manner. By tracking model
sensitivity and effective dimensionality, 2SEDFOSGD dynamically modulates the
exponent to mitigate oscillations and hasten convergence. Theoretically, for
onoconvex optimization problems, this approach preserves the advantages of
fractional memory without the sluggish or unstable behavior observed in na\"ive
fractional SGD. Empirical evaluations in Gaussian and $\alpha$-stable noise
scenarios using an autoregressive (AR) model highlight faster convergence and
more robust parameter estimates compared to baseline methods, underscoring the
potential of dimension-aware fractional techniques for advanced modeling and
estimation tasks.

</details>


### [46] [Radio: Rate-Distortion Optimization for Large Language Model Compression](https://arxiv.org/abs/2505.03031)
*Sean I. Young*

Main category: cs.LG

TL;DR: 论文提出了一种基于率失真理论的LLM量化技术，支持在训练后根据用户需求灵活压缩模型大小或精度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）部署在资源有限的设备上时面临挑战，作者希望通过量化技术降低计算成本和环境影响。

Method: 从率失真理论角度建立LLM量化基础，提出一种基于简单率失真优化的量化技术。

Result: 该技术可扩展到包含数百亿权重参数的模型，并允许用户根据需要压缩模型大小或精度。

Conclusion: 论文提出的量化技术为LLM的高效部署提供了灵活且可扩展的解决方案。

Abstract: In recent years, the compression of large language models (LLMs) has emerged
as a key problem in facilitating LLM deployment on resource-limited devices,
reducing compute costs, and mitigating the environmental footprint due to
large-scale AI infrastructure. Here, we establish the foundations of LLM
quantization from a rate-distortion theory perspective and propose a
quantization technique based on simple rate-distortion optimization. Our
technique scales to models containing hundreds of billions of weight parameters
and offers users the flexibility to compress models, post-training, to a model
size or accuracy specified by the user.

</details>


### [47] [A New Perspective To Understanding Multi-resolution Hash Encoding For Neural Fields](https://arxiv.org/abs/2505.03042)
*Steven Tin Sui Luo*

Main category: cs.LG

TL;DR: 论文探讨了Instant-NGP多分辨率哈希网格结构的工作原理，提出了一种名为‘域操作’的新视角来解释其如何提升神经网络的表达能力。通过实验验证了该视角的合理性，并展示了其在高维信号中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管Instant-NPG的多分辨率哈希网格结构在信号拟合任务中表现优异，但其工作原理仍缺乏理论支持。缺乏对这一结构的理论理解使得其超参数只能通过经验调优。论文旨在通过‘域操作’的新视角解释哈希网格的工作原理。

Method: 论文提出‘域操作’视角，从底层解释哈希网格如何通过学习目标信号以及人为创建已有线段的多个副本来提升神经场的表达能力。并通过在一维信号上的实验验证和支持这一理论。

Result: 实验结果表明‘域操作’视角能够直观解释多分辨率哈希网格结构的工作原理。这一视角不仅在一维信号中得到了验证，而且展示了其在高维信号中的泛化能力。

Conclusion: 论文为Instant-NGP的多分辨率哈希网格提供了理论支持，提出‘域操作’的新视角能够帮助理解其工作原理和提升表达能力的机制。这为超参数的选择提供了理论启发和高维应用的可能性。

Abstract: Instant-NGP has been the state-of-the-art architecture of neural fields in
recent years. Its incredible signal-fitting capabilities are generally
attributed to its multi-resolution hash grid structure and have been used and
improved in numerous following works. However, it is unclear how and why such a
hash grid structure improves the capabilities of a neural network by such great
margins. A lack of principled understanding of the hash grid also implies that
the large set of hyperparameters accompanying Instant-NGP could only be tuned
empirically without much heuristics. To provide an intuitive explanation of the
working principle of the hash grid, we propose a novel perspective, namely
domain manipulation. This perspective provides a ground-up explanation of how
the feature grid learns the target signal and increases the expressivity of the
neural field by artificially creating multiples of pre-existing linear
segments. We conducted numerous experiments on carefully constructed
1-dimensional signals to support our claims empirically and aid our
illustrations. While our analysis mainly focuses on 1-dimensional signals, we
show that the idea is generalizable to higher dimensions.

</details>


### [48] [34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery](https://arxiv.org/abs/2505.03049)
*Yoel Zimmermann,Adib Bazgir,Alexander Al-Feghali,Mehrad Ansari,L. Catherine Brinson,Yuan Chiang,Defne Circi,Min-Hsueh Chiu,Nathan Daelman,Matthew L. Evans,Abhijeet S. Gangan,Janine George,Hassan Harb,Ghazal Khalighinejad,Sartaaj Takrim Khan,Sascha Klawohn,Magdalena Lederbauer,Soroush Mahjoubi,Bernadette Mohr,Seyed Mohamad Moosavi,Aakash Naik,Aleyna Beste Ozhan,Dieter Plessers,Aritra Roy,Fabian Schöppach,Philippe Schwaller,Carla Terboven,Katharina Ueltzen,Shang Zhu,Jan Janssen,Calvin Li,Ian Foster,Ben Blaiszik*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）正在改变材料科学与化学研究的多个领域，涵盖分子性质预测、材料设计等，并通过34个项目展示了其多功能性。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在科学研究全周期中的应用潜力，尤其是在材料科学与化学中的前沿能力。

Method: 通过第二届年度LLM Hackathon的34个项目，围绕七个关键研究领域（如分子设计、自动化等）进行综述分析。

Result: LLMs展现了作为预测模型和领域工具快速原型平台的潜力，尤其在低数据环境和跨学科研究中表现突出。

Conclusion: LLMs的持续改进为科学工作流带来新机遇，但也需解决可靠性、可解释性等挑战，需进一步研究。

Abstract: Large Language Models (LLMs) are reshaping many aspects of materials science
and chemistry research, enabling advances in molecular property prediction,
materials design, scientific automation, knowledge extraction, and more. Recent
developments demonstrate that the latest class of models are able to integrate
structured and unstructured data, assist in hypothesis generation, and
streamline research workflows. To explore the frontier of LLM capabilities
across the research lifecycle, we review applications of LLMs through 34 total
projects developed during the second annual Large Language Model Hackathon for
Applications in Materials Science and Chemistry, a global hybrid event. These
projects spanned seven key research areas: (1) molecular and material property
prediction, (2) molecular and material design, (3) automation and novel
interfaces, (4) scientific communication and education, (5) research data
management and automation, (6) hypothesis generation and evaluation, and (7)
knowledge extraction and reasoning from the scientific literature.
Collectively, these applications illustrate how LLMs serve as versatile
predictive models, platforms for rapid prototyping of domain-specific tools,
and much more. In particular, improvements in both open source and proprietary
LLM performance through the addition of reasoning, additional training data,
and new techniques have expanded effectiveness, particularly in low-data
environments and interdisciplinary research. As LLMs continue to improve, their
integration into scientific workflows presents both new opportunities and new
challenges, requiring ongoing exploration, continued refinement, and further
research to address reliability, interpretability, and reproducibility.

</details>


### [49] [Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/abs/2505.03084)
*Shashank Kapoor,Sanjay Surendranath Girija,Lakshit Arora,Dipen Pradhan,Ankit Shetgaonkar,Aman Raj*

Main category: cs.LG

TL;DR: 论文提供了对多模态模型中对抗性攻击的首个全面综述，涵盖文本、图像、视频和音频四种模态，填补了实践应用视角的空白。


<details>
  <summary>Details</summary>
Motivation: 多模态模型虽为AI带来突破，但其跨模态的对抗性攻击威胁被放大，而现有研究缺乏面向实践者的攻击类型总结。

Method: 通过调查针对文本、图像、视频和音频四种模态的对抗性攻击，梳理攻击类型及其在多模态环境中的演变。

Result: 首次全面总结了多模态世界的对抗性威胁，提供了攻击视角的实践指南。

Conclusion: 该研究填补了多模态对抗性攻击的实践空白，为开发者在部署模型时提供防御依据。

Abstract: The introduction of multimodal models is a huge step forward in Artificial
Intelligence. A single model is trained to understand multiple modalities:
text, image, video, and audio. Open-source multimodal models have made these
breakthroughs more accessible. However, considering the vast landscape of
adversarial attacks across these modalities, these models also inherit
vulnerabilities of all the modalities, and ultimately, the adversarial threat
amplifies. While broad research is available on possible attacks within or
across these modalities, a practitioner-focused view that outlines attack types
remains absent in the multimodal world. As more Machine Learning Practitioners
adopt, fine-tune, and deploy open-source models in real-world applications,
it's crucial that they can view the threat landscape and take the preventive
actions necessary. This paper addresses the gap by surveying adversarial
attacks targeting all four modalities: text, image, video, and audio. This
survey provides a view of the adversarial attack landscape and presents how
multimodal adversarial threats have evolved. To the best of our knowledge, this
survey is the first comprehensive summarization of the threat landscape in the
multimodal world.

</details>


### [50] [Deep Learning in Renewable Energy Forecasting: A Cross-Dataset Evaluation of Temporal and Spatial Models](https://arxiv.org/abs/2505.03109)
*Lutfu Sua,Haibo Wang,Jun Huang*

Main category: cs.LG

TL;DR: 该论文探讨了在可再生能源领域中使用深度学习（DL）模型的必要性，并比较了多种DL和传统机器学习（ML）方法在两种不同数据集上的表现，最终发现LSTM和MLP模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 可再生能源数据具有非线性和复杂性，传统ML模型难以有效捕捉变量间的复杂关系，因此需要研究DL模型在这一领域的适用性和准确性影响因素。

Method: 通过比较七种不同的DL和ML方法（包括LSTM、CNN等），并使用两种数据集（天气与发电数据、光伏面板输出数据），结合正则化技术（如早停、神经元丢弃等）来减少过拟合问题。

Result: LSTM和MLP模型表现最优，其验证数据的均方根误差非常低。

Conclusion: DL模型（尤其是LSTM和MLP）在可再生能源数据中表现出色，适合用于处理复杂的非线性关系。

Abstract: Unpredictability of renewable energy sources coupled with the complexity of
those methods used for various purposes in this area calls for the development
of robust methods such as DL models within the renewable energy domain. Given
the nonlinear relationships among variables in renewable energy datasets, DL
models are preferred over traditional machine learning (ML) models because they
can effectively capture and model complex interactions between variables. This
research aims to identify the factors responsible for the accuracy of DL
techniques, such as sampling, stationarity, linearity, and hyperparameter
optimization for different algorithms. The proposed DL framework compares
various methods and alternative training/test ratios. Seven ML methods, such as
Long-Short Term Memory (LSTM), Stacked LSTM, Convolutional Neural Network
(CNN), CNN-LSTM, Deep Neural Network (DNN), Multilayer Perceptron (MLP), and
Encoder-Decoder (ED), were evaluated on two different datasets. The first
dataset contains the weather and power generation data. It encompasses two
distinct datasets, hourly energy demand data and hourly weather data in Spain,
while the second dataset includes power output generated by the photovoltaic
panels at 12 locations. This study deploys regularization approaches, including
early stopping, neuron dropping, and L2 regularization, to reduce the
overfitting problem associated with DL models. The LSTM and MLP models show
superior performance. Their validation data exhibit exceptionally low root mean
square error values.

</details>


### [51] [Plug-and-Play AMC: Context Is King in Training-Free, Open-Set Modulation with LLMs](https://arxiv.org/abs/2505.03112)
*Mohammad Rostami,Atik Faysal,Reihaneh Gh. Roshan,Huaxia Wang,Nikhil Muralidhar,Yu-Dong Yao*

Main category: cs.LG

TL;DR: 论文提出了一种创新的自动调制分类（AMC）框架，结合传统信号处理技术与大语言模型（LLMs），通过高阶统计和累积量估计将信号特征转化为结构化自然语言提示，实现无需额外训练或预处理的单次分类，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: AMC在频谱管理和无线通信中至关重要，但信号干扰和噪声的复杂性使其仍具挑战性。为了解决这一问题，作者提出结合传统信号处理和LLMs的创新方法，旨在减少开发特定信道模型的成本，并提升分类的鲁棒性和可扩展性。

Method: 利用高阶统计和累积量估计提取信号特征，将其转化为结构化自然语言提示，结合LLMs对传统信号处理的固有知识，实现无需额外训练的单次分类。

Result: 在合成数据集（包括无噪声和有噪声条件）上的实验表明，该方法在多种调制方案和信噪比（SNR）下均表现优异，为无线通信中的基础模型提供了新方向。

Conclusion: 该框架为下一代无线网络中可扩展、可解释且通用的信号分类系统奠定了基础，显著降低了信道特定模型的开发成本。

Abstract: Automatic Modulation Classification (AMC) is critical for efficient spectrum
management and robust wireless communications. However, AMC remains challenging
due to the complex interplay of signal interference and noise. In this work, we
propose an innovative framework that integrates traditional signal processing
techniques with Large-Language Models (LLMs) to address AMC. Our approach
leverages higher-order statistics and cumulant estimation to convert
quantitative signal features into structured natural language prompts. By
incorporating exemplar contexts into these prompts, our method exploits the
LLM's inherent familiarity with classical signal processing, enabling effective
one-shot classification without additional training or preprocessing (e.g.,
denoising). Experimental evaluations on synthetically generated datasets,
spanning both noiseless and noisy conditions, demonstrate that our framework
achieves competitive performance across diverse modulation schemes and
Signal-to-Noise Ratios (SNRs). Moreover, our approach paves the way for robust
foundation models in wireless communications across varying channel conditions,
significantly reducing the expense associated with developing channel-specific
models. This work lays the foundation for scalable, interpretable, and
versatile signal classification systems in next-generation wireless networks.
The source code is available at https://github.com/RU-SIT/context-is-king

</details>


### [52] [Adaptive Thresholding for Multi-Label Classification via Global-Local Signal Fusion](https://arxiv.org/abs/2505.03118)
*Dmytro Shamatrin*

Main category: cs.LG

TL;DR: 论文提出了一种自适应阈值机制，结合全局和局部信号，用于多标签分类，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统多标签分类方法使用固定阈值或独立处理标签，忽略了上下文和全局稀疏性，因此需要更灵活的方法。

Method: 采用自适应阈值机制，融合全局（基于IDF）和局部（基于KNN）信号，生成每个标签和实例的阈值，并将其作为可微损失惩罚。

Result: 在AmazonCat-13K基准测试中，宏F1达到0.1712，优于基于树和预训练Transformer的方法。

Conclusion: 提出的方法轻量、可解释且模块化，代码已开源以便复现和扩展。

Abstract: Multi-label classification (MLC) requires predicting multiple labels per
sample, often under heavy class imbalance and noisy conditions. Traditional
approaches apply fixed thresholds or treat labels independently, overlooking
context and global rarity. We introduce an adaptive thresholding mechanism that
fuses global (IDF-based) and local (KNN-based) signals to produce per-label,
per-instance thresholds. Instead of applying these as hard cutoffs, we treat
them as differentiable penalties in the loss, providing smooth supervision and
better calibration. Our architecture is lightweight, interpretable, and highly
modular. On the AmazonCat-13K benchmark, it achieves a macro-F1 of 0.1712,
substantially outperforming tree-based and pretrained transformer-based
methods. We release full code for reproducibility and future extensions.

</details>


### [53] [Rethinking the Global Convergence of Softmax Policy Gradient with Linear Function Approximation](https://arxiv.org/abs/2505.03155)
*Max Qiushi Lin,Jincheng Mei,Matin Aghaei,Michael Lu,Bo Dai,Alekh Agarwal,Dale Schuurmans,Csaba Szepesvari,Sharan Vaswani*

Main category: cs.LG

TL;DR: 论文研究了带线性函数近似的Softmax策略梯度方法（Lin-SPG），证明近似误差不影响算法的全局收敛性，并提出了保证全局收敛的特征表示条件，同时还证明了无论学习率如何，Lin-SPG都可以渐进收敛到最优策略。


<details>
  <summary>Details</summary>
Motivation: 策略梯度方法在强化学习中取得了显著的成功，但在处理大的状态-动作空间时，通常需要结合函数近似技术。目前，近似误差对全局收敛的影响尚不明确，本研究旨在探索Lin-SPG方法的收敛性及其近似误差的影响。

Method: 本文通过理论分析，识别出保证Lin-SPG方法全局收敛的必要与充分特征表示条件，并进一步考察了问题特定学习率及任意恒定学习率条件下的收敛行为。

Result: 研究发现：1) 近似误差不影响Lin-SPG的全局收敛性；2) 在特定特征条件下，T次迭代可实现O(1/T)的最优策略收敛；3) 任意恒定学习率下，Lin-SPG均可渐进收敛到最优策略。

Conclusion: 研究为Lin-SPG的全局收敛性提供了理论保障，拓展了对策略梯度方法在实际应用中稳定性的理解，为进一步研究奠定了理论基础。

Abstract: Policy gradient (PG) methods have played an essential role in the empirical
successes of reinforcement learning. In order to handle large state-action
spaces, PG methods are typically used with function approximation. In this
setting, the approximation error in modeling problem-dependent quantities is a
key notion for characterizing the global convergence of PG methods. We focus on
Softmax PG with linear function approximation (referred to as
$\texttt{Lin-SPG}$) and demonstrate that the approximation error is irrelevant
to the algorithm's global convergence even for the stochastic bandit setting.
Consequently, we first identify the necessary and sufficient conditions on the
feature representation that can guarantee the asymptotic global convergence of
$\texttt{Lin-SPG}$. Under these feature conditions, we prove that $T$
iterations of $\texttt{Lin-SPG}$ with a problem-specific learning rate result
in an $O(1/T)$ convergence to the optimal policy. Furthermore, we prove that
$\texttt{Lin-SPG}$ with any arbitrary constant learning rate can ensure
asymptotic global convergence to the optimal policy.

</details>


### [54] [Improving the Reproducibility of Deep Learning Software: An Initial Investigation through a Case Study Analysis](https://arxiv.org/abs/2505.03165)
*Nikita Ravi,Abhinav Goel,James C. Davis,George K. Thiruvathukal*

Main category: cs.LG

TL;DR: 该论文提出了一种系统性方法来分析和提高深度学习模型的可复现性，通过案例研究展示了相关指导原则，包括环境复制、算法实现、设计透明化等，并进行了敏感性分析。


<details>
  <summary>Details</summary>
Motivation: 深度学习领域虽取得重大突破，但结果的可复现性问题日益突出，影响研究的可靠性和有效性。超过70%的研究者无法复现他人实验，50%无法复现自己的实验，亟需解决。

Method: 论文通过案例研究展示了一套指导原则，如复制原始软件环境、实现端到端算法、公开架构设计、增强数据处理和训练管道的透明度，并进行敏感性分析。

Result: 通过实施这些策略，论文展示了如何有效提高深度学习模型的可复现性，弥合研究与实际应用之间的差距。

Conclusion: 本文提出的系统性方法和指导原则为深度学习模型的可复现性提供了实用解决方案，有助于研究结果的可靠部署和应用。

Abstract: The field of deep learning has witnessed significant breakthroughs, spanning
various applications, and fundamentally transforming current software
capabilities. However, alongside these advancements, there have been increasing
concerns about reproducing the results of these deep learning methods. This is
significant because reproducibility is the foundation of reliability and
validity in software development, particularly in the rapidly evolving domain
of deep learning. The difficulty of reproducibility may arise due to several
reasons, including having differences from the original execution environment,
incompatible software libraries, proprietary data and source code, lack of
transparency, and the stochastic nature in some software. A study conducted by
the Nature journal reveals that more than 70% of researchers failed to
reproduce other researchers experiments and over 50% failed to reproduce their
own experiments. Irreproducibility of deep learning poses significant
challenges for researchers and practitioners. To address these concerns, this
paper presents a systematic approach at analyzing and improving the
reproducibility of deep learning models by demonstrating these guidelines using
a case study. We illustrate the patterns and anti-patterns involved with these
guidelines for improving the reproducibility of deep learning models. These
guidelines encompass establishing a methodology to replicate the original
software environment, implementing end-to-end training and testing algorithms,
disclosing architectural designs, and enhancing transparency in data processing
and training pipelines. We also conduct a sensitivity analysis to understand
the model performance across diverse conditions. By implementing these
strategies, we aim to bridge the gap between research and practice, so that
innovations in deep learning can be effectively reproduced and deployed within
software.

</details>


### [55] [Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2505.03172)
*Caleb Chuck,Fan Feng,Carl Qi,Chang Shi,Siddhant Agarwal,Amy Zhang,Scott Niekum*

Main category: cs.LG

TL;DR: 论文提出HInt方法，通过结合交互与事后重标签，提高目标导向强化学习的样本效率，并在对象为中心的任务中显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 在对象为中心的领域，传统的事后重标签方法难以有效分配奖励，导致学习失败。论文通过定义交互行为来改进这一问题。

Method: 提出HInt方法，结合交互与事后重标签，并使用Null Counterfactual Interaction Inference（NCII）来推断交互行为。

Result: 在多个动态机器人任务中实现了显著的交互推断准确性提升，样本效率提高了4倍。

Conclusion: 该方法在对象为中心的任务中有效，显著提升了样本效率和性能。

Abstract: Hindsight relabeling is a powerful tool for overcoming sparsity in
goal-conditioned reinforcement learning (GCRL), especially in certain domains
such as navigation and locomotion. However, hindsight relabeling can struggle
in object-centric domains. For example, suppose that the goal space consists of
a robotic arm pushing a particular target block to a goal location. In this
case, hindsight relabeling will give high rewards to any trajectory that does
not interact with the block. However, these behaviors are only useful when the
object is already at the goal -- an extremely rare case in practice. A dataset
dominated by these kinds of trajectories can complicate learning and lead to
failures. In object-centric domains, one key intuition is that meaningful
trajectories are often characterized by object-object interactions such as
pushing the block with the gripper. To leverage this intuition, we introduce
Hindsight Relabeling using Interactions (HInt), which combines interactions
with hindsight relabeling to improve the sample efficiency of downstream RL.
However because interactions do not have a consensus statistical definition
tractable for downstream GCRL, we propose a definition of interactions based on
the concept of null counterfactual: a cause object is interacting with a target
object if, in a world where the cause object did not exist, the target object
would have different transition dynamics. We leverage this definition to infer
interactions in Null Counterfactual Interaction Inference (NCII), which uses a
"nulling'' operation with a learned model to infer interactions. NCII is able
to achieve significantly improved interaction inference accuracy in both simple
linear dynamics domains and dynamic robotic domains in Robosuite, Robot Air
Hockey, and Franka Kitchen and HInt improves sample efficiency by up to 4x.

</details>


### [56] [RADE: Learning Risk-Adjustable Driving Environment via Multi-Agent Conditional Diffusion](https://arxiv.org/abs/2505.03178)
*Jiawei Wang,Xintao Yan,Yao Mu,Haowei Sun,Zhong Cao,Henry X. Liu*

Main category: cs.LG

TL;DR: RADE是一个基于多智能体扩散架构的仿真框架，用于生成统计上真实且风险可调的交通场景，直接通过数据学习风险条件下的行为，并保持自然的多智能体交互，同时通过令牌化动力学检查模块确保物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过设计复杂的目标操纵单一车辆的轨迹以引发对抗性交互，但牺牲了真实性和可扩展性。RADE旨在生成统计上真实且风险可调的交通场景，以更高效地测试自动驾驶车辆的安全性。

Method: RADE基于多智能体扩散架构，联合建模环境中所有智能体的行为，并通过代理风险度量条件化其轨迹。引入令牌化动力学检查模块过滤生成的轨迹以确保物理合理性。

Result: 在真实世界的rounD数据集上验证表明，RADE在不同风险水平下均保持统计真实性，并随着风险水平的提升自然增加安全关键事件的概率。

Conclusion: RADE作为一种可扩展且真实的工具，具有在自动驾驶安全性评估中的潜力。

Abstract: Generating safety-critical scenarios in high-fidelity simulations offers a
promising and cost-effective approach for efficient testing of autonomous
vehicles. Existing methods typically rely on manipulating a single vehicle's
trajectory through sophisticated designed objectives to induce adversarial
interactions, often at the cost of realism and scalability. In this work, we
propose the Risk-Adjustable Driving Environment (RADE), a simulation framework
that generates statistically realistic and risk-adjustable traffic scenes.
Built upon a multi-agent diffusion architecture, RADE jointly models the
behavior of all agents in the environment and conditions their trajectories on
a surrogate risk measure. Unlike traditional adversarial methods, RADE learns
risk-conditioned behaviors directly from data, preserving naturalistic
multi-agent interactions with controllable risk levels. To ensure physical
plausibility, we incorporate a tokenized dynamics check module that efficiently
filters generated trajectories using a motion vocabulary. We validate RADE on
the real-world rounD dataset, demonstrating that it preserves statistical
realism across varying risk levels and naturally increases the likelihood of
safety-critical events as the desired risk level grows up. Our results
highlight RADE's potential as a scalable and realistic tool for AV safety
evaluation.

</details>


### [57] [VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making](https://arxiv.org/abs/2505.03181)
*Jake Grigsby,Yuke Zhu,Michael Ryoo,Juan Carlos Niebles*

Main category: cs.LG

TL;DR: 该研究通过离线到在线强化学习方法优化视觉语言模型（VLM）在代理任务中的表现，弥补其在严格输出语法要求上的不足，并展示了在三个多模态代理领域中的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决视觉语言模型在代理任务中不如大型语言模型灵活，尤其是在严格输出语法要求上的不足，研究提出了一种改进方法。

Method: 采用离线到在线强化学习（RL）框架，结合监督微调（SFT）和从模型自身或更强大模型的失败决策中学习，以稳定且简单的方式优化VLM。

Result: 研究在三个多模态代理领域中验证了该方法，证明其能够有效提升VLM在代理任务中的表现。

Conclusion: 通过强化学习方法，VLM能够在代理任务中自我改进并学习低质量数据集，为多模态代理应用提供了新思路。

Abstract: Recent research looks to harness the general knowledge and reasoning of large
language models (LLMs) into agents that accomplish user-specified goals in
interactive environments. Vision-language models (VLMs) extend LLMs to
multi-modal data and provide agents with the visual reasoning necessary for new
applications in areas such as computer automation. However, agent tasks
emphasize skills where accessible open-weight VLMs lag behind their LLM
equivalents. For example, VLMs are less capable of following an environment's
strict output syntax requirements and are more focused on open-ended question
answering. Overcoming these limitations requires supervised fine-tuning (SFT)
on task-specific expert demonstrations. Our work approaches these challenges
from an offline-to-online reinforcement learning (RL) perspective. RL lets us
fine-tune VLMs to agent tasks while learning from the unsuccessful decisions of
our own model or more capable (larger) models. We explore an off-policy RL
solution that retains the stability and simplicity of the widely used SFT
workflow while allowing our agent to self-improve and learn from low-quality
datasets. We demonstrate this technique with two open-weight VLMs across three
multi-modal agent domains.

</details>


### [58] [Convergence Of Consistency Model With Multistep Sampling Under General Data Assumptions](https://arxiv.org/abs/2505.03194)
*Yiding Chen,Yiyi Zhang,Owen Oertell,Wen Sun*

Main category: cs.LG

TL;DR: 一致性模型通过学习从噪声直接映射到数据的函数，实现快速一步生成和多步采样以提高样本质量。本文分析了自一致性在训练分布下近似成立时的收敛性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在数据生成任务中表现优异，但其迭代采样过程计算成本高。研究一致性模型在自一致性条件下的收敛性，是希望找到更高效的生成方式。

Method: 分析了一致性模型在自一致性近似成立时的收敛性，针对不同数据分布（有界支撑或快速衰减的尾部分布）给出了样本与目标分布在Wasserstein距离或总变差距离下的近似程度。

Result: 当目标分布满足一定条件时，一致性模型生成的样本与目标分布在特定距离度量下接近；多步采样的益处通过常用前向过程的案例研究得到验证。

Conclusion: 一致性模型在特定条件下能有效逼近目标分布，多步采样可进一步提升样本质量。

Abstract: Diffusion models accomplish remarkable success in data generation tasks
across various domains. However, the iterative sampling process is
computationally expensive. Consistency models are proposed to learn consistency
functions to map from noise to data directly, which allows one-step fast data
generation and multistep sampling to improve sample quality. In this paper, we
study the convergence of consistency models when the self-consistency property
holds approximately under the training distribution. Our analysis requires only
mild data assumption and applies to a family of forward processes. When the
target data distribution has bounded support or has tails that decay
sufficiently fast, we show that the samples generated by the consistency model
are close to the target distribution in Wasserstein distance; when the target
distribution satisfies some smoothness assumption, we show that with an
additional perturbation step for smoothing, the generated samples are close to
the target distribution in total variation distance. We provide two case
studies with commonly chosen forward processes to demonstrate the benefit of
multistep sampling.

</details>


### [59] [Transformers for Learning on Noisy and Task-Level Manifolds: Approximation and Generalization Insights](https://arxiv.org/abs/2505.03205)
*Zhaiming Shen,Alex Havrilla,Rongjie Lai,Alexander Cloninger,Wenjing Liao*

Main category: cs.LG

TL;DR: 这篇论文为Transformer在带噪数据的流形回归任务中建立了理论基础，发现其性能依赖于数据的内在维度，即使输入数据受到高维噪声干扰。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据和学习任务通常具有低维结构，同时伴随噪声或测量误差。尽管Transformer在实际中表现优异，但其理论理解尚不完善。论文旨在填补这一空白，研究Transformer在带噪数据流形上的回归任务中的表现。

Method: 通过分析输入数据为流形管状邻域的情况，研究Transformer如何从带噪数据中学习依赖于流形投影的真实函数。采用新颖的证明技术构建Transformer对基本算术运算的表示。

Result: 论文证明了Transformer的近似和泛化误差依赖于流形的内在维度，表明即使输入数据受高维噪声干扰，Transformer仍可利用学习任务中的低复杂性结构。

Conclusion: 这项研究为Transformer在带噪数据任务中的理论表现提供了新的见解，其证明技术可能具有独立的研究价值。

Abstract: Transformers serve as the foundational architecture for large language and
video generation models, such as GPT, BERT, SORA and their successors.
Empirical studies have demonstrated that real-world data and learning tasks
exhibit low-dimensional structures, along with some noise or measurement error.
The performance of transformers tends to depend on the intrinsic dimension of
the data/tasks, though theoretical understandings remain largely unexplored for
transformers. This work establishes a theoretical foundation by analyzing the
performance of transformers for regression tasks involving noisy input data on
a manifold. Specifically, the input data are in a tubular neighborhood of a
manifold, while the ground truth function depends on the projection of the
noisy data onto the manifold. We prove approximation and generalization errors
which crucially depend on the intrinsic dimension of the manifold. Our results
demonstrate that transformers can leverage low-complexity structures in
learning task even when the input data are perturbed by high-dimensional noise.
Our novel proof technique constructs representations of basic arithmetic
operations by transformers, which may hold independent interest.

</details>


### [60] [Partial Label Clustering](https://arxiv.org/abs/2505.03207)
*Yutong Xie,Fuchao Yang,Yuheng Jia*

Main category: cs.LG

TL;DR: 该论文首次研究了部分标签聚类问题，通过利用有限的标签信息提升聚类性能，提出了一种结合特征空间关系、标签消歧和对偶图学习的联合模型，实验证明了其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 部分标签学习（PLL）中每个训练样本有一组候选标签但仅一个真实标签，作者希望利用这些有限的部分标签信息提升聚类性能。

Method: 1. 基于特征空间关系构建样本权重矩阵，消歧候选标签估计真实标签；2. 基于消歧结果构建必须链接和不可链接约束；3. 通过对抗性先验促进的双图学习传播约束；4. 将权重矩阵、标签消歧和约束传播整合为联合模型。

Result: 实验表明，该方法在有限标注样本下优于现有的约束聚类方法、PLL及半监督PLL方法。

Conclusion: 论文证明了更好的标签消歧能提升聚类性能，提出的联合模型在多任务协同中实现了性能突破。

Abstract: Partial label learning (PLL) is a significant weakly supervised learning
framework, where each training example corresponds to a set of candidate labels
and only one label is the ground-truth label. For the first time, this paper
investigates the partial label clustering problem, which takes advantage of the
limited available partial labels to improve the clustering performance.
Specifically, we first construct a weight matrix of examples based on their
relationships in the feature space and disambiguate the candidate labels to
estimate the ground-truth label based on the weight matrix. Then, we construct
a set of must-link and cannot-link constraints based on the disambiguation
results. Moreover, we propagate the initial must-link and cannot-link
constraints based on an adversarial prior promoted dual-graph learning
approach. Finally, we integrate weight matrix construction, label
disambiguation, and pairwise constraints propagation into a joint model to
achieve mutual enhancement. We also theoretically prove that a better
disambiguated label matrix can help improve clustering performance.
Comprehensive experiments demonstrate our method realizes superior performance
when comparing with state-of-the-art constrained clustering methods, and
outperforms PLL and semi-supervised PLL methods when only limited samples are
annotated. The code is publicly available at https://github.com/xyt-ml/PLC.

</details>


### [61] [DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning](https://arxiv.org/abs/2505.03209)
*Borui Wang,Kathleen McKeown,Rex Ying*

Main category: cs.LG

TL;DR: DYSTIL结合大型语言模型（LLMs）提出了一种新的强化学习框架，通过动态生成文本策略并优化策略，提升了模型泛化能力和样本效率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于专家演示的强化学习方法泛化能力差、样本效率低且模型可解释性差。DYSTIL通过利用LLM的推理能力，动态生成策略，以克服这些局限。

Method: DYSTIL动态查询策略生成LLM，基于优势估计和专家演示生成文本策略，并通过策略优化逐步将这些策略内化到RL智能体中。

Result: 在Minigrid和BabyAI的挑战性环境中，DYSTIL比现有方法的平均成功率提升了17.75%，且样本效率更高。

Conclusion: DYSTIL通过结合LLM的动态策略生成和策略优化，显著提升了强化学习的性能，同时增强了策略的可解释性。

Abstract: Reinforcement learning from expert demonstrations has long remained a
challenging research problem, and existing state-of-the-art methods using
behavioral cloning plus further RL training often suffer from poor
generalization, low sample efficiency, and poor model interpretability.
Inspired by the strong reasoning abilities of large language models (LLMs), we
propose a novel strategy-based reinforcement learning framework integrated with
LLMs called DYnamic STrategy Induction with Llms for reinforcement learning
(DYSTIL) to overcome these limitations. DYSTIL dynamically queries a
strategy-generating LLM to induce textual strategies based on advantage
estimations and expert demonstrations, and gradually internalizes induced
strategies into the RL agent through policy optimization to improve its
performance through boosting policy generalization and enhancing sample
efficiency. It also provides a direct textual channel to observe and interpret
the evolution of the policy's underlying strategies during training. We test
DYSTIL over challenging RL environments from Minigrid and BabyAI, and
empirically demonstrate that DYSTIL significantly outperforms state-of-the-art
baseline methods by 17.75% in average success rate while also enjoying higher
sample efficiency during the learning process.

</details>


### [62] [Joint Resource Management for Energy-efficient UAV-assisted SWIPT-MEC: A Deep Reinforcement Learning Approach](https://arxiv.org/abs/2505.03230)
*Yue Chen,Hui Kang,Jiahui Li,Geng Su,Boxiong Wang,Jiacheng Wang,Cong Liang,Shuang Liang,Dusit Niyato*

Main category: cs.LG

TL;DR: 该论文提出了一种基于无人机的移动边缘计算系统，结合定向天线技术，为6G物联网终端提供计算资源和能量支持，并通过改进的强化学习算法优化能量效率和终端电池可持续性。


<details>
  <summary>Details</summary>
Motivation: 针对6G物联网在基础设施缺失场景（如偏远地区或灾难环境）中无线信息与能量同时传输的挑战，研究如何通过无人机辅助系统平衡能量消耗、终端电池寿命和资源分配。

Method: 通过双目标优化问题建模，将其转化为马尔可夫决策过程，并改进软演员-评论家算法（SAC），加入动作简化机制以提高收敛性和泛化能力。

Result: 仿真结果表明，该方法在多种场景下优于基线算法，能够高效管理能量并保持高计算性能，且在复杂环境中表现出强泛化能力。

Conclusion: 提出的方法有效解决了无人机辅助系统的多目标优化问题，验证了边界惩罚和充电奖励机制的设计有效性。

Abstract: The integration of simultaneous wireless information and power transfer
(SWIPT) technology in 6G Internet of Things (IoT) networks faces significant
challenges in remote areas and disaster scenarios where ground infrastructure
is unavailable. This paper proposes a novel unmanned aerial vehicle
(UAV)-assisted mobile edge computing (MEC) system enhanced by directional
antennas to provide both computational resources and energy support for ground
IoT terminals. However, such systems require multiple trade-off policies to
balance UAV energy consumption, terminal battery levels, and computational
resource allocation under various constraints, including limited UAV battery
capacity, non-linear energy harvesting characteristics, and dynamic task
arrivals. To address these challenges comprehensively, we formulate a
bi-objective optimization problem that simultaneously considers system energy
efficiency and terminal battery sustainability. We then reformulate this
non-convex problem with a hybrid solution space as a Markov decision process
(MDP) and propose an improved soft actor-critic (SAC) algorithm with an action
simplification mechanism to enhance its convergence and generalization
capabilities. Simulation results have demonstrated that our proposed approach
outperforms various baselines in different scenarios, achieving efficient
energy management while maintaining high computational performance.
Furthermore, our method shows strong generalization ability across different
scenarios, particularly in complex environments, validating the effectiveness
of our designed boundary penalty and charging reward mechanisms.

</details>


### [63] [MDPs with a State Sensing Cost](https://arxiv.org/abs/2505.03280)
*Vansh Kapoor,Jayakrishnan Nair*

Main category: cs.LG

TL;DR: 论文提出了一种在感知成本存在时的序列决策问题，扩展了马尔可夫决策过程（MDP）模型，并提出了一种高效启发式算法。


<details>
  <summary>Details</summary>
Motivation: 实际决策问题中，感知环境状态会带来成本，需要权衡感知与行动的价值。

Method: 将问题建模为扩展的MDP，限制连续非感知动作的数量，并设计基于策略改进的启发式算法。

Result: 理论上界定了策略次优性差距，实践中算法表现接近最优策略。

Conclusion: 通过数值案例验证了方法的有效性，为高成本感知场景提供了实用解决方案。

Abstract: In many practical sequential decision-making problems, tracking the state of
the environment incurs a sensing/communication/computation cost. In these
settings, the agent's interaction with its environment includes the additional
component of deciding $\textit{when}$ to sense the state, in a manner that
balances the value associated with optimal (state-specific) actions and the
cost of sensing. We formulate this as an expected discounted cost Markov
Decision Process (MDP), wherein the agent incurs an additional cost for sensing
its next state, but has the option to take actions while remaining 'blind' to
the system state.
  We pose this problem as a classical discounted cost MDP with an expanded
(countably infinite) state space. While computing the optimal policy for this
MDP is intractable in general, we bound the sub-optimality gap associated with
optimal policies in a restricted class, where the number of consecutive
non-sensing (a.k.a., blind) actions is capped. We also design a computationally
efficient heuristic algorithm based on policy improvement, which in practice
performs close to the optimal policy. Finally, we benchmark against the state
of the art via a numerical case study.

</details>


### [64] [Physics-inspired Energy Transition Neural Network for Sequence Learning](https://arxiv.org/abs/2505.03281)
*Zhou Wu,Junyi An,Baile Xu,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种名为PETNN的新型循环神经网络结构，受物理能量转换模型启发，旨在解决Transformer在长序列任务中的不足。实验证明PETNN在多种序列任务中优于Transformer，且复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在序列建模中表现优越，但其长依赖捕捉能力主要源于复杂的配对建模而非固有的序列语义偏置。因此，研究者探索纯RNN的潜力，并重新评估其长时学习机制。

Method: 受物理能量转换模型的启发，提出PETNN，通过模拟能量变化来设计其记忆机制，以高效存储长时依赖信息。

Result: 实验显示PETNN在多种序列任务中超越Transformer方法，同时因循环特性具备显著更低的复杂度。

Conclusion: PETNN为一种高效的循环架构，展示了纯RNN在当前Transformer主导领域中的潜力。

Abstract: Recently, the superior performance of Transformers has made them a more
robust and scalable solution for sequence modeling than traditional recurrent
neural networks (RNNs). However, the effectiveness of Transformer in capturing
long-term dependencies is primarily attributed to their comprehensive
pair-modeling process rather than inherent inductive biases toward sequence
semantics. In this study, we explore the capabilities of pure RNNs and reassess
their long-term learning mechanisms. Inspired by the physics energy transition
models that track energy changes over time, we propose a effective recurrent
structure called the``Physics-inspired Energy Transition Neural Network"
(PETNN). We demonstrate that PETNN's memory mechanism effectively stores
information over long-term dependencies. Experimental results indicate that
PETNN outperforms transformer-based methods across various sequence tasks.
Furthermore, owing to its recurrent nature, PETNN exhibits significantly lower
complexity. Our study presents an optimal foundational recurrent architecture
and highlights the potential for developing effective recurrent neural networks
in fields currently dominated by Transformer.

</details>


### [65] [Unraveling the Rainbow: can value-based methods schedule?](https://arxiv.org/abs/2505.03323)
*Arthur Corrêa,Alexandre Jesus,Cristóvão Silva,Samuel Moniz*

Main category: cs.LG

TL;DR: 本文通过实证研究，揭示在组合优化问题中，基于价值的深度强化学习方法（如DQN及其扩展）可以媲美甚至优于流行的基于策略的方法（如PPO），挑战了后者在领域内的主导地位。


<details>
  <summary>Details</summary>
Motivation: 尽管基于价值的深度强化学习在Atari游戏等领域表现出色，但组合优化领域主要采用基于策略的方法。作者质疑这种偏好是否合理，旨在评估基于价值方法在复杂组合优化问题中的潜力。

Method: 研究在作业车间调度和灵活作业车间调度两类经典工业问题上，系统对比了深度Q网络（DQN）及其多种改进算法与近端策略优化（PPO）的表现，实验设计严谨。

Result: 实验结果表明，基于价值的方法（如Rainbow DQN）在两项任务中可达到与PPO相当或更优的性能，打破‘基于策略方法必然更优’的传统认知。

Conclusion: 研究表明基于价值的深度强化学习在组合优化领域具有被低估的潜力，呼吁学界重新审视其价值并投入更多研究关注。代码已开源以促进复现。

Abstract: Recently, deep reinforcement learning has emerged as a promising approach for
solving complex combinatorial optimization problems. Broadly, deep
reinforcement learning methods fall into two categories: policy-based and
value-based. While value-based approaches have achieved notable success in
domains such as the Arcade Learning Environment, the combinatorial optimization
community has predominantly favored policy-based methods, often overlooking the
potential of value-based algorithms. In this work, we conduct a comprehensive
empirical evaluation of value-based algorithms, including the deep q-network
and several of its advanced extensions, within the context of two complex
combinatorial problems: the job-shop and the flexible job-shop scheduling
problems, two fundamental challenges with multiple industrial applications. Our
results challenge the assumption that policy-based methods are inherently
superior for combinatorial optimization. We show that several value-based
approaches can match or even outperform the widely adopted proximal policy
optimization algorithm, suggesting that value-based strategies deserve greater
attention from the combinatorial optimization community. Our code is openly
available at: https://github.com/AJ-Correa/Unraveling-the-Rainbow.

</details>


### [66] [Absolute Zero: Reinforced Self-play Reasoning with Zero Data](https://arxiv.org/abs/2505.03335)
*Andrew Zhao,Yiran Wu,Yang Yue,Tong Wu,Quentin Xu,Yang Yue,Matthieu Lin,Shenzhi Wang,Qingyun Wu,Zilong Zheng,Gao Huang*

Main category: cs.LG

TL;DR: 提出了一种名为Absolute Zero的新RLVR范式，通过自我生成任务并验证奖励，无需外部数据，实现了在数学和编码推理任务上的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法依赖人工标注数据，难以扩展且在AI超越人类智能时潜力有限。Absolute Zero通过自我驱动的任务生成和验证，解决了这些问题。

Method: 引入Absolute Zero Reasoner（AZR），利用代码执行器自我生成任务并验证答案，实现无外部数据的开放学习。

Result: AZR在数学和编码推理任务上表现优于依赖大量人工数据的现有方法，且适用于不同模型规模和类型。

Conclusion: Absolute Zero为RLVR提供了一种无需外部数据的自我驱动学习框架，扩展了其在超人类智能场景下的适用性。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown promise in
enhancing the reasoning capabilities of large language models by learning
directly from outcome-based rewards. Recent RLVR works that operate under the
zero setting avoid supervision in labeling the reasoning process, but still
depend on manually curated collections of questions and answers for training.
The scarcity of high-quality, human-produced examples raises concerns about the
long-term scalability of relying on human supervision, a challenge already
evident in the domain of language model pretraining. Furthermore, in a
hypothetical future where AI surpasses human intelligence, tasks provided by
humans may offer limited learning potential for a superintelligent system. To
address these concerns, we propose a new RLVR paradigm called Absolute Zero, in
which a single model learns to propose tasks that maximize its own learning
progress and improves reasoning by solving them, without relying on any
external data. Under this paradigm, we introduce the Absolute Zero Reasoner
(AZR), a system that self-evolves its training curriculum and reasoning ability
by using a code executor to both validate proposed code reasoning tasks and
verify answers, serving as an unified source of verifiable reward to guide
open-ended yet grounded learning. Despite being trained entirely without
external data, AZR achieves overall SOTA performance on coding and mathematical
reasoning tasks, outperforming existing zero-setting models that rely on tens
of thousands of in-domain human-curated examples. Furthermore, we demonstrate
that AZR can be effectively applied across different model scales and is
compatible with various model classes.

</details>


### [67] [Geospatial Mechanistic Interpretability of Large Language Models](https://arxiv.org/abs/2505.03368)
*Stef De Sabbata,Stefano Mizzaro,Kevin Roitero*

Main category: cs.LG

TL;DR: 该论文提出了一种新框架，用于研究大型语言模型（LLMs）如何处理地理信息，通过空间分析揭示其内部机制。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自然语言处理任务中表现出色，但其处理地理信息的内部机制尚不明确，论文旨在填补这一研究空白。

Method: 作者结合探测技术和机械可解释性方法（如稀疏自编码器），通过空间自相关性分析揭示LLMs内部的地理信息表征。

Result: 实验表明，地名特征的空间模式与地理位置相关，揭示了LLMs处理地理信息的机制。

Conclusion: 该框架为地理学中基础模型的研究和应用提供了新视角，促进了对其内部地理信息处理机制的理解。

Abstract: Large Language Models (LLMs) have demonstrated unprecedented capabilities
across various natural language processing tasks. Their ability to process and
generate viable text and code has made them ubiquitous in many fields, while
their deployment as knowledge bases and "reasoning" tools remains an area of
ongoing research. In geography, a growing body of literature has been focusing
on evaluating LLMs' geographical knowledge and their ability to perform spatial
reasoning. However, very little is still known about the internal functioning
of these models, especially about how they process geographical information.
  In this chapter, we establish a novel framework for the study of geospatial
mechanistic interpretability - using spatial analysis to reverse engineer how
LLMs handle geographical information. Our aim is to advance our understanding
of the internal representations that these complex models generate while
processing geographical information - what one might call "how LLMs think about
geographic information" if such phrasing was not an undue anthropomorphism.
  We first outline the use of probing in revealing internal structures within
LLMs. We then introduce the field of mechanistic interpretability, discussing
the superposition hypothesis and the role of sparse autoencoders in
disentangling polysemantic internal representations of LLMs into more
interpretable, monosemantic features. In our experiments, we use spatial
autocorrelation to show how features obtained for placenames display spatial
patterns related to their geographic location and can thus be interpreted
geospatially, providing insights into how these models process geographical
information. We conclude by discussing how our framework can help shape the
study and use of foundation models in geography.

</details>


### [68] [SPAP: Structured Pruning via Alternating Optimization and Penalty Methods](https://arxiv.org/abs/2505.03373)
*Hanyu Hu,Xiaoming Yuan*

Main category: cs.LG

TL;DR: SPAP提出了一种基于优化理论的结构化剪枝框架，通过交替优化和惩罚方法有效减少LLMs的计算和内存需求，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有结构化剪枝方法存在性能下降、依赖启发式指标或微调成本高的问题，本文旨在提出一种高效、优化驱动的解决方案。

Method: SPAP采用混合整数优化模型，结合惩罚方法减少剪枝误差，并设计交替最小化算法优化权重更新和性能恢复。

Result: 在多个LLM模型上的实验显示，SPAP在30%稀疏度下实现1.29倍推理加速和内存线性减少，优于现有方法。

Conclusion: SPAP为LLM剪枝提供了实用的优化驱动方法，显著提升了效率且不牺牲性能。

Abstract: The deployment of large language models (LLMs) is often constrained by their
substantial computational and memory demands. While structured pruning presents
a viable approach by eliminating entire network components, existing methods
suffer from performance degradation, reliance on heuristic metrics, or
expensive finetuning. To address these challenges, we propose SPAP (Structured
Pruning via Alternating Optimization and Penalty Methods), a novel and
efficient structured pruning framework for LLMs grounded in optimization
theory. SPAP formulates the pruning problem through a mixed-integer
optimization model, employs a penalty method that effectively makes pruning
decisions to minimize pruning errors, and introduces an alternating
minimization algorithm tailored to the splittable problem structure for
efficient weight updates and performance recovery. Extensive experiments on
OPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over
state-of-the-art methods, delivering linear inference speedups (1.29$\times$ at
30% sparsity) and proportional memory reductions. Our work offers a practical,
optimization-driven solution for pruning LLMs while preserving model
performance.

</details>


### [69] [Physics-informed neural network estimation of active material properties in time-dependent cardiac biomechanical models](https://arxiv.org/abs/2505.03382)
*Matthias Höfler,Francesco Regazzoni,Stefano Pagani,Elias Karabelas,Christoph Augustin,Gundolf Haase,Gernot Plank,Federica Caforio*

Main category: cs.LG

TL;DR: 该研究利用物理信息神经网络（PINNs）从医学影像数据中推断心脏生物力学模型中的主动收缩参数，通过优化网络参数实现了高空间分辨率的主动应力场重建。


<details>
  <summary>Details</summary>
Motivation: 现有临床环境中难以准确评估心肌主动应力参数，尤其是仅依靠医学影像的位移和应变数据时。研究旨在从这些数据中推断参数，从而更好地理解心肌功能。

Method: 通过参数化状态和参数场为两个神经网络，并构建能量最小化问题优化网络参数，同时结合自适应权重、正则化策略和傅里叶特征等改进传统PINN算法。

Result: 在各种设置下成功重建了带噪声的主动应力场，并分析了权重对参数重建的影响，进一步应用于心肌组织异质性表征和纤维化瘢痕检测。

Conclusion: 该方法为心脏纤维化相关疾病的诊断和治疗规划提供了新途径。

Abstract: Active stress models in cardiac biomechanics account for the mechanical
deformation caused by muscle activity, thus providing a link between the
electrophysiological and mechanical properties of the tissue. The accurate
assessment of active stress parameters is fundamental for a precise
understanding of myocardial function but remains difficult to achieve in a
clinical setting, especially when only displacement and strain data from
medical imaging modalities are available. This work investigates, through an
in-silico study, the application of physics-informed neural networks (PINNs)
for inferring active contractility parameters in time-dependent cardiac
biomechanical models from these types of imaging data. In particular, by
parametrising the sought state and parameter field with two neural networks,
respectively, and formulating an energy minimisation problem to search for the
optimal network parameters, we are able to reconstruct in various settings
active stress fields in the presence of noise and with a high spatial
resolution. To this end, we also advance the vanilla PINN learning algorithm
with the use of adaptive weighting schemes, ad-hoc regularisation strategies,
Fourier features, and suitable network architectures. In addition, we
thoroughly analyse the influence of the loss weights in the reconstruction of
active stress parameters. Finally, we apply the method to the characterisation
of tissue inhomogeneities and detection of fibrotic scars in myocardial tissue.
This approach opens a new pathway to significantly improve the diagnosis,
treatment planning, and management of heart conditions associated with cardiac
fibrosis.

</details>


### [70] [Improving Omics-Based Classification: The Role of Feature Selection and Synthetic Data Generation](https://arxiv.org/abs/2505.03387)
*Diego Perazzolo,Pietro Fanton,Ilaria Barison,Marny Fedrigo,Annalisa Angelini,Chiara Castellani,Enrico Grisan*

Main category: cs.LG

TL;DR: 该论文提出了一种结合特征选择与数据增强的机器学习分类框架，旨在提升小样本高维组学数据的分类性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 面对高维组学数据样本量小、模型可解释性差的挑战，研究旨在平衡分类准确性与特征选择，通过数据增强提升模型泛化能力。

Method: 采用基于机器学习的分类框架，集成特征选择与数据增强技术，并在公开数据集（E-MTAB 8026）上通过六种二分类场景进行自举分析验证。

Result: 实验表明，该框架在小样本数据上具有稳定的交叉验证性能，且训练后的分类器在更大的测试集上保持性能一致性。

Conclusion: 研究强调了准确性与特征选择的平衡，证明合成数据的引入能有效提升模型在极有限样本下的泛化能力。

Abstract: Given the increasing complexity of omics datasets, a key challenge is not
only improving classification performance but also enhancing the transparency
and reliability of model decisions. Effective model performance and feature
selection are fundamental for explainability and reliability. In many cases,
high dimensional omics datasets suffer from limited number of samples due to
clinical constraints, patient conditions, phenotypes rarity and others
conditions. Current omics based classification models often suffer from narrow
interpretability, making it difficult to discern meaningful insights where
trust and reproducibility are critical. This study presents a machine learning
based classification framework that integrates feature selection with data
augmentation techniques to achieve high standard classification accuracy while
ensuring better interpretability. Using the publicly available dataset (E MTAB
8026), we explore a bootstrap analysis in six binary classification scenarios
to evaluate the proposed model's behaviour. We show that the proposed pipeline
yields cross validated perfomance on small dataset that is conserved when the
trained classifier is applied to a larger test set. Our findings emphasize the
fundamental balance between accuracy and feature selection, highlighting the
positive effect of introducing synthetic data for better generalization, even
in scenarios with very limited samples availability.

</details>


### [71] [Concept Factorization via Self-Representation and Adaptive Graph Structure Learning](https://arxiv.org/abs/2505.03390)
*Zhengqin Yang,Di Wu,Jia Chen,Xin Luo*

Main category: cs.LG

TL;DR: 提出了一种基于自表示和自适应图结构学习的概念分解模型（CFSRAG），通过动态学习数据的几何结构来提升聚类性能，并在实验中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统概念分解模型的聚类性能高度依赖初始图结构构建，因此需要一种能自适应学习图结构的方法来提升准确性和适应性。

Method: CFSRAG结合自表示方法学习数据间的相似性关系，并利用学到的相似矩阵实现动态图正则化约束，从而动态学习数据的内部几何结构。

Result: 在四个真实数据集上的实验表明，CFSRAG模型性能优于其他最先进的模型。

Conclusion: CFSRAG通过自适应图结构学习有效提升了聚类性能，为数据挖掘领域提供了一种更优的解决方案。

Abstract: Concept Factorization (CF) models have attracted widespread attention due to
their excellent performance in data clustering. In recent years, many variant
models based on CF have achieved great success in clustering by taking into
account the internal geometric manifold structure of the dataset and using
graph regularization techniques. However, their clustering performance depends
greatly on the construction of the initial graph structure. In order to enable
adaptive learning of the graph structure of the data, we propose a Concept
Factorization Based on Self-Representation and Adaptive Graph Structure
Learning (CFSRAG) Model. CFSRAG learns the affinity relationship between data
through a self-representation method, and uses the learned affinity matrix to
implement dynamic graph regularization constraints, thereby ensuring dynamic
learning of the internal geometric structure of the data. Finally, we give the
CFSRAG update rule and convergence analysis, and conduct comparative
experiments on four real datasets. The results show that our model outperforms
other state-of-the-art models.

</details>


### [72] [Automatic Calibration for Membership Inference Attack on Large Language Models](https://arxiv.org/abs/2505.03392)
*Saleh Zare Zade,Yao Qiang,Xiangyu Zhou,Hui Zhu,Mohammad Amin Roshani,Prashant Khanduri,Dongxiao Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种名为ACMIA的新框架，通过可调温度校准输出来改善大型语言模型（LLMs）的成员推断攻击（MIAs）准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有成员推断攻击方法存在高误报率或依赖额外参考模型的问题，限制了其实际应用。

Method: ACMIA利用可调温度校准输出概率，并通过最大似然估计的理论洞察设计三种配置以适应不同模型访问级别。

Result: 实验表明，ACMIA在多种开源LLMs上表现优于现有方法，具有高效性、鲁棒性和普适性。

Conclusion: ACMIA显著提升了成员推断的可靠性和鲁棒性，为LLMs的隐私保护提供了更强大的评估工具。

Abstract: Membership Inference Attacks (MIAs) have recently been employed to determine
whether a specific text was part of the pre-training data of Large Language
Models (LLMs). However, existing methods often misinfer non-members as members,
leading to a high false positive rate, or depend on additional reference models
for probability calibration, which limits their practicality. To overcome these
challenges, we introduce a novel framework called Automatic Calibration
Membership Inference Attack (ACMIA), which utilizes a tunable temperature to
calibrate output probabilities effectively. This approach is inspired by our
theoretical insights into maximum likelihood estimation during the pre-training
of LLMs. We introduce ACMIA in three configurations designed to accommodate
different levels of model access and increase the probability gap between
members and non-members, improving the reliability and robustness of membership
inference. Extensive experiments on various open-source LLMs demonstrate that
our proposed attack is highly effective, robust, and generalizable, surpassing
state-of-the-art baselines across three widely used benchmarks. Our code is
available at:
\href{https://github.com/Salehzz/ACMIA}{\textcolor{blue}{Github}}.

</details>


### [73] [Prediction Models That Learn to Avoid Missing Values](https://arxiv.org/abs/2505.03393)
*Lena Stempfle,Anton Matsson,Newton Mwai,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 论文提出了一种缺失避免（MA）机器学习框架，通过特定正则化项减少模型对缺失值的依赖，同时保持预测性能和高可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理缺失值时往往引入偏差或复杂模型结构，牺牲可解释性。本文旨在开发一种既能减少对缺失值的依赖，又不影响预测性能的方法。

Method: 提出MA框架，通过分类器特定的正则化项训练模型（如决策树、树集成、稀疏线性模型），利用上下文缺失性降低对缺失值的依赖。

Result: 实验表明MA-DT、MA-LASSO、MA-RF和MA-GBT能有效减少对缺失值的依赖，且预测性能与原模型相当。

Conclusion: MA框架为处理测试时缺失值提供了一种高效且可解释的解决方案。

Abstract: Handling missing values at test time is challenging for machine learning
models, especially when aiming for both high accuracy and interpretability.
Established approaches often add bias through imputation or excessive model
complexity via missingness indicators. Moreover, either method can obscure
interpretability, making it harder to understand how the model utilizes the
observed variables in predictions. We propose missingness-avoiding (MA) machine
learning, a general framework for training models to rarely require the values
of missing (or imputed) features at test time. We create tailored MA learning
algorithms for decision trees, tree ensembles, and sparse linear models by
incorporating classifier-specific regularization terms in their learning
objectives. The tree-based models leverage contextual missingness by reducing
reliance on missing values based on the observed context. Experiments on
real-world datasets demonstrate that MA-DT, MA-LASSO, MA-RF, and MA-GBT
effectively reduce the reliance on features with missing values while
maintaining predictive performance competitive with their unregularized
counterparts. This shows that our framework gives practitioners a powerful tool
to maintain interpretability in predictions with test-time missing values.

</details>


### [74] [Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey](https://arxiv.org/abs/2505.03418)
*Da Zheng,Lun Du,Junwei Su,Yuchen Tian,Yuqi Zhu,Jintian Zhang,Lanning Wei,Ningyu Zhang,Huajun Chen*

Main category: cs.LG

TL;DR: 本文综述了大语言模型（LLMs）在复杂问题解决中的能力和限制，探讨了链式思考推理（CoT）、知识增强及多种验证技术，并讨论了当前LLM解决方案的基本局限性和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的进步，LLMs已成为解决跨领域复杂问题的强大工具，但其在现实问题中的应用仍面临多步推理、领域知识整合和结果验证等挑战。

Method: 本文通过分析链式思考推理（CoT）、知识增强及多种LLM和工具验证技术，探讨了LLMs在复杂问题解决中的方法。

Result: 研究发现，尽管LLMs在多步推理和知识整合方面取得了一定进展，但仍需进一步改进以应对实际挑战，尤其是在软件工程、数学推理及科学研究等领域。

Conclusion: LLMs在复杂问题解决领域潜力巨大，但需突破多步推理、领域知识整合和结果验证等技术瓶颈，未来应朝这些方向发展。

Abstract: Problem-solving has been a fundamental driver of human progress in numerous
domains. With advancements in artificial intelligence, Large Language Models
(LLMs) have emerged as powerful tools capable of tackling complex problems
across diverse domains. Unlike traditional computational systems, LLMs combine
raw computational power with an approximation of human reasoning, allowing them
to generate solutions, make inferences, and even leverage external
computational tools. However, applying LLMs to real-world problem-solving
presents significant challenges, including multi-step reasoning, domain
knowledge integration, and result verification. This survey explores the
capabilities and limitations of LLMs in complex problem-solving, examining
techniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,
and various LLM-based and tool-based verification techniques. Additionally, we
highlight domain-specific challenges in various domains, such as software
engineering, mathematical reasoning and proving, data analysis and modeling,
and scientific research. The paper further discusses the fundamental
limitations of the current LLM solutions and the future directions of LLM-based
complex problems solving from the perspective of multi-step reasoning, domain
knowledge integration and result verification.

</details>


### [75] [Framework GNN-AID: Graph Neural Network Analysis Interpretation and Defense](https://arxiv.org/abs/2505.03424)
*Kirill Lukyanov,Mikhail Drobyshevskiy,Georgii Sazonov,Mikhail Soloviov,Ilya Makarov*

Main category: cs.LG

TL;DR: GNN-AID is an open-source Python framework for analyzing and defending Graph Neural Networks (GNNs), integrating interpretability and robustness for graph data.


<details>
  <summary>Details</summary>
Motivation: The growing need for Trusted AI (TAI) highlights gaps in existing tools that overlook graph data and fail to combine interpretability and robustness in a single solution.

Method: GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models, customizable interfaces, and a web UI for visualization and no-code experimentation. It supports MLOps for reproducibility.

Result: The framework provides tools for developers and researchers to analyze, defend, and interpret GNNs, while revealing conflicts between defense strategies against evasion and poisoning attacks.

Conclusion: GNN-AID bridges the gap in Trusted AI for graph data, offering a flexible and efficient solution for GNN analysis, defense, and interpretability.

Abstract: The growing need for Trusted AI (TAI) highlights the importance of
interpretability and robustness in machine learning models. However, many
existing tools overlook graph data and rarely combine these two aspects into a
single solution. Graph Neural Networks (GNNs) have become a popular approach,
achieving top results across various tasks. We introduce GNN-AID (Graph Neural
Network Analysis, Interpretation, and Defense), an open-source framework
designed for graph data to address this gap. Built as a Python library, GNN-AID
supports advanced trust methods and architectural layers, allowing users to
analyze graph datasets and GNN behavior using attacks, defenses, and
interpretability methods.
  GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models,
and support for any GNNs through customizable interfaces. It also includes a
web interface with tools for graph visualization and no-code features like an
interactive model builder, simplifying the exploration and analysis of GNNs.
The framework also supports MLOps techniques, ensuring reproducibility and
result versioning to track and revisit analyses efficiently.
  GNN-AID is a flexible tool for developers and researchers. It helps
developers create, analyze, and customize graph models, while also providing
access to prebuilt datasets and models for quick experimentation. Researchers
can use the framework to explore advanced topics on the relationship between
interpretability and robustness, test defense strategies, and combine methods
to protect against different types of attacks.
  We also show how defenses against evasion and poisoning attacks can conflict
when applied to graph data, highlighting the complex connections between
defense strategies.
  GNN-AID is available at
\href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}

</details>


### [76] [Wasserstein Convergence of Score-based Generative Models under Semiconvexity and Discontinuous Gradients](https://arxiv.org/abs/2505.03432)
*Stefano Bruno,Sotirios Sabanis*

Main category: cs.LG

TL;DR: 该论文提出了针对半凸分布的基于评分的生成模型（SGMs）的非渐近Wasserstein-2收敛保证，突破了传统分析中对数据分布的强正则性假设。


<details>
  <summary>Details</summary>
Motivation: 现有Wasserstein-2收敛分析通常假设数据分布具有强正则性（如平滑性或严格对数凹性），而这在实际中很少满足。本文旨在填补SGMs在非平滑、复杂数据场景下的理论空白。

Method: 通过利用半凸性（无需潜在函数的可微性假设），提出了一种框架，适用于包括对称修正半正态分布、高斯混合、双阱势等广泛的实际相关分布。

Result: 实现了在数据维度$d$上的最优依赖关系$O(\sqrt{d})$和阶数一的收敛速率，显著扩展了SGMs的理论基础。

Conclusion: 本文的工作极大地拓展了SGMs的理论基础，弥合了非平滑、复杂数据场景下实证成功与严格理论保证之间的差距。

Abstract: Score-based Generative Models (SGMs) approximate a data distribution by
perturbing it with Gaussian noise and subsequently denoising it via a learned
reverse diffusion process. These models excel at modeling complex data
distributions and generating diverse samples, achieving state-of-the-art
performance across domains such as computer vision, audio generation,
reinforcement learning, and computational biology. Despite their empirical
success, existing Wasserstein-2 convergence analysis typically assume strong
regularity conditions-such as smoothness or strict log-concavity of the data
distribution-that are rarely satisfied in practice. In this work, we establish
the first non-asymptotic Wasserstein-2 convergence guarantees for SGMs
targeting semiconvex distributions with potentially discontinuous gradients.
Our upper bounds are explicit and sharp in key parameters, achieving optimal
dependence of $O(\sqrt{d})$ on the data dimension $d$ and convergence rate of
order one. The framework accommodates a wide class of practically relevant
distributions, including symmetric modified half-normal distributions, Gaussian
mixtures, double-well potentials, and elastic net potentials. By leveraging
semiconvexity without requiring smoothness assumptions on the potential such as
differentiability, our results substantially broaden the theoretical
foundations of SGMs, bridging the gap between empirical success and rigorous
guarantees in non-smooth, complex data regimes.

</details>


### [77] [A new membership inference attack that spots memorization in generative and predictive models: Loss-Based with Reference Model algorithm (LBRM)](https://arxiv.org/abs/2505.03490)
*Faiz Taleb,Ivan Gazeau,Maryline Laurent*

Main category: cs.LG

TL;DR: 论文提出LBRM算法，通过参考模型提高时间序列插值模型中训练数据记忆现象的检测精度，成员推断攻击的AUROC提升40%-60%。


<details>
  <summary>Details</summary>
Motivation: 生成模型可能无意中记忆训练数据，带来隐私风险，尤其针对时间序列插值模型中的记忆现象进行研究。

Method: 提出基于参考模型的LBRM算法，通过成员推断攻击区分训练和测试数据，优化记忆数据的提取与识别。

Result: 未经微调时AUROC提升约40%，微调后提升约60%，并在两种时间序列插值架构中验证了方法的鲁棒性。

Conclusion: LBRM方法显著提升了检测精度，有效应对时间序列插值模型中的隐私风险。

Abstract: Generative models can unintentionally memorize training data, posing
significant privacy risks. This paper addresses the memorization phenomenon in
time series imputation models, introducing the Loss-Based with Reference Model
(LBRM) algorithm. The LBRM method leverages a reference model to enhance the
accuracy of membership inference attacks, distinguishing between training and
test data. Our contributions are twofold: first, we propose an innovative
method to effectively extract and identify memorized training data,
significantly improving detection accuracy. On average, without fine-tuning,
the AUROC improved by approximately 40\%. With fine-tuning, the AUROC increased
by approximately 60\%. Second, we validate our approach through membership
inference attacks on two types of architectures designed for time series
imputation, demonstrating the robustness and versatility of the LBRM approach
in different contexts. These results highlight the significant enhancement in
detection accuracy provided by the LBRM approach, addressing privacy risks in
time series imputation models.

</details>


### [78] [AnomalyMatch: Discovering Rare Objects of Interest with Semi-supervised and Active Learning](https://arxiv.org/abs/2505.03509)
*Pablo Gómez,David O'Ryan*

Main category: cs.LG

TL;DR: 该论文提出了一种名为AnomalyMatch的异常检测框架，结合半监督FixMatch算法与主动学习，适用于标签稀缺的大规模数据集（如天文和自然图像）。该方法在GalaxyMNIST和miniImageNet数据集上表现优异，通过少量初始标注和主动学习循环达到高精度（AUROC 0.95/0.86）。


<details>
  <summary>Details</summary>
Motivation: 解决大规模数据集中异常检测的标签稀缺问题，传统监督方法需要大量标注，实际应用中不现实。

Method: 结合半监督FixMatch算法（基于EfficientNet分类器）与主动学习，通过专家验证迭代优化模型，将异常检测转化为半监督二分类问题。

Result: 在miniImageNet（1%异常）和GalaxyMNIST上，初始5-10个标注样本经3轮主动学习后，AUROC分别达0.95和0.86；最高排名的1%图像中异常识别精度达71%-93%。

Conclusion: AnomalyMatch在大规模应用中高效且可扩展（如单GPU处理1亿图像仅需3天），尤其适用于标签稀缺领域，已在欧洲航天局数据平台部署。

Abstract: Anomaly detection in large datasets is essential in fields such as astronomy
and computer vision; however, supervised methods typically require extensive
anomaly labelling, which is often impractical. We present AnomalyMatch, an
anomaly detection framework combining the semi-supervised FixMatch algorithm
using EfficientNet classifiers with active learning. By treating anomaly
detection as a semi-supervised binary classification problem, we efficiently
utilise limited labelled and abundant unlabelled images. We allow iterative
model refinement in a user interface for expert verification of high-confidence
anomalies and correction of false positives. Built for astronomical data,
AnomalyMatch generalises readily to other domains facing similar data
challenges. Evaluations on the GalaxyMNIST astronomical dataset and the
miniImageNet natural-image benchmark under severe class imbalance (1% anomalies
for miniImageNet) display strong performance: starting from five to ten
labelled anomalies and after three active learning cycles, we achieve an
average AUROC of 0.95 (miniImageNet) and 0.86 (GalaxyMNIST), with respective
AUPRC of 0.77 and 0.71. After active learning cycles, anomalies are ranked with
71% (miniImageNet) to 93% precision in the 1% of the highest-ranked images.
AnomalyMatch is tailored for large-scale applications, efficiently processing
predictions for 100 million images within three days on a single GPU.
Integrated into ESAs Datalabs platform, AnomalyMatch facilitates targeted
discovery of scientifically valuable anomalies in vast astronomical datasets.
Our results underscore the exceptional utility and scalability of this approach
for anomaly discovery, highlighting the value of specialised approaches for
domains characterised by severe label scarcity.

</details>


### [79] [Uncovering the Limitations of Model Inversion Evaluation: Benchmarks and Connection to Type-I Adversarial Attacks](https://arxiv.org/abs/2505.03519)
*Sy-Tuyen Ho,Koh Jun Hao,Ngoc-Bao Nguyen,Alexander Binder,Ngai-Man Cheung*

Main category: cs.LG

TL;DR: 该论文深入研究了模型反转（MI）攻击的评估框架，发现其存在大量假阳性问题，导致之前报告的MI攻击成功率被高估。


<details>
  <summary>Details</summary>
Motivation: 现有MI攻击/防御的评估框架存在假阳性问题，可能导致对隐私泄漏程度的误解，因此需要深入分析其准确性和局限性。

Method: 构建首个全面的MI攻击样本人工标注数据集，分析评估框架的假阳性问题，并通过控制实验揭示类型I对抗特征的影响。

Result: 发现评估框架的假阳性问题显著，实际隐私泄漏程度低于以往报告，且类型I对抗特征与对抗可转移性存在关联。

Conclusion: 提出改进评估框架以减少假阳性，并建议将人工评估作为主要方法，同时呼吁开发更鲁棒的自动评估框架。

Abstract: Model Inversion (MI) attacks aim to reconstruct information of private
training data by exploiting access to machine learning models. The most common
evaluation framework for MI attacks/defenses relies on an evaluation model that
has been utilized to assess progress across almost all MI attacks and defenses
proposed in recent years. In this paper, for the first time, we present an
in-depth study of MI evaluation. Firstly, we construct the first comprehensive
human-annotated dataset of MI attack samples, based on 28 setups of different
MI attacks, defenses, private and public datasets. Secondly, using our dataset,
we examine the accuracy of the MI evaluation framework and reveal that it
suffers from a significant number of false positives. These findings raise
questions about the previously reported success rates of SOTA MI attacks.
Thirdly, we analyze the causes of these false positives, design controlled
experiments, and discover the surprising effect of Type I adversarial features
on MI evaluation, as well as adversarial transferability, highlighting a
relationship between two previously distinct research areas. Our findings
suggest that the performance of SOTA MI attacks has been overestimated, with
the actual privacy leakage being significantly less than previously reported.
In conclusion, we highlight critical limitations in the widely used MI
evaluation framework and present our methods to mitigate false positive rates.
We remark that prior research has shown that Type I adversarial attacks are
very challenging, with no existing solution. Therefore, we urge to consider
human evaluation as a primary MI evaluation framework rather than merely a
supplement as in previous MI research. We also encourage further work on
developing more robust and reliable automatic evaluation frameworks.

</details>


### [80] [Causal Intervention Framework for Variational Auto Encoder Mechanistic Interpretability](https://arxiv.org/abs/2505.03530)
*Dip Roy*

Main category: cs.LG

TL;DR: 这篇论文提出了一个用于变分自编码器(VAE)机制可解释性的因果干预框架，通过多层次的干预技术分析其内部电路功能，并引入量化指标比较不同VAE变体的表现。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的机制可解释性至关重要，但生成模型（如VAE）的可解释性仍具挑战性。本文旨在填补这一空白，为VAEs提供系统性分析工具。

Method: 采用了因果干预框架，包括输入操控、潜空间扰动、激活补丁和因果中介分析等技术，结合合成数据集和标准解纠缠基准进行验证。

Result: 实验表明，该框架能有效识别功能电路，FactorVAE在解纠缠分数（0.084）和因果效应强度（均值4.59）上优于标准VAE和Beta-VAE。

Conclusion: 该框架提升了生成模型的机制可解释性，为设计和优化更透明、可控的VAE架构提供了工具。

Abstract: Mechanistic interpretability of deep learning models has emerged as a crucial
research direction for understanding the functioning of neural networks. While
significant progress has been made in interpreting discriminative models like
transformers, understanding generative models such as Variational Autoencoders
(VAEs) remains challenging. This paper introduces a comprehensive causal
intervention framework for mechanistic interpretability of VAEs. We develop
techniques to identify and analyze "circuit motifs" in VAEs, examining how
semantic factors are encoded, processed, and disentangled through the network
layers. Our approach uses targeted interventions at different levels: input
manipulations, latent space perturbations, activation patching, and causal
mediation analysis. We apply our framework to both synthetic datasets with
known causal relationships and standard disentanglement benchmarks. Results
show that our interventions can successfully isolate functional circuits, map
computational graphs to causal graphs of semantic factors, and distinguish
between polysemantic and monosemantic units. Furthermore, we introduce metrics
for causal effect strength, intervention specificity, and circuit modularity
that quantify the interpretability of VAE components. Experimental results
demonstrate clear differences between VAE variants, with FactorVAE achieving
higher disentanglement scores (0.084) and effect strengths (mean 4.59) compared
to standard VAE (0.064, 3.99) and Beta-VAE (0.051, 3.43). Our framework
advances the mechanistic understanding of generative models and provides tools
for more transparent and controllable VAE architectures.

</details>


### [81] [Small-Scale-Fading-Aware Resource Allocation in Wireless Federated Learning](https://arxiv.org/abs/2505.03533)
*Jiacheng Wang,Le Liang,Hao Ye,Chongtao Guo,Shi Jin*

Main category: cs.LG

TL;DR: 该论文提出了一种基于多智能体强化学习（MARL）的资源分配策略，以优化无线网络中联邦学习（FL）的性能，通过动态调整频谱和功率分配来应对快速信道波动。


<details>
  <summary>Details</summary>
Motivation: 现有的资源分配策略通常忽略FL梯度上传过程中的快速信道波动，导致训练性能下降，因此需要一种更动态的分配方法。

Method: 使用QMIX算法解决部分可观测的马尔可夫决策过程（Dec-POMDP）问题，每个客户端作为智能体动态分配频谱和功率。

Result: 实验表明，该方法在统计异构性不同的情况下显著优于基线方法，并通过消融研究验证了考虑小尺度衰落的重要性。

Conclusion: 该策略通过MARL框架优化资源分配，提升了FL训练性能和实用性。

Abstract: Judicious resource allocation can effectively enhance federated learning (FL)
training performance in wireless networks by addressing both system and
statistical heterogeneity. However, existing strategies typically rely on block
fading assumptions, which overlooks rapid channel fluctuations within each
round of FL gradient uploading, leading to a degradation in FL training
performance. Therefore, this paper proposes a small-scale-fading-aware resource
allocation strategy using a multi-agent reinforcement learning (MARL)
framework. Specifically, we establish a one-step convergence bound of the FL
algorithm and formulate the resource allocation problem as a decentralized
partially observable Markov decision process (Dec-POMDP), which is subsequently
solved using the QMIX algorithm. In our framework, each client serves as an
agent that dynamically determines spectrum and power allocations within each
coherence time slot, based on local observations and a reward derived from the
convergence analysis. The MARL setting reduces the dimensionality of the action
space and facilitates decentralized decision-making, enhancing the scalability
and practicality of the solution. Experimental results demonstrate that our
QMIX-based resource allocation strategy significantly outperforms baseline
methods across various degrees of statistical heterogeneity. Additionally,
ablation studies validate the critical importance of incorporating small-scale
fading dynamics, highlighting its role in optimizing FL performance.

</details>


### [82] [Efficient Training of Physics-enhanced Neural ODEs via Direct Collocation and Nonlinear Programming](https://arxiv.org/abs/2505.03552)
*Linus Langenkamp,Philip Hannebohm,Bernhard Bachmann*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新颖的训练物理增强神经ODE（PeNODEs）的方法，通过将训练过程表达为动态优化问题，提高了稳定性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了改进基于ODE求解器的训练方法在稳定性、运行时间和准确性方面的局限性，同时结合物理约束以提高模型性能。

Method: 采用高阶隐式Runge-Kutta方法和翻转Legendre-Gauss-Radau点离散化模型，生成大规模非线性程序（NLP），并利用Ipopt等NLP求解器高效求解。

Result: 通过Quarter Vehicle Model和Van-der-Pol振荡器的基准测试，展示了该方法在准确性、速度和泛化能力上的优势，且可以使用更小的网络。

Conclusion: 论文提出了一种高效、稳定的PeNODEs训练方法，未来计划整合到OpenModelica中以支持神经DAE的易用训练。

Abstract: We propose a novel approach for training Physics-enhanced Neural ODEs
(PeNODEs) by expressing the training process as a dynamic optimization problem.
The full model, including neural components, is discretized using a high-order
implicit Runge-Kutta method with flipped Legendre-Gauss-Radau points, resulting
in a large-scale nonlinear program (NLP) efficiently solved by state-of-the-art
NLP solvers such as Ipopt. This formulation enables simultaneous optimization
of network parameters and state trajectories, addressing key limitations of ODE
solver-based training in terms of stability, runtime, and accuracy. Extending
on a recent direct collocation-based method for Neural ODEs, we generalize to
PeNODEs, incorporate physical constraints, and present a custom, parallelized,
open-source implementation. Benchmarks on a Quarter Vehicle Model and a
Van-der-Pol oscillator demonstrate superior accuracy, speed, and generalization
with smaller networks compared to other training techniques. We also outline a
planned integration into OpenModelica to enable accessible training of Neural
DAEs.

</details>


### [83] [Rapid AI-based generation of coverage paths for dispensing applications](https://arxiv.org/abs/2505.03560)
*Simon Baeuerle,Ian F. Mendonca,Kristof Van Laerhoven,Ralf Mikut,Andreas Steimer*

Main category: cs.LG

TL;DR: 提出了一种基于AI的新方法，用于生成热界面材料（TIM）的喷涂路径，替代传统手动或高计算量的优化方法。


<details>
  <summary>Details</summary>
Motivation: 目前TIM的喷涂路径规划依赖专家手动或计算密集的优化方法，亟需更高效且自动化的解决方案。

Method: 采用无标签训练的人工神经网络（ANN），直接根据目标冷却区域生成喷涂路径。

Result: 生成的路径可直接用于自动化制造设备，且避免了空气夹带问题，展示了在多目标区域的可行性。

Conclusion: 该方法可实时预测制造工艺参数，并可能推广至其他制造流程。

Abstract: Coverage Path Planning of Thermal Interface Materials (TIM) plays a crucial
role in the design of power electronics and electronic control units. Up to
now, this is done manually by experts or by using optimization approaches with
a high computational effort. We propose a novel AI-based approach to generate
dispense paths for TIM and similar dispensing applications. It is a drop-in
replacement for optimization-based approaches. An Artificial Neural Network
(ANN) receives the target cooling area as input and directly outputs the
dispense path. Our proposed setup does not require labels and we show its
feasibility on multiple target areas. The resulting dispense paths can be
directly transferred to automated manufacturing equipment and do not exhibit
air entrapments. The approach of using an ANN to predict process parameters for
a desired target state in real-time could potentially be transferred to other
manufacturing processes.

</details>


### [84] [Ergodic Generative Flows](https://arxiv.org/abs/2505.03561)
*Leo Maxime Brunswic,Mateo Clemente,Rui Heng Yang,Adam Sigal,Amir Rasouli,Yinchuan Li*

Main category: cs.LG

TL;DR: 本文提出了一类称为遍历生成流(EGFs)的生成流，解决了传统生成流网络(GFNs)在连续环境和模仿学习中的挑战，如流匹配损失的难处理性和对单独奖励模型的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统GFNs在连续环境和模仿学习中面临诸多挑战，如流匹配损失的难处理性和需要单独奖励模型。本文旨在通过EGFs解决这些问题。

Method: 提出了EGFs，利用遍历性构建简单的生成流，具有全局定义的变换和可处理的流匹配损失；并设计了KL-weakFM损失，用于无需单独奖励模型的模仿学习训练。

Result: 在2D玩具任务和NASA真实数据集上评估了IL-EGFs，使用KL-weakFM损失；并在2D强化学习实验中验证了FM损失的有效性。

Conclusion: EGFs通过遍历性和新设计的损失函数，有效解决了GFNs在连续环境和模仿学习中的问题，展现了广泛的适用性。

Abstract: Generative Flow Networks (GFNs) were initially introduced on directed acyclic
graphs to sample from an unnormalized distribution density. Recent works have
extended the theoretical framework for generative methods allowing more
flexibility and enhancing application range. However, many challenges remain in
training GFNs in continuous settings and for imitation learning (IL), including
intractability of flow-matching loss, limited tests of non-acyclic training,
and the need for a separate reward model in imitation learning. The present
work proposes a family of generative flows called Ergodic Generative Flows
(EGFs) which are used to address the aforementioned issues. First, we leverage
ergodicity to build simple generative flows with finitely many globally defined
transformations (diffeomorphisms) with universality guarantees and tractable
flow-matching loss (FM loss). Second, we introduce a new loss involving
cross-entropy coupled to weak flow-matching control, coined KL-weakFM loss. It
is designed for IL training without a separate reward model. We evaluate
IL-EGFs on toy 2D tasks and real-world datasets from NASA on the sphere, using
the KL-weakFM loss. Additionally, we conduct toy 2D reinforcement learning
experiments with a target reward, using the FM loss.

</details>


### [85] [Anant-Net: Breaking the Curse of Dimensionality with Scalable and Interpretable Neural Surrogate for High-Dimensional PDEs](https://arxiv.org/abs/2505.03595)
*Sidharth S. Menon,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: Anant-Net是一种高效的神经代理方法，用于解决高维偏微分方程的维数灾难问题，尤其在超立方体域上表现出高精度和高效性。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在高维PDE中因计算复杂度指数级增长而失效，尤其在超立方体域上，Anant-Net旨在克服这一挑战。

Method: 通过神经代理Anant-Net高效处理高维边界条件并最小化PDE残差，同时引入Kolmogorov-Arnold网络提升可解释性。

Result: 在Poisson、Sine-Gordon和Allen-Cahn等方程中表现出高精度和鲁棒性，单GPU几小时内解决300维问题。

Conclusion: Anant-Net为高维PDE提供了准确、可解释且可扩展的解决方案。

Abstract: High-dimensional partial differential equations (PDEs) arise in diverse
scientific and engineering applications but remain computationally intractable
due to the curse of dimensionality. Traditional numerical methods struggle with
the exponential growth in computational complexity, particularly on hypercubic
domains, where the number of required collocation points increases rapidly with
dimensionality. Here, we introduce Anant-Net, an efficient neural surrogate
that overcomes this challenge, enabling the solution of PDEs in high
dimensions. Unlike hyperspheres, where the internal volume diminishes as
dimensionality increases, hypercubes retain or expand their volume (for unit or
larger length), making high-dimensional computations significantly more
demanding. Anant-Net efficiently incorporates high-dimensional boundary
conditions and minimizes the PDE residual at high-dimensional collocation
points. To enhance interpretability, we integrate Kolmogorov-Arnold networks
into the Anant-Net architecture. We benchmark Anant-Net's performance on
several linear and nonlinear high-dimensional equations, including the Poisson,
Sine-Gordon, and Allen-Cahn equations, demonstrating high accuracy and
robustness across randomly sampled test points from high-dimensional space.
Importantly, Anant-Net achieves these results with remarkable efficiency,
solving 300-dimensional problems on a single GPU within a few hours. We also
compare Anant-Net's results for accuracy and runtime with other
state-of-the-art methods. Our findings establish Anant-Net as an accurate,
interpretable, and scalable framework for efficiently solving high-dimensional
PDEs.

</details>


### [86] [Understand the Effect of Importance Weighting in Deep Learning on Dataset Shift](https://arxiv.org/abs/2505.03617)
*Thien Nhan Vo,Thanh Xuan Truong*

Main category: cs.LG

TL;DR: 研究了重要性加权在深度神经网络中对标签偏移和协变量偏移的有效性，发现其效果随训练时间减弱，复杂数据中效果有限。


<details>
  <summary>Details</summary>
Motivation: 探索重要性加权在实际分布偏移场景中的实用性和局限性。

Method: 在合成2D数据和CIFAR-10上测试逻辑回归和MLP，结合L2正则化和dropout进行比较。

Result: 加权在早期影响明显但后期减弱，仅在L2正则化下表现较好，复杂数据中无显著提升。

Conclusion: 重要性加权在真实世界分布偏移中的实用性存疑。

Abstract: We evaluate the effectiveness of importance weighting in deep neural networks
under label shift and covariate shift. On synthetic 2D data (linearly separable
and moon-shaped) using logistic regression and MLPs, we observe that weighting
strongly affects decision boundaries early in training but fades with prolonged
optimization. On CIFAR-10 with various class imbalances, only L2 regularization
(not dropout) helps preserve weighting effects. In a covariate-shift
experiment, importance weighting yields no significant performance gain,
highlighting challenges on complex data. Our results call into question the
practical utility of importance weighting for real-world distribution shifts.

</details>


### [87] [ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders](https://arxiv.org/abs/2505.03646)
*Chethan Krishnamurthy Ramanaik,Arjun Roy,Eirini Ntoutsi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于层条件对抗优化的新方法，用于增强自编码器的对抗鲁棒性评估，并通过实验验证了其优于现有攻击方法的效果。


<details>
  <summary>Details</summary>
Motivation: 尽管自编码器在关键应用中广泛使用，但其对抗鲁棒性研究相比分类模型仍显不足。现有基于白盒攻击的评估框架未能充分挖掘自编码器中条件不佳的中层脆弱性。

Method: 论文提出了一种新的层条件对抗优化目标，通过增强对抗损失梯度的信息传播，有效引导对抗映射接近局部Lipschitz边界。

Result: 实验表明，该方法在通用和样本特定场景下均优于现有攻击方法。论文还提出了一种推理时对抗训练防御插件，用于缓解对抗样本的影响。

Conclusion: 该方法显著提升了自编码器的对抗鲁棒性评估效果，并为防御对抗攻击提供了新的解决方案。

Abstract: Despite the extensive use of deep autoencoders (AEs) in critical
applications, their adversarial robustness remains relatively underexplored
compared to classification models. AE robustness is characterized by the
Lipschitz bounds of its components. Existing robustness evaluation frameworks
based on white-box attacks do not fully exploit the vulnerabilities of
intermediate ill-conditioned layers in AEs. In the context of optimizing
imperceptible norm-bounded additive perturbations to maximize output damage,
existing methods struggle to effectively propagate adversarial loss gradients
throughout the network, often converging to less effective perturbations. To
address this, we propose a novel layer-conditioning-based adversarial
optimization objective that effectively guides the adversarial map toward
regions of local Lipschitz bounds by enhancing loss gradient information
propagation during attack optimization. We demonstrate through extensive
experiments on state-of-the-art AEs that our adversarial objective results in
stronger attacks, outperforming existing methods in both universal and
sample-specific scenarios. As a defense method against this attack, we
introduce an inference-time adversarially trained defense plugin that mitigates
the effects of adversarial examples.

</details>


### [88] [Mitigating mode collapse in normalizing flows by annealing with an adaptive schedule: Application to parameter estimation](https://arxiv.org/abs/2505.03652)
*Yihang Wang,Chris Chi,Aaron R. Dinner*

Main category: cs.LG

TL;DR: 本文提出了一种基于有效样本大小（ESS）的自适应退火调度方法，用于缓解归一化流（NFs）在多模态分布中的模式塌陷问题，并在生化振荡器模型中展示了其高效性。


<details>
  <summary>Details</summary>
Motivation: 归一化流（NFs）在参数估计中因能提供复杂分布的无关联样本而备受关注，但其在多模态分布中易出现模式塌陷的问题限制了其实际应用。

Method: 通过基于有效样本大小（ESS）的自适应退火调度，缓解模式塌陷问题，并辅以样本修剪以减少方差。

Result: 在生化振荡器模型中，该方法比广泛使用的集成马尔可夫链蒙特卡洛（MCMC）方法快十倍收敛边际似然。

Conclusion: 该方法为NFs采样提供了通用改进方向，并探讨了进一步优化的可能性。

Abstract: Normalizing flows (NFs) provide uncorrelated samples from complex
distributions, making them an appealing tool for parameter estimation. However,
the practical utility of NFs remains limited by their tendency to collapse to a
single mode of a multimodal distribution. In this study, we show that annealing
with an adaptive schedule based on the effective sample size (ESS) can mitigate
mode collapse. We demonstrate that our approach can converge the marginal
likelihood for a biochemical oscillator model fit to time-series data in
ten-fold less computation time than a widely used ensemble Markov chain Monte
Carlo (MCMC) method. We show that the ESS can also be used to reduce variance
by pruning the samples. We expect these developments to be of general use for
sampling with NFs and discuss potential opportunities for further improvements.

</details>


### [89] [Neural Integral Operators for Inverse problems in Spectroscopy](https://arxiv.org/abs/2505.03677)
*Emanuele Zappala,Alice Giola,Andreas Kramer,Enrico Greco*

Main category: cs.LG

TL;DR: 论文提出了一种基于积分方程的深度学习模型，用于小数据集下的分子光谱分类，解决了传统深度学习方法在小数据集上的过拟合问题，性能优于传统机器学习和其他深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 由于光谱数据通常较少，传统深度学习方法容易过拟合，而传统机器学习方法精度有限。本研究旨在开发一种在小数据集上表现优越的深度学习方法。

Method: 通过第一类积分方程学习积分算子，构建了一种对抗过拟合的深度学习模型，结合光谱逆问题的数学框架。

Result: 实验表明，该方法在小数据集上优于决策树、支持向量机和其他深度学习模型，同时保持了深度学习的强大性能。

Conclusion: 该模型成功解决了深度学习方法在小数据集光谱分析中的过拟合问题，展示了其在数据稀缺场景下的优越性和实用性。

Abstract: Deep learning has shown high performance on spectroscopic inverse problems
when sufficient data is available. However, it is often the case that data in
spectroscopy is scarce, and this usually causes severe overfitting problems
with deep learning methods. Traditional machine learning methods are viable
when datasets are smaller, but the accuracy and applicability of these methods
is generally more limited.
  We introduce a deep learning method for classification of molecular spectra
based on learning integral operators via integral equations of the first kind,
which results in an algorithm that is less affected by overfitting issues on
small datasets, compared to other deep learning models.
  The problem formulation of the deep learning approach is based on inverse
problems, which have traditionally found important applications in
spectroscopy. We perform experiments on real world data to showcase our
algorithm. It is seen that the model outperforms traditional machine learning
approaches such as decision tree and support vector machine, and for small
datasets it outperforms other deep learning models. Therefore, our methodology
leverages the power of deep learning, still maintaining the performance when
the available data is very limited, which is one of the main issues that deep
learning faces in spectroscopy, where datasets are often times of small size.

</details>


### [90] [Learning Survival Distributions with the Asymmetric Laplace Distribution](https://arxiv.org/abs/2505.03712)
*Deming Sheng,Ricardo Henao*

Main category: cs.LG

TL;DR: 论文提出了基于非对称拉普拉斯分布（ALD）的参数化生存分析方法，通过最大似然估计学习个体层面的分布参数，在准确性和校准方面优于现有参数和非参数方法。


<details>
  <summary>Details</summary>
Motivation: 现有的非参数生存分析方法通常通过离散化间接估计生存分布，而本研究旨在利用ALD分布的特性直接提供事件时间的关键统计量（如均值、中位数、分位数等）。

Method: 采用ALD分布进行参数化建模，通过最大似然估计学习个体的位置、尺度和不对称性参数，从而直接计算事件的统计量。

Result: 在合成和真实数据上的实验表明，该方法在准确性、区分度和校准方面优于现有参数和非参数方法。

Conclusion: 基于ALD的参数化生存分析方法提供了更高效且准确的生存分析工具，尤其在需要直接获取事件时间统计量的场景中表现优异。

Abstract: Probabilistic survival analysis models seek to estimate the distribution of
the future occurrence (time) of an event given a set of covariates. In recent
years, these models have preferred nonparametric specifications that avoid
directly estimating survival distributions via discretization. Specifically,
they estimate the probability of an individual event at fixed times or the time
of an event at fixed probabilities (quantiles), using supervised learning.
Borrowing ideas from the quantile regression literature, we propose a
parametric survival analysis method based on the Asymmetric Laplace
Distribution (ALD). This distribution allows for closed-form calculation of
popular event summaries such as mean, median, mode, variation, and quantiles.
The model is optimized by maximum likelihood to learn, at the individual level,
the parameters (location, scale, and asymmetry) of the ALD distribution.
Extensive results on synthetic and real-world data demonstrate that the
proposed method outperforms parametric and nonparametric approaches in terms of
accuracy, discrimination and calibration.

</details>


### [91] [Sustainable Smart Farm Networks: Enhancing Resilience and Efficiency with Decision Theory-Guided Deep Reinforcement Learning](https://arxiv.org/abs/2505.03721)
*Dian Chen,Zelin Wan,Dong Sam Ha,Jin-Hee Cho*

Main category: cs.LG

TL;DR: 该研究提出了一种可持续智能农场网络，结合深度强化学习（DRL）、迁移学习（TL）和决策理论（DT），以优化动物监控效果与能源效率，同时抵御网络攻击和适应动态能源供应。实验表明，DT引导的DRL模型性能优于TL增强的DRL，训练时间减少47.5%。


<details>
  <summary>Details</summary>
Motivation: 当前太阳能传感器监控系统在网络安全和动态能源供应方面的适应性研究不足，亟需一种能够应对网络攻击和能源波动的智能农场解决方案。

Method: 采用深度强化学习（DRL）设计优化策略，并结合迁移学习（TL）和决策理论（DT）加速学习过程，以提升监控质量与能源可持续性。

Result: 实验证明，DT引导的DRL模型在性能和训练效率上优于TL增强的DRL，训练时间减少47.5%，且监控效果显著提升。

Conclusion: 该研究为智能农场提供了一种高效、可持续的监控方案，通过DRL与DT的结合，显著提高了系统性能和训练效率。

Abstract: Solar sensor-based monitoring systems have become a crucial agricultural
innovation, advancing farm management and animal welfare through integrating
sensor technology, Internet-of-Things, and edge and cloud computing. However,
the resilience of these systems to cyber-attacks and their adaptability to
dynamic and constrained energy supplies remain largely unexplored. To address
these challenges, we propose a sustainable smart farm network designed to
maintain high-quality animal monitoring under various cyber and adversarial
threats, as well as fluctuating energy conditions. Our approach utilizes deep
reinforcement learning (DRL) to devise optimal policies that maximize both
monitoring effectiveness and energy efficiency. To overcome DRL's inherent
challenge of slow convergence, we integrate transfer learning (TL) and decision
theory (DT) to accelerate the learning process. By incorporating DT-guided
strategies, we optimize monitoring quality and energy sustainability,
significantly reducing training time while achieving comparable performance
rewards. Our experimental results prove that DT-guided DRL outperforms
TL-enhanced DRL models, improving system performance and reducing training
runtime by 47.5%.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [92] [Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach](https://arxiv.org/abs/2505.02952)
*Fabrizio Marozzo*

Main category: cs.AI

TL;DR: 论文提出了一种迭代方法，通过结构化澄清问题和示例逐步解决自然语言指令的模糊性，最终生成精确解决方案，优于传统一次性方法。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统虽革新了人机交互，但自然语言的模糊性常导致不精确的指令，迫使用户反复测试和修正提示。

Method: 采用迭代方法，通过结构化澄清问题、替代方案提议及输入/输出示例，逐步消除模糊性。

Result: 在多样化数据集上验证，该方法准确性更高、解决时间竞争性强、用户满意度优于传统一次性方法。

Conclusion: 结构化迭代方法能有效解决自然语言模糊性，提升生成式AI系统的实用性和用户体验。

Abstract: Generative AI systems have revolutionized human interaction by enabling
natural language-based coding and problem solving. However, the inherent
ambiguity of natural language often leads to imprecise instructions, forcing
users to iteratively test, correct, and resubmit their prompts. We propose an
iterative approach that systematically narrows down these ambiguities through a
structured series of clarification questions and alternative solution
proposals, illustrated with input/output examples as well. Once every
uncertainty is resolved, a final, precise solution is generated. Evaluated on a
diverse dataset spanning coding, data analysis, and creative writing, our
method demonstrates superior accuracy, competitive resolution times, and higher
user satisfaction compared to conventional one-shot solutions, which typically
require multiple manual iterations to achieve a correct output.

</details>


### [93] [The Multimodal Paradox: How Added and Missing Modalities Shape Bias and Performance in Multimodal AI](https://arxiv.org/abs/2505.03020)
*Kishore Sampath,Pratheesh,Ayaazuddin Mohammad,Resmi Ramachandranpillai*

Main category: cs.AI

TL;DR: 多模态学习在提升性能的同时，可能影响公平性；模态缺失会降低性能和公平性，影响模型在实际应用中的稳健性。


<details>
  <summary>Details</summary>
Motivation: 探讨多模态学习中性能提升与公平性之间的关系，以及模态缺失对模型性能和公平性的影响。

Method: 通过实验分析多模态模型的性能和公平性，使用包含图像、时间序列和结构化数据的医疗数据集。

Result: 新模态的加入提升性能，但公平性表现不一致；模态缺失会导致性能和公平性下降。

Conclusion: 多模态学习的实际部署需权衡性能、公平性和稳健性。

Abstract: Multimodal learning, which integrates diverse data sources such as images,
text, and structured data, has proven superior to unimodal counterparts in
high-stakes decision-making. However, while performance gains remain the gold
standard for evaluating multimodal systems, concerns around bias and robustness
are frequently overlooked. In this context, this paper explores two key
research questions (RQs): (i) RQ1 examines whether adding a modality
con-sistently enhances performance and investigates its role in shaping
fairness measures, assessing whether it mitigates or amplifies bias in
multimodal models; (ii) RQ2 investigates the impact of missing modalities at
inference time, analyzing how multimodal models generalize in terms of both
performance and fairness. Our analysis reveals that incorporating new
modalities during training consistently enhances the performance of multimodal
models, while fairness trends exhibit variability across different evaluation
measures and datasets. Additionally, the absence of modalities at inference
degrades performance and fairness, raising concerns about its robustness in
real-world deployment. We conduct extensive experiments using multimodal
healthcare datasets containing images, time series, and structured information
to validate our findings.

</details>


### [94] [Evaluating the Impact of AI-Powered Audiovisual Personalization on Learner Emotion, Focus, and Learning Outcomes](https://arxiv.org/abs/2505.03033)
*George Xi Wang,Jingying Deng,Safinah Ali*

Main category: cs.AI

TL;DR: 论文提出了一种基于LLM的个性化多感官学习环境系统，旨在通过定制视听元素减少分心并提升学习者的情感稳定性。研究结合生物特征和学习表现，评估了该系统的效果。


<details>
  <summary>Details</summary>
Motivation: 现有教育技术多关注内容适配和反馈，忽视了学习的情感与感官环境。学习者常因分心或情感波动影响学习效果，需要更个性化和沉浸式的辅助工具。

Method: 使用混合方法设计，通过LLM生成个性化视听元素（如视觉主题和听觉背景），并结合生物指标和学习表现评估其效果。

Result: 研究发现个性化视听环境能显著降低认知负荷并提升学习专注度，验证了LLM在多感官学习环境中的潜力。

Conclusion: 该系统推动了情感响应教育技术的发展，并为LLM在自主学习感官维度的应用提供了新方向。

Abstract: Independent learners often struggle with sustaining focus and emotional
regulation in unstructured or distracting settings. Although some rely on
ambient aids such as music, ASMR, or visual backgrounds to support
concentration, these tools are rarely integrated into cohesive,
learner-centered systems. Moreover, existing educational technologies focus
primarily on content adaptation and feedback, overlooking the emotional and
sensory context in which learning takes place. Large language models have
demonstrated powerful multimodal capabilities including the ability to generate
and adapt text, audio, and visual content. Educational research has yet to
fully explore their potential in creating personalized audiovisual learning
environments. To address this gap, we introduce an AI-powered system that uses
LLMs to generate personalized multisensory study environments. Users select or
generate customized visual themes (e.g., abstract vs. realistic, static vs.
animated) and auditory elements (e.g., white noise, ambient ASMR, familiar vs.
novel sounds) to create immersive settings aimed at reducing distraction and
enhancing emotional stability. Our primary research question investigates how
combinations of personalized audiovisual elements affect learner cognitive load
and engagement. Using a mixed-methods design that incorporates biometric
measures and performance outcomes, this study evaluates the effectiveness of
LLM-driven sensory personalization. The findings aim to advance emotionally
responsive educational technologies and extend the application of multimodal
LLMs into the sensory dimension of self-directed learning.

</details>


### [95] [BLAB: Brutally Long Audio Bench](https://arxiv.org/abs/2505.03054)
*Orevaoghene Ahia,Martijn Bartelds,Kabir Ahuja,Hila Gonen,Valentin Hofmann,Siddhant Arora,Shuyue Stella Li,Vishal Puttagunta,Mofetoluwa Adeyemi,Charishma Buchireddy,Ben Walls,Noah Bennett,Shinji Watanabe,Noah A. Smith,Yulia Tsvetkov,Sachin Kumar*

Main category: cs.AI

TL;DR: 该论文提出了一个名为BLAB的长音频基准测试，用于评估音频语言模型在长对话任务中的性能，发现现有模型在长音频理解上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 开发能够理解多样化语音交互的大型音频语言模型，以更贴近自然用户交互的多模态需求，并提升语言技术的可访问性。

Method: 引入了BLAB基准测试，包含833+小时的多样化长音频片段（平均51分钟），并配有人工标注的文本问答对，评估了六种开源和专有音频语言模型。

Result: 所有模型（包括Gemini 2.0 Pro和GPT-4o）在BLAB任务中表现不佳，尤其在定位、时间推理、计数等任务上，性能随音频时长增加而下降。

Conclusion: BLAB为开发具有长音频理解能力的音频语言模型提供了挑战性评估框架，揭示了当前模型在长语音任务中的局限性。

Abstract: Developing large audio language models (LMs) capable of understanding diverse
spoken interactions is essential for accommodating the multimodal nature of
human communication and can increase the accessibility of language technologies
across different user populations. Recent work on audio LMs has primarily
evaluated their performance on short audio segments, typically under 30
seconds, with limited exploration of long-form conversational speech segments
that more closely reflect natural user interactions with these models. We
introduce Brutally Long Audio Bench (BLAB), a challenging long-form audio
benchmark that evaluates audio LMs on localization, duration estimation,
emotion, and counting tasks using audio segments averaging 51 minutes in
length. BLAB consists of 833+ hours of diverse, full-length audio clips, each
paired with human-annotated, text-based natural language questions and answers.
Our audio data were collected from permissively licensed sources and underwent
a human-assisted filtering process to ensure task compliance. We evaluate six
open-source and proprietary audio LMs on BLAB and find that all of them,
including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the
tasks in BLAB. Our comprehensive analysis reveals key insights into the
trade-offs between task difficulty and audio duration. In general, we find that
audio LMs struggle with long-form speech, with performance declining as
duration increases. They perform poorly on localization, temporal reasoning,
counting, and struggle to understand non-phonemic information, relying more on
prompts than audio content. BLAB serves as a challenging evaluation framework
to develop audio LMs with robust long-form audio understanding capabilities.

</details>


### [96] [Is AI currently capable of identifying wild oysters? A comparison of human annotators against the AI model, ODYSSEE](https://arxiv.org/abs/2505.03108)
*Brendan Campbell,Alan Williams,Kleio Baxevani,Alyssa Campbell,Rushabh Dhoke,Rileigh E. Hudock,Xiaomin Lin,Vivek Mange,Bernhard Neuberger,Arjun Suresh,Alhim Vera,Arthur Trembanis,Herbert G. Tanner,Edward Hale*

Main category: cs.AI

TL;DR: 论文提出了一种基于深度学习的ODYSSEE模型，用于通过视频或图像识别活牡蛎以评估其数量，但模型准确性（63%）低于专家（74%）和非专家（75%）。图像质量对模型和人工标注的准确性有重要影响。未来需优化模型训练以提高预测能力。


<details>
  <summary>Details</summary>
Motivation: 当前牡蛎礁监测方法依赖破坏性采样和大量人工工作，不适用于小规模或敏感环境。ODYSSEE模型旨在通过非侵入性技术提高监测效率。

Method: 使用深度学习技术开发ODYSSEE模型，通过视频或图像自动识别活牡蛎，并与专家和非专家的标注结果进行对比，分析预测错误来源。

Result: 模型推断速度显著快于人工（39.6秒 vs. 几小时），但活牡蛎识别准确性较低（63% vs. 专家74%、非专家75%）。图像质量对模型和人工准确性均有影响。

Conclusion: 尽管模型准确性不足，未来通过更高质图像训练、增加活体图像和标注类别可提升性能。需进一步研究活牡蛎与死牡蛎的区分方法。

Abstract: Oysters are ecologically and commercially important species that require
frequent monitoring to track population demographics (e.g. abundance, growth,
mortality). Current methods of monitoring oyster reefs often require
destructive sampling methods and extensive manual effort. Therefore, they are
suboptimal for small-scale or sensitive environments. A recent alternative, the
ODYSSEE model, was developed to use deep learning techniques to identify live
oysters using video or images taken in the field of oyster reefs to assess
abundance. The validity of this model in identifying live oysters on a reef was
compared to expert and non-expert annotators. In addition, we identified
potential sources of prediction error. Although the model can make inferences
significantly faster than expert and non-expert annotators (39.6 s, $2.34 \pm
0.61$ h, $4.50 \pm 1.46$ h, respectively), the model overpredicted the number
of live oysters, achieving lower accuracy (63\%) in identifying live oysters
compared to experts (74\%) and non-experts (75\%) alike. Image quality was an
important factor in determining the accuracy of the model and the annotators.
Better quality images improved human accuracy and worsened model accuracy.
Although ODYSSEE was not sufficiently accurate, we anticipate that future
training on higher-quality images, utilizing additional live imagery, and
incorporating additional annotation training classes will greatly improve the
model's predictive power based on the results of this analysis. Future research
should address methods that improve the detection of living vs. dead oysters.

</details>


### [97] [Holmes: Automated Fact Check with Large Language Models](https://arxiv.org/abs/2505.03135)
*Haoran Ou,Gelei Deng,Xingshuo Han,Jie Zhang,Xinlei He,Han Qiu,Shangwei Guo,Tianwei Zhang*

Main category: cs.AI

TL;DR: 论文探讨了利用大型语言模型（LLMs）检测多模态虚假信息的挑战，提出了Holmes框架，通过改进证据检索显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: 互联网虚假信息的传播威胁社会信任与安全，传统深度学习方法难以应对多模态虚假信息的复杂性。

Method: 提出Holmes框架，结合LLM驱动的内容摘要和新型证据质量评估算法，优化证据检索以辅助LLMs验证信息。

Result: Holmes在开放数据集和实时验证任务中分别达到88.3%和90.2%的准确率，证据检索改进使事实核查准确性提升30.8%。

Conclusion: Holmes框架通过高效证据检索显著提升了LLMs在虚假信息检测中的表现，为复杂场景下的信息验证提供了解决方案。

Abstract: The rise of Internet connectivity has accelerated the spread of
disinformation, threatening societal trust, decision-making, and national
security. Disinformation has evolved from simple text to complex multimodal
forms combining images and text, challenging existing detection methods.
Traditional deep learning models struggle to capture the complexity of
multimodal disinformation. Inspired by advances in AI, this study explores
using Large Language Models (LLMs) for automated disinformation detection. The
empirical study shows that (1) LLMs alone cannot reliably assess the
truthfulness of claims; (2) providing relevant evidence significantly improves
their performance; (3) however, LLMs cannot autonomously search for accurate
evidence. To address this, we propose Holmes, an end-to-end framework featuring
a novel evidence retrieval method that assists LLMs in collecting high-quality
evidence. Our approach uses (1) LLM-powered summarization to extract key
information from open sources and (2) a new algorithm and metrics to evaluate
evidence quality. Holmes enables LLMs to verify claims and generate
justifications effectively. Experiments show Holmes achieves 88.3% accuracy on
two open-source datasets and 90.2% in real-time verification tasks. Notably,
our improved evidence retrieval boosts fact-checking accuracy by 30.8% over
existing methods

</details>


### [98] [CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics](https://arxiv.org/abs/2505.03171)
*Junqi Liu,Xiaohan Lin,Jonas Bayer,Yael Dillies,Weijie Jiang,Xiaodan Liang,Roman Soletskyi,Haiming Wang,Yunzhou Xie,Beibei Xiong,Zhengfeng Yang,Jujian Zhang,Lihong Zhi,Jia Li,Zhengying Liu*

Main category: cs.AI

TL;DR: 该论文介绍了CombiBench，一个涵盖100个组合问题的综合基准测试集，旨在弥补组合数学领域缺乏合适基准和定理库的问题，并提出了评估框架Fine-Eval。


<details>
  <summary>Details</summary>
Motivation: 组合数学领域缺乏适当的基准和定理库，导致该领域的自动化推理研究进展缓慢。

Method: 提出了CombiBench基准测试集，并开发了Fine-Eval评估框架，支持形式化数学问题的评估，包括填空式问题。

Result: 测试了多个LLM模型，结果表明它们在形式化解决组合问题方面能力有限（最高解决7/100问题）。

Conclusion: CombiBench和Fine-Eval为组合数学的形式化推理研究提供了重要工具，但现有模型在该领域的表现仍有提升空间。

Abstract: Neurosymbolic approaches integrating large language models with formal
reasoning have recently achieved human-level performance on mathematics
competition problems in algebra, geometry and number theory. In comparison,
combinatorics remains a challenging domain, characterized by a lack of
appropriate benchmarks and theorem libraries. To address this gap, we introduce
CombiBench, a comprehensive benchmark comprising 100 combinatorial problems,
each formalized in Lean~4 and paired with its corresponding informal statement.
The problem set covers a wide spectrum of difficulty levels, ranging from
middle school to IMO and university level, and span over ten combinatorial
topics. CombiBench is suitable for testing IMO solving capabilities since it
includes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its
statement contain an images). Furthermore, we provide a comprehensive and
standardized evaluation framework, dubbed Fine-Eval (for
$\textbf{F}$ill-in-the-blank $\textbf{in}$ L$\textbf{e}$an Evaluation), for
formal mathematics. It accommodates not only proof-based problems but also, for
the first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval
as the evaluation method and Kimina Lean Server as the backend, we benchmark
several LLMs on CombiBench and observe that their capabilities for formally
solving combinatorial problems remain limited. Among all models tested (none of
which has been trained for this particular task), Kimina-Prover attains the
best results, solving 7 problems (out of 100) under both ``with solution'' and
``without solution'' scenarios. We open source the benchmark dataset alongside
with the code of the proposed evaluation method at
https://github.com/MoonshotAI/CombiBench/.

</details>


### [99] [Patterns and Mechanisms of Contrastive Activation Engineering](https://arxiv.org/abs/2505.03189)
*Yixiong Hao,Ayush Panda,Stepan Shabalin,Sheikh Abdur Raheem Ali*

Main category: cs.AI

TL;DR: 对比激活工程（CAE）作为一种零成本、推理时调整大型语言模型输出的方法，虽在分布内有效但存在局限性，如对抗输入脆弱性、困惑度降低等。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的复杂性和不透明性，传统微调方法计算成本高，CAE提供了一种灵活、任务特定的调整方式。

Method: 通过对比激活工程技术，在推理时针对性调整模型内部表示，分析其在分布内外场景的效果及稳定性。

Result: 研究发现CAE仅适用于分布内场景，样本量超过80后收益递减，易受对抗输入影响并降低模型困惑度，但大模型抗性较强。

Conclusion: CAE虽具潜力，但需谨慎部署以应对其局限性，尤其是在分布外和对抗性场景下的表现。

Abstract: Controlling the behavior of Large Language Models (LLMs) remains a
significant challenge due to their inherent complexity and opacity. While
techniques like fine-tuning can modify model behavior, they typically require
extensive computational resources. Recent work has introduced a class of
contrastive activation engineering (CAE) techniques as promising approaches for
steering LLM outputs through targeted modifications to their internal
representations. Applied at inference-time with zero cost, CAE has the
potential to introduce a new paradigm of flexible, task-specific LLM behavior
tuning. We analyze the performance of CAE in in-distribution,
out-of-distribution settings, evaluate drawbacks, and begin to develop
comprehensive guidelines for its effective deployment. We find that 1. CAE is
only reliably effective when applied to in-distribution contexts. 2. Increasing
the number of samples used to generate steering vectors has diminishing returns
at around 80 samples. 3. Steering vectors are susceptible to adversarial inputs
that reverses the behavior that is steered for. 4. Steering vectors harm the
overall model perplexity. 5. Larger models are more resistant to
steering-induced degradation.

</details>


### [100] [RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation](https://arxiv.org/abs/2505.03275)
*Tiantian Gan,Qiyao Sun*

Main category: cs.AI

TL;DR: RAG-MCP框架通过语义检索减少大型语言模型（LLM）使用外部工具时的提示膨胀和选择复杂度，显著提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在处理大量外部工具时的提示膨胀和工具选择复杂性问题。

Method: 引入RAG-MCP框架，利用语义检索从外部索引中筛选相关工具描述，仅将选中的工具传递给LLM。

Result: 实验显示，RAG-MCP减少提示令牌超过50%，工具选择准确率从13.62%提升至43.13%。

Conclusion: RAG-MCP为LLM提供了可扩展且准确的外部工具集成方案。

Abstract: Large language models (LLMs) struggle to effectively utilize a growing number
of external tools, such as those defined by the Model Context Protocol
(MCP)\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We
introduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes
this challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to
identify the most relevant MCP(s) for a given query from an external index
before engaging the LLM. Only the selected tool descriptions are passed to the
model, drastically reducing prompt size and simplifying decision-making.
Experiments, including an MCP stress test, demonstrate RAG-MCP significantly
cuts prompt tokens (e.g., by over 50%) and more than triples tool selection
accuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables
scalable and accurate tool integration for LLMs.

</details>


### [101] [Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for Reusing Existing Libraries and Interfaces](https://arxiv.org/abs/2505.03295)
*Luis Miguel Vieira da Silva,Aljosha Köcher,Nicolas König,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: 提出了一种利用大语言模型基于自然语言输入生成技能实现代码的方法，解决了技能开发耗时且具有挑战性的问题。


<details>
  <summary>Details</summary>
Motivation: 面对模块化架构中技能实现开发耗时且复杂的挑战，研究旨在利用大语言模型简化这一过程。

Method: 将能力视为技能实现的契约，结合自然语言输入和大语言模型生成代码，并集成现有软件库和接口技术。

Result: 在基于Python和ROS 2的自主移动机器人上验证了方法的可行性和灵活性。

Conclusion: 通过集成用户自定义库和接口，该方法为跨语言技能实现生成提供了高效解决方案。

Abstract: Modern automation systems increasingly rely on modular architectures, with
capabilities and skills as one solution approach. Capabilities define the
functions of resources in a machine-readable form and skills provide the
concrete implementations that realize those capabilities. However, the
development of a skill implementation conforming to a corresponding capability
remains a time-consuming and challenging task. In this paper, we present a
method that treats capabilities as contracts for skill implementations and
leverages large language models to generate executable code based on natural
language user input. A key feature of our approach is the integration of
existing software libraries and interface technologies, enabling the generation
of skill implementations across different target languages. We introduce a
framework that allows users to incorporate their own libraries and resource
interfaces into the code generation process through a retrieval-augmented
generation architecture. The proposed method is evaluated using an autonomous
mobile robot controlled via Python and ROS 2, demonstrating the feasibility and
flexibility of the approach.

</details>


### [102] [Artificial Behavior Intelligence: Technology, Challenges, and Future Directions](https://arxiv.org/abs/2505.03315)
*Kanghyun Jo,Jehwan Choi,Kwanho Kim,Seongmin Kim,Duy-Linh Nguyen,Xuan-Thuy Vo,Adri Priadana,Tien-Dat Tran*

Main category: cs.AI

TL;DR: 本文提出了人工行为智能（ABI）的技术框架，用于分析和解释人类姿态、表情、情绪、行为序列及上下文线索，并探讨了预训练大模型对其准确性和可解释性的提升。研究团队专注于开发轻量级模型以高效推断复杂行为，并提出了应对数据有限性、行为预测不确定性及模型优化的挑战方案。


<details>
  <summary>Details</summary>
Motivation: 理解和预测人类行为在自动驾驶、智能医疗等多个AI应用领域至关重要。本文旨在通过ABI技术框架，结合多模态数据和预训练大模型，提升行为识别的能力。

Method: 基于姿态估计、表情与情绪识别、序列行为分析和上下文建模等关键技术，研究团队探索了轻量级Transformer、图架构、能耗感知损失函数和多模态知识蒸馏等优化策略。

Result: 研究重点在于开发能够高效推断复杂行为的轻量级模型，并验证了其在实时环境中的适用性，同时识别了数据限制、预测不确定性和模型优化等技术挑战。

Conclusion: ABI框架及其优化方法为行为智能的实时高效应用提供了可能，未来研究方向包括解决数据稀缺性、提升预测稳定性及优化模型性能。

Abstract: Understanding and predicting human behavior has emerged as a core capability
in various AI application domains such as autonomous driving, smart healthcare,
surveillance systems, and social robotics. This paper defines the technical
framework of Artificial Behavior Intelligence (ABI), which comprehensively
analyzes and interprets human posture, facial expressions, emotions, behavioral
sequences, and contextual cues. It details the essential components of ABI,
including pose estimation, face and emotion recognition, sequential behavior
analysis, and context-aware modeling. Furthermore, we highlight the
transformative potential of recent advances in large-scale pretrained models,
such as large language models (LLMs), vision foundation models, and multimodal
integration models, in significantly improving the accuracy and
interpretability of behavior recognition. Our research team has a strong
interest in the ABI domain and is actively conducting research, particularly
focusing on the development of intelligent lightweight models capable of
efficiently inferring complex human behaviors. This paper identifies several
technical challenges that must be addressed to deploy ABI in real-world
applications including learning behavioral intelligence from limited data,
quantifying uncertainty in complex behavior prediction, and optimizing model
structures for low-power, real-time inference. To tackle these challenges, our
team is exploring various optimization strategies including lightweight
transformers, graph-based recognition architectures, energy-aware loss
functions, and multimodal knowledge distillation, while validating their
applicability in real-time environments.

</details>


### [103] [AI-Driven Scholarly Peer Review via Persistent Workflow Prompting, Meta-Prompting, and Meta-Reasoning](https://arxiv.org/abs/2505.03332)
*Evgeny Markhasin*

Main category: cs.AI

TL;DR: 论文提出了一种名为持久工作流提示（PWP）的方法，旨在通过标准LLM聊天界面（无需编码或API）改进科学手稿的同行评审，展示了其在实验化学领域识别方法缺陷等复杂任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在科学稿件同行评审中的局限性，如数据不足和专家推理复杂性，通过一种零代码的普适性提示工程方法。

Method: 采用分层模块化架构的PWP提示，通过元提示技术和元推理迭代开发，编码专家评审工作流（包括隐性知识），提交后可持续触发系统化评估。

Result: PWP引导的LLM成功识别测试案例中的主要方法缺陷，减少输入偏差，处理复杂任务如区分主张与证据、多模态分析等。

Conclusion: PWP基于工作流形式化，展示了利用现有LLM完成复杂科学任务的潜力，并提供了透明化的开发过程和资源以支持复现。

Abstract: Critical peer review of scientific manuscripts presents a significant
challenge for Large Language Models (LLMs), partly due to data limitations and
the complexity of expert reasoning. This report introduces Persistent Workflow
Prompting (PWP), a potentially broadly applicable prompt engineering
methodology designed to bridge this gap using standard LLM chat interfaces
(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical
analysis of experimental chemistry manuscripts, featuring a hierarchical,
modular architecture (structured via Markdown) that defines detailed analysis
workflows. We develop this PWP prompt through iterative application of
meta-prompting techniques and meta-reasoning aimed at systematically codifying
expert review workflows, including tacit knowledge. Submitted once at the start
of a session, this PWP prompt equips the LLM with persistent workflows
triggered by subsequent queries, guiding modern reasoning LLMs through
systematic, multimodal evaluations. Demonstrations show the PWP-guided LLM
identifying major methodological flaws in a test case while mitigating LLM
input bias and performing complex tasks, including distinguishing claims from
evidence, integrating text/photo/figure analysis to infer parameters, executing
quantitative feasibility checks, comparing estimates against claims, and
assessing a priori plausibility. To ensure transparency and facilitate
replication, we provide full prompts, detailed demonstration analyses, and logs
of interactive chats as supplementary resources. Beyond the specific
application, this work offers insights into the meta-development process
itself, highlighting the potential of PWP, informed by detailed workflow
formalization, to enable sophisticated analysis using readily available LLMs
for complex scientific tasks.

</details>


### [104] [Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection](https://arxiv.org/abs/2505.03359)
*June-Woo Kim,Haram Yoon,Wonkyo Oh,Dawoon Jung,Sung-Hoon Yoon,Dae-Jin Kim,Dong-Ho Lee,Sang-Yeol Lee,Chan-Mo Yang*

Main category: cs.AI

TL;DR: 该论文提出了一种基于领域对抗训练的语音AI模型，用于减少性别偏见，提升抑郁症和PTSD检测的公平性与准确性，实验显示F1分数提高了13.29%。


<details>
  <summary>Details</summary>
Motivation: 语音AI模型在心理健康评估中存在性别偏见，导致预测不公和不准，研究旨在通过领域对抗训练解决这一问题。

Method: 将不同性别视为独立领域，融入预训练的语音基础模型，并通过E-DAIC数据集验证其效果。

Result: 方法显著提升了检测性能，F1分数比基线提高了13.29个百分点。

Conclusion: 研究强调了在AI心理健康评估中解决人口统计学差异的重要性。

Abstract: Speech-based AI models are emerging as powerful tools for detecting
depression and the presence of Post-traumatic stress disorder (PTSD), offering
a non-invasive and cost-effective way to assess mental health. However, these
models often struggle with gender bias, which can lead to unfair and inaccurate
predictions. In this study, our study addresses this issue by introducing a
domain adversarial training approach that explicitly considers gender
differences in speech-based depression and PTSD detection. Specifically, we
treat different genders as distinct domains and integrate this information into
a pretrained speech foundation model. We then validate its effectiveness on the
E-DAIC dataset to assess its impact on performance. Experimental results show
that our method notably improves detection performance, increasing the F1-score
by up to 13.29 percentage points compared to the baseline. This highlights the
importance of addressing demographic disparities in AI-driven mental health
assessment.

</details>


### [105] [Validating the Effectiveness of a Large Language Model-based Approach for Identifying Children's Development across Various Free Play Settings in Kindergarten](https://arxiv.org/abs/2505.03369)
*Yuanyuan Yang,Yuan Shen,Tianchen Sun,Yangbin Xie*

Main category: cs.AI

TL;DR: 该论文提出了一种结合大语言模型(LLM)和学习分析技术的方法，用于分析儿童自由游戏中的自我叙述，以评估其发展能力。该方法在识别认知、运动和社交能力方面准确率超过90%，并揭示了不同游戏环境对儿童发展的独特贡献。


<details>
  <summary>Details</summary>
Motivation: 自由游戏对儿童发展至关重要，但传统评估方法难以全面捕捉其动态和非结构化特点。该研究旨在通过技术手段提供更客观、即时的评估，以支持个性化教育。

Method: 研究使用LLM分析儿童的游戏叙述，并结合学习分析技术计算不同游戏环境下的表现评分。数据来自29名幼儿园儿童的2,224条游戏叙述，覆盖一学期的四个游戏区域。

Result: LLM模型在多数领域的识别准确率超过90%，且不同游戏环境对特定能力的发展有显著差异。

Conclusion: 该方法有效识别儿童在多游戏环境中的发展，展示了LLM与学习分析在教育中的潜力，为个性化学习提供了数据支持。

Abstract: Free play is a fundamental aspect of early childhood education, supporting
children's cognitive, social, emotional, and motor development. However,
assessing children's development during free play poses significant challenges
due to the unstructured and spontaneous nature of the activity. Traditional
assessment methods often rely on direct observations by teachers, parents, or
researchers, which may fail to capture comprehensive insights from free play
and provide timely feedback to educators. This study proposes an innovative
approach combining Large Language Models (LLMs) with learning analytics to
analyze children's self-narratives of their play experiences. The LLM
identifies developmental abilities, while performance scores across different
play settings are calculated using learning analytics techniques. We collected
2,224 play narratives from 29 children in a kindergarten, covering four
distinct play areas over one semester. According to the evaluation results from
eight professionals, the LLM-based approach achieved high accuracy in
identifying cognitive, motor, and social abilities, with accuracy exceeding 90%
in most domains. Moreover, significant differences in developmental outcomes
were observed across play settings, highlighting each area's unique
contributions to specific abilities. These findings confirm that the proposed
approach is effective in identifying children's development across various free
play settings. This study demonstrates the potential of integrating LLMs and
learning analytics to provide child-centered insights into developmental
trajectories, offering educators valuable data to support personalized learning
and enhance early childhood education practices.

</details>


### [106] [Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in LLM-Based Agents](https://arxiv.org/abs/2505.03434)
*Schaun Wheeler,Olivier Jeunen*

Main category: cs.AI

TL;DR: 论文指出大型语言模型（LLMs）依赖程序性记忆，在复杂多变的环境中表现受限，提出通过模块化架构增强语义记忆和联想学习系统以提升适应性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决LLMs在真实复杂环境中因依赖程序性记忆而表现不足的问题。

Method: 采用模块化架构，将语义记忆和联想学习系统与LLMs解耦，以增强其适应性。

Result: 理论分析表明增强后的系统能更好地应对规则多变、反馈模糊的复杂环境。

Conclusion: 通过整合多种认知功能，LLMs可突破程序性记忆的局限，实现更接近人类智能的适应能力。

Abstract: Large Language Models (LLMs) represent a landmark achievement in Artificial
Intelligence (AI), demonstrating unprecedented proficiency in procedural tasks
such as text generation, code completion, and conversational coherence. These
capabilities stem from their architecture, which mirrors human procedural
memory -- the brain's ability to automate repetitive, pattern-driven tasks
through practice. However, as LLMs are increasingly deployed in real-world
applications, it becomes impossible to ignore their limitations operating in
complex, unpredictable environments. This paper argues that LLMs, while
transformative, are fundamentally constrained by their reliance on procedural
memory. To create agents capable of navigating ``wicked'' learning environments
-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must
augment LLMs with semantic memory and associative learning systems. By adopting
a modular architecture that decouples these cognitive functions, we can bridge
the gap between narrow procedural expertise and the adaptive intelligence
required for real-world problem-solving.

</details>


### [107] [The Steganographic Potentials of Language Models](https://arxiv.org/abs/2505.03439)
*Artem Karpov,Tinuade Adeleke,Seong Hah Cho,Natalia Perez-Campanero*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLMs）通过强化学习微调后隐藏信息（隐写）的能力，发现当前模型在安全和容量上表现出初步隐写能力，而明确的算法指导显著提升了其信息隐藏能力。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs通过隐写技术隐藏信息的潜力及其对未对齐AI代理检测和LLMs推理忠实性的挑战。

Method: 通过强化学习微调LLMs，开发隐蔽编码方案，并在提示和未提示的 realistic 场景中测试其隐写能力。

Result: 当前模型在隐写的安全和容量方面表现出初步能力，而算法指导显著提升了其信息隐藏效果。

Conclusion: 研究发现LLMs具备初步隐写能力，且通过算法指导可以显著提升其信息隐藏性能，这对AI安全和LLMs的可靠性具有重要意义。

Abstract: The potential for large language models (LLMs) to hide messages within plain
text (steganography) poses a challenge to detection and thwarting of unaligned
AI agents, and undermines faithfulness of LLMs reasoning. We explore the
steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)
to: (1) develop covert encoding schemes, (2) engage in steganography when
prompted, and (3) utilize steganography in realistic scenarios where hidden
reasoning is likely, but not prompted. In these scenarios, we detect the
intention of LLMs to hide their reasoning as well as their steganography
performance. Our findings in the fine-tuning experiments as well as in
behavioral non fine-tuning evaluations reveal that while current models exhibit
rudimentary steganographic abilities in terms of security and capacity,
explicit algorithmic guidance markedly enhances their capacity for information
concealment.

</details>


### [108] [am-ELO: A Stable Framework for Arena-based LLM Evaluation](https://arxiv.org/abs/2505.03475)
*Zirui Liu,Jiatong Li,Yan Zhuang,Qi Liu,Shuanghong Shen,Jie Ouyang,Mingyue Cheng,Shijin Wang*

Main category: cs.AI

TL;DR: 论文提出了一种新的稳定竞技场框架m-ELO和am-ELO，通过最大似然估计和考虑标注者能力，解决了现有ELO评分系统的不稳定问题，提升了大型语言模型评估的稳健性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于ELO评分系统的竞技场评估框架存在排名不一致导致的稳定性问题，且未考虑标注者能力的差异。论文旨在通过改进ELO系统，提出更稳定的评估方法。

Method: 提出m-ELO，用最大似然估计替代迭代更新方法，并理论证明其一致性和稳定性；进一步提出am-ELO，修改ELO的概率函数以纳入标注者能力，同时估计模型得分和标注者可靠性。

Result: 实验证明该方法能确保稳定性，提供了更稳健、准确和稳定的LLMs评估框架。

Conclusion: m-ELO和am-ELO通过理论改进和实验验证，显著提升了竞技场评估的稳定性和准确性，为LLMs评估提供了更可靠的解决方案。

Abstract: Arena-based evaluation is a fundamental yet significant evaluation paradigm
for modern AI models, especially large language models (LLMs). Existing
framework based on ELO rating system suffers from the inevitable instability
problem due to ranking inconsistency and the lack of attention to the varying
abilities of annotators. In this paper, we introduce a novel stable arena
framework to address these issues by enhancing the ELO Rating System.
Specifically, we replace the iterative update method with a Maximum Likelihood
Estimation (MLE) approach, m-ELO, and provide theoretical proof of the
consistency and stability of the MLE approach for model ranking. Additionally,
we proposed the am-ELO, which modify the Elo Rating's probability function to
incorporate annotator abilities, enabling the simultaneous estimation of model
scores and annotator reliability. Experiments demonstrate that this method
ensures stability, proving that this framework offers a more robust, accurate,
and stable evaluation method for LLMs.

</details>


### [109] [STORY2GAME: Generating (Almost) Everything in an Interactive Fiction Game](https://arxiv.org/abs/2505.03547)
*Eric Zhou,Shreyas Basavatia,Moontashir Siam,Zexin Chen,Mark O. Riedl*

Main category: cs.AI

TL;DR: STORY2GAME利用大型语言模型生成基于文本的交互式小说游戏，通过生成故事、填充世界并构建游戏引擎中的动作代码，使故事能够互动展开。动态生成动作和游戏状态更新是核心创新。


<details>
  <summary>Details</summary>
Motivation: 传统硬编码动作可能限制故事生成的开放性，而动态生成动作和游戏状态跟踪能提供更自由的互动体验。

Method: 利用LLM生成动作的前置条件和效果，动态生成新动作以适应玩家行为，并实时更新游戏引擎状态。

Result: 评估显示动作代码生成的成功率取决于玩家是否能完整互动体验生成的故事。

Conclusion: STORY2GAME通过动态动作生成和状态管理，实现了更开放的互动故事生成，为交互式叙事提供了新思路。

Abstract: We introduce STORY2GAME, a novel approach to using Large Language Models to
generate text-based interactive fiction games that starts by generating a
story, populates the world, and builds the code for actions in a game engine
that enables the story to play out interactively. Whereas a given set of
hard-coded actions can artificially constrain story generation, the ability to
generate actions means the story generation process can be more open-ended but
still allow for experiences that are grounded in a game state. The key to
successful action generation is to use LLM-generated preconditions and effects
of actions in the stories as guides for what aspects of the game state must be
tracked and changed by the game engine when a player performs an action. We
also introduce a technique for dynamically generating new actions to
accommodate the player's desire to perform actions that they think of that are
not part of the story. Dynamic action generation may require on-the-fly updates
to the game engine's state representation and revision of previously generated
actions. We evaluate the success rate of action code generation with respect to
whether a player can interactively play through the entire generated story.

</details>


### [110] [A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model Reasoning](https://arxiv.org/abs/2505.03553)
*Kolawole E. Ogunsina,Morayo A. Ogunsina*

Main category: cs.AI

TL;DR: 该论文提出了一种新型共识机制，受分布式账本技术启发，通过黑箱同行的方式验证和收敛多个大型语言模型的输出，以减少不一致和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在不一致输出和幻觉方面存在显著问题，这限制了可靠AI系统的发展。不同专有推理模型（RMs）在相同复杂请求下可能产生差异巨大的结果，影响了系统的可信度。

Method: 论文基于Hashgraph共识算法，提出了一种新型的共识机制。该方法通过‘gossip-about-gossip’通信和虚拟投票在多个推理模型间达成一致。每个模型在迭代交换和更新其答案的过程中，逐步提高准确性和置信度。该方法不仅采用多数表决，还结合了每个模型的知识和交叉验证内容。

Result: 该方法展现出在减少不准确输出方面的优势，超越了传统的集成技术。作者构建了原型系统架构，初步评估表明在收敛性和准确性上的潜力，但也提出了实施的挑战。

Conclusion: 该共识机制为多智能体AI系统提供了一种新的自验证手段，能够在复杂任务中提供高保真度的应答，具有重要的研究前景和应用潜力。

Abstract: Inconsistent outputs and hallucinations from large language models (LLMs) are
major obstacles to reliable AI systems. When different proprietary reasoning
models (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI,
are given the same complex request, they often produce divergent results due to
variations in training and inference. This paper proposes a novel consensus
mechanism, inspired by distributed ledger technology, to validate and converge
these outputs, treating each RM as a black-box peer. Building on the Hashgraph
consensus algorithm, our approach employs gossip-about-gossip communication and
virtual voting to achieve agreement among an ensemble of RMs. We present an
architectural design for a prototype system in which RMs iteratively exchange
and update their answers, using information from each round to improve accuracy
and confidence in subsequent rounds. This approach goes beyond simple majority
voting by incorporating the knowledge and cross-verification content of every
model. We justify the feasibility of this Hashgraph-inspired consensus for AI
ensembles and outline its advantages over traditional ensembling techniques in
reducing nonfactual outputs. Preliminary considerations for implementation,
evaluation criteria for convergence and accuracy, and potential challenges are
discussed. The proposed mechanism demonstrates a promising direction for
multi-agent AI systems to self-validate and deliver high-fidelity responses in
complex tasks.

</details>


### [111] [OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents](https://arxiv.org/abs/2505.03570)
*Mariya Davydova,Daniel Jeffries,Patrick Barker,Arturo Márquez Flores,Sinéad Ryan*

Main category: cs.AI

TL;DR: OSUniverse是一个为高级GUI导航AI代理设计的基准测试，专注于易用性、可扩展性、全面覆盖测试用例和自动化验证。其任务复杂度从基础点击到多步骤跨应用测试递增，当前版本确保SOTA代理成功率不超过50%，而普通白领可完美完成。基准测试支持手动和自动化评分（误差率<2%），为衡量AI代理GUI导航能力提供可靠标准。


<details>
  <summary>Details</summary>
Motivation: 为先进GUI导航AI代理创建一个综合基准测试，填补现有基准在复杂度、覆盖范围和自动化验证方面的不足，以准确衡量代理的能力和进展。

Method: 任务按复杂度分层设计（从基础点击到跨应用多步骤任务），并引入自动化验证机制（误差率<2%）以确保评分可靠性。任务难度校准至SOTA代理成功率≤50%，普通人类可完美完成。

Result: 当前基准版本显示SOTA代理任务完成率不足50%，自动化验证误差率低于2%，有效区分AI与人类能力差距。

Conclusion: OSUniverse为GUI导航AI代理的能力评估提供了标准化、自动化且可靠的基准，支持短期和中期技术进展的量化衡量。

Abstract: In this paper, we introduce OSUniverse: a benchmark of complex, multimodal
desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on
ease of use, extensibility, comprehensive coverage of test cases, and automated
validation. We divide the tasks in increasing levels of complexity, from basic
precision clicking to multistep, multiapplication tests requiring dexterity,
precision, and clear thinking from the agent. In version one of the benchmark,
presented here, we have calibrated the complexity of the benchmark test cases
to ensure that the SOTA (State of the Art) agents (at the time of publication)
do not achieve results higher than 50%, while the average white collar worker
can perform all these tasks with perfect accuracy. The benchmark can be scored
manually, but we also introduce an automated validation mechanism that has an
average error rate less than 2%. Therefore, this benchmark presents solid
ground for fully automated measuring of progress, capabilities and the
effectiveness of GUI-navigation AI agents over the short and medium-term
horizon. The source code of the benchmark is available at
https://github.com/agentsea/osuniverse.

</details>


### [112] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and Manipulating Human Perceptual Variability](https://arxiv.org/abs/2505.03641)
*Chen Wei,Chi Zhang,Jiachen Zou,Haotian Deng,Dietmar Heinke,Quanying Liu*

Main category: cs.AI

TL;DR: 这篇论文提出了一个名为BAM的计算框架，结合了人工神经网络中的感知边界采样和人类行为实验，以系统性研究人类决策的变异性，并通过大规模实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 理解人类在认知任务和日常生活中的决策变异性，探索感知和决策机制如何应对不确定性和模糊性。

Method: 结合了ANN的感知边界采样算法和大规模人类行为实验，生成了变异性刺激，并验证其有效性。

Result: 通过246名参与者的116,715次实验，创建了包含19,943张标注图像的variMNIST数据集，并展示了同时预测和操纵参与者感知决策的能力。

Conclusion: BAM框架填补了计算模型与人类个体差异研究之间的空白，为个性化感知分析提供了新工具。

Abstract: Human decision-making in cognitive tasks and daily life exhibits considerable
variability, shaped by factors such as task difficulty, individual preferences,
and personal experiences. Understanding this variability across individuals is
essential for uncovering the perceptual and decision-making mechanisms that
humans rely on when faced with uncertainty and ambiguity. We present a
computational framework BAM (Boundary Alignment & Manipulation framework) that
combines perceptual boundary sampling in ANNs and human behavioral experiments
to systematically investigate this phenomenon. Our perceptual boundary sampling
algorithm generates stimuli along ANN decision boundaries that intrinsically
induce significant perceptual variability. The efficacy of these stimuli is
empirically validated through large-scale behavioral experiments involving 246
participants across 116,715 trials, culminating in the variMNIST dataset
containing 19,943 systematically annotated images. Through personalized model
alignment and adversarial generation, we establish a reliable method for
simultaneously predicting and manipulating the divergent perceptual decisions
of pairs of participants. This work bridges the gap between computational
models and human individual difference research, providing new tools for
personalized perception analysis.

</details>


### [113] [BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems](https://arxiv.org/abs/2505.03643)
*Chelsea Sidrane,Jana Tumova*

Main category: cs.AI

TL;DR: 提出了一种用于计算非线性离散时间神经反馈回路的下近似后向可达集的算法，用于验证目标可达性。


<details>
  <summary>Details</summary>
Motivation: 现有学习启用的规划和控制算法缺乏严格的性能或安全性保证，需一种能验证学习启用系统属性的方法。

Method: 通过过近似系统动力学函数，利用混合整数线性规划计算下近似后向可达集。

Result: 算法在数值示例中得到验证，扩展了可验证学习启用系统属性的类别。

Conclusion: 该算法为学习启用系统提供了更严格的验证方法，增强了其安全性和可靠性。

Abstract: Learning-enabled planning and control algorithms are increasingly popular,
but they often lack rigorous guarantees of performance or safety. We introduce
an algorithm for computing underapproximate backward reachable sets of
nonlinear discrete time neural feedback loops. We then use the backward
reachable sets to check goal-reaching properties. Our algorithm is based on
overapproximating the system dynamics function to enable computation of
underapproximate backward reachable sets through solutions of mixed-integer
linear programs. We rigorously analyze the soundness of our algorithm and
demonstrate it on a numerical example. Our work expands the class of properties
that can be verified for learning-enabled systems.

</details>


### [114] [Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time](https://arxiv.org/abs/2505.03668)
*Celeste Veronese,Daniele Meli,Alessandro Farinelli*

Main category: cs.AI

TL;DR: 该论文提出了一种结合时间逻辑推理与部分可观察马尔可夫决策过程（POMDPs）的方法，通过线性时序逻辑（LTL）和事件演算（EC）生成持久宏动作，显著减少推理时间并提升性能。


<details>
  <summary>Details</summary>
Motivation: 动机在于解决不确定性下的决策问题，通过结合时间逻辑和POMDPs实现可解释的决策，同时减少对人工设计启发式的依赖。

Method: 方法包括利用LTL和EC生成持久宏动作，通过归纳逻辑编程（ILP）从少量执行轨迹中学习宏动作，并结合蒙特卡洛树搜索（MCTS）优化POMDP求解。

Result: 在Pocman和Rocksample基准测试中，学习的宏动表现出更高的表达能力和通用性，显著提升计算效率。

Conclusion: 结论表明，结合时间逻辑的宏动作能高效解决不确定环境下的决策问题，减少推理时间并保持鲁棒性能。

Abstract: This paper proposes an integration of temporal logical reasoning and
Partially Observable Markov Decision Processes (POMDPs) to achieve
interpretable decision-making under uncertainty with macro-actions. Our method
leverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus
(EC) to generate \emph{persistent} (i.e., constant) macro-actions, which guide
Monte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon,
significantly reducing inference time while ensuring robust performance. Such
macro-actions are learnt via Inductive Logic Programming (ILP) from a few
traces of execution (belief-action pairs), thus eliminating the need for
manually designed heuristics and requiring only the specification of the POMDP
transition model. In the Pocman and Rocksample benchmark scenarios, our learned
macro-actions demonstrate increased expressiveness and generality when compared
to time-independent heuristics, indeed offering substantial computational
efficiency improvements.

</details>


### [115] [Gap the (Theory of) Mind: Sharing Beliefs About Teammates' Goals Boosts Collaboration Perception, Not Performance](https://arxiv.org/abs/2505.03674)
*Yotam Amitai,Reuth Mirsky,Ofra Amir*

Main category: cs.AI

TL;DR: 研究探讨了AI代理通过共享推断的人类队友目标是否能提升任务表现和协作感知。实验发现，目标共享虽未显著改善任务表现或满意度，但支持了策略调整和协作感知，且在认知负荷上无额外负担。


<details>
  <summary>Details</summary>
Motivation: 在人类-AI团队中，直接共享目标并非总是可行，需通过行动推断意图。研究旨在验证AI共享推断目标是否能提升协作效果。

Method: 采用实验比较三种条件：无目标识别（NR）、可行目标（VG）及按需可行目标（VGod），评估任务表现、满意度及认知负荷。

Result: 目标共享未显著提升任务表现或满意度，但支持策略调整和协作感知；认知负荷无差异，表明信息量与简洁性的平衡难题。

Conclusion: 目标共享在增强信任和协作感知上有价值，但可能牺牲客观性能，需权衡其应用场景。

Abstract: In human-agent teams, openly sharing goals is often assumed to enhance
planning, collaboration, and effectiveness. However, direct communication of
these goals is not always feasible, requiring teammates to infer their
partner's intentions through actions. Building on this, we investigate whether
an AI agent's ability to share its inferred understanding of a human teammate's
goals can improve task performance and perceived collaboration. Through an
experiment comparing three conditions-no recognition (NR), viable goals (VG),
and viable goals on-demand (VGod) - we find that while goal-sharing information
did not yield significant improvements in task performance or overall
satisfaction scores, thematic analysis suggests that it supported strategic
adaptations and subjective perceptions of collaboration. Cognitive load
assessments revealed no additional burden across conditions, highlighting the
challenge of balancing informativeness and simplicity in human-agent
interactions. These findings highlight the nuanced trade-off of goal-sharing:
while it fosters trust and enhances perceived collaboration, it can
occasionally hinder objective performance gains.

</details>


### [116] [Graph Drawing for LLMs: An Empirical Evaluation](https://arxiv.org/abs/2505.03678)
*Walter Didimo,Fabrizio Montecchiani,Tommaso Piselli*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型（LLMs）在处理图相关任务时，输入图的可视化布局、美观性及提示技术对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过优化图的布局和提示技术，提升LLMs在图任务中的表现。

Method: 通过实验分析不同布局范式、绘图美学和提示技术对模型性能的影响，并提出了三个研究问题。

Result: 研究发现，选择正确的布局范式和优化绘图可读性可显著提升模型性能，同时提示技术的选择对性能至关重要。

Conclusion: 优化图的视觉输入和提示技术是提升LLMs在图任务中性能的关键因素。

Abstract: Our work contributes to the fast-growing literature on the use of Large
Language Models (LLMs) to perform graph-related tasks. In particular, we focus
on usage scenarios that rely on the visual modality, feeding the model with a
drawing of the graph under analysis. We investigate how the model's performance
is affected by the chosen layout paradigm, the aesthetics of the drawing, and
the prompting technique used for the queries. We formulate three corresponding
research questions and present the results of a thorough experimental analysis.
Our findings reveal that choosing the right layout paradigm and optimizing the
readability of the input drawing from a human perspective can significantly
improve the performance of the model on the given task. Moreover, selecting the
most effective prompting technique is a challenging yet crucial task for
achieving optimal performance.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [117] [Taskmaster Deconstructed: A Quantitative Look at Tension, Volatility, and Viewer Ratings](https://arxiv.org/abs/2505.02886)
*David H. Silver*

Main category: physics.soc-ph

TL;DR: 研究分析了英国电视节目《Taskmaster》的评分动态对观众吸引力的影响，发现评分指标与IMDb评分无显著关联，观众兴趣更多受选手行为而非游戏机制影响。


<details>
  <summary>Details</summary>
Motivation: 探讨《Taskmaster》中结构化评分系统是否真正影响观众参与度。

Method: 对18季162集的十五项评分指标（如排名波动、分数差距等）进行统计分析，并分析选手的排名轨迹。

Result: 评分动态与IMDb评分无显著关联；长期趋势显示平均分数上升，波动性略降，排名差距稳定；选手行为模式更影响观众兴趣。

Conclusion: 节目通过调整竞争可视性保持结构平衡，观众兴趣主要由选手行为驱动，而非评分机制。

Abstract: Taskmaster is a British television show that combines comedic performance
with a formal scoring system. Despite the appearance of structured competition,
it remains unclear whether scoring dynamics contribute meaningfully to audience
engagement. We conducted a statistical analysis of 162 episodes across 18
series, using fifteen episode-level metrics to quantify rank volatility, point
spread, lead changes, and winner dominance. None of these metrics showed a
significant association with IMDb ratings, even after controlling for series
effects. Long-term trends suggest that average points have increased over time,
while volatility has slightly declined and rank spread has remained stable.
These patterns indicate an attempt to enhance competitive visibility without
altering the show's structural equilibrium. We also analyzed contestant rank
trajectories and identified five recurring archetypes describing performance
styles. These patterns suggest that viewer interest is shaped more by
contestant behavior than by game mechanics.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [118] [Neural Orchestration for Multi-Agent Systems: A Deep Learning Framework for Optimal Agent Selection in Multi-Domain Task Environments](https://arxiv.org/abs/2505.02861)
*Kushagra Agrawal,Nisharg Nargund*

Main category: cs.MA

TL;DR: 论文提出了一种名为MetaOrch的神经协调框架，通过监督学习和模糊评估模块动态选择多域任务环境中最合适的代理，显著提升了选择准确性和系统适应性。


<details>
  <summary>Details</summary>
Motivation: 传统多代理系统（MAS）架构的协调机制过于僵化，难以适应动态任务环境，因此需一种更灵活、自适应的解决方案。

Method: MetaOrch采用监督学习方法建模任务上下文、代理历史记录和预期响应质量，并通过模糊评估模块对代理响应进行评分，动态预测最佳代理。

Result: 在模拟环境中，MetaOrch实现了86.3%的选择准确率，显著优于随机选择和轮询调度等基线策略。

Conclusion: 神经协调为多代理系统提供了更高的自主性、可解释性和适应性，适用于多样化的任务领域。

Abstract: Multi-agent systems (MAS) are foundational in simulating complex real-world
scenarios involving autonomous, interacting entities. However, traditional MAS
architectures often suffer from rigid coordination mechanisms and difficulty
adapting to dynamic tasks. We propose MetaOrch, a neural orchestration
framework for optimal agent selection in multi-domain task environments. Our
system implements a supervised learning approach that models task context,
agent histories, and expected response quality to select the most appropriate
agent for each task. A novel fuzzy evaluation module scores agent responses
along completeness, relevance, and confidence dimensions, generating soft
supervision labels for training the orchestrator. Unlike previous methods that
hard-code agent-task mappings, MetaOrch dynamically predicts the most suitable
agent while estimating selection confidence. Experiments in simulated
environments with heterogeneous agents demonstrate that our approach achieves
86.3% selection accuracy, significantly outperforming baseline strategies
including random selection and round-robin scheduling. The modular architecture
emphasizes extensibility, allowing agents to be registered, updated, and
queried independently. Results suggest that neural orchestration offers a
powerful approach to enhancing the autonomy, interpretability, and adaptability
of multi-agent systems across diverse task domains.

</details>


### [119] [Assessing and Enhancing the Robustness of LLM-based Multi-Agent Systems Through Chaos Engineering](https://arxiv.org/abs/2505.03096)
*Joshua Owotogbe*

Main category: cs.MA

TL;DR: 这篇论文提出了一种混沌工程框架，用于增强大型语言模型多代理系统（LLM-MAS）在真实生产环境中的鲁棒性，以应对突发错误和中断。


<details>
  <summary>Details</summary>
Motivation: LLM-MAS在任务处理中具有广泛潜力，但在生产或预生产环境中易受突发错误（如幻觉、代理失败和通信故障）的影响，需要增强其鲁棒性。

Method: 提出了一种混沌工程框架，用于主动识别LLM-MAS的脆弱点，评估并构建其抗干扰能力。

Result: 该框架能够帮助LLM-MAS在关键应用中实现可靠性能。

Conclusion: 混沌工程是确保LLM-MAS在真实环境下稳健运行的有效方法。

Abstract: This study explores the application of chaos engineering to enhance the
robustness of Large Language Model-Based Multi-Agent Systems (LLM-MAS) in
production-like environments under real-world conditions. LLM-MAS can
potentially improve a wide range of tasks, from answering questions and
generating content to automating customer support and improving decision-making
processes. However, LLM-MAS in production or preproduction environments can be
vulnerable to emergent errors or disruptions, such as hallucinations, agent
failures, and agent communication failures. This study proposes a chaos
engineering framework to proactively identify such vulnerabilities in LLM-MAS,
assess and build resilience against them, and ensure reliable performance in
critical applications.

</details>


### [120] [Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation](https://arxiv.org/abs/2505.03586)
*Songchen Fu,Siang Chen,Shaojing Zhao,Letian Bai,Ta Li,Yonghong Yan*

Main category: cs.MA

TL;DR: 论文提出了一个处理多智能体系统中观测延迟的框架RDC，通过扩展Dec-POMDP模型并引入RAINBOW方法，有效缓解了延迟对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中普遍存在的观测延迟问题，提升智能体在延迟环境下的决策能力。

Method: 提出DSID-POMDP模型扩展Dec-POMDP，并设计RAINBOW Delay Compensation (RDC)训练框架。

Result: 实验表明RDC能显著减轻延迟对性能的影响，某些情况下甚至能达到无延迟性能。

Conclusion: 研究为多智能体延迟观测问题提供了新视角和有效的解决框架。

Abstract: In real-world multi-agent systems (MASs), observation delays are ubiquitous,
preventing agents from making decisions based on the environment's true state.
An individual agent's local observation often consists of multiple components
from other agents or dynamic entities in the environment. These discrete
observation components with varying delay characteristics pose significant
challenges for multi-agent reinforcement learning (MARL). In this paper, we
first formulate the decentralized stochastic individual delay partially
observable Markov decision process (DSID-POMDP) by extending the standard
Dec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL
training framework for addressing stochastic individual delays, along with
recommended implementations for its constituent modules. We implement the
DSID-POMDP's observation generation pattern using standard MARL benchmarks,
including MPE and SMAC. Experiments demonstrate that baseline MARL methods
suffer severe performance degradation under fixed and unfixed delays. The
RDC-enhanced approach mitigates this issue, remarkably achieving ideal
delay-free performance in certain delay scenarios while maintaining
generalization capability. Our work provides a novel perspective on multi-agent
delayed observation problems and offers an effective solution framework.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [121] [Solar Flare Forecast: A Comparative Analysis of Machine Learning Algorithms for Solar Flare Class Prediction](https://arxiv.org/abs/2505.03385)
*Julia Bringewald*

Main category: astro-ph.SR

TL;DR: 该研究评估了三种机器学习算法（随机森林、KNN和XGBoost）在太阳耀斑分类中的表现，发现随机森林和XGBoost在多维度数据下表现优异。


<details>
  <summary>Details</summary>
Motivation: 太阳耀斑对空间天气和基础设施有重大影响，需要准确预测其发生和强度。

Method: 使用13个SHARP参数数据集，结合不同维度降维（8和100个主成分），通过10折分层交叉验证和网格搜索调参，评估模型在二元和多分类任务中的表现。

Result: 随机森林和XGBoost在所有指标中表现稳定，尤其在更高维度数据下效果显著。

Conclusion: 研究结果为未来天文物理任务的模型选择和维度降维优化提供了参考，有助于开发更准确的空间天气预报系统。

Abstract: Solar flares are among the most powerful and dynamic events in the solar
system, resulting from the sudden release of magnetic energy stored in the
Sun's atmosphere. These energetic bursts of electromagnetic radiation can
release up to 10^32 erg of energy, impacting space weather and posing risks to
technological infrastructure and therefore require accurate forecasting of
solar flare occurrences and intensities. This study evaluates the predictive
performance of three machine learning algorithms: Random Forest, k-Nearest
Neighbors (KNN), and Extreme Gradient Boosting (XGBoost) for classifying solar
flares into 4 categories (B, C, M, X). Using the dataset of 13 SHARP
parameters, the effectiveness of the models was evaluated in binary and
multiclass classification tasks. The analysis utilized 8 principal components
(PC), capturing 95% of data variance, and 100 PCs, capturing 97.5% of variance.
Our approach uniquely combines binary and multiclass classification with
different levels of dimensionality reduction, an innovative methodology not
previously explored in the context of solar flare prediction. Employing a
10-fold stratified cross-validation and grid search for hyperparameter tuning
ensured robust model evaluation. Our findings indicate that Random Forest and
XGBoost consistently demonstrate strong performance across all metrics,
benefiting significantly from increased dimensionality. The insights of this
study enhance future research by optimizing dimensionality reduction techniques
and informing model selection for astrophysical tasks. By integrating this
newly acquired knowledge into future research, more accurate space weather
forecasting systems can be developed, along with a deeper understanding of
solar physics.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [122] [Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation](https://arxiv.org/abs/2505.03105)
*Xule Lin*

Main category: cs.HC

TL;DR: 论文提出Cognitio Emergens (CE)框架，通过动态的人类与AI合作模型解决现有模型的局限，强调科学理解通过递归互动涌现的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有模型过于静态或侧重狭窄指标，无法捕捉人类与AI合作中科学理解的动态涌现过程。

Method: CE整合了机构配置（动态分配人类与AI的权威）、认知维度（六种合作能力）和合作动态（关系演化的驱动力）。

Result: CE揭示了知识共同创造通过角色、价值和结构的持续谈判涌现，为平衡人类参与和AI突破提供工具。

Conclusion: CE重构了人类与AI的科学合作，既不盲目乐观也不过度恐惧AI角色，为培育有意义的人类参与的伙伴关系提供概念工具。

Abstract: Scientific knowledge creation is fundamentally transforming as humans and AI
systems evolve beyond tool-user relationships into co-evolutionary epistemic
partnerships. When AlphaFold revolutionized protein structure prediction,
researchers described engaging with an epistemic partner that reshaped how they
conceptualized fundamental relationships. This article introduces Cognitio
Emergens (CE), a framework addressing critical limitations in existing models
that focus on static roles or narrow metrics while failing to capture how
scientific understanding emerges through recursive human-AI interaction over
time. CE integrates three components addressing these limitations: Agency
Configurations describing how authority distributes between humans and AI
(Directed, Contributory, Partnership), with partnerships dynamically
oscillating between configurations rather than following linear progression;
Epistemic Dimensions capturing six specific capabilities emerging through
collaboration across Discovery, Integration, and Projection axes, creating
distinctive "capability signatures" that guide development; and Partnership
Dynamics identifying forces shaping how these relationships evolve,
particularly the risk of epistemic alienation where researchers lose
interpretive control over knowledge they formally endorse. Drawing from
autopoiesis theory, social systems theory, and organizational modularity, CE
reveals how knowledge co-creation emerges through continuous negotiation of
roles, values, and organizational structures. By reconceptualizing human-AI
scientific collaboration as fundamentally co-evolutionary, CE offers a balanced
perspective that neither uncritically celebrates nor unnecessarily fears AI's
evolving role, instead providing conceptual tools for cultivating partnerships
that maintain meaningful human participation while enabling transformative
scientific breakthroughs.

</details>


### [123] [Augmenting Human Cognition through Everyday AR](https://arxiv.org/abs/2505.03492)
*Xiaoan Liu*

Main category: cs.HC

TL;DR: 论文探讨了常开式AR如何无缝连接数字认知与物理功能，以增强人类任务表现和理解。


<details>
  <summary>Details</summary>
Motivation: 随着空间计算和多模态LLM的发展，AR逐渐成为一种直观的‘思维工具’，将语义和上下文感知智能嵌入日常环境中。

Method: 研究探索了常开式AR技术，通过语义和上下文感知智能实现主动、上下文敏感的人机交互。

Result: 通过AR技术的应用，能够有效提升人类的任务表现和理解能力。

Conclusion: 常开式AR作为桥梁，能够将数字智能与物理世界无缝结合，为人机交互带来新的可能性。

Abstract: As spatial computing and multimodal LLMs mature, AR is tending to become an
intuitive "thinking tool," embedding semantic and context-aware intelligence
directly into everyday environments. This paper explores how always-on AR can
seamlessly bridge digital cognition and physical affordances, enabling
proactive, context-sensitive interactions that enhance human task performance
and understanding.

</details>


### [124] [BCause: Human-AI collaboration to improve hybrid mapping and ideation in argumentation-grounded deliberation](https://arxiv.org/abs/2505.03584)
*Lucas Anastasiou,Anna De Liddo*

Main category: cs.HC

TL;DR: 本文介绍了BCause讨论系统，利用生成式AI和人机协作，将公共议题的无序对话转化为结构化、可操作的民主进程。


<details>
  <summary>Details</summary>
Motivation: 公共讨论常因低效、肤浅、缺乏行动导向而受限，BCause旨在通过技术改进这一现状。

Method: 系统通过三大创新实现目标：将无序对话转为论辩讨论、基于地理的Telegram机器人议题报告、智能报告生成工具。

Result: BCause系统实现了结构化公共讨论，并保持了人类参与以确保伦理和上下文相关性。

Conclusion: 通过人机协作，BCause成功提升了公共讨论的质量和行动导向性。

Abstract: Public deliberation, as in open discussion of issues of public concern, often
suffers from scattered and shallow discourse, poor sensemaking, and a
disconnect from actionable policy outcomes. This paper introduces BCause, a
discussion system leveraging generative AI and human-machine collaboration to
transform unstructured dialogue around public issues (such as urban living,
policy changes, and current socio-economic transformations) into structured,
actionable democratic processes. We present three innovations: (i) importing
and transforming unstructured transcripts into argumentative discussions, (ii)
geo-deliberated problem-sensing via a Telegram bot for local issue reporting,
and (iii) smart reporting with customizable widgets (e.g., summaries, topic
modelling, policy recommendations, clustered arguments). The system's human-AI
partnership preserves critical human participation to ensure ethical oversight,
contextual relevance, and creative synthesis.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [125] [Nonnegative Low-rank Matrix Recovery Can Have Spurious Local Minima](https://arxiv.org/abs/2505.03717)
*Richard Y. Zhang*

Main category: math.OC

TL;DR: 该论文研究了带非负约束的低秩矩阵恢复问题，发现在完全观测情况下秩为1时仍具良性非凸性，但在部分观测情况下即使RIP常数趋近于0时失效，揭示了理论上的重要差异。


<details>
  <summary>Details</summary>
Motivation: 探讨低秩矩阵恢复在引入非负约束后是否仍保持其良性非凸特性，填补理论空白。

Method: 分析秩为1的非负因子矩阵在完全观测和部分观测情况下的行为，对比有无RIP条件下的收敛性。

Result: 完全观测下（RIP常数δ=0）保持良性非凸性，但部分观测下（δ→0⁺）失效，即使秩过参数化也无济于事。

Conclusion: 非负约束会破坏低秩矩阵恢复的连续性论证框架，导致部分观测场景下良性非凸性不复存在。

Abstract: The classical low-rank matrix recovery problem is well-known to exhibit
\emph{benign nonconvexity} under the restricted isometry property (RIP): local
optimization is guaranteed to converge to the global optimum, where the ground
truth is recovered. We investigate whether benign nonconvexity continues to
hold when the factor matrices are constrained to be elementwise nonnegative --
a common practical requirement. In the simple setting of a rank-1 nonnegative
ground truth, we confirm that benign nonconvexity holds in the fully-observed
case with RIP constant $\delta=0$. Surprisingly, however, this property fails
to extend to the partially-observed case with any arbitrarily small RIP
constant $\delta\to0^{+}$, irrespective of rank overparameterization. This
finding exposes a critical theoretical gap: the continuity argument widely used
to explain the empirical robustness of low-rank matrix recovery fundamentally
breaks down once nonnegative constraints are imposed.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [126] [Accelerating Evolution: Integrating PSO Principles into Real-Coded Genetic Algorithm Crossover](https://arxiv.org/abs/2505.03217)
*Xiaobo Jin,JiaShu Tu*

Main category: cs.NE

TL;DR: 提出一种名为PSOX的新型交叉算子，结合当前最优和历史最优信息，提升遗传算法在多样性保持和收敛速度上的性能。在15个基准测试函数上验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统交叉算子仅在同代个体间交换信息，无法充分利用历史最优信息。PSOX旨在结合全局最优和历史最优，提升算法性能。

Method: 开发PSOX交叉算子，融合粒子群优化思想，利用当前和历史最优解指导搜索，并通过15个基准函数对比实验验证效果。

Result: PSOX在解质量、稳定性和收敛速度上均优于5种先进交叉算子，尤其在搭配合适变异策略时表现更佳。

Conclusion: PSOX为实数编码遗传算法提供高效交叉方案，实验证明其优越性，并给出变异率调参的实用建议。

Abstract: This study introduces an innovative crossover operator named Particle Swarm
Optimization-inspired Crossover (PSOX), which is specifically developed for
real-coded genetic algorithms. Departing from conventional crossover approaches
that only exchange information between individuals within the same generation,
PSOX uniquely incorporates guidance from both the current global best solution
and historical optimal solutions across multiple generations. This novel
mechanism enables the algorithm to maintain population diversity while
simultaneously accelerating convergence toward promising regions of the search
space. The effectiveness of PSOX is rigorously evaluated through comprehensive
experiments on 15 benchmark test functions with diverse characteristics,
including unimodal, multimodal, and highly complex landscapes. Comparative
analysis against five state-of-the-art crossover operators reveals that PSOX
consistently delivers superior performance in terms of solution accuracy,
algorithmic stability, and convergence speed, especially when combined with an
appropriate mutation strategy. Furthermore, the study provides an in-depth
investigation of how different mutation rates influence PSOX's performance,
yielding practical guidelines for parameter tuning when addressing optimization
problems with varying landscape properties.

</details>


### [127] [From Neurons to Computation: Biological Reservoir Computing for Pattern Recognition](https://arxiv.org/abs/2505.03510)
*Ludovico Iannello,Luca Ciampi,Gabriele Lagani,Fabrizio Tonelli,Eleonora Crocco,Lucio Maria Calcagnile,Angelo Di Garbo,Federico Cremisi,Giuseppe Amato*

Main category: cs.NE

TL;DR: 本文提出一种利用培养生物神经元作为储备池的新型储备计算（BRC）范式，与传统人工计算单元不同，其神经网络活动由生物神经元产生，并通过线性分类器高效实现模式识别。


<details>
  <summary>Details</summary>
Motivation: 探索生物神经网络在计算任务中的应用潜力，为生物启发式计算系统和生物混合计算提供新思路。

Method: 通过多电极阵列（MEA）记录培养神经元的电活动，输入数据通过部分电极引入，其余电极捕获神经活动，形成非线性映射到高维生物特征空间。

Result: 实验表明，BRC能有效处理位置编码、不同方向的条形图以及数字识别任务，验证了生物神经网络替代传统人工神经网络的可行性。

Conclusion: BRC为神经形态工程和生物混合计算开辟了新途径，展现了生物神经网络在计算任务中的潜力。

Abstract: In this paper, we introduce a novel paradigm for reservoir computing (RC)
that leverages a pool of cultured biological neurons as the reservoir
substrate, creating a biological reservoir computing (BRC). This system
operates similarly to an echo state network (ESN), with the key distinction
that the neural activity is generated by a network of cultured neurons, rather
than being modeled by traditional artificial computational units. The neuronal
activity is recorded using a multi-electrode array (MEA), which enables
high-throughput recording of neural signals. In our approach, inputs are
introduced into the network through a subset of the MEA electrodes, while the
remaining electrodes capture the resulting neural activity. This generates a
nonlinear mapping of the input data to a high-dimensional biological feature
space, where distinguishing between data becomes more efficient and
straightforward, allowing a simple linear classifier to perform pattern
recognition tasks effectively. To evaluate the performance of our proposed
system, we present an experimental study that includes various input patterns,
such as positional codes, bars with different orientations, and a digit
recognition task. The results demonstrate the feasibility of using biological
neural networks to perform tasks traditionally handled by artificial neural
networks, paving the way for further exploration of biologically-inspired
computing systems, with potential applications in neuromorphic engineering and
bio-hybrid computing.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [128] [Single-Sample and Robust Online Resource Allocation](https://arxiv.org/abs/2505.02963)
*Rohan Ghuge,Sahil Singla,Yifan Wang*

Main category: cs.DS

TL;DR: 本文针对在线资源分配问题提出了一种新颖的指数定价算法，仅需单一样本即实现近似最优解，并具备抗干扰和激励兼容特性。


<details>
  <summary>Details</summary>
Motivation: 研究在线资源分配问题的可学习性和鲁棒性，解决现有方法对完整分布信息或特定到达模式的依赖，同时增强算法的抗干扰能力。

Method: 提出指数定价算法，通过资源价格的动态调整（指数变化）实现资源分配的优化，避免资源耗尽问题。

Result: 算法在单样本条件下实现$(1-\epsilon)$-近似最优，并在干扰模型下保持性能，同时确保激励兼容性。

Conclusion: 指数定价算法为在线资源分配问题提供了高效、鲁棒的解决方案，特别适用于预算充足且样本有限的场景。

Abstract: Online Resource Allocation problem is a central problem in many areas of
Computer Science, Operations Research, and Economics. In this problem, we
sequentially receive $n$ stochastic requests for $m$ kinds of shared resources,
where each request can be satisfied in multiple ways, consuming different
amounts of resources and generating different values. The goal is to achieve a
$(1-\epsilon)$-approximation to the hindsight optimum, where $\epsilon>0$ is a
small constant, assuming each resource has a large budget.
  In this paper, we investigate the learnability and robustness of online
resource allocation. Our primary contribution is a novel Exponential Pricing
algorithm with the following properties: 1. It requires only a \emph{single
sample} from each of the $n$ request distributions to achieve a
$(1-\epsilon)$-approximation for online resource allocation with large budgets.
Such an algorithm was previously unknown, even with access to polynomially many
samples, as prior work either assumed full distributional knowledge or was
limited to i.i.d.\,or random-order arrivals. 2. It is robust to corruptions in
the outliers model and the value augmentation model. Specifically, it maintains
its $(1 - \epsilon)$-approximation guarantee under both these robustness
models, resolving the open question posed in Argue, Gupta, Molinaro, and Singla
(SODA'22). 3. It operates as a simple item-pricing algorithm that ensures
incentive compatibility.
  The intuition behind our Exponential Pricing algorithm is that the price of a
resource should adjust exponentially as it is overused or underused. It differs
from conventional approaches that use an online learning algorithm for item
pricing. This departure guarantees that the algorithm will never run out of any
resource, but loses the usual no-regret properties of online learning
algorithms, necessitating a new analytical approach.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [129] [Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories](https://arxiv.org/abs/2505.03443)
*Valerio Bellandi*

Main category: cs.DC

TL;DR: 本文比较了集中式和分布式系统的优缺点，提出了一种为意大利司法部开发的分布式文档存储系统，利用边缘存储库分析文本数据和元数据，提升语义探索能力。


<details>
  <summary>Details</summary>
Motivation: 探讨集中式与分布式系统的适用场景，针对大规模环境中的高可用性和性能需求，开发分布式文档存储系统以满足语义探索的需求。

Method: 通过边缘存储库分析文本数据和元数据，设计并实现分布式文档存储系统。

Result: 系统成功提升了语义探索能力，适用于大规模环境中的高可用性和性能需求。

Conclusion: 分布式系统在大规模环境中表现优异，特别是在需要高可用性和语义探索能力的场景下。

Abstract: Centralized and distributed systems are two main approaches to organizing ICT
infrastructure, each with its pros and cons. Centralized systems concentrate
resources in one location, making management easier but creating single points
of failure. Distributed systems, on the other hand, spread resources across
multiple nodes, offering better scalability and fault tolerance, but requiring
more complex management. The choice between them depends on factors like
application needs, scalability, and data sensitivity. Centralized systems suit
applications with limited scalability and centralized control, while
distributed systems excel in large-scale environments requiring high
availability and performance. This paper explores a distributed document
repository system developed for the Italian Ministry of Justice, using edge
repositories to analyze textual data and metadata, enhancing semantic
exploration capabilities.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [130] [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/abs/2505.03414)
*Fangming Cui,Yonggang Zhang,Xuan Wang,Xinmei Tian,Jun Yu*

Main category: cs.CV

TL;DR: 论文提出了一种名为特征矩阵（FM）正则化的新方法，旨在提升大型视觉语言模型在目标非特定任务上的表现，解决现有提示学习方法易过拟合的问题，并在实验中验证了其有效性和兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法在目标非特异性任务上表现不佳，主要因过拟合导致模型遗忘通用知识。为解决这一问题，提出FM正则化方法以保留和利用通用知识。

Method: 通过提取并利用通用知识构建特征矩阵（FM），从深度和细粒度角度捕捉多样化输入的语义，避免过拟合。

Result: 实验表明FM能兼容现有框架，并显著提升目标非特定任务表现，达到最优性能。

Conclusion: FM正则化是一种通用且灵活的模块，能有效解决过拟合问题并提升目标非特异性任务的性能。

Abstract: Recent developments in prompt learning of large vision-language models have
significantly improved performance in target-specific tasks. However, these
prompt optimizing methods often struggle to tackle the target-unspecific or
generalizable tasks effectively. It may be attributed to the fact that
overfitting training causes the model to forget its general knowledge having
strong promotion on target-unspecific tasks. To alleviate this issue, we
propose a novel Features Matrix (FM) regularization approach designed to
enhance these models on target-unspecific tasks. Our method extracts and
leverages general knowledge, shaping a Features Matrix (FM). Specifically, the
FM captures the semantics of diverse inputs from a deep and fine perspective,
preserving essential general knowledge, which mitigates the risk of
overfitting. Representative evaluations demonstrate that: 1) the FM is
compatible with existing frameworks as a generic and flexible module, and 2)
the FM significantly showcases its effectiveness in enhancing target-unspecific
tasks, achieving state-of-the-art performance.

</details>


### [131] [Generating Narrated Lecture Videos from Slides with Synchronized Highlights](https://arxiv.org/abs/2505.02966)
*Alexander Holmberg*

Main category: cs.CV

TL;DR: 该论文提出了一种自动化系统，将静态幻灯片转换为带有AI生成讲解和动态视觉重点的视频讲座，核心是新颖的高亮对齐模块，利用多种策略精确同步讲解与视觉元素，效果优于传统方法且成本极低。


<details>
  <summary>Details</summary>
Motivation: 将静态幻灯片转换为视频讲座通常耗时耗力，需要人工录制讲解和视觉引导。本研究旨在通过自动化系统解决这一问题，提供高效且低成本的方案。

Method: 开发了一个端到端系统，结合高亮对齐模块（使用Levenshtein距离和LLM语义分析）和TTS时间同步，生成动态高亮与讲解同步的视频。

Result: 在1000个样本的数据集上，LLM对齐的准确率（F1>92%）显著优于其他方法，尤其对复杂数学内容更有效，且生成成本低于1美元/小时。

Conclusion: 该系统以高精度和极低成本实现了幻灯片到视频讲座的自动化转换，具有广泛应用潜力。

Abstract: Turning static slides into engaging video lectures takes considerable time
and effort, requiring presenters to record explanations and visually guide
their audience through the material. We introduce an end-to-end system designed
to automate this process entirely. Given a slide deck, this system synthesizes
a video lecture featuring AI-generated narration synchronized precisely with
dynamic visual highlights. These highlights automatically draw attention to the
specific concept being discussed, much like an effective presenter would. The
core technical contribution is a novel highlight alignment module. This module
accurately maps spoken phrases to locations on a given slide using diverse
strategies (e.g., Levenshtein distance, LLM-based semantic analysis) at
selectable granularities (line or word level) and utilizes timestamp-providing
Text-to-Speech (TTS) for timing synchronization. We demonstrate the system's
effectiveness through a technical evaluation using a manually annotated slide
dataset with 1000 samples, finding that LLM-based alignment achieves high
location accuracy (F1 > 92%), significantly outperforming simpler methods,
especially on complex, math-heavy content. Furthermore, the calculated
generation cost averages under $1 per hour of video, offering potential savings
of two orders of magnitude compared to conservative estimates of manual
production costs. This combination of high accuracy and extremely low cost
positions this approach as a practical and scalable tool for transforming
static slides into effective, visually-guided video lectures.

</details>


### [132] [Lesion-Aware Generative Artificial Intelligence for Virtual Contrast-Enhanced Mammography in Breast Cancer](https://arxiv.org/abs/2505.03018)
*Aurora Rofena,Arianna Manchia,Claudia Lucia Piccolo,Bruno Beomonte Zobel,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: Seg-CycleGAN是一种深度学习框架，通过低能量图像生成高保真的双能量减影图像，减少辐射和对比剂副作用，提升诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 解决对比增强乳腺摄影（CESM）中高辐射和对比剂副作用的问题，探索无对比剂的替代方案。

Method: 提出Seg-CycleGAN，基于CycleGAN架构，利用病灶分割图指导生成过程，并通过局部损失函数优化病灶重建。

Result: 在CESM@UCBM数据集上，Seg-CycleGAN在PSNR和SSIM上优于基线，同时保持竞争力的MSE和VIF。定性评估显示病灶保真度提升。

Conclusion: 研究表明，基于分割的生成模型为无对比剂CESM提供可行路径。

Abstract: Contrast-Enhanced Spectral Mammography (CESM) is a dual-energy mammographic
technique that improves lesion visibility through the administration of an
iodinated contrast agent. It acquires both a low-energy image, comparable to
standard mammography, and a high-energy image, which are then combined to
produce a dual-energy subtracted image highlighting lesion contrast
enhancement. While CESM offers superior diagnostic accuracy compared to
standard mammography, its use entails higher radiation exposure and potential
side effects associated with the contrast medium. To address these limitations,
we propose Seg-CycleGAN, a generative deep learning framework for Virtual
Contrast Enhancement in CESM. The model synthesizes high-fidelity dual-energy
subtracted images from low-energy images, leveraging lesion segmentation maps
to guide the generative process and improve lesion reconstruction. Building
upon the standard CycleGAN architecture, Seg-CycleGAN introduces localized loss
terms focused on lesion areas, enhancing the synthesis of diagnostically
relevant regions. Experiments on the CESM@UCBM dataset demonstrate that
Seg-CycleGAN outperforms the baseline in terms of PSNR and SSIM, while
maintaining competitive MSE and VIF. Qualitative evaluations further confirm
improved lesion fidelity in the generated images. These results suggest that
segmentation-aware generative models offer a viable pathway toward
contrast-free CESM alternatives.

</details>


### [133] [VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis](https://arxiv.org/abs/2505.03132)
*Xinyuan Yan,Xiwei Xuan,Jorge Piazentin Ono,Jiajing Guo,Vikram Mohanty,Shekar Arvind Kumar,Liang Gou,Bei Wang,Liu Ren*

Main category: cs.CV

TL;DR: 论文提出了一个名为VISLIX的视觉分析框架，利用先进的基础模型帮助专家分析计算机视觉模型中的数据切片，解决了传统数据切片方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的机器学习模型（如自动驾驶和监控）需要严格验证，但传统的数据切片方法在计算机视觉任务中存在诸多挑战，如依赖额外元数据、人工需求高且缺乏交互性。

Method: 引入VISLIX框架，通过基础模型自动生成自然语言见解，并支持用户交互式测试数据切片假设，无需图像元数据或视觉概念。

Result: 通过专家研究和三个用例证明了VISLIX在验证目标检测模型时的有效性，能提供全面见解。

Conclusion: VISLIX克服了传统数据切片的限制，提升了计算机视觉模型验证的效率和可操作性。

Abstract: Real-world machine learning models require rigorous evaluation before
deployment, especially in safety-critical domains like autonomous driving and
surveillance. The evaluation of machine learning models often focuses on data
slices, which are subsets of the data that share a set of characteristics. Data
slice finding automatically identifies conditions or data subgroups where
models underperform, aiding developers in mitigating performance issues.
Despite its popularity and effectiveness, data slicing for vision model
validation faces several challenges. First, data slicing often needs additional
image metadata or visual concepts, and falls short in certain computer vision
tasks, such as object detection. Second, understanding data slices is a
labor-intensive and mentally demanding process that heavily relies on the
expert's domain knowledge. Third, data slicing lacks a human-in-the-loop
solution that allows experts to form hypothesis and test them interactively. To
overcome these limitations and better support the machine learning operations
lifecycle, we introduce VISLIX, a novel visual analytics framework that employs
state-of-the-art foundation models to help domain experts analyze slices in
computer vision models. Our approach does not require image metadata or visual
concepts, automatically generates natural language insights, and allows users
to test data slice hypothesis interactively. We evaluate VISLIX with an expert
study and three use cases, that demonstrate the effectiveness of our tool in
providing comprehensive insights for validating object detection models.

</details>


### [134] [Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)](https://arxiv.org/abs/2505.03149)
*Joseph William Kettelkamp,Ludovica Romanin,Sarv Priya,Mathews Jacob*

Main category: cs.CV

TL;DR: 提出一种无监督运动补偿图像重建算法用于自由呼吸和门控3D心脏MRI，通过低秩模型紧凑表示运动参数以提升重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有的运动分辨和运动补偿算法在自由呼吸3D心脏MRI中效果有限，需要更紧凑的运动表示方法来提升重建质量。

Method: 利用低秩模型表示运动相位参数化后的速度场，通过积分路径获得变形场，直接从k空间数据无监督学习静态模板与运动参数。

Result: 相较现有方法，提出的低秩运动模型在自由呼吸3D心脏MRI中重建效果更优。

Conclusion: 低秩运动模型为无监督心脏MRI重建提供了更有效的运动补偿方案，显著提升图像质量。

Abstract: We introduce an unsupervised motion-compensated image reconstruction
algorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging
(MRI). We express the image volume corresponding to each specific motion phase
as the deformation of a single static image template. The main contribution of
the work is the low-rank model for the compact joint representation of the
family of diffeomorphisms, parameterized by the motion phases. The
diffeomorphism at a specific motion phase is obtained by integrating a
parametric velocity field along a path connecting the reference template phase
to the motion phase. The velocity field at different phases is represented
using a low-rank model. The static template and the low-rank motion model
parameters are learned directly from the k-space data in an unsupervised
fashion. The more constrained motion model is observed to offer improved
recovery compared to current motion-resolved and motion-compensated algorithms
for free-breathing 3D cine MRI.

</details>


### [135] [StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data](https://arxiv.org/abs/2505.03154)
*Yuxuan Mu,Hung Yu Ling,Yi Shi,Ismael Baira Ojeda,Pengcheng Xi,Chang Shu,Fabio Zinno,Xue Bin Peng*

Main category: cs.CV

TL;DR: StableMotion提出了一种直接从非配对的损坏动作数据中训练动作清理模型的方法，通过引入动作质量指示器实现高质量动作生成，显著减少了人工清理的负担。


<details>
  <summary>Details</summary>
Motivation: 动作捕捉数据常因传感器不准确或后处理导致视觉上的不自然，人工清理成本高且耗时。现有数据驱动方法需配对数据，而高质量数据获取困难。

Method: 提出StableMotion，利用可手动或启发式标注的动作质量指示器，在扩散模型框架下训练质量感知的动作生成模型，统一生成与判别功能。

Result: 在SoccerMocap数据集上测试，模型有效修复多种动作瑕疵，动作卡顿和冻结帧分别减少68%和81%。

Conclusion: StableMotion为动作数据清理提供了高效且无需配对数据的解决方案，显著提升了自动化清理的效果。

Abstract: Motion capture (mocap) data often exhibits visually jarring artifacts due to
inaccurate sensors and post-processing. Cleaning this corrupted data can
require substantial manual effort from human experts, which can be a costly and
time-consuming process. Previous data-driven motion cleanup methods offer the
promise of automating this cleanup process, but often require in-domain paired
corrupted-to-clean training data. Constructing such paired datasets requires
access to high-quality, relatively artifact-free motion clips, which often
necessitates laborious manual cleanup. In this work, we present StableMotion, a
simple yet effective method for training motion cleanup models directly from
unpaired corrupted datasets that need cleanup. The core component of our method
is the introduction of motion quality indicators, which can be easily annotated
through manual labeling or heuristic algorithms and enable training of
quality-aware motion generation models on raw motion data with mixed quality.
At test time, the model can be prompted to generate high-quality motions using
the quality indicators. Our method can be implemented through a simple
diffusion-based framework, leading to a unified motion generate-discriminate
model, which can be used to both identify and fix corrupted frames. We
demonstrate that our proposed method is effective for training motion cleanup
models on raw mocap data in production scenarios by applying StableMotion to
SoccerMocap, a 245-hour soccer mocap dataset containing real-world motion
artifacts. The trained model effectively corrects a wide range of motion
artifacts, reducing motion pops and frozen frames by 68% and 81%, respectively.
See https://youtu.be/3Y7MMAH02B4 for more results.

</details>


### [136] [RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph](https://arxiv.org/abs/2505.03173)
*Sameer Malik,Moyuru Yamada,Ayush Singh,Dishank Aggarwal*

Main category: cs.CV

TL;DR: RAVU提出了一种检索增强的长视频理解框架，通过时空图记忆机制解决现有LMM处理长视频的局限性，显著提升了复杂推理任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LMM因缺乏显式记忆与检索机制，难以处理分钟至小时级长视频，尤其在多跳推理和跨帧目标追踪任务中表现不佳。

Method: 构建视频的时空图表示作为长期记忆，通过检索与组合式推理分解复杂查询，在图中逐步执行推理步骤。

Result: 在NExT-QA和EgoSchema数据集上，仅用5-10帧检索即可超越SOTA方法与基线模型。

Conclusion: RAVU证明了时空图与检索增强机制对长视频理解的有效性，为多模态推理任务提供了新方向。

Abstract: Comprehending long videos remains a significant challenge for Large
Multi-modal Models (LMMs). Current LMMs struggle to process even minutes to
hours videos due to their lack of explicit memory and retrieval mechanisms. To
address this limitation, we propose RAVU (Retrieval Augmented Video
Understanding), a novel framework for video understanding enhanced by retrieval
with compositional reasoning over a spatio-temporal graph. We construct a graph
representation of the video, capturing both spatial and temporal relationships
between entities. This graph serves as a long-term memory, allowing us to track
objects and their actions across time. To answer complex queries, we decompose
the queries into a sequence of reasoning steps and execute these steps on the
graph, retrieving relevant key information. Our approach enables more accurate
understanding of long videos, particularly for queries that require multi-hop
reasoning and tracking objects across frames. Our approach demonstrate superior
performances with limited retrieved frames (5-10) compared with other SOTA
methods and baselines on two major video QA datasets, NExT-QA and EgoSchema.

</details>


### [137] [seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models](https://arxiv.org/abs/2505.03176)
*Hafez Ghaemi,Eilif Muller,Shahab Bakhtiari*

Main category: cs.CV

TL;DR: seq-JEPA通过处理图像多视图序列，同时学习不变和等变表示，解决了传统双视图方法在下游任务中的灵活性限制，平衡了分类与等变任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统双视图自监督学习因过度依赖数据增强和掩码，导致不变性和等变性任务性能难以兼顾，限制了表示灵活性。

Method: seq-JEPA基于联合嵌入预测架构，处理多视图序列并融合相对变换信息，通过Transformer编码器生成聚合表示，预测下一视图表征。

Result: seq-JEPA在等变基准和分类任务中表现优异，且擅长序列观测聚合任务（如路径整合和眼动预测）。

Conclusion: seq-JEPA通过多视图序列建模有效平衡了不变性与等变性需求，为自监督学习提供了更灵活的表示框架。

Abstract: Current self-supervised algorithms mostly rely on transformations such as
data augmentation and masking to learn visual representations. This is achieved
by inducing invariance or equivariance with respect to these transformations
after encoding two views of an image. This dominant two-view paradigm can limit
the flexibility of learned representations for downstream adaptation by
creating performance trade-offs between invariance-related tasks such as image
classification and more fine-grained equivariance-related tasks. In this work,
we introduce \emph{seq-JEPA}, a world modeling paradigm based on
joint-embedding predictive architecture that leverages architectural inductive
biases to resolve this trade-off. Without requiring an additional equivariance
predictor or loss term, seq-JEPA simultaneously learns two architecturally
segregated representations: one equivariant to the specified transformations
and another invariant to them and suited for tasks such as classification. To
do so, our model processes a short sequence of different views (observations)
of an input image. Each encoded view is concatenated with embeddings
corresponding to the relative transformation (action) producing the next
observation in the sequence. A transformer encoder outputs an aggregate
representation of this sequence, which is subsequently conditioned on the
action leading to the next observation to predict its representation.
Empirically, seq-JEPA achieves strong performance on equivariant benchmarks and
image classification without sacrificing one for the other. Additionally, our
framework excels at tasks that inherently require aggregating a sequence of
observations, such as path integration across actions and predictive learning
across eye movements.

</details>


### [138] [DCS-ST for Classification of Breast Cancer Histopathology Images with Limited Annotations](https://arxiv.org/abs/2505.03204)
*Liu Suxing,Byungwon Min*

Main category: cs.CV

TL;DR: 深度学习在乳腺癌组织病理图像分类中表现良好，但在标注数据有限时性能下降，这是医学影像中的重要挑战。


<details>
  <summary>Details</summary>
Motivation: 由于医学影像标注成本高且需要专业知识，获取大量标注数据困难，而现有深度学习方法在数据有限时性能不佳。

Method: 

Result: 

Conclusion: 

Abstract: Deep learning methods have shown promise in classifying breast cancer
histopathology images, but their performance often declines with limited
annotated data, a critical challenge in medical imaging due to the high cost
and expertise required for annotations.

</details>


### [139] [Enhancing Glass Defect Detection with Diffusion Models: Addressing Imbalanced Datasets in Manufacturing Quality Control](https://arxiv.org/abs/2505.03134)
*Sajjad Rezvani Boroujeni,Hossein Abedi,Tom Bush*

Main category: cs.CV

TL;DR: 论文提出了一种基于Denoising Diffusion Probabilistic Models (DDPMs)生成合成缺陷玻璃产品图像的方法，用于数据增强，解决制造业质量控制和自动化视觉检测中的类别不平衡问题。通过增加少数类别样本，显著提升了CNN架构（ResNet50V2、EfficientNetB0和MobileNetV2）在缺陷检测中的分类性能，尤其是在召回率上的提升，同时保持了高精度。在ResNet50V2上，分类准确率从78%提升至93%。


<details>
  <summary>Details</summary>
Motivation: 工业玻璃制造中视觉缺陷检测面临着缺陷产品频率低、数据集不平衡的问题，这限制了深度学习模型和计算机视觉系统的表现。为解决这一问题，研究提出了生成合成数据以增强少数类别的方法。

Method: 使用Denoising Diffusion Probabilistic Models (DDPMs)生成合成缺陷玻璃产品图像，作为数据增强手段。随后利用ResNet50V2、EfficientNetB0和MobileNetV2等CNN架构进行训练和测试。

Result: 实验结果表明，该方法在所有测试的深度神经网络架构中显著提升了缺陷样本的召回率，同时保持了完美精度。ResNet50V2的分类准确率从78%提升至93%，提升最为显著。

Conclusion: 该方法为自动化缺陷检测提供了一种可扩展且经济高效的解决方案，不仅适用于玻璃制造业，还可扩展至其他面临类似类别不平衡问题的工业质量保证系统。

Abstract: Visual defect detection in industrial glass manufacturing remains a critical
challenge due to the low frequency of defective products, leading to imbalanced
datasets that limit the performance of deep learning models and computer vision
systems. This paper presents a novel approach using Denoising Diffusion
Probabilistic Models (DDPMs) to generate synthetic defective glass product
images for data augmentation, effectively addressing class imbalance issues in
manufacturing quality control and automated visual inspection. The methodology
significantly enhances image classification performance of standard CNN
architectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting
anomalies by increasing the minority class representation. Experimental results
demonstrate substantial improvements in key machine learning metrics,
particularly in recall for defective samples across all tested deep neural
network architectures while maintaining perfect precision. The most dramatic
improvement was observed in ResNet50V2's overall classification accuracy, which
increased from 78 percent to 93 percent when trained with the augmented data.
This work provides a scalable, cost-effective approach to enhancing automated
defect detection in glass manufacturing that can potentially be extended to
other industrial quality assurance systems and industries with similar class
imbalance challenges.

</details>


### [140] [Seeing the Abstract: Translating the Abstract Language for Vision Language Models](https://arxiv.org/abs/2505.03242)
*Davide Talon,Federico Girella,Ziyue Liu,Marco Cristani,Yiming Wang*

Main category: cs.CV

TL;DR: 论文分析了视觉语言模型（VLMs）中未被充分研究的抽象导向语言，特别是在时尚领域。研究发现抽象词汇在许多方面占据主导地位，但现有VLMs因预训练数据集中缺乏足够抽象词汇而表现不足。提出了一种无需训练的方法ACT，通过将抽象表示映射到已充分表示的具象词汇空间，显著提升了文本到图像检索任务的性能。


<details>
  <summary>Details</summary>
Motivation: 揭示并验证抽象导向语言在视觉语言模型中的广泛存在及其被低估的价值，特别是在时尚领域这种富含抽象表达的场景。

Method: 提出一种无需训练且与模型无关的方法——抽象到具象转换器（ACT），利用预训练模型和多模态数据库，将抽象词汇映射到VLM潜在空间中已充分表示的具象词汇。

Result: 在文本到图像检索任务中，ACT在相同数据集和跨数据集设置下均优于微调的VLMs，表现出强大的泛化能力，且改进效果与不同VLMs兼容。

Conclusion: ACT是一种即插即用的解决方案，能显著提升VLMs处理抽象语言的能力，尤其在时尚领域的多模态任务中具有广泛的应用潜力。

Abstract: Natural language goes beyond dryly describing visual content. It contains
rich abstract concepts to express feeling, creativity and properties that
cannot be directly perceived. Yet, current research in Vision Language Models
(VLMs) has not shed light on abstract-oriented language. Our research breaks
new ground by uncovering its wide presence and under-estimated value, with
extensive analysis. Particularly, we focus our investigation on the fashion
domain, a highly-representative field with abstract expressions. By analyzing
recent large-scale multimodal fashion datasets, we find that abstract terms
have a dominant presence, rivaling the concrete ones, providing novel
information, and being useful in the retrieval task. However, a critical
challenge emerges: current general-purpose or fashion-specific VLMs are
pre-trained with databases that lack sufficient abstract words in their text
corpora, thus hindering their ability to effectively represent
abstract-oriented language. We propose a training-free and model-agnostic
method, Abstract-to-Concrete Translator (ACT), to shift abstract
representations towards well-represented concrete ones in the VLM latent space,
using pre-trained models and existing multimodal databases. On the
text-to-image retrieval task, despite being training-free, ACT outperforms the
fine-tuned VLMs in both same- and cross-dataset settings, exhibiting its
effectiveness with a strong generalization capability. Moreover, the
improvement introduced by ACT is consistent with various VLMs, making it a
plug-and-play solution.

</details>


### [141] [Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach](https://arxiv.org/abs/2505.03299)
*Pierre Adorni,Minh-Tan Pham,Stéphane May,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: 该论文提出了一种名为‘能力编码’的新方法，用于预测基础模型在多个下游任务上的性能，无需微调，以简化模型选择并提供现有文献的新视角。


<details>
  <summary>Details</summary>
Motivation: 在遥感领域，已开发了超过75种视觉基础模型，但没有一种在所有下游任务中始终优于其他模型。为了简化模型选择并比较不同模型，作者提出了这一方法。

Method: 通过‘能力编码’这一成本效益高的方法，预测基础模型在多个下游任务上的性能，无需逐一微调。

Result: 该方法能有效简化基础模型的选择，并为未来研究提供新视角。代码已开源。

Conclusion: 论文提出的方法为遥感领域的基础模型选择提供了一种高效且实用的解决方案，并为相关研究提供了新方向。

Abstract: Foundation models constitute a significant advancement in computer vision:
after a single, albeit costly, training phase, they can address a wide array of
tasks. In the field of Earth observation, over 75 remote sensing vision
foundation models have been developed in the past four years. However, none has
consistently outperformed the others across all available downstream tasks. To
facilitate their comparison, we propose a cost-effective method for predicting
a model's performance on multiple downstream tasks without the need for
fine-tuning on each one. This method is based on what we call "capabilities
encoding." The utility of this novel approach is twofold: we demonstrate its
potential to simplify the selection of a foundation model for a given new task,
and we employ it to offer a fresh perspective on the existing literature,
suggesting avenues for future research. Codes are available at
https://github.com/pierreadorni/capabilities-encoding.

</details>


### [142] [Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices](https://arxiv.org/abs/2505.03303)
*Tasnim Shahriar*

Main category: cs.CV

TL;DR: 本研究评估了五种轻量级深度学习模型（MobileNetV3 Small、ResNet18、SqueezeNet、EfficientNetV2-S和ShuffleNetV2）在资源受限环境下的图像分类性能，发现EfficientNetV2精度最高，MobileNetV3平衡性最佳，SqueezeNet速度最快。


<details>
  <summary>Details</summary>
Motivation: 探讨轻量级模型在资源受限环境（如低内存设备）中的适用性，为边缘计算和移动平台优化深度学习系统提供依据。

Method: 在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上，评估五种模型的分类精度、推理时间、FLOPs和模型大小，并对比预训练与从头训练的MobileNetV3 Small。

Result: 迁移学习显著提升模型精度和计算效率，尤其对复杂数据集；EfficientNetV2精度最高，MobileNetV3平衡性最佳，SqueezeNet速度和紧凑性最优。

Conclusion: 研究揭示了精度与效率之间的权衡，为资源受限场景下的轻量级模型部署提供了实用指导。

Abstract: This paper presents a comprehensive evaluation of lightweight deep learning
models for image classification, emphasizing their suitability for deployment
in resource-constrained environments such as low-memory devices. Five
state-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,
EfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse
datasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using
four key performance metrics: classification accuracy, inference time,
floating-point operations (FLOPs), and model size. Additionally, we investigate
the impact of hyperparameter tuning, data augmentation, and training paradigms
by comparing pretrained models with scratch-trained counterparts, focusing on
MobileNetV3 Small. Our findings reveal that transfer learning significantly
enhances model accuracy and computational efficiency, particularly for complex
datasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest
accuracy, while MobileNetV3 offers the best balance between accuracy and
efficiency, and SqueezeNet excels in inference speed and compactness. This
study highlights critical trade-offs between accuracy and efficiency, offering
actionable insights for deploying lightweight models in real-world applications
where computational resources are limited. By addressing these challenges, this
research contributes to optimizing deep learning systems for edge computing and
mobile platforms.

</details>


### [143] [PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs](https://arxiv.org/abs/2505.03254)
*Lukas Meiner,Jens Mehnert,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: PROM提出一种针对深度可分离卷积网络的高效量化方法，通过选择性使用三元和8-bit权重，降低能量消耗和存储大小。


<details>
  <summary>Details</summary>
Motivation: 针对现代深度可分离卷积网络中计算成本分布不均的问题，现有量化方法无法充分利用效率潜力。

Method: 采用双位宽量化策略，对点卷积使用三元权重，其他模块使用8-bit权重，并通过量化感知训练实现。

Result: 在MobileNetV2上，能量消耗降低23.9倍，存储大小减少2.7倍，同时保持ImageNet分类性能。

Conclusion: PROM为量化深度可分离卷积网络提供了简单高效的方法，显著优化了能量与精度的权衡。

Abstract: Convolutional neural networks (CNNs) are crucial for computer vision tasks on
resource-constrained devices. Quantization effectively compresses these models,
reducing storage size and energy cost. However, in modern depthwise-separable
architectures, the computational cost is distributed unevenly across its
components, with pointwise operations being the most expensive. By applying a
general quantization scheme to this imbalanced cost distribution, existing
quantization approaches fail to fully exploit potential efficiency gains. To
this end, we introduce PROM, a straightforward approach for quantizing modern
depthwise-separable convolutional networks by selectively using two distinct
bit-widths. Specifically, pointwise convolutions are quantized to ternary
weights, while the remaining modules use 8-bit weights, which is achieved
through a simple quantization-aware training procedure. Additionally, by
quantizing activations to 8-bit, our method transforms pointwise convolutions
with ternary weights into int8 additions, which enjoy broad support across
hardware platforms and effectively eliminates the need for expensive
multiplications. Applying PROM to MobileNetV2 reduces the model's energy cost
by more than an order of magnitude (23.9x) and its storage size by 2.7x
compared to the float16 baseline while retaining similar classification
performance on ImageNet. Our method advances the Pareto frontier for energy
consumption vs. top-1 accuracy for quantized convolutional models on ImageNet.
PROM addresses the challenges of quantizing depthwise-separable convolutional
networks to both ternary and 8-bit weights, offering a simple way to reduce
energy cost and storage size.

</details>


### [144] [SD-VSum: A Method and Dataset for Script-Driven Video Summarization](https://arxiv.org/abs/2505.03319)
*Manolis Mylonas,Evlampios Apostolidis,Vasileios Mezaris*

Main category: cs.CV

TL;DR: 本文介绍了脚本驱动的视频摘要任务，通过用户提供的脚本选择最相关片段生成摘要，扩展了现有数据集并提出了新的网络架构SD-VSum，实验证明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统视频摘要方法无法根据用户需求灵活调整的问题，本文提出脚本驱动的视频摘要任务，旨在通过用户提供的脚本生成定制化的视频摘要。

Method: 本文扩展了VideoXum数据集，增加摘要的自然语言描述，并提出了SD-VSum网络架构，利用跨模态注意力机制对齐和融合视觉与文本信息。

Result: 实验结果显示，SD-VSum在脚本驱动的视频摘要任务中性能优于现有的查询驱动和通用摘要方法，能够生成符合用户需求的内容。

Conclusion: SD-VSum通过跨模态注意力机制有效融合视觉与文本信息，为用户提供了灵活且高质量的视频摘要解决方案。

Abstract: In this work, we introduce the task of script-driven video summarization,
which aims to produce a summary of the full-length video by selecting the parts
that are most relevant to a user-provided script outlining the visual content
of the desired summary. Following, we extend a recently-introduced large-scale
dataset for generic video summarization (VideoXum) by producing natural
language descriptions of the different human-annotated summaries that are
available per video. In this way we make it compatible with the introduced
task, since the available triplets of ``video, summary and summary
description'' can be used for training a method that is able to produce
different summaries for a given video, driven by the provided script about the
content of each summary. Finally, we develop a new network architecture for
script-driven video summarization (SD-VSum), that relies on the use of a
cross-modal attention mechanism for aligning and fusing information from the
visual and text modalities. Our experimental evaluations demonstrate the
advanced performance of SD-VSum against state-of-the-art approaches for
query-driven and generic (unimodal and multimodal) summarization from the
literature, and document its capacity to produce video summaries that are
adapted to each user's needs about their content.

</details>


### [145] [Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning](https://arxiv.org/abs/2505.03327)
*José-Luis Bueso-Bello,Benjamin Chauvel,Daniel Carcereri,Philipp Posovszky,Pietro Milillo,Jennifer Ruiz,Juan-Carlos Fernández-Diaz,Carolina González,Michele Martone,Ronny Hänsch,Paola Rizzoli*

Main category: cs.CV

TL;DR: 该论文提出了一种结合自监督学习和监督学习的框架，利用TanDEM-X干涉SAR数据进行高分辨率（6米）森林制图，解决了传统方法在缺乏大量标注数据时的限制，并在亚马逊雨林的实际应用中显著提升了分类精度。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型在中等分辨率森林制图中表现良好，但需要大量标注数据。高分辨率制图（如6米）面临标注数据不足的挑战，尤其是在检测狭窄道路和精确划定森林边界时。因此，作者旨在通过自监督学习减少对标注数据的依赖。

Method: 采用自监督学习从输入特征中提取高信息量表示，然后使用少量可靠标注数据进行监督训练。通过美国宾夕法尼亚州的1米分辨率森林/非森林参考地图验证不同训练方法，最终在亚马逊雨林的实际场景中应用最佳方法。

Result: 在标注数据极少的亚马逊雨林场景中，提出的自监督框架相比全监督方法显著提升了分类精度，为大规模高分辨率森林制图提供了可行方案。

Conclusion: 自监督学习能有效减少对标注数据的依赖，在高分辨率森林制图中表现优于传统全监督方法，具有广泛的应用潜力。

Abstract: Deep learning models have shown encouraging capabilities for mapping
accurately forests at medium resolution with TanDEM-X interferometric SAR data.
Such models, as most of current state-of-the-art deep learning techniques in
remote sensing, are trained in a fully-supervised way, which requires a large
amount of labeled data for training and validation. In this work, our aim is to
exploit the high-resolution capabilities of the TanDEM-X mission to map forests
at 6 m. The goal is to overcome the intrinsic limitations posed by
midresolution products, which affect, e.g., the detection of narrow roads
within vegetated areas and the precise delineation of forested regions
contours. To cope with the lack of extended reliable reference datasets at such
a high resolution, we investigate self-supervised learning techniques for
extracting highly informative representations from the input features, followed
by a supervised training step with a significantly smaller number of reliable
labels. A 1 m resolution forest/non-forest reference map over Pennsylvania,
USA, allows for comparing different training approaches for the development of
an effective forest mapping framework with limited labeled samples. We select
the best-performing approach over this test region and apply it in a real-case
forest mapping scenario over the Amazon rainforest, where only very few labeled
data at high resolution are available. In this challenging scenario, the
proposed self-supervised framework significantly enhances the classification
accuracy with respect to fully-supervised methods, trained using the same
amount of labeled data, representing an extremely promising starting point for
large-scale, very high-resolution forest mapping with TanDEM-X data.

</details>


### [146] [Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant](https://arxiv.org/abs/2505.03380)
*Haonan Wang,Jiaji Mao,Lehan Wang,Qixiang Zhang,Marawan Elbatel,Yi Qin,Huijun Hu,Baoxun Li,Wenhui Deng,Weifeng Qin,Hongrui Li,Jialin Liang,Jun Shen,Xiaomeng Li*

Main category: cs.CV

TL;DR: RCMed是一个全栈AI助手，通过层次化的视觉-语言对齐提升多模态输入的准确性，实现精确解剖定位和可靠诊断。它在165个临床任务中表现优异，并在外部验证中展示了卓越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI助手在多模态内容准确性和实际临床验证方面存在不足，RCMed旨在通过改进多模态对齐解决这些挑战，推动精准医疗的发展。

Method: RCMed采用自增强的视觉-语言关联机制，结合颜色区域描述策略，训练了2000万图像-掩码-描述三元组，实现跨尺度的形状-位置-文本关系学习。

Result: RCMed在165个临床任务中表现出色，细胞分割精度相对提升23.5%，并在20种癌症类型的验证中达到最新技术水平。

Conclusion: RCMed展示了集成多模态模型在复杂医疗场景中如何实现细粒度模式捕捉和人类水平的解释能力，推动了以人为本的AI医疗发展。

Abstract: Medical AI assistants support doctors in disease diagnosis, medical image
analysis, and report generation. However, they still face significant
challenges in clinical use, including limited accuracy with multimodal content
and insufficient validation in real-world settings. We propose RCMed, a
full-stack AI assistant that improves multimodal alignment in both input and
output, enabling precise anatomical delineation, accurate localization, and
reliable diagnosis through hierarchical vision-language grounding. A
self-reinforcing correlation mechanism allows visual features to inform
language context, while language semantics guide pixel-wise attention, forming
a closed loop that refines both modalities. This correlation is enhanced by a
color region description strategy, translating anatomical structures into
semantically rich text to learn shape-location-text relationships across
scales. Trained on 20 million image-mask-description triplets, RCMed achieves
state-of-the-art precision in contextualizing irregular lesions and subtle
anatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It
achieved a 23.5% relative improvement in cell segmentation from microscopy
images over prior methods. RCMed's strong vision-language alignment enables
exceptional generalization, with state-of-the-art performance in external
validation across 20 clinically significant cancer types, including novel
tasks. This work demonstrates how integrated multimodal models capture
fine-grained patterns, enabling human-level interpretation in complex scenarios
and advancing human-centric AI healthcare.

</details>


### [147] [DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation](https://arxiv.org/abs/2505.03401)
*Shanshan Song,Hui Tang,Honglong Yang,Xiaomeng Li*

Main category: cs.CV

TL;DR: 论文提出了一种动态差异感知时间残差网络（DDaTR），用于改进纵向放射学报告生成（LRRG），通过捕捉空间和时间相关性，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有LRRG方法在特征提取过程中难以有效捕捉空间和时间相关性，导致差异信息捕获不足，影响了报告生成的质量。

Method: DDaTR网络引入了动态特征对齐模块（DFAM）和动态差异感知模块（DDAM），以多级提取空间相关性，并通过动态残差网络建模时间相关性。

Result: 在三个基准测试中，DDaTR表现出优于现有方法的性能，证明了其在RRG和LRRG任务中的有效性。

Conclusion: DDaTR通过改进特征提取和建模时间相关性，显著提升了放射学报告生成的性能。

Abstract: Radiology Report Generation (RRG) automates the creation of radiology reports
from medical imaging, enhancing the efficiency of the reporting process.
Longitudinal Radiology Report Generation (LRRG) extends RRG by incorporating
the ability to compare current and prior exams, facilitating the tracking of
temporal changes in clinical findings. Existing LRRG approaches only extract
features from prior and current images using a visual pre-trained encoder,
which are then concatenated to generate the final report. However, these
methods struggle to effectively capture both spatial and temporal correlations
during the feature extraction process. Consequently, the extracted features
inadequately capture the information of difference across exams and thus
underrepresent the expected progressions, leading to sub-optimal performance in
LRRG. To address this, we develop a novel dynamic difference-aware temporal
residual network (DDaTR). In DDaTR, we introduce two modules at each stage of
the visual encoder to capture multi-level spatial correlations. The Dynamic
Feature Alignment Module (DFAM) is designed to align prior features across
modalities for the integrity of prior clinical information. Prompted by the
enriched prior features, the dynamic difference-aware module (DDAM) captures
favorable difference information by identifying relationships across exams.
Furthermore, our DDaTR employs the dynamic residual network to unidirectionally
transmit longitudinal information, effectively modelling temporal correlations.
Extensive experiments demonstrated superior performance over existing methods
on three benchmarks, proving its efficacy in both RRG and LRRG tasks.

</details>


### [148] [Blending 3D Geometry and Machine Learning for Multi-View Stereopsis](https://arxiv.org/abs/2505.03470)
*Vibhas Vats,Md. Alimoor Reza,David Crandall,Soon-heung Jung*

Main category: cs.CV

TL;DR: GC MVSNet++提出了一种在多视图和多尺度下主动实施几何一致性的新方法，显著加速学习过程并提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统的MVS方法依赖后处理的几何一致性检查，而现代学习方法未在学习阶段充分利用几何一致性。作者旨在通过引入多视角、多尺度的几何一致性监督改进学习效率和质量。

Method: 通过在多视图和多尺度下直接惩罚几何不一致像素，加速学习；提出稠密连接的代价正则化网络，包含两种块设计以增强特征连接。

Result: 在DTU和BlendedMVS数据集上达到新SOTA，在Tanks and Temples榜单位居第二。

Conclusion: GC MVSNet++首次在学习阶段实施多视图、多尺度的几何一致性监督，显著提升性能和学习效率。

Abstract: Traditional multi-view stereo (MVS) methods primarily depend on photometric
and geometric consistency constraints. In contrast, modern learning-based
algorithms often rely on the plane sweep algorithm to infer 3D geometry,
applying explicit geometric consistency (GC) checks only as a post-processing
step, with no impact on the learning process itself. In this work, we introduce
GC MVSNet plus plus, a novel approach that actively enforces geometric
consistency of reference view depth maps across multiple source views (multi
view) and at various scales (multi scale) during the learning phase (see Fig.
1). This integrated GC check significantly accelerates the learning process by
directly penalizing geometrically inconsistent pixels, effectively halving the
number of training iterations compared to other MVS methods. Furthermore, we
introduce a densely connected cost regularization network with two distinct
block designs simple and feature dense optimized to harness dense feature
connections for enhanced regularization. Extensive experiments demonstrate that
our approach achieves a new state of the art on the DTU and BlendedMVS datasets
and secures second place on the Tanks and Temples benchmark. To our knowledge,
GC MVSNet plus plus is the first method to enforce multi-view, multi-scale
supervised geometric consistency during learning. Our code is available.

</details>


### [149] [Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI Synthesis: Advancing Pretraining and Clinical Applications](https://arxiv.org/abs/2505.03426)
*Ziyu Li,Yujian Hu,Zhengyao Ding,Yiheng Mao,Haitao Li,Fan Yi,Hongkun Zhang,Zhengxing Huang*

Main category: cs.CV

TL;DR: CPGG提出了一种生成多样化心脏磁共振（CMR）数据的新方法，解决了数据稀缺问题，显著提升了AI模型在下游任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决大规模高质量CMR数据稀缺问题，以支持AI模型在心脏健康评估中的有效应用。

Method: 采用两阶段方法：首先生成模型学习CMR数据的心脏表型，随后利用掩码自回归扩散模型生成高保真CMR序列。

Result: 生成的合成CMR数据质量高，显著提高了诊断和表型预测等下游任务的性能。

Conclusion: CPGG框架通过生成多样化数据，有效解决了CMR数据不足问题，推动了AI在心脏健康领域的应用。

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for
diagnosing heart diseases and evaluating cardiac health. However, the limited
availability of large-scale, high-quality CMR datasets poses a major challenge
to the effective application of artificial intelligence (AI) in this domain.
Even the amount of unlabeled data and the health status it covers are difficult
to meet the needs of model pretraining, which hinders the performance of AI
models on downstream tasks. In this study, we present Cardiac Phenotype-Guided
CMR Generation (CPGG), a novel approach for generating diverse CMR data that
covers a wide spectrum of cardiac health status. The CPGG framework consists of
two stages: in the first stage, a generative model is trained using cardiac
phenotypes derived from CMR data; in the second stage, a masked autoregressive
diffusion model, conditioned on these phenotypes, generates high-fidelity CMR
cine sequences that capture both structural and functional features of the
heart in a fine-grained manner. We synthesized a massive amount of CMR to
expand the pretraining data. Experimental results show that CPGG generates
high-quality synthetic CMR data, significantly improving performance on various
downstream tasks, including diagnosis and cardiac phenotypes prediction. These
gains are demonstrated across both public and private datasets, highlighting
the effectiveness of our approach. Code is availabel at
https://anonymous.4open.science/r/CPGG.

</details>


### [150] [Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks](https://arxiv.org/abs/2505.03522)
*Haotong Cheng,Zhiqi Zhang,Hao Li,Xinshang Zhang*

Main category: cs.CV

TL;DR: 该论文提出了模块'普适性'的概念及其评估方程(UAE)，用于量化模块的迁移能力，并设计了两种优化模块(CRB和DCRB)，实验表明这些模块能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注性能提升，而忽略了架构组件的迁移能力量化。论文旨在通过定义'普适性'和设计评估指标，揭示模块普适性与模型泛化能力的关系。

Method: 提出UAE作为模块迁移能力的量化指标，并基于此设计了两种优化模块(CRB和DCRB)。

Result: 在多种数据集和设备部署中，嵌入这些模块的网络性能优于现有方法，PSNR提升最高达0.83dB，或参数减少71.3%且重建质量几乎无损。

Conclusion: 模块的普适性对模型性能至关重要，UAE和优化模块的设计为未来研究提供了新方向。

Abstract: Deep learning has substantially advanced the Single Image Super-Resolution
(SISR). However, existing researches have predominantly focused on raw
performance gains, with little attention paid to quantifying the
transferability of architectural components. In this paper, we introduce the
concept of "Universality" and its associated definitions which extend the
traditional notion of "Generalization" to encompass the modules' ease of
transferability, thus revealing the relationships between module universality
and model generalizability. Then we propose the Universality Assessment
Equation (UAE), a metric for quantifying how readily a given module could be
transplanted across models. Guided by the UAE results of standard residual
blocks and other plug-and-play modules, we further design two optimized
modules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).
Through comprehensive experiments on natural-scene benchmarks, remote-sensing
datasets, extreme-industrial imagery and on-device deployments, we demonstrate
that networks embedded with the proposed plug-and-play modules outperform
several state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or
enabling a 71.3% reduction in parameters with negligible loss in reconstruction
fidelity.

</details>


### [151] [Generating Synthetic Data via Augmentations for Improved Facial Resemblance in DreamBooth and InstantID](https://arxiv.org/abs/2505.03557)
*Koray Ulusan,Benjamin Kiefer*

Main category: cs.CV

TL;DR: 本文研究如何使用增强技术提升Stable Diffusion（SDXL）生成肖像的面部相似度，重点关注DreamBooth和InstantID两种个性化方法，并通过FaceDistance评估生成效果。


<details>
  <summary>Details</summary>
Motivation: 探索如何在业余照片生成的肖像中提高面部相似度，以满足专业肖像生成的需求。

Method: 采用DreamBooth和InstantID两种个性化技术，结合多种增强策略，并使用FaceDistance（基于FaceNet的封装）量化评估生成肖像的面部相似度。

Result: 研究表明增强技术能显著提升生成肖像的面部相似性，为下游应用提供了实用策略。

Conclusion: 增强技术对提升SDXL生成肖像的面部相似度具有重要作用，需根据具体需求选择合适的增强策略。

Abstract: The personalization of Stable Diffusion for generating professional portraits
from amateur photographs is a burgeoning area, with applications in various
downstream contexts. This paper investigates the impact of augmentations on
improving facial resemblance when using two prominent personalization
techniques: DreamBooth and InstantID. Through a series of experiments with
diverse subject datasets, we assessed the effectiveness of various augmentation
strategies on the generated headshots' fidelity to the original subject. We
introduce FaceDistance, a wrapper around FaceNet, to rank the generations based
on facial similarity, which aided in our assessment. Ultimately, this research
provides insights into the role of augmentations in enhancing facial
resemblance in SDXL-generated portraits, informing strategies for their
effective deployment in downstream applications.

</details>


### [152] [Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning](https://arxiv.org/abs/2505.03703)
*François Role,Sébastien Meyer,Victor Amblard*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法和指标来衡量和减少视觉语言模型中的模态间隙问题，并验证了其对下游任务的有效性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型中的模态间隙（modality gap）问题导致跨模态表示不匹配，影响了下游任务（如多模态检索、聚类和零样本分类）的性能。目前缺乏有效的方法来评估和解决这一问题。

Method: 作者提出了基于谱方法和最优传输技术的新方法，以测量并减少模态间隙。

Result: 在多个图像-文本数据集和模型上的实验表明，所提方法能有效减少模态间隙，并显著提升下游任务的性能。

Conclusion: 该研究为视觉语言模型中的模态间隙问题提供了实用的评估和优化方案，为后续研究奠定了基础。

Abstract: Vision-language models (VLMs) allow to embed texts and images in a shared
representation space. However, it has been shown that these models are subject
to a modality gap phenomenon meaning there exists a clear separation between
the embeddings from one modality and another in the embedding space. While this
misalignment is detrimental for downstream tasks such as multimodal retrieval,
multimodal clustering or zero-shot classification, etc. no generic and
practical methods have so far been proposed to assess it precisely and even
reduce it. We therefore propose novel measures and effective techniques
(spectral- and optimal transport-based methods) to achieve this goal. Extensive
experiments conducted on several image-text datasets and models demonstrate
their effectiveness and beneficial effects on downstream tasks. Our code is
available at the URL provided in the paper's abstract.

</details>


### [153] [Real-Time Person Image Synthesis Using a Flow Matching Model](https://arxiv.org/abs/2505.03562)
*Jiwoo Jeong,Kirok Kim,Wooju Kim,Nam-Joon Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种基于流匹配（FM）的生成模型RPFM，用于实时生成姿态引导的人物图像（PGPIS），在保持图像质量的同时显著提升生成速度。


<details>
  <summary>Details</summary>
Motivation: 实时生成高质量的人物图像在应用（如手语视频、AR/VR等）中至关重要，但现有扩散模型速度慢，无法满足实时性需求。

Method: 提出基于流匹配的生成模型，支持隐空间条件生成，实现了快速、稳定的训练与采样。

Result: 在DeepFashion数据集上，RPFM在生成速度上提升两倍以上，接近实时，同时图像质量与现有最佳模型相当。

Conclusion: RPFM通过轻微牺牲生成精度换取了显著的实时性提升，适合速度与质量并重的实时PGPIS应用。

Abstract: Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images
conditioned on a target pose and a source image. This task plays a key role in
various real-world applications, such as sign language video generation, AR/VR,
gaming, and live streaming. In these scenarios, real-time PGPIS is critical for
providing immediate visual feedback and maintaining user immersion.However,
achieving real-time performance remains a significant challenge due to the
complexity of synthesizing high-fidelity images from diverse and dynamic human
poses. Recent diffusion-based methods have shown impressive image quality in
PGPIS, but their slow sampling speeds hinder deployment in time-sensitive
applications. This latency is particularly problematic in tasks like generating
sign language videos during live broadcasts, where rapid image updates are
required. Therefore, developing a fast and reliable PGPIS model is a crucial
step toward enabling real-time interactive systems. To address this challenge,
we propose a generative model based on flow matching (FM). Our approach enables
faster, more stable, and more efficient training and sampling. Furthermore, the
proposed model supports conditional generation and can operate in latent space,
making it especially suitable for real-time PGPIS applications where both speed
and quality are critical. We evaluate our proposed method, Real-Time Person
Image Synthesis Using a Flow Matching Model (RPFM), on the widely used
DeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves
near-real-time sampling speeds while maintaining performance comparable to the
state-of-the-art models. Our methodology trades off a slight, acceptable
decrease in generated-image accuracy for over a twofold increase in generation
speed, thereby ensuring real-time performance.

</details>


### [154] [ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language and Vision Assistant](https://arxiv.org/abs/2505.03654)
*Yifan Xiang,Zhenxi Zhang,Bin Li,Yixuan Weng,Shoujun Zhou,Yangfan He,Keqin Li*

Main category: cs.CV

TL;DR: 该论文提出了ReGraP数据集和ReGraP-LLaVA模型，旨在解决现有个性化MLLM在关系推理方面的不足。通过结合知识图谱（KG）和思维链（CoT）问答对，模型能够更好地进行关系推理，并在多个任务类型中实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有个性化MLLM在关系推理方面存在局限性，训练数据缺乏多对象集且模型忽视个性化概念间的关系。论文旨在通过新的数据集和模型解决这些问题。

Method: 论文提出ReGraP数据集，包含图像、知识图谱和思维链问答对，并设计ReGraP-LLaVA模型，采用软硬图提示方法对齐知识图谱与模型语义空间。

Result: 实验表明，ReGraP-LLaVA不仅学习个性化知识，还能进行关系推理，在多样化任务中达到最先进性能。

Conclusion: 论文通过ReGraP数据集和ReGraP-LLaVA模型成功提升了MLLM在关系推理和知识连接能力上的表现。

Abstract: Recent advances in personalized MLLMs enable effective capture of
user-specific concepts, supporting both recognition of personalized concepts
and contextual captioning. However, humans typically explore and reason over
relations among objects and individuals, transcending surface-level information
to achieve more personalized and contextual understanding. To this end,
existing methods may face three main limitations: Their training data lacks
multi-object sets in which relations among objects are learnable. Building on
the limited training data, their models overlook the relations between
different personalized concepts and fail to reason over them. Their experiments
mainly focus on a single personalized concept, where evaluations are limited to
recognition and captioning tasks. To address the limitations, we present a new
dataset named ReGraP, consisting of 120 sets of personalized knowledge. Each
set includes images, KGs, and CoT QA pairs derived from the KGs, enabling more
structured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an
MLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard
graph prompting methods are designed to align KGs within the model's semantic
space. We establish the ReGraP Benchmark, which contains diverse task types:
multiple-choice, fill-in-the-blank, True/False, and descriptive questions in
both open- and closed-ended settings. The proposed benchmark is designed to
evaluate the relational reasoning and knowledge-connection capability of
personalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and
other competitive MLLMs. Results show that the proposed model not only learns
personalized knowledge but also performs relational reasoning in responses,
achieving the SoTA performance compared with the competitive methods. All the
codes and datasets are released at: https://github.com/xyfyyds/ReGraP.

</details>


### [155] [Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps from T1-Weighted MRI using CycleGAN Models](https://arxiv.org/abs/2505.03662)
*Xin Du,Francesca M. Cozzi,Rajesh Jena*

Main category: cs.CV

TL;DR: 提出了基于CycleGAN的方法，直接从T1加权MRI扫描生成FA图，提高了白质完整性和结构连通性评估的效率，尤其是在肿瘤区域表现优异。


<details>
  <summary>Details</summary>
Motivation: FA图与纤维束追踪图谱的空间错位影响了其与预测模型的有效整合，需一种直接从T1加权MRI生成FA图的方法。

Method: 采用CycleGAN模型，利用未配对数据训练，生成高保真FA图，并通过SSIM和PSNR进行严格评估。

Result: 模型在肿瘤区域表现尤为优异，放射学评估表明其可减少额外扫描需求，优化临床工作流程。

Conclusion: 该方法为FA图的生成提供了一种AI驱动的替代方案，具有显著的临床潜力。

Abstract: Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are
essential for evaluating white matter integrity and structural connectivity in
neuroimaging. However, the spatial misalignment between FA maps and
tractography atlases hinders their effective integration into predictive
models. To address this issue, we propose a CycleGAN based approach for
generating FA maps directly from T1-weighted MRI scans, representing the first
application of this technique to both healthy and tumour-affected tissues. Our
model, trained on unpaired data, produces high fidelity maps, which have been
rigorously evaluated using Structural Similarity Index (SSIM) and Peak
Signal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in
tumour regions. Radiological assessments further underscore the model's
potential to enhance clinical workflows by providing an AI-driven alternative
that reduces the necessity for additional scans.

</details>


### [156] [FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios](https://arxiv.org/abs/2505.03730)
*Shiyi Zhang,Junhao Zhuang,Zhaoyang Zhang,Ying Shan,Yansong Tang*

Main category: cs.CV

TL;DR: FlexiAct提出了一种新方法，通过RefAdapter和FAE技术，实现从参考视频到任意目标图像的动作转移，打破了现有方法在空间结构上的严格限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动作定制中受限于空间结构的严格约束（如布局、骨架和视角一致性），难以适应多样化主体和场景。FlexiAct旨在克服这些限制，提升灵活性和适应性。

Method: FlexiAct结合了RefAdapter（轻量级图像条件适配器）和FAE（频率感知动作提取）技术。RefAdapter专注于空间适应和一致性保持，而FAE在去噪过程中直接提取动作，避免传统时空分离架构的复杂性。

Result: 实验表明，FlexiAct能有效将动作转移至具有不同布局、骨架和视角的主体上，在保持外观一致性的同时提供更高的结构灵活性。

Conclusion: FlexiAct为动作定制提供了更灵活的解决方案，通过创新的RefAdapter和FAE技术，实现了多样化场景下的高质量动作转移。

Abstract: Action customization involves generating videos where the subject performs
actions dictated by input control signals. Current methods use pose-guided or
global motion customization but are limited by strict constraints on spatial
structure, such as layout, skeleton, and viewpoint consistency, reducing
adaptability across diverse subjects and scenarios. To overcome these
limitations, we propose FlexiAct, which transfers actions from a reference
video to an arbitrary target image. Unlike existing methods, FlexiAct allows
for variations in layout, viewpoint, and skeletal structure between the subject
of the reference video and the target image, while maintaining identity
consistency. Achieving this requires precise action control, spatial structure
adaptation, and consistency preservation. To this end, we introduce RefAdapter,
a lightweight image-conditioned adapter that excels in spatial adaptation and
consistency preservation, surpassing existing methods in balancing appearance
consistency and structural flexibility. Additionally, based on our
observations, the denoising process exhibits varying levels of attention to
motion (low frequency) and appearance details (high frequency) at different
timesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike
existing methods that rely on separate spatial-temporal architectures, directly
achieves action extraction during the denoising process. Experiments
demonstrate that our method effectively transfers actions to subjects with
diverse layouts, skeletons, and viewpoints. We release our code and model
weights to support further research at
https://shiyi-zh0408.github.io/projectpages/FlexiAct/

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [157] [Rational Retrieval Acts: Leveraging Pragmatic Reasoning to Improve Sparse Retrieval](https://arxiv.org/abs/2505.03676)
*Arthur Satouf,Gabriel Ben Zenou,Benjamin Piwowarski,Habiboulaye Amadou Boubacar,Pablo Piantanida*

Main category: cs.IR

TL;DR: 该论文提出了一种基于理性言语行为（RSA）框架的稀疏神经信息检索方法，通过动态调整文档集合中词项权重的交互作用，显著提升了检索模型的性能，尤其在跨域数据集上达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏神经信息检索方法及传统模型如BM25未能充分考虑文档集合中词项权重的复杂交互作用，导致文档表示不够准确。论文旨在通过RSA框架优化这种交互，提升检索效果。

Method: 方法上，论文将RSA框架（一种语言学中用于最小化特征数量的方法）适配到信息检索场景，通过动态调节词项-文档的交互关系，考虑文档集合的整体影响，从而更准确地表示文档。

Result: 实验结果表明，引入RSA的检索模型在多项稀疏检索任务中表现显著提升，尤其是在BEIR基准测试的跨域数据集上达到了最优结果。

Conclusion: 论文验证了RSA框架在稀疏神经信息检索中的有效性，表明动态考虑文档集合的全局信息可以显著改善检索性能，为未来研究提供了新方向。

Abstract: Current sparse neural information retrieval (IR) methods, and to a lesser
extent more traditional models such as BM25, do not take into account the
document collection and the complex interplay between different term weights
when representing a single document. In this paper, we show how the Rational
Speech Acts (RSA), a linguistics framework used to minimize the number of
features to be communicated when identifying an object in a set, can be adapted
to the IR case -- and in particular to the high number of potential features
(here, tokens). RSA dynamically modulates token-document interactions by
considering the influence of other documents in the dataset, better contrasting
document representations. Experiments show that incorporating RSA consistently
improves multiple sparse retrieval models and achieves state-of-the-art
performance on out-of-domain datasets from the BEIR benchmark.
https://github.com/arthur-75/Rational-Retrieval-Acts

</details>


### [158] [Feature Staleness Aware Incremental Learning for CTR Prediction](https://arxiv.org/abs/2505.02844)
*Zhikai Wang,Yanyan Shen,Zibin Zhang,Kangyi Lin*

Main category: cs.IR

TL;DR: 论文提出了一种名为FeSAIL的方法，用于解决CTR预测中的特征陈旧问题，通过动态回放陈旧特征样本和正则化机制提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现实推荐系统中的CTR预测每天处理数十亿用户交互，增量更新模型时，未被新数据覆盖的特征嵌入会陈旧，导致性能下降。

Method: FeSAIL包含两部分：(1) 高效采样陈旧样本的SAS算法；(2) 精细控制特征嵌入更新的SAR正则化机制。

Result: 实验证明FeSAIL在四个基准数据集上优于现有方法。

Conclusion: FeSAIL通过动态处理特征陈旧问题显著提升CTR预测模型的增量学习效果。

Abstract: Click-through Rate (CTR) prediction in real-world recommender systems often
deals with billions of user interactions every day. To improve the training
efficiency, it is common to update the CTR prediction model incrementally using
the new incremental data and a subset of historical data. However, the feature
embeddings of a CTR prediction model often get stale when the corresponding
features do not appear in current incremental data. In the next period, the
model would have a performance degradation on samples containing stale
features, which we call the feature staleness problem. To mitigate this
problem, we propose a Feature Staleness Aware Incremental Learning method for
CTR prediction (FeSAIL) which adaptively replays samples containing stale
features. We first introduce a staleness aware sampling algorithm (SAS) to
sample a fixed number of stale samples with high sampling efficiency. We then
introduce a staleness aware regularization mechanism (SAR) for a fine-grained
control of the feature embedding updating. We instantiate FeSAIL with a general
deep learning-based CTR prediction model and the experimental results
demonstrate FeSAIL outperforms various state-of-the-art methods on four
benchmark datasets.

</details>


### [159] [Avoid Recommending Out-of-Domain Items: Constrained Generative Recommendation with LLMs](https://arxiv.org/abs/2505.03336)
*Hao Liao,Wensheng Lu,Jianxun Lian,Mingqi Wu,Shuo Wang,Yong Zhang,Yitian Huang,Mingyang Zhou,Xing Xie*

Main category: cs.IR

TL;DR: 论文提出两种方法（RecLM-ret和RecLM-cgen）解决大语言模型推荐系统中的OOD问题，实验表明RecLM-cgen在准确性和消除OOD推荐方面表现更优，且易于集成。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推荐系统中展现潜力，但OOD推荐问题尚未解决。需要有效方法确保推荐内容在领域内。

Method: 研究两种方法：基于检索的RecLM-ret和基于约束生成的RecLM-cgen，两者均可与现有大语言模型无缝集成。

Result: 在三个推荐数据集上的实验显示，RecLM-cgen在准确性和消除OOD推荐方面优于RecLM-ret及其他基于大语言模型的推荐方法。

Conclusion: RecLM-cgen是更优方法，兼具轻量化和易集成性，为实际应用带来重要价值。

Abstract: Large Language Models (LLMs) have shown promise for generative recommender
systems due to their transformative capabilities in user interaction. However,
ensuring they do not recommend out-of-domain (OOD) items remains a challenge.
We study two distinct methods to address this issue: RecLM-ret, a
retrieval-based method, and RecLM-cgen, a constrained generation method. Both
methods integrate seamlessly with existing LLMs to ensure in-domain
recommendations. Comprehensive experiments on three recommendation datasets
demonstrate that RecLM-cgen consistently outperforms RecLM-ret and existing
LLM-based recommender models in accuracy while eliminating OOD recommendations,
making it the preferred method for adoption. Additionally, RecLM-cgen maintains
strong generalist capabilities and is a lightweight plug-and-play module for
easy integration into LLMs, offering valuable practical benefits for the
community. Source code is available at https://github.com/microsoft/RecAI

</details>


### [160] [Modeling Musical Genre Trajectories through Pathlet Learning](https://arxiv.org/abs/2505.03480)
*Lilian Marey,Charlotte Laclau,Bruno Sguerra,Tiphaine Viard,Manuel Moussallam*

Main category: cs.IR

TL;DR: 论文提出了一种基于字典学习的方法，分析用户在音乐流媒体平台上的音乐类型轨迹，揭示了用户听歌偏好的演变模式。


<details>
  <summary>Details</summary>
Motivation: 随着音乐流媒体平台上用户数据的增加，分析用户音乐消费行为成为可能，但追踪用户偏好的变化仍具挑战性。

Method: 采用字典学习范式，定义了一种称为`pathlets`的框架，捕捉音乐类型轨迹中的重复模式，生成可理解的轨迹嵌入。

Result: 实验表明，`pathlets`能有效揭示用户听歌模式，并可定性定量分析；同时发布了Deezer提供的2000名用户17个月的音乐类型标签数据集。

Conclusion: 研究增进了对用户与音乐互动行为的理解，为推荐系统优化及用户行为多样性研究提供了新方向。

Abstract: The increasing availability of user data on music streaming platforms opens
up new possibilities for analyzing music consumption. However, understanding
the evolution of user preferences remains a complex challenge, particularly as
their musical tastes change over time. This paper uses the dictionary learning
paradigm to model user trajectories across different musical genres. We define
a new framework that captures recurring patterns in genre trajectories, called
pathlets, enabling the creation of comprehensible trajectory embeddings. We
show that pathlet learning reveals relevant listening patterns that can be
analyzed both qualitatively and quantitatively. This work improves our
understanding of users' interactions with music and opens up avenues of
research into user behavior and fostering diversity in recommender systems. A
dataset of 2000 user histories tagged by genre over 17 months, supplied by
Deezer (a leading music streaming company), is also released with the code.

</details>


### [161] [Counterfactual Inference for Eliminating Sentiment Bias in Recommender Systems](https://arxiv.org/abs/2505.03655)
*Le Pan,Yuanjiang Cao,Chengkai Huang,Wenjie Zhang,Lina Yao*

Main category: cs.IR

TL;DR: 该研究针对推荐系统中的情感偏见问题，提出了一种基于反事实推理的两阶段方法，通过建模情感对评分的影响并解耦直接与间接效应，有效减轻偏见，提升推荐公平性。


<details>
  <summary>Details</summary>
Motivation: 研究发现基于评论的推荐系统存在情感偏见，负面评论用户或物品的推荐准确性较差，导致对批评用户和小众物品不公平。为解决这一问题，作者从反事实推理的角度展开研究。

Method: 采用两阶段方法：1) 训练阶段构建因果图建模情感对评分的影响；2) 推理阶段通过反事实推理解耦直接与间接效应，消除情感偏见的间接影响。

Result: 实验结果表明，该方法在评分预测上表现优异，同时有效缓解情感偏见。

Conclusion: 这是首次将反事实推理应用于推荐系统中的情感偏见缓解，为提升推荐公平性提供了新思路。

Abstract: Recommender Systems (RSs) aim to provide personalized recommendations for
users. A newly discovered bias, known as sentiment bias, uncovers a common
phenomenon within Review-based RSs (RRSs): the recommendation accuracy of users
or items with negative reviews deteriorates compared with users or items with
positive reviews. Critical users and niche items are disadvantaged by such
unfair recommendations. We study this problem from the perspective of
counterfactual inference with two stages. At the model training stage, we build
a causal graph and model how sentiment influences the final rating score.
During the inference stage, we decouple the direct and indirect effects to
mitigate the impact of sentiment bias and remove the indirect effect using
counterfactual inference. We have conducted extensive experiments, and the
results validate that our model can achieve comparable performance on rating
prediction for better recommendations and effective mitigation of sentiment
bias. To the best of our knowledge, this is the first work to employ
counterfactual inference on sentiment bias mitigation in RSs.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [162] [CreoPep: A Universal Deep Learning Framework for Target-Specific Peptide Design and Optimization](https://arxiv.org/abs/2505.02887)
*Cheng Ge,Han-Shen Tae,Zhenqiang Zhang,Lu Lu,Zhijie Huang,Yilin Wang,Tao Jiang,Wenqing Cai,Shan Chang,David J. Adams,Rilei Yu*

Main category: q-bio.BM

TL;DR: CreoPep是一个基于深度学习的条件生成框架，用于设计高亲和力肽变体并发现新结构基序，通过整合掩码语言模型和渐进掩码方案，结合FoldX能量筛选和温度控制多态采样，成功设计出针对α7烟碱型乙酰胆碱受体的强效抑制剂，展示了其在肽疗法设计中的潜力。


<details>
  <summary>Details</summary>
Motivation: 天然肽变体的多样性有限且传统优化策略费时费力，限制了靶向肽（如芋螺毒素）在治疗中的应用潜力，因此需要一种高效的计算方法加速肽药物设计。

Method: 采用深度学习的条件生成框架CreoPep，结合掩码语言模型和渐进掩码方案，通过FoldX能量筛选和温度控制多态采样生成结构功能多样的肽变体。

Result: 成功设计出针对α7烟碱型乙酰胆碱受体的亚微摩尔级强效抑制剂，结构分析显示变体具有保守和新颖的结合模式，包括无二硫键形式，突破了传统设计范式。

Conclusion: CreoPep为计算肽设计与实验验证提供了强大且通用性强的平台，有望加速下一代肽疗法的发现。

Abstract: Target-specific peptides, such as conotoxins, exhibit exceptional binding
affinity and selectivity toward ion channels and receptors. However, their
therapeutic potential remains underutilized due to the limited diversity of
natural variants and the labor-intensive nature of traditional optimization
strategies. Here, we present CreoPep, a deep learning-based conditional
generative framework that integrates masked language modeling with a
progressive masking scheme to design high-affinity peptide mutants while
uncovering novel structural motifs. CreoPep employs an integrative augmentation
pipeline, combining FoldX-based energy screening with temperature-controlled
multinomial sampling, to generate structurally and functionally diverse
peptides that retain key pharmacological properties. We validate this approach
by designing conotoxin inhibitors targeting the $\alpha$7 nicotinic
acetylcholine receptor, achieving submicromolar potency in electrophysiological
assays. Structural analysis reveals that CreoPep-generated variants engage in
both conserved and novel binding modes, including disulfide-deficient forms,
thus expanding beyond conventional design paradigms. Overall, CreoPep offers a
robust and generalizable platform that bridges computational peptide design
with experimental validation, accelerating the discovery of next-generation
peptide therapeutics.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [163] [A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case](https://arxiv.org/abs/2505.03196)
*Haoxiang Luo,Gang Sun,Yinqiu Liu,Dusit Niyato,Hongfang Yu,Mohammed Atiquzzaman,Schahram Dustdar*

Main category: cs.NI

TL;DR: 论文提出了一种基于区块链的多LLM协作框架（MultiLLMN），旨在通过联合评估提升复杂网络优化问题的响应可靠性和质量，并以FBS攻击防御为例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 不同LLM因结构和训练数据差异可能导致优化策略不一致，加之单一LLM的数据局限性和潜在恶意风险，需构建可信的多LLM协作网络。

Method: 设计基于区块链的多LLM协作框架，联合评估并筛选最优响应；以FBS攻击防御为例进行实证。

Result: 框架有效提升网络优化问题的响应可靠性和质量，FBS案例验证其可行性。

Conclusion: MultiLLMN为解决LLM协作与信任问题提供了新思路，未来可扩展至更多网络场景。

Abstract: Large Language Models (LLMs) demonstrate strong potential across a variety of
tasks in communications and networking due to their advanced reasoning
capabilities. However, because different LLMs have different model structures
and are trained using distinct corpora and methods, they may offer varying
optimization strategies for the same network issues. Moreover, the limitations
of an individual LLM's training data, aggravated by the potential maliciousness
of its hosting device, can result in responses with low confidence or even
bias. To address these challenges, we propose a blockchain-enabled
collaborative framework that connects multiple LLMs into a Trustworthy
Multi-LLM Network (MultiLLMN). This architecture enables the cooperative
evaluation and selection of the most reliable and high-quality responses to
complex network optimization problems. Specifically, we begin by reviewing
related work and highlighting the limitations of existing LLMs in collaboration
and trust, emphasizing the need for trustworthiness in LLM-based systems. We
then introduce the workflow and design of the proposed Trustworthy MultiLLMN
framework. Given the severity of False Base Station (FBS) attacks in B5G and 6G
communication systems and the difficulty of addressing such threats through
traditional modeling techniques, we present FBS defense as a case study to
empirically validate the effectiveness of our approach. Finally, we outline
promising future research directions in this emerging area.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [164] [Vector valued optimal transport: from dynamic to static formulations](https://arxiv.org/abs/2505.03670)
*Katy Craig,Nicolás García Trillos,Đorđe Nikolić*

Main category: math.AP

TL;DR: 本文提出了一种统一向量值最优运输理论的框架，将动态（Benamou-Brenier）和静态（Kantorovich）方法整合，证明四种距离的Bi-Hölder等价性，并探讨其在多物种PDE和数据分析中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于向量值度量的分类和多物种偏微分方程（PDE）的应用需求，旨在统一现有的向量值最优运输理论。

Method: 通过将向量值度量建模为积空间上的概率测度（其中G为加权图），结合图的几何特性，提出动态和静态距离理论，并证明其等价性。

Result: 获得了四种向量值最优运输距离的Bi-Hölder等价性，并展示了每种度量在理论和实践中的优势。

Conclusion: 该框架为多物种PDE和数据分析提供了潜在应用价值，尤其是其中一种静态公式可通过线性化加速计算。

Abstract: Motivated by applications in classification of vector valued measures and
multispecies PDE, we develop a theory that unifies existing notions of vector
valued optimal transport, from dynamic formulations (\`a la Benamou-Brenier) to
static formulations (\`a la Kantorovich). In our framework, vector valued
measures are modeled as probability measures on a product space $\mathbb{R}^d
\times G$, where $G$ is a weighted graph over a finite set of nodes and the
graph geometry strongly influences the associated dynamic and static distances.
We obtain sharp inequalities relating four notions of vector valued optimal
transport and prove that the distances are mutually bi-H\"older equivalent. We
discuss the theoretical and practical advantages of each metric and indicate
potential applications in multispecies PDE and data analysis. In particular,
one of the static formulations discussed in the paper is amenable to
linearization, a technique that has been explored in recent years to accelerate
the computation of pairwise optimal transport distances.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [165] [MORE: Mobile Manipulation Rearrangement Through Grounded Language Reasoning](https://arxiv.org/abs/2505.03035)
*Mohammad Mohammadi,Daniel Honerkamp,Martin Büchner,Matteo Cassinelli,Tim Welschehold,Fabien Despinoy,Igor Gilitschenski,Abhinav Valada*

Main category: cs.RO

TL;DR: MORE是一种新型方法，通过场景图表示环境和实例区分，增强语言模型在零样本移动操作任务中的规划能力，显著提升任务解决率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前基于基础模型的方法在大量物体和大规模环境中性能下降，因此提出MORE来解决这些限制。

Method: MORE利用场景图表示环境，结合实例区分和主动筛选方案，提取任务相关的子图，实现有界规划问题。

Result: 在BEHAVIOR-1K基准测试的81个任务中表现优异，成为首个显著解决该基准的方法，并成功应用于复杂现实任务。

Conclusion: MORE通过场景图和子图筛选有效提升规划可靠性，为移动操作任务提供了新颖解决方案。

Abstract: Autonomous long-horizon mobile manipulation encompasses a multitude of
challenges, including scene dynamics, unexplored areas, and error recovery.
Recent works have leveraged foundation models for scene-level robotic reasoning
and planning. However, the performance of these methods degrades when dealing
with a large number of objects and large-scale environments. To address these
limitations, we propose MORE, a novel approach for enhancing the capabilities
of language models to solve zero-shot mobile manipulation planning for
rearrangement tasks. MORE leverages scene graphs to represent environments,
incorporates instance differentiation, and introduces an active filtering
scheme that extracts task-relevant subgraphs of object and region instances.
These steps yield a bounded planning problem, effectively mitigating
hallucinations and improving reliability. Additionally, we introduce several
enhancements that enable planning across both indoor and outdoor environments.
We evaluate MORE on 81 diverse rearrangement tasks from the BEHAVIOR-1K
benchmark, where it becomes the first approach to successfully solve a
significant share of the benchmark, outperforming recent foundation model-based
approaches. Furthermore, we demonstrate the capabilities of our approach in
several complex real-world tasks, mimicking everyday activities. We make the
code publicly available at https://more-model.cs.uni-freiburg.de.

</details>


### [166] [Latent Adaptive Planner for Dynamic Manipulation](https://arxiv.org/abs/2505.03077)
*Donghun Noh,Deqian Kong,Minglu Zhao,Andrew Lizarraga,Jianwen Xie,Ying Nian Wu,Dennis Hong*

Main category: cs.RO

TL;DR: 论文提出Latent Adaptive Planner (LAP)，一种通过潜在空间推断从人类示范视频学习的动态非抓取操作规划方法，结合变分重规划和贝叶斯更新，在复杂场景中实现高效适应与最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决动态非抓取操作任务中的规划问题，特别是通过人类示范视频学习视动策略的挑战，包括保持时序一致性和适应环境变化。

Method: 采用潜在空间推断的变分重规划框架，结合贝叶斯更新逐步优化计划，并通过基于模型的比例映射填补人与机器人之间的具身鸿沟。

Result: 在多个复杂操作基准测试中，LAP在成功率、轨迹平滑性和能效方面优于现有方法，尤其在动态适应场景中表现突出。

Conclusion: LAP为机器人提供了类人类适应性的复杂交互能力，其可扩展框架适用于多种机器人平台，并基于相同的人类示范视频。

Abstract: This paper presents Latent Adaptive Planner (LAP), a novel approach for
dynamic nonprehensile manipulation tasks that formulates planning as latent
space inference, effectively learned from human demonstration videos. Our
method addresses key challenges in visuomotor policy learning through a
principled variational replanning framework that maintains temporal consistency
while efficiently adapting to environmental changes. LAP employs Bayesian
updating in latent space to incrementally refine plans as new observations
become available, striking an optimal balance between computational efficiency
and real-time adaptability. We bridge the embodiment gap between humans and
robots through model-based proportional mapping that regenerates accurate
kinematic-dynamic joint states and object positions from human demonstrations.
Experimental evaluations across multiple complex manipulation benchmarks
demonstrate that LAP achieves state-of-the-art performance, outperforming
existing approaches in success rate, trajectory smoothness, and energy
efficiency, particularly in dynamic adaptation scenarios. Our approach enables
robots to perform complex interactions with human-like adaptability while
providing an expandable framework applicable to diverse robotic platforms using
the same human demonstration videos.

</details>


### [167] [Learn to Swim: Data-Driven LSTM Hydrodynamic Model for Quadruped Robot Gait Optimization](https://arxiv.org/abs/2505.03146)
*Fei Han,Pengming Guo,Hao Chen,Weikun Li,Jingbo Ren,Naijun Liu,Ning Yang,Dixia Fan*

Main category: cs.RO

TL;DR: 本文提出了基于LSTM网络的流体实验数据驱动模型（FED-LSTM），用于预测水下四足机器人的非稳态非线性水动力，并在实验数据上表现优于传统经验公式。


<details>
  <summary>Details</summary>
Motivation: 传统经验公式难以准确捕捉复杂流体动力学，尤其是在机器人直线游动和转向优化中。

Method: 通过实验数据训练FED-LSTM模型，结合NSGA-II算法优化直线和转向步态。

Result: FED-LSTM在直线游动中减少了偏差，并改善了转向时间，硬件实验验证了其精确性和稳定性。

Conclusion: 该方法为提升水下腿式机器人的游泳性能提供了可靠框架，为未来水下机器人运动研究奠定了基础。

Abstract: This paper presents a Long Short-Term Memory network-based Fluid Experiment
Data-Driven model (FED-LSTM) for predicting unsteady, nonlinear hydrodynamic
forces on the underwater quadruped robot we constructed. Trained on
experimental data from leg force and body drag tests conducted in both a
recirculating water tank and a towing tank, FED-LSTM outperforms traditional
Empirical Formulas (EF) commonly used for flow prediction over flat surfaces.
The model demonstrates superior accuracy and adaptability in capturing complex
fluid dynamics, particularly in straight-line and turning-gait optimizations
via the NSGA-II algorithm. FED-LSTM reduces deflection errors during
straight-line swimming and improves turn times without increasing the turning
radius. Hardware experiments further validate the model's precision and
stability over EF. This approach provides a robust framework for enhancing the
swimming performance of legged robots, laying the groundwork for future
advances in underwater robotic locomotion.

</details>


### [168] [Systematic Evaluation of Initial States and Exploration-Exploitation Strategies in PID Auto-Tuning: A Framework-Driven Approach Applied on Mobile Robots](https://arxiv.org/abs/2505.03159)
*Zaid Ghazal,Ali Al-Bustami,Khouloud Gaaloul,Jaerock Kwon*

Main category: cs.RO

TL;DR: 该论文研究了PID控制器自动调谐过程中初始系统状态对收敛性的影响，以及探索与开发之间的平衡问题，提出了一种评估框架，并在两种不同类型的移动机器人上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 探讨初始系统状态和探索与开发的平衡对PID自动调谐的影响，以优化控制性能。

Method: 引入新框架评估贝叶斯优化和差分进化在PID自动调谐中的作用，并在两种机器人平台上进行测试。

Result: 实验结果揭示了系统变化对收敛速度、稳定时间、上升时间和超调百分比的影响。

Conclusion: 研究为PID控制器自动调谐提供了实证基础，有助于未来该领域的进一步研究。

Abstract: PID controllers are widely used in control systems because of their
simplicity and effectiveness. Although advanced optimization techniques such as
Bayesian Optimization and Differential Evolution have been applied to address
the challenges of automatic tuning of PID controllers, the influence of initial
system states on convergence and the balance between exploration and
exploitation remains underexplored. Moreover, experimenting the influence
directly on real cyber-physical systems such as mobile robots is crucial for
deriving realistic insights. In the present paper, a novel framework is
introduced to evaluate the impact of systematically varying these factors on
the PID auto-tuning processes that utilize Bayesian Optimization and
Differential Evolution. Testing was conducted on two distinct PID-controlled
robotic platforms, an omnidirectional robot and a differential drive mobile
robot, to assess the effects on convergence rate, settling time, rise time, and
overshoot percentage. As a result, the experimental outcomes yield evidence on
the effects of the systematic variations, thereby providing an empirical basis
for future research studies in the field.

</details>


### [169] [Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets](https://arxiv.org/abs/2505.03174)
*Guillermo Roque,Erika Maquiling,Jose Giovanni Tapia Lopez,Ross Greer*

Main category: cs.RO

TL;DR: This paper explores using GPS and NLP to automatically generate instruction-action (IA) data pairs for training autonomous vehicles, reducing human annotation costs and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of IA data for robotic systems is costly and inefficient, so the authors seek an automated solution using GPS and NLP.

Method: Collect GPS voice instructions and video data to form vision-language-action triads, then categorize instructions into eight classes using their prototype ADVLAT-Engine.

Result: Demonstrated successful automated collection and categorization of diverse IA pairs, highlighting scalability and cost reduction.

Conclusion: Automated IA data generation via GPS/NLP has potential to accelerate high-quality dataset creation for vision-language-action models in autonomous systems.

Abstract: Instruction-Action (IA) data pairs are valuable for training robotic systems,
especially autonomous vehicles (AVs), but having humans manually annotate this
data is costly and time-inefficient. This paper explores the potential of using
mobile application Global Positioning System (GPS) references and Natural
Language Processing (NLP) to automatically generate large volumes of IA
commands and responses without having a human generate or retroactively tag the
data. In our pilot data collection, by driving to various destinations and
collecting voice instructions from GPS applications, we demonstrate a means to
collect and categorize the diverse sets of instructions, further accompanied by
video data to form complete vision-language-action triads. We provide details
on our completely automated data collection prototype system, ADVLAT-Engine. We
characterize collected GPS voice instructions into eight different
classifications, highlighting the breadth of commands and referentialities
available for curation from freely available mobile applications. Through
research and exploration into the automation of IA data pairs using GPS
references, the potential to increase the speed and volume at which
high-quality IA datasets are created, while minimizing cost, can pave the way
for robust vision-language-action (VLA) models to serve tasks in
vision-language navigation (VLN) and human-interactive autonomous systems.

</details>


### [170] [The Unreasonable Effectiveness of Discrete-Time Gaussian Process Mixtures for Robot Policy Learning](https://arxiv.org/abs/2505.03296)
*Jan Ole von Hartz,Adrian Röfer,Joschka Boedecker,Abhinav Valada*

Main category: cs.RO

TL;DR: 论文提出了一种名为MiDiGap的新方法，用于机器人操作的灵活策略表示与模仿学习，能够在仅需5次演示的情况下从相机观测中学习，并在多样化任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为机器人操作提供一种高效、灵活的模仿学习方法，解决传统方法在样本效率、任务多样性和泛化能力上的不足。

Method: 采用混合离散时间高斯过程（MiDiGap），利用少量演示和相机观测进行学习，并开发了推理时引导工具以增强泛化能力。

Result: 在各类任务中表现优异，如RLBench任务成功率提升76%，轨迹成本降低67%，多模态任务成功率提升48%，样本效率提高20倍。

Conclusion: MiDiGap在机器人操作任务中表现出高效性和泛化能力，为模仿学习提供了新的解决方案。

Abstract: We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel
approach for flexible policy representation and imitation learning in robot
manipulation. MiDiGap enables learning from as few as five demonstrations using
only camera observations and generalizes across a wide range of challenging
tasks. It excels at long-horizon behaviors such as making coffee, highly
constrained motions such as opening doors, dynamic actions such as scooping
with a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns
these tasks on a CPU in less than a minute and scales linearly to large
datasets. We also develop a rich suite of tools for inference-time steering
using evidence such as collision signals and robot kinematic constraints. This
steering enables novel generalization capabilities, including obstacle
avoidance and cross-embodiment policy transfer. MiDiGap achieves
state-of-the-art performance on diverse few-shot manipulation benchmarks. On
constrained RLBench tasks, it improves policy success by 76 percentage points
and reduces trajectory cost by 67%. On multimodal tasks, it improves policy
success by 48 percentage points and increases sample efficiency by a factor of
20. In cross-embodiment transfer, it more than doubles policy success. We make
the code publicly available at https://midigap.cs.uni-freiburg.de.

</details>


### [171] [RIFT: Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic Simulation](https://arxiv.org/abs/2505.03344)
*Keyu Chen,Wenchao Sun,Hao Cheng,Sifa Zheng*

Main category: cs.RO

TL;DR: 论文提出了一种双阶段自动驾驶仿真框架，结合数据驱动和物理仿真，提升真实性和可控性，并通过RIFT策略优化强化学习微调过程。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶仿真中真实性与可控性难以兼顾的问题，数据驱动方法存在协变量偏移，物理仿真缺乏真实性。

Method: 采用双阶段框架：数据驱动仿真的开环模仿学习预训练，结合物理仿真的闭环强化学习微调；提出RIFT策略，保留多模态性并增强可控性。

Result: 实验表明，RIFT显著提升了生成交通场景的真实性和可控性，为自动驾驶性能评估提供了强大平台。

Conclusion: 提出的双阶段框架和RIFT策略有效解决了仿真中的真实性与可控性冲突，为自动驾驶测试提供了更可靠的模拟环境。

Abstract: Achieving both realism and controllability in interactive closed-loop traffic
simulation remains a key challenge in autonomous driving. Data-driven
simulation methods reproduce realistic trajectories but suffer from covariate
shift in closed-loop deployment, compounded by simplified dynamics models that
further reduce reliability. Conversely, physics-based simulation methods
enhance reliable and controllable closed-loop interactions but often lack
expert demonstrations, compromising realism. To address these challenges, we
introduce a dual-stage AV-centered simulation framework that conducts open-loop
imitation learning pre-training in a data-driven simulator to capture
trajectory-level realism and multimodality, followed by closed-loop
reinforcement learning fine-tuning in a physics-based simulator to enhance
controllability and mitigate covariate shift. In the fine-tuning stage, we
propose RIFT, a simple yet effective closed-loop RL fine-tuning strategy that
preserves the trajectory-level multimodality through a GRPO-style
group-relative advantage formulation, while enhancing controllability and
training stability by replacing KL regularization with the dual-clip mechanism.
Extensive experiments demonstrate that RIFT significantly improves the realism
and controllability of generated traffic scenarios, providing a robust platform
for evaluating autonomous vehicle performance in diverse and interactive
scenarios.

</details>


### [172] [Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach](https://arxiv.org/abs/2505.03702)
*Srecharan Selvam,Abhishesh Silwal,George Kanter*

Main category: cs.RO

TL;DR: 论文提出了一种混合几何-神经方法，通过自监督学习结合传统计算机视觉和神经网络，实现了88.0%的抓取成功率，显著优于纯几何（75.3%）和纯神经（60.2%）方法。


<details>
  <summary>Details</summary>
Motivation: 农业场景中叶片操作的自动化面临植物形态多变和叶片可变形的挑战，需要一种能结合领域专家知识与机器学习能力的新方法。

Method: 采用YOLOv8实例分割和RAFT-Stereo 3D深度估计构建叶片表征，结合几何特征评分管道和神经细化模块（GraspPointCNN），通过置信度加权融合动态平衡两者贡献，并利用几何管道作为专家教师生成自监督训练数据。

Result: 在受控环境和真实温室条件下，分别达到88.0%和84.7%的成功率，优于纯几何（75.3%）和纯神经方法（60.2%）。

Conclusion: 该研究为农业机器人领域提供了一种融合专家知识与机器学习的新范式，为全自动作物监测系统奠定了基础。

Abstract: Automating leaf manipulation in agricultural settings faces significant
challenges, including the variability of plant morphologies and deformable
leaves. We propose a novel hybrid geometric-neural approach for autonomous leaf
grasping that combines traditional computer vision with neural networks through
self-supervised learning. Our method integrates YOLOv8 for instance
segmentation and RAFT-Stereo for 3D depth estimation to build rich leaf
representations, which feed into both a geometric feature scoring pipeline and
a neural refinement module (GraspPointCNN). The key innovation is our
confidence-weighted fusion mechanism that dynamically balances the contribution
of each approach based on prediction certainty. Our self-supervised framework
uses the geometric pipeline as an expert teacher to automatically generate
training data. Experiments demonstrate that our approach achieves an 88.0%
success rate in controlled environments and 84.7% in real greenhouse
conditions, significantly outperforming both purely geometric (75.3%) and
neural (60.2%) methods. This work establishes a new paradigm for agricultural
robotics where domain expertise is seamlessly integrated with machine learning
capabilities, providing a foundation for fully automated crop monitoring
systems.

</details>


### [173] [AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control](https://arxiv.org/abs/2505.03738)
*Jialong Li,Xuxin Cheng,Tianshu Huang,Shiqi Yang,Ri-Zhao Qiu,Xiaolong Wang*

Main category: cs.RO

TL;DR: 摘要介绍了自适应运动优化（AMO）框架，用于解决仿人机器人高自由度和非线性动态带来的控制挑战，通过结合模拟到现实的强化学习与轨迹优化，实现实时全身控制，并在实验中验证其稳定性和扩展的工作空间。


<details>
  <summary>Details</summary>
Motivation: 解决仿人机器人在高自由度和非线性动态下的实时自适应全身控制问题，以扩展其工作空间并提升稳定性。

Method: 提出AMO框架，结合模拟到现实的强化学习（RL）与轨迹优化，构建混合数据集训练网络，实现鲁棒的自适应控制。

Result: 在29自由度的Unitree G1机器人上验证，AMO展现出优于基准方法的稳定性和工作空间扩展能力，并支持通过模仿学习实现自主任务执行。

Conclusion: AMO框架通过结合RL与轨迹优化，不仅解决了复杂动态系统的控制挑战，还展示了其多功能性和鲁棒性，为仿人机器人的实际应用提供了有效解决方案。

Abstract: Humanoid robots derive much of their dexterity from hyper-dexterous
whole-body movements, enabling tasks that require a large operational
workspace: such as picking objects off the ground. However, achieving these
capabilities on real humanoids remains challenging due to their high degrees of
freedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization
(AMO), a framework that integrates sim-to-real reinforcement learning (RL) with
trajectory optimization for real-time, adaptive whole-body control. To mitigate
distribution bias in motion imitation RL, we construct a hybrid AMO dataset and
train a network capable of robust, on-demand adaptation to potentially O.O.D.
commands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid
robot, demonstrating superior stability and an expanded workspace compared to
strong baselines. Finally, we show that AMO's consistent performance supports
autonomous task execution via imitation learning, underscoring the system's
versatility and robustness.

</details>


### [174] [Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid](https://arxiv.org/abs/2505.03694)
*Parv Kapoor,Ian Higgins,Nikhil Keetha,Jay Patrikar,Brady Moon,Zelin Ye,Yao He,Ivan Cisneros,Yaoyu Hu,Changliu Liu,Eunsuk Kang,Sebastian Scherer*

Main category: cs.RO

TL;DR: ViSafe是一种基于视觉的高速空中避碰系统，结合学习型边缘AI框架和多摄像头硬件原型，通过感知输入控制屏障函数提供安全运行时保证，并通过实验验证其在高速场景中的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了实现高密度空中交通操作中的安全保障，尤其是在资源受限的空中系统中，开发一种高效、可靠的避碰系统是必要的。

Method: ViSafe集成了学习型边缘AI框架和定制多摄像头硬件原型，利用感知输入控制屏障函数（CBF）设计和强制执行安全阈值。

Result: ViSafe在模拟和真实飞行测试中表现优异，最高避碰速度达144 km/h，为纯视觉自主避碰设立了新标准。

Conclusion: ViSafe为高速空中导航提供了一种可证明安全的解决方案，提升了安全性和操作密度。

Abstract: Assured safe-separation is essential for achieving seamless high-density
operation of airborne vehicles in a shared airspace. To equip
resource-constrained aerial systems with this safety-critical capability, we
present ViSafe, a high-speed vision-only airborne collision avoidance system.
ViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by
tightly integrating a learning-based edge-AI framework with a custom
multi-camera hardware prototype designed under SWaP-C constraints. By
leveraging perceptual input-focused control barrier functions (CBF) to design,
encode, and enforce safety thresholds, ViSafe can provide provably safe runtime
guarantees for self-separation in high-speed aerial operations. We evaluate
ViSafe's performance through an extensive test campaign involving both
simulated digital twins and real-world flight scenarios. By independently
varying agent types, closure rates, interaction geometries, and environmental
conditions (e.g., weather and lighting), we demonstrate that ViSafe
consistently ensures self-separation across diverse scenarios. In
first-of-its-kind real-world high-speed collision avoidance tests with closure
rates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous
collision avoidance, establishing a new standard for safety in high-speed
aerial navigation.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [175] [Parameter estimation for land-surface models using machine learning libraries](https://arxiv.org/abs/2505.02979)
*Ruiyue Huang,Claire E. Heaney,Maarten van Reeuwijk*

Main category: physics.ao-ph

TL;DR: 该研究使用NN4PDEs方法和PyTorch的反向传播引擎，通过合成数据集测试地表模型的参数反演。结果表明，单层土壤温度数据无法可靠估计参数，而双层数据虽能估参但无法区分感热和潜热通量。在实际应用中，结合有效反照率数据可可靠估算热导率、热容及感热-潜热联合系数，模型能准确预测辐射和热通量。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过神经网络和反向传播技术解决地表模型参数反演问题，尤其是因观测数据有限导致参数估计不准确的挑战。

Method: 使用PyTorch反向传播引擎，通过合成数据集（已知参数模拟的土壤温度时间序列）验证NN4PDEs方法的反演能力。

Result: 单层土壤温度数据无法可靠估参；双层数据可估参但无法区分感热与潜热通量。实际应用中，结合有效反照率可准确估算热导率、热容及感热-潜热联合系数，模型预测辐射和热通量效果良好。

Conclusion: NN4PDEs方法在结合多层观测数据时能有效反演地表模型参数，但需进一步解决感热与潜热通量的区分问题。实际应用中联合反照率数据可提升模型预测精度。

Abstract: The Neural Networks for Partial Differential Equations (NN4PDEs) approach is
used to determine the parameters of a simple land-surface model using PyTorch's
backpropagation engine. In order to test the inverse model, a synthetic dataset
is created by running the model in forward mode with known parameter values to
create soil temperature time series that can be used as observations for the
inverse model. We show that it is not possible to obtain a reliable parameter
estimation using a single observed soil temperature time series. Using
measurements at two depths, reliable parameter estimates can be obtained
although it is not possible to differentiate between latent and sensible heat
fluxes. We apply the inverse model to urban flux tower data in Phoenix, United
States, and show that the thermal conductivity, volumetric heat capacity, and
the combined sensible-latent heat transfer coefficient can be reliably
estimated using an observed value for the effective surface albedo. The
resulting model accurately predicts the outgoing longwave radiation, conductive
soil fluxes and the combined sensible-latent heat fluxes.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [176] [Snakemaker: Seamlessly transforming ad-hoc analyses into sustainable Snakemake workflows with generative AI](https://arxiv.org/abs/2505.02841)
*Marco Masera,Alessandro Leone,Johannes Köster,Ivan Molineris*

Main category: cs.SE

TL;DR: Snakemaker是一个利用生成式AI将非结构化代码转换为Snakemake工作流程的工具，旨在提升生物信息学软件的可重复性和可持续性。


<details>
  <summary>Details</summary>
Motivation: 生物信息学软件开发和复杂工作流的快速演变常导致工具难以持续或适应。Snakemaker旨在填补这一空白，提升计算的可重复性。

Method: Snakemaker通过跟踪终端执行记录和分析执行模式，生成符合最佳实践的Snakemake工作流。支持将Ipython Notebook转换为模块化流程，并通过聊天助手提供自然语言控制。

Result: Snakemaker能够生成高质量的Snakemake工作流，包括跟踪Conda环境、通用规则生成和循环展开。

Conclusion: 通过降低原型代码与生产级代码之间的门槛，Snakemaker解决了生物信息学研究中计算可重复性的关键问题。

Abstract: Reproducibility and sustainability present significant challenges in
bioinformatics software development, where rapidly evolving tools and complex
workflows often result in short-lived or difficult-to-adapt pipelines. This
paper introduces Snakemaker, a tool that leverages generative AI to facilitate
researchers build sustainable data analysis pipelines by converting
unstructured code into well-defined Snakemake workflows. Snakemaker
non-invasively tracks the work performed in the terminal by the researcher,
analyzes execution patterns, and generates Snakemake workflows that can be
integrated into existing pipelines. Snakemaker also supports the transformation
of monolithic Ipython Notebooks into modular Snakemake pipelines, resolving the
global state of the notebook into discrete, file-based interactions between
rules. An integrated chat assistant provides users with fine-grained control
through natural language instructions. Snakemaker generates high-quality
Snakemake workflows by adhering to the best practices, including Conda
environment tracking, generic rule generation and loop unrolling. By lowering
the barrier between prototype and production-quality code, Snakemaker addresses
a critical gap in computational reproducibility for bioinformatics research.

</details>


### [177] [The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models](https://arxiv.org/abs/2505.02931)
*Fernando Vallecillos Ruiz,Max Hort,Leon Moonen*

Main category: cs.SE

TL;DR: 论文研究了如何在自动程序修复（APR）中平衡多输出生成与多轮迭代优化，使用少量微调数据（<1%）即可显著提升修复效果。


<details>
  <summary>Details</summary>
Motivation: 旨在减少手动修复代码错误的工作量，并探索在有限补丁数量（10个/错误）下平衡两种主流APR策略（多输出生成与迭代优化）的效果。

Method: 采用三种指令调优的LLM模型（DeepSeekCoder-Instruct、Codellama-Instruct、Llama3.1-Instruct），在三种数据集规模（1K、30K、65K）和两种微调技术（全微调与LoRA）下测试，评估其在HumanEval-Java和Defects4J基准上的表现。

Result: 仅用少量微调数据（<1%）即可将可行补丁生成率提升78%；迭代策略对基础模型效果显著，复杂基准中优势更明显，但过度微调会因过拟合导致收益递减。

Conclusion: 需结合多输出生成与迭代优化的平衡策略，微调虽有效但需谨慎，迭代对复杂问题尤其重要。

Abstract: Automatic program repair (APR) aims to reduce the manual efforts required to
identify and fix errors in source code. Before the rise of LLM-based agents, a
common strategy was to increase the number of generated patches, sometimes to
the thousands, to achieve better repair results on benchmarks. More recently,
self-iterative capabilities enabled LLMs to refine patches over multiple rounds
guided by feedback. However, literature often focuses on many iterations and
disregards different numbers of outputs.
  We investigate an APR pipeline that balances these two approaches, the
generation of multiple outputs and multiple rounds of iteration, while imposing
a limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs
- DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct - to the APR
task. We further fine-tune each model on an APR dataset with three sizes (1K,
30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess
their repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.
  Our results show that by using only a fraction (<1%) of the fine-tuning
dataset, we can achieve improvements of up to 78% in the number of plausible
patches generated, challenging prior studies that reported limited gains using
Full Fine-Tuning. However, we find that exceeding certain thresholds leads to
diminishing outcomes, likely due to overfitting. Moreover, we show that base
models greatly benefit from creating patches in an iterative fashion rather
than generating them all at once. In addition, the benefit of iterative
strategies becomes more pronounced in complex benchmarks. Even fine-tuned
models, while benefiting less from iterations, still gain advantages,
particularly on complex benchmarks. The research underscores the need for
balanced APR strategies that combine multi-output generation and iterative
refinement.

</details>


### [178] [DocSpiral: A Platform for Integrated Assistive Document Annotation through Human-in-the-Spiral](https://arxiv.org/abs/2505.03214)
*Qiang Sun,Sirui Li,Tingting Bi,Du Huynh,Mark Reynolds,Yuanyi Luo,Wei Liu*

Main category: cs.SE

TL;DR: DocSpiral是一个人类参与的辅助文档注释平台，通过迭代循环减少人工干预，提高从图像文档中提取结构化数据的效率，实验显示注释时间减少至少41%。


<details>
  <summary>Details</summary>
Motivation: 由于领域特定的图像文档存在变异性且多为非机器可读文本，需要人工注释训练自动化提取系统，开发DocSpiral旨在降低这类文档处理的AI/ML模型开发门槛。

Method: 采用螺旋设计，通过文档格式标准化、综合注释界面、评估指标面板和API端点的集成工作流，实现渐进式模型训练。

Result: 实验表明平台在模型训练的三个迭代中表现稳定，注释时间减少至少41%。

Conclusion: 免费开放的DocSpiral平台有助于推动图像密集型领域（如地学与医疗）中大语言模型的应用，提升文档处理效率。

Abstract: Acquiring structured data from domain-specific, image-based documents such as
scanned reports is crucial for many downstream tasks but remains challenging
due to document variability. Many of these documents exist as images rather
than as machine-readable text, which requires human annotation to train
automated extraction systems. We present DocSpiral, the first
Human-in-the-Spiral assistive document annotation platform, designed to address
the challenge of extracting structured information from domain-specific,
image-based document collections. Our spiral design establishes an iterative
cycle in which human annotations train models that progressively require less
manual intervention. DocSpiral integrates document format normalization,
comprehensive annotation interfaces, evaluation metrics dashboard, and API
endpoints for the development of AI / ML models into a unified workflow.
Experiments demonstrate that our framework reduces annotation time by at least
41\% while showing consistent performance gains across three iterations during
model training. By making this annotation platform freely accessible, we aim to
lower barriers to AI/ML models development in document processing, facilitating
the adoption of large language models in image-based, document-intensive fields
such as geoscience and healthcare. The system is freely available at:
https://app.ai4wa.com. The demonstration video is available:
https://app.ai4wa.com/docs/docspiral/demo.

</details>


### [179] [Synthline: A Product Line Approach for Synthetic Requirements Engineering Data Generation using Large Language Models](https://arxiv.org/abs/2505.03265)
*Abdelkarim El-Hajjami,Camille Salinesi*

Main category: cs.SE

TL;DR: 该论文提出了Synthline方法，利用大型语言模型生成合成需求工程数据，以解决高质量数据集稀缺的问题。实验表明，合成数据虽然多样性不足，但与真实数据结合可显著提升模型性能，精确度提高85%，召回率翻倍。


<details>
  <summary>Details</summary>
Motivation: 现代需求工程依赖自然语言处理和机器学习技术，但高质量数据集稀缺限制了其效果。本文旨在通过生成合成数据缓解这一问题。

Method: 提出Synthline方法，利用大型语言模型系统性生成合成需求工程数据，并通过实验评估生成数据的多样性和对下游模型的训练效果。

Result: 实验显示，合成数据多样性不如真实数据，但可作为有效训练资源。合成与真实数据结合使模型精确度提升85%，召回率翻倍。

Conclusion: 基于产品线的合成数据生成能有效解决需求工程中的数据稀缺问题，提升模型性能。研究公开了实现和生成数据集以支持领域发展。

Abstract: While modern Requirements Engineering (RE) heavily relies on natural language
processing and Machine Learning (ML) techniques, their effectiveness is limited
by the scarcity of high-quality datasets. This paper introduces Synthline, a
Product Line (PL) approach that leverages Large Language Models to
systematically generate synthetic RE data for classification-based use cases.
Through an empirical evaluation conducted in the context of using ML for the
identification of requirements specification defects, we investigated both the
diversity of the generated data and its utility for training downstream models.
Our analysis reveals that while synthetic datasets exhibit less diversity than
real data, they are good enough to serve as viable training resources.
Moreover, our evaluation shows that combining synthetic and real data leads to
substantial performance improvements. Specifically, hybrid approaches achieve
up to 85% improvement in precision and a 2x increase in recall compared to
models trained exclusively on real data. These findings demonstrate the
potential of PL-based synthetic data generation to address data scarcity in RE.
We make both our implementation and generated datasets publicly available to
support reproducibility and advancement in the field.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [180] [SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation](https://arxiv.org/abs/2505.03273)
*Zhaoxi Mu,Xinyu Yang,Gang Wang*

Main category: cs.SD

TL;DR: SepALM是一种利用音频语言模型（ALM）在文本域校正和重新合成语音的创新方法，通过四个核心组件（分离器、校正器、合成器和对齐器）提升语音分离的精度和适应性。


<details>
  <summary>Details</summary>
Motivation: 解决现有语音分离技术在嘈杂和混响环境中易产生伪影或失真的问题，通过ALM在文本域的校正优化分离效果。

Method: 提出SepALM框架，结合分离、校正、合成和对齐四个模块，并引入Chain-of-Thought提示和知识蒸馏技术优化ALM训练。

Result: 实验表明，SepALM显著提高了语音分离的准确性，并在新声学环境中展现出更强的适应性。

Conclusion: SepALM通过ALM的端到端错误校正机制，有效避免了传统方法的误差累积问题，为复杂环境下的语音分离提供了新思路。

Abstract: While contemporary speech separation technologies adeptly process lengthy
mixed audio waveforms, they are frequently challenged by the intricacies of
real-world environments, including noisy and reverberant settings, which can
result in artifacts or distortions in the separated speech. To overcome these
limitations, we introduce SepALM, a pioneering approach that employs audio
language models (ALMs) to rectify and re-synthesize speech within the text
domain following preliminary separation. SepALM comprises four core components:
a separator, a corrector, a synthesizer, and an aligner. By integrating an
ALM-based end-to-end error correction mechanism, we mitigate the risk of error
accumulation and circumvent the optimization hurdles typically encountered in
conventional methods that amalgamate automatic speech recognition (ASR) with
large language models (LLMs). Additionally, we have developed Chain-of-Thought
(CoT) prompting and knowledge distillation techniques to facilitate the
reasoning and training processes of the ALM. Our experiments substantiate that
SepALM not only elevates the precision of speech separation but also markedly
bolsters adaptability in novel acoustic environments.

</details>


### [181] [A study on audio synchronous steganography detection and distributed guide inference model based on sliding spectral features and intelligent inference drive](https://arxiv.org/abs/2505.03193)
*Wei Meng*

Main category: cs.SD

TL;DR: 提出了一种基于滑动频谱特征和智能推理的同步隐写检测与分布式引导重建模型，验证了其在开放平台隐蔽通信分析中的有效性。


<details>
  <summary>Details</summary>
Motivation: 为解决传统技术在检测同步隐写中的局限性，研究基于中国南海舰队在TikTok发布的短视频样本，提出新的检测方法。

Method: 采用25毫秒滑动窗口和短时傅里叶变换（STFT）提取主频轨迹，构建同步帧检测模型（M1）和结构化模型（M2）解码有效载荷。

Result: 在音频段（36-45秒）发现低熵重复字节序列和高集中频谱能量，验证了同步帧的存在和军事通信协议特征。

Conclusion: 滑动频谱特征对同步隐写检测有效，建立的可扩展推理模型适用于隐蔽通信分析和战术引导模拟。

Abstract: With the rise of short video platforms in global communication, embedding
steganographic data in audio synchronization streams has emerged as a new
covert communication method. To address the limitations of traditional
techniques in detecting synchronized steganography, this paper proposes a
detection and distributed guidance reconstruction model based on short video
"Yupan" samples released by China's South Sea Fleet on TikTok. The method
integrates sliding spectrum feature extraction and intelligent inference
mechanisms. A 25 ms sliding window with short-time Fourier transform (STFT) is
used to extract the main frequency trajectory and construct the synchronization
frame detection model (M1), identifying a frame flag "FFFFFFFFFFFFFFFFFF80".
The subsequent 32-byte payload is decoded by a structured model (M2) to infer
distributed guidance commands. Analysis reveals a low-entropy, repetitive byte
sequence in the 36 to 45 second audio segment with highly concentrated spectral
energy, confirming the presence of synchronization frames. Although plaintext
semantics are not restored, the consistency in command field layout suggests
features of military communication protocols. The multi-segment splicing model
further shows cross-video embedding and centralized decoding capabilities. The
proposed framework validates the effectiveness of sliding spectral features for
synchronized steganography detection and builds an extensible inference model
for covert communication analysis and tactical guidance simulation on open
platforms.

</details>


### [182] [Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic Music Generation](https://arxiv.org/abs/2505.03314)
*Jincheng Zhang,György Fazekas,Charalampos Saitis*

Main category: cs.SD

TL;DR: 该论文提出了一种将符号音乐表示为图像类钢琴卷帘的方法，并引入了结合Transformer-Mamba块和可学习小波变换的新扩散模型，用于生成高质量且可控的符号音乐，效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像合成中表现出色，但其在符号音乐生成中的应用尚未充分探索，主要原因在于符号音乐的离散性使得标准扩散模型难以直接应用。

Method: 将符号音乐表示为钢琴卷帘，提出了一种包含Transformer-Mamba块和可学习小波变换的扩散模型，并结合无分类器指导生成具有目标和弦的音乐。

Result: 实验表明，该方法在音乐质量和可控性方面表现优异，超越了钢琴卷帘生成的基线模型。

Conclusion: 研究证明了扩散模型在符号音乐生成中的潜力，并提出了一种有效的方法来克服离散数据带来的挑战。

Abstract: The recent surge in the popularity of diffusion models for image synthesis
has attracted new attention to their potential for generation tasks in other
domains. However, their applications to symbolic music generation remain
largely under-explored because symbolic music is typically represented as
sequences of discrete events and standard diffusion models are not well-suited
for discrete data. We represent symbolic music as image-like pianorolls,
facilitating the use of diffusion models for the generation of symbolic music.
Moreover, this study introduces a novel diffusion model that incorporates our
proposed Transformer-Mamba block and learnable wavelet transform.
Classifier-free guidance is utilised to generate symbolic music with target
chords. Our evaluation shows that our method achieves compelling results in
terms of music quality and controllability, outperforming the strong baseline
in pianoroll generation. Our code is available at
https://github.com/jinchengzhanggg/proffusion.

</details>


### [183] [Knowledge Distillation for Speech Denoising by Latent Representation Alignment with Cosine Distance](https://arxiv.org/abs/2505.03442)
*Diep Luong,Mikko Heikkinen,Konstantinos Drossos,Tuomas Virtanen*

Main category: cs.SD

TL;DR: 提出了一种基于知识蒸馏的语音降噪方法，通过解耦教师模型的特征限制，提升学生模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法限制了学生模型的学习能力，导致在低资源设备上表现不佳，需要改进。

Method: 结合降噪自编码器框架、线性倒置瓶颈结构和余弦相似性特性，优化知识蒸馏过程。

Result: 实验显示，该方法使学生模型在性能上优于基准方法，且能适应更大的师生模型不匹配情况。

Conclusion: 提出的方法有效提升了学生模型的语音降噪能力，适用于低资源场景。

Abstract: Speech denoising is a generally adopted and impactful task, appearing in many
common and everyday-life use cases. Although there are very powerful methods
published, most of those are too complex for deployment in everyday and
low-resources computational environments, like hand-held devices, intelligent
glasses, hearing aids, etc. Knowledge distillation (KD) is a prominent way for
alleviating this complexity mismatch and is based on the
transferring/distilling of knowledge from a pre-trained complex model, the
teacher, to another less complex one, the student. Existing KD methods for
speech denoising are based on processes that potentially hamper the KD by
bounding the learning of the student to the distribution, information ordering,
and feature dimensionality learned by the teacher. In this paper, we present
and assess a method that tries to treat this issue, by exploiting the
well-known denoising-autoencoder framework, the linear inverted bottlenecks,
and the properties of the cosine similarity. We use a public dataset and
conduct repeated experiments with different mismatching scenarios between the
teacher and the student, reporting the mean and standard deviation of the
metrics of our method and another, state-of-the-art method that is used as a
baseline. Our results show that with the proposed method, the student can
perform better and can also retain greater mismatching conditions compared to
the teacher.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [184] [BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models](https://arxiv.org/abs/2505.03501)
*Zihan Wang,Hongwei Li,Rui Zhang,Wenbo Jiang,Kangjie Chen,Tianwei Zhang,Qingchuan Zhao,Guowen Xu*

Main category: cs.CR

TL;DR: 论文提出了针对大语言模型的新型后门攻击——语言后门攻击，利用语言本身作为触发条件，诱导模型生成煽动性言论。通过设计任务无关的BadLingual攻击方法，显著提升了攻击的泛化能力。实验结果验证了其有效性，同时揭示了多语言大语言模型的潜在漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着全球互联网的发展和多语言大语言模型的普及，确保大语言模型的安全性和鲁棒性变得尤为重要。恶意势力可能利用模型的特性发起特定语言群体的定向攻击，这加剧了种族歧视等社会问题。为了深入了解这些威胁并激发防御研究，研究者提出了新型的语言后门攻击方法。

Method: 论文首先实现了一个基于翻译触发的基准语言后门攻击，但由于其泛化性能差，难以应用于实际场景。为此，研究者设计了任务无关的攻击方法BadLingual，并采用PPL约束的贪婪坐标梯度搜索（PGCG）进行对抗训练，以扩展后门的决策边界，提升其跨任务泛化能力。

Result: 实验表明，基准攻击在特定任务上的攻击成功率（ASR）超过90%，但在任务无关场景下仅为37.61%。相比之下，BadLingual在任务无关场景下实现了比基准攻击高37.35%的改进效果。

Conclusion: 语言后门攻击为大语言模型安全研究提供了新视角，揭示了多语言能力的潜在漏洞。未来需要进一步研究防御方法，以增强大语言模型的鲁棒性和安全性。

Abstract: In this paper, we present a new form of backdoor attack against Large
Language Models (LLMs): lingual-backdoor attacks. The key novelty of
lingual-backdoor attacks is that the language itself serves as the trigger to
hijack the infected LLMs to generate inflammatory speech. They enable the
precise targeting of a specific language-speaking group, exacerbating racial
discrimination by malicious entities. We first implement a baseline
lingual-backdoor attack, which is carried out by poisoning a set of training
data for specific downstream tasks through translation into the trigger
language. However, this baseline attack suffers from poor task generalization
and is impractical in real-world settings. To address this challenge, we design
BadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any
downstream tasks within the chat LLMs, regardless of the specific questions of
these tasks. We design a new approach using PPL-constrained Greedy Coordinate
Gradient-based Search (PGCG) based adversarial training to expand the decision
boundary of lingual-backdoor, thereby enhancing the generalization ability of
lingual-backdoor across various tasks. We perform extensive experiments to
validate the effectiveness of our proposed attacks. Specifically, the baseline
attack achieves an ASR of over 90% on the specified tasks. However, its ASR
reaches only 37.61% across six tasks in the task-agnostic scenario. In
contrast, BadLingual brings up to 37.35% improvement over the baseline. Our
study sheds light on a new perspective of vulnerabilities in LLMs with
multilingual capabilities and is expected to promote future research on the
potential defenses to enhance the LLMs' robustness

</details>


### [185] [Adversarial Sample Generation for Anomaly Detection in Industrial Control Systems](https://arxiv.org/abs/2505.03120)
*Abdul Mustafa,Muhammad Talha Khan,Muhammad Azmi Umer,Zaki Masood,Chuadhry Mujeeb Ahmed*

Main category: cs.CR

TL;DR: 论文研究了基于机器学习的入侵检测系统对对抗样本的脆弱性，使用JSMA生成对抗样本，并在工业控制系统中验证其泛化性和可扩展性。对抗训练后的模型在真实攻击数据上达到95%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击可能利用机器学习模型的漏洞，尤其在入侵检测系统中，识别对抗样本对确保系统安全至关重要。

Method: 使用Jacobian Saliency Map Attack (JSMA) 生成对抗样本，并在工业控制系统（ICS）的实际攻击数据上验证其效果。

Result: 对抗训练后的模型在未参与训练的真实攻击数据上达到95%的检测准确率。

Conclusion: 对抗训练能有效提升入侵检测系统对对抗攻击的鲁棒性，尤其适用于工业控制系统等关键基础设施。

Abstract: Machine learning (ML)-based intrusion detection systems (IDS) are vulnerable
to adversarial attacks. It is crucial for an IDS to learn to recognize
adversarial examples before malicious entities exploit them. In this paper, we
generated adversarial samples using the Jacobian Saliency Map Attack (JSMA). We
validate the generalization and scalability of the adversarial samples to
tackle a broad range of real attacks on Industrial Control Systems (ICS). We
evaluated the impact by assessing multiple attacks generated using the proposed
method. The model trained with adversarial samples detected attacks with 95%
accuracy on real-world attack data not used during training. The study was
conducted using an operational secure water treatment (SWaT) testbed.

</details>


### [186] [Detecting Quishing Attacks with Machine Learning Techniques Through QR Code Analysis](https://arxiv.org/abs/2505.03451)
*Fouad Trad,Ali Chehab*

Main category: cs.CR

TL;DR: 论文提出了首个直接分析二维码结构和像素模式的quishing检测框架，无需提取嵌入内容，通过机器学习模型实现了高AUC性能。


<details>
  <summary>Details</summary>
Motivation: 二维码钓鱼攻击（Quishing）日益严重，现有基于URL的检测方法不仅需要提取二维码内容，还可能暴露用户于恶意内容，且无法覆盖二维码编码的其他类型数据。

Method: 生成钓鱼和良性二维码数据集，训练并评估多种机器学习模型（如XGBoost），通过特征重要性分析优化特征集。

Result: 最佳模型（XGBoost）AUC达0.9106，优化特征集后提升至0.9133，发现二维码结构特征与钓鱼风险强相关。

Conclusion: 研究证实直接分析二维码的可行性，为quishing防御奠定基础，并凸显其作为现代钓鱼防御关键层的潜力。

Abstract: The rise of QR code based phishing ("Quishing") poses a growing cybersecurity
threat, as attackers increasingly exploit QR codes to bypass traditional
phishing defenses. Existing detection methods predominantly focus on URL
analysis, which requires the extraction of the QR code payload, and may
inadvertently expose users to malicious content. Moreover, QR codes can encode
various types of data beyond URLs, such as Wi-Fi credentials and payment
information, making URL-based detection insufficient for broader security
concerns. To address these gaps, we propose the first framework for quishing
detection that directly analyzes QR code structure and pixel patterns without
extracting the embedded content. We generated a dataset of phishing and benign
QR codes and we used it to train and evaluate multiple machine learning models,
including Logistic Regression, Decision Trees, Random Forest, Naive Bayes,
LightGBM, and XGBoost. Our best-performing model (XGBoost) achieves an AUC of
0.9106, demonstrating the feasibility of QR-centric detection. Through feature
importance analysis, we identify key visual indicators of malicious intent and
refine our feature set by removing non-informative pixels, improving
performance to an AUC of 0.9133 with a reduced feature space. Our findings
reveal that the structural features of QR code correlate strongly with phishing
risk. This work establishes a foundation for quishing mitigation and highlights
the potential of direct QR analysis as a critical layer in modern phishing
defenses.

</details>


### [187] [LlamaFirewall: An open source guardrail system for building secure AI agents](https://arxiv.org/abs/2505.03574)
*Sahana Chennabasappa,Cyrus Nikolaidis,Daniel Song,David Molnar,Stephanie Ding,Shengye Wan,Spencer Whitman,Lauren Deason,Nicholas Doucette,Abraham Montilla,Alekhya Gampa,Beto de Paola,Dominik Gabi,James Crnkovich,Jean-Christophe Testud,Kat He,Rashnil Chaturvedi,Wu Zhou,Joshua Saxe*

Main category: cs.CR

TL;DR: LlamaFirewall是一个开源的安全护栏框架，旨在作为AI代理相关安全风险的最后一层防御。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的提升，AI代理可能执行高风险任务，而现有安全措施无法完全应对这些新风险，因此需要实时护栏监控来保障安全。

Method: 框架通过三个核心护栏来实现安全防护：PromptGuard 2（通用的越狱检测器）、Agent Alignment Checks（思维链审计工具）和CodeShield（在线静态分析引擎）。

Result: LlamaFirewall在防止提示注入、代理不对齐和不安全代码生成方面表现出色，尤其是PromptGuard 2展示了最先进的性能。

Conclusion: LlamaFirewall提供了一种灵活且高效的方法来缓解AI代理的安全风险，并支持开发者自定义安全规则。

Abstract: Large language models (LLMs) have evolved from simple chatbots into
autonomous agents capable of performing complex tasks such as editing
production code, orchestrating workflows, and taking higher-stakes actions
based on untrusted inputs like webpages and emails. These capabilities
introduce new security risks that existing security measures, such as model
fine-tuning or chatbot-focused guardrails, do not fully address. Given the
higher stakes and the absence of deterministic solutions to mitigate these
risks, there is a critical need for a real-time guardrail monitor to serve as a
final layer of defense, and support system level, use case specific safety
policy definition and enforcement. We introduce LlamaFirewall, an open-source
security focused guardrail framework designed to serve as a final layer of
defense against security risks associated with AI Agents. Our framework
mitigates risks such as prompt injection, agent misalignment, and insecure code
risks through three powerful guardrails: PromptGuard 2, a universal jailbreak
detector that demonstrates clear state of the art performance; Agent Alignment
Checks, a chain-of-thought auditor that inspects agent reasoning for prompt
injection and goal misalignment, which, while still experimental, shows
stronger efficacy at preventing indirect injections in general scenarios than
previously proposed approaches; and CodeShield, an online static analysis
engine that is both fast and extensible, aimed at preventing the generation of
insecure or dangerous code by coding agents. Additionally, we include
easy-to-use customizable scanners that make it possible for any developer who
can write a regular expression or an LLM prompt to quickly update an agent's
security guardrails.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [188] [An Active Inference perspective on Neurofeedback Training](https://arxiv.org/abs/2505.03308)
*Côme Annicchiarico,Fabien Lotte,Jérémie Mattout*

Main category: q-bio.NC

TL;DR: 本文提出了一个基于主动推断的计算模型，用于模拟神经反馈训练（NFT）的闭环过程，分析反馈质量和先验信念等因素对训练效果的影响。


<details>
  <summary>Details</summary>
Motivation: 由于神经反馈训练的效果差异大且机制不明确，难以验证其有效性，因此需要一个计算模型来解决这些问题。

Method: 采用主动推断（一种贝叶斯框架）模拟代理与NFT环境的交互，测试设计和个体因素对训练的影响。

Result: 模拟结果显示训练效果对反馈噪声、偏差和先验信念敏感，且完美反馈无法确保高性能。

Conclusion: 该模型为评估NFT变异性、解释实证数据和开发个性化训练协议提供了工具。

Abstract: Neurofeedback training (NFT) aims to teach self-regulation of brain activity
through real-time feedback, but suffers from highly variable outcomes and
poorly understood mechanisms, hampering its validation. To address these
issues, we propose a formal computational model of the NFT closed loop. Using
Active Inference, a Bayesian framework modelling perception, action, and
learning, we simulate agents interacting with an NFT environment. This enables
us to test the impact of design choices (e.g., feedback quality, biomarker
validity) and subject factors (e.g., prior beliefs) on training. Simulations
show that training effectiveness is sensitive to feedback noise or bias, and to
prior beliefs (highlighting the importance of guiding instructions), but also
reveal that perfect feedback is insufficient to guarantee high performance.
This approach provides a tool for assessing and predicting NFT variability,
interpret empirical data, and potentially develop personalized training
protocols.

</details>


### [189] [Binding threshold units with artificial oscillatory neurons](https://arxiv.org/abs/2505.03648)
*Vladimir Fanaskov,Ivan Oseledets*

Main category: q-bio.NC

TL;DR: 论文提出了一种理论框架，将振荡神经元与阈值单区分开，并建立了它们之间的耦合机制。从生物角度出发，振荡神经元和阈值单实现不同的神经编码，前者通过频率调制促进信息交换，后者模拟神经元放电强度。研究表明可以通过Lyapunov约束将这两种单耦合起来，形成Hopfield-Kuramoto联想记忆模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在区别振荡神经元与阈值单的编码机制，并探索它们之间的耦合，以结合双方优势，提高任务表现。

Method: 通过Lyapunov函数约束振荡神经元（广义Kuramoto模型）和阈值单（Hopfield联想记忆模型）的动态系统，实现两者耦合。

Result: 成功构建了Hopfield-Kuramoto联想记忆耦合模型，验证了振荡神经元可修正Hopfield网络的权重矩阵，并在玩具实验中实现该耦合。

Conclusion: 耦合机制为两类神经元协同工作提供了理论支持，在任务中表现优于单独使用阈值单，并在生物神经编码研究和机器学习应用中具有潜力。

Abstract: Artificial Kuramoto oscillatory neurons were recently introduced as an
alternative to threshold units. Empirical evidence suggests that oscillatory
units outperform threshold units in several tasks including unsupervised object
discovery and certain reasoning problems. The proposed coupling mechanism for
these oscillatory neurons is heterogeneous, combining a generalized Kuramoto
equation with standard coupling methods used for threshold units. In this
research note, we present a theoretical framework that clearly distinguishes
oscillatory neurons from threshold units and establishes a coupling mechanism
between them. We argue that, from a biological standpoint, oscillatory and
threshold units realise distinct aspects of neural coding: roughly, threshold
units model intensity of neuron firing, while oscillatory units facilitate
information exchange by frequency modulation. To derive interaction between
these two types of units, we constrain their dynamics by focusing on dynamical
systems that admit Lyapunov functions. For threshold units, this leads to
Hopfield associative memory model, and for oscillatory units it yields a
specific form of generalized Kuramoto model. The resulting dynamical systems
can be naturally coupled to form a Hopfield-Kuramoto associative memory model,
which also admits a Lyapunov function. Various forms of coupling are possible.
Notably, oscillatory neurons can be employed to implement a low-rank correction
to the weight matrix of a Hopfield network. This correction can be viewed
either as a form of Hebbian learning or as a popular LoRA method used for
fine-tuning of large language models. We demonstrate the practical realization
of this particular coupling through illustrative toy experiments.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [190] [Robustly Invertible Nonlinear Dynamics and the BiLipREN: Contracting Neural Models with Contracting Inverses](https://arxiv.org/abs/2505.03069)
*Yurui Zhang,Ruigang Wang,Ian R. Manchester*

Main category: eess.SY

TL;DR: 这篇论文研究了非线性动力系统的可逆性，提出了一种新的可逆循环神经网络模型BiLipREN，并探讨了其稳健性和构造方法。


<details>
  <summary>Details</summary>
Motivation: 研究非线性动力系统的可逆性及其稳健性，以解决输入扰动和初始条件不确定性下的输入重建问题。

Method: 基于收缩和增量稳定性分析，提出BiLipREN模型，确保模型及其逆都是收缩且Lipschitz的，从而稳健地重建输入序列。

Result: BiLipREN模型通过构造实现稳健可逆性，并能与正交线性系统结合构建更一般的bi-Lipschitz动态模型。

Conclusion: BiLipREN为非线性动力系统的稳健可逆性提供了一种有效方法，并通过数值示例验证了其实用性。

Abstract: We study the invertibility of nonlinear dynamical systems from the
perspective of contraction and incremental stability analysis and propose a new
invertible recurrent neural model: the BiLipREN. In particular, we consider a
nonlinear state space model to be robustly invertible if an inverse exists with
a state space realisation, and both the forward model and its inverse are
contracting, i.e. incrementally exponentially stable, and Lipschitz, i.e. have
bounded incremental gain. This property of bi-Lipschitzness implies both
robustness in the sense of sensitivity to input perturbations, as well as
robust distinguishability of different inputs from their corresponding outputs,
i.e. the inverse model robustly reconstructs the input sequence despite small
perturbations to the initial conditions and measured output. Building on this
foundation, we propose a parameterization of neural dynamic models:
bi-Lipschitz recurrent equilibrium networks (biLipREN), which are robustly
invertible by construction. Moreover, biLipRENs can be composed with orthogonal
linear systems to construct more general bi-Lipschitz dynamic models, e.g., a
nonlinear analogue of minimum-phase/all-pass (inner/outer) factorization. We
illustrate the utility of our proposed approach with numerical examples.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [191] [GeoERM: Geometry-Aware Multi-Task Representation Learning on Riemannian Manifolds](https://arxiv.org/abs/2505.02972)
*Aoran Chen,Yang Feng*

Main category: stat.ML

TL;DR: GeoERM是一种几何感知的多任务学习框架，通过在黎曼流形上优化共享表示，提升了任务异质性或对抗性环境下的稳健性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多任务学习方法在欧几里得空间中处理潜在表示矩阵，忽略了其非欧几何特性，导致在任务异质或对抗时性能下降。GeoERM旨在解决这一问题。

Method: GeoERM将共享表示嵌入自然的黎曼流形，通过黎曼梯度步和极坐标回缩操作优化，确保几何保真度。

Result: 在合成实验和 wearable-sensor 活动识别基准测试中，GeoERM显著提高了估计精度，减少了负面迁移，并在对抗性标签噪声下保持稳定。

Conclusion: GeoERM通过几何感知优化，在多任务学习中展现了优越的性能和稳健性，为复杂任务场景提供了有效解决方案。

Abstract: Multi-Task Learning (MTL) seeks to boost statistical power and learning
efficiency by discovering structure shared across related tasks.
State-of-the-art MTL representation methods, however, usually treat the latent
representation matrix as a point in ordinary Euclidean space, ignoring its
often non-Euclidean geometry, thus sacrificing robustness when tasks are
heterogeneous or even adversarial. We propose GeoERM, a geometry-aware MTL
framework that embeds the shared representation on its natural Riemannian
manifold and optimizes it via explicit manifold operations. Each training cycle
performs (i) a Riemannian gradient step that respects the intrinsic curvature
of the search space, followed by (ii) an efficient polar retraction to remain
on the manifold, guaranteeing geometric fidelity at every iteration. The
procedure applies to a broad class of matrix-factorized MTL models and retains
the same per-iteration cost as Euclidean baselines. Across a set of synthetic
experiments with task heterogeneity and on a wearable-sensor
activity-recognition benchmark, GeoERM consistently improves estimation
accuracy, reduces negative transfer, and remains stable under adversarial label
noise, outperforming leading MTL and single-task alternatives.

</details>


### [192] [Modeling Spatial Extremes using Non-Gaussian Spatial Autoregressive Models via Convolutional Neural Networks](https://arxiv.org/abs/2505.03034)
*Sweta Rai,Douglas W. Nychka,Soutir Bandyopadhyay*

Main category: stat.ML

TL;DR: 论文提出了一种空间自回归建模框架，结合广义极值分布创新项，用于处理非高斯场和极端空间行为，并通过卷积神经网络加速参数估计，应用于北美最大降水数据分析。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理大规模、非高斯且空间异质性强的网格数据时面临挑战，尤其是在填补缺失网格或模拟极端空间行为时效果不佳。

Method: 采用空间自回归模型（SAR）结合广义极值分布创新项，通过邻域观测映射到独立随机变量，并利用卷积神经网络快速估计参数。

Result: 该框架在模拟和填补网格数据时表现灵活高效，尤其适用于极端空间行为的建模，并通过神经网络大幅提升了参数估计速度。

Conclusion: 提出的方法为非高斯场和极端空间行为的建模提供了有效工具，尤其在处理大规模网格数据时展现出显著优势。

Abstract: Data derived from remote sensing or numerical simulations often have a
regular gridded structure and are large in volume, making it challenging to
find accurate spatial models that can fill in missing grid cells or simulate
the process effectively, especially in the presence of spatial heterogeneity
and heavy-tailed marginal distributions. To overcome this issue, we present a
spatial autoregressive modeling framework, which maps observations at a
location and its neighbors to independent random variables. This is a highly
flexible modeling approach and well-suited for non-Gaussian fields, providing
simpler interpretability. In particular, we consider the SAR model with
Generalized Extreme Value distribution innovations to combine the observation
at a central grid location with its neighbors, capturing extreme spatial
behavior based on the heavy-tailed innovations. While these models are fast to
simulate by exploiting the sparsity of the key matrices in the computations,
the maximum likelihood estimation of the parameters is prohibitive due to the
intractability of the likelihood, making optimization challenging. To overcome
this, we train a convolutional neural network on a large training set that
covers a useful parameter space, and then use the trained network for fast
parameter estimation. Finally, we apply this model to analyze annual maximum
precipitation data from ERA-Interim-driven Weather Research and Forecasting
(WRF) simulations, allowing us to explore its spatial extreme behavior across
North America.

</details>


### [193] [A Symbolic and Statistical Learning Framework to Discover Bioprocessing Regulatory Mechanism: Cell Culture Example](https://arxiv.org/abs/2505.03177)
*Keilung Choy,Wei Xie,Keqi Wang*

Main category: stat.ML

TL;DR: 本文提出了一种结合符号与统计学习的方法，用于识别生物过程中的关键调控机制并量化模型不确定性，通过贝叶斯学习和高效算法提升计算效率与模型精度。


<details>
  <summary>Details</summary>
Motivation: 生物过程机制建模对生物制造的智能数字孪生至关重要，但由于复杂的细胞内调控、随机系统行为和实验数据有限，仍存在挑战。

Method: 采用基于生物知识的随机微分方程和候选调控机制集合，结合贝叶斯混合模型联合学习动力学参数和调控结构，并开发了Metropolis-adjusted Langevin算法和伴随敏感性分析以提升计算效率。

Result: 与先进的贝叶斯推断方法相比，该框架在样本效率和模型选择上表现更优，实证研究显示其能恢复缺失调控机制并在数据有限条件下提高模型保真度。

Conclusion: 该框架为数据稀缺环境下生物过程建模提供了高效且稳健的解决方案，有助于推动数字孪生技术的发展。

Abstract: Bioprocess mechanistic modeling is essential for advancing intelligent
digital twin representation of biomanufacturing, yet challenges persist due to
complex intracellular regulation, stochastic system behavior, and limited
experimental data. This paper introduces a symbolic and statistical learning
framework to identify key regulatory mechanisms and quantify model uncertainty.
Bioprocess dynamics is formulated with stochastic differential equations
characterizing intrinsic process variability, with a predefined set of
candidate regulatory mechanisms constructed from biological knowledge. A
Bayesian learning approach is developed, which is based on a joint learning of
kinetic parameters and regulatory structure through a formulation of the
mixture model. To enhance computational efficiency, a Metropolis-adjusted
Langevin algorithm with adjoint sensitivity analysis is developed for posterior
exploration. Compared to state-of-the-art Bayesian inference approaches, the
proposed framework achieves improved sample efficiency and robust model
selection. An empirical study demonstrates its ability to recover missing
regulatory mechanisms and improve model fidelity under data-limited conditions.

</details>


### [194] [Weighted Average Gradients for Feature Attribution](https://arxiv.org/abs/2505.03201)
*Kien Tran Duc Tuan,Tam Nguyen Trong,Son Nguyen Hoang,Khoat Than,Anh Nguyen Duc*

Main category: stat.ML

TL;DR: 该论文提出了Weighted Average Gradients (WG)方法，通过无监督评估基线适用性并选择有效基线，优于传统Expected Gradients (EG)方法，性能提升10-35%，同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 在可解释AI中，基线的选择对Integrated Gradients (IG)技术的解释结果影响很大。传统EG方法假设基线可均匀采样且权重相同，但实际基线不应等同对待。

Method: 提出WG方法，该技术无监督评估基线适用性并选择有效基线，通过加权平均计算贡献，满足解释方法的理论要求且稳定性更高。

Result: 实验表明WG在多种场景下优于EG，性能提升10-35%，且能通过筛选有效基线减少计算成本，保持高准确性。

Conclusion: WG是一种更优的基线选择和加权方法，提升了IG的解释效果和计算效率，代码已开源。

Abstract: In explainable AI, Integrated Gradients (IG) is a widely adopted technique
for assessing the significance of feature attributes of the input on model
outputs by evaluating contributions from a baseline input to the current input.
The choice of the baseline input significantly influences the resulting
explanation. While the traditional Expected Gradients (EG) method assumes
baselines can be uniformly sampled and averaged with equal weights, this study
argues that baselines should not be treated equivalently. We introduce Weighted
Average Gradients (WG), a novel approach that unsupervisedly evaluates baseline
suitability and incorporates a strategy for selecting effective baselines.
Theoretical analysis demonstrates that WG satisfies essential explanation
method criteria and offers greater stability than prior approaches.
Experimental results further confirm that WG outperforms EG across diverse
scenarios, achieving an improvement of 10-35\% on main metrics. Moreover, by
evaluating baselines, our method can filter a subset of effective baselines for
each input to calculate explanations, maintaining high accuracy while reducing
computational cost. The code is available at:
https://github.com/Tamnt240904/weighted_baseline.

</details>


### [195] [Lower Bounds for Greedy Teaching Set Constructions](https://arxiv.org/abs/2505.03223)
*Spencer Compton,Chirag Pabbaraju,Nikita Zhivotovskiy*

Main category: stat.ML

TL;DR: 本文研究了在有限VC维度的概念类中，贪婪算法在构造教学集时的性能下界。结果表明，对于小的k值（如k=1和k=2），算法性能有限且存在匹配下界，暗示可能需要研究高阶交互才能解决TS_min=O(d)的猜想。


<details>
  <summary>Details</summary>
Motivation: 学习理论中的一个核心问题是确定有限VC维度d的概念类C的最佳教学维度TS_min。解决这一问题将有助于验证Simon和Zilles在2015年提出的关于递归教学维度的猜想。此前的研究通过贪婪算法证明了TS_min的上界，但对其性能的下界研究不足。

Method: 本文通过分析贪婪算法在小k值（如k=1和k=2）下的性能，构造了对应的下界。特别地，对于k=1，算法无法优于基于对分法的O(log|C|)边界；对于k=2，给出了与已知上界O(log log|C|)匹配的下界。

Result: 研究结果显示，对于k≤⌈cd⌉（c为小常数），贪婪算法的性能存在显著下界，表明现有方法可能不足以达到TS_min=O(d)的目标，需要更高阶的交互分析。

Conclusion: 本文揭示了贪婪算法在小k值下的局限性，并指出解决TS_min=O(d)猜想可能需要更复杂的高阶交互研究。

Abstract: A fundamental open problem in learning theory is to characterize the
best-case teaching dimension $\operatorname{TS}_{\min}$ of a concept class
$\mathcal{C}$ with finite VC dimension $d$. Resolving this problem will, in
particular, settle the conjectured upper bound on Recursive Teaching Dimension
posed by [Simon and Zilles; COLT 2015]. Prior work used a natural greedy
algorithm to construct teaching sets recursively, thereby proving upper bounds
on $\operatorname{TS}_{\min}$, with the best known bound being $O(d^2)$ [Hu,
Wu, Li, and Wang; COLT 2017]. In each iteration, this greedy algorithm chooses
to add to the teaching set the $k$ labeled points that restrict the concept
class the most. In this work, we prove lower bounds on the performance of this
greedy approach for small $k$. Specifically, we show that for $k = 1$, the
algorithm does not improve upon the halving-based bound of
$O(\log(|\mathcal{C}|))$. Furthermore, for $k = 2$, we complement the upper
bound of $O\left(\log(\log(|\mathcal{C}|))\right)$ from [Moran, Shpilka,
Wigderson, and Yuhudayoff; FOCS 2015] with a matching lower bound. Most
consequentially, our lower bound extends up to $k \le \lceil c d \rceil$ for
small constant $c>0$: suggesting that studying higher-order interactions may be
necessary to resolve the conjecture that $\operatorname{TS}_{\min} = O(d)$.

</details>


### [196] [Decision Making under Model Misspecification: DRO with Robust Bayesian Ambiguity Sets](https://arxiv.org/abs/2505.03585)
*Charita Dellaporta,Patrick O'Hara,Theodoros Damoulas*

Main category: stat.ML

TL;DR: 提出DRO-RoBAS方法，通过结合稳健后验预测分布解决模型误设问题，在数据噪声和有限样本条件下优于传统贝叶斯和实证DRO方法。


<details>
  <summary>Details</summary>
Motivation: 针对模型误设导致决策过于保守的问题，结合贝叶斯方法提出更稳健的解决方案。

Method: 引入DRO-RoBAS，使用基于最大均值差异的模糊集，以稳健后验预测分布为中心，结合对数据生成过程的信念。

Result: 在Newsvendor和Portfolio问题上，DRO-RoBAS在样本外表现优于其他贝叶斯和实证DRO方法。

Conclusion: DRO-RoBAS有效应对模型误设，提升了决策的稳健性和性能。

Abstract: Distributionally Robust Optimisation (DRO) protects risk-averse
decision-makers by considering the worst-case risk within an ambiguity set of
distributions based on the empirical distribution or a model. To further guard
against finite, noisy data, model-based approaches admit Bayesian formulations
that propagate uncertainty from the posterior to the decision-making problem.
However, when the model is misspecified, the decision maker must stretch the
ambiguity set to contain the data-generating process (DGP), leading to overly
conservative decisions. We address this challenge by introducing DRO with
Robust, to model misspecification, Bayesian Ambiguity Sets (DRO-RoBAS). These
are Maximum Mean Discrepancy ambiguity sets centred at a robust posterior
predictive distribution that incorporates beliefs about the DGP. We show that
the resulting optimisation problem obtains a dual formulation in the
Reproducing Kernel Hilbert Space and we give probabilistic guarantees on the
tolerance level of the ambiguity set. Our method outperforms other Bayesian and
empirical DRO approaches in out-of-sample performance on the Newsvendor and
Portfolio problems with various cases of model misspecification.

</details>


### [197] [Physics-Informed Sylvester Normalizing Flows for Bayesian Inference in Magnetic Resonance Spectroscopy](https://arxiv.org/abs/2505.03590)
*Julian P. Merkofer,Dennis M. J. van de Sande,Alex A. Bhogal,Ruud J. G. van Sloun*

Main category: stat.ML

TL;DR: 本文提出了一种基于Sylvester归一化流（SNFs）的贝叶斯推断框架，用于提高磁共振波谱（MRS）中代谢物定量的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统方法如线性组合模型在代谢物定量中受限于谱重叠、低信噪比和伪影等问题，且仅提供理论精度下限。

Method: 采用Sylvester归一化流（SNFs）近似代谢物浓度的后验分布，并结合基于物理的解码器引入MRS信号形成的先验知识。

Result: 在模拟的7T质子MRS数据上验证了方法的准确性、不确定度校准良好，并揭示了参数相关性和多模态分布。

Conclusion: 该方法显著提升了代谢物定量的可靠性，为MRS分析提供了新工具。

Abstract: Magnetic resonance spectroscopy (MRS) is a non-invasive technique to measure
the metabolic composition of tissues, offering valuable insights into
neurological disorders, tumor detection, and other metabolic dysfunctions.
However, accurate metabolite quantification is hindered by challenges such as
spectral overlap, low signal-to-noise ratio, and various artifacts. Traditional
methods like linear-combination modeling are susceptible to ambiguities and
commonly only provide a theoretical lower bound on estimation accuracy in the
form of the Cram\'er-Rao bound. This work introduces a Bayesian inference
framework using Sylvester normalizing flows (SNFs) to approximate posterior
distributions over metabolite concentrations, enhancing quantification
reliability. A physics-based decoder incorporates prior knowledge of MRS signal
formation, ensuring realistic distribution representations. We validate the
method on simulated 7T proton MRS data, demonstrating accurate metabolite
quantification, well-calibrated uncertainties, and insights into parameter
correlations and multi-modal distributions.

</details>


### [198] [Weighted Random Dot Product Graphs](https://arxiv.org/abs/2505.03649)
*Bernardo Marenco,Paola Bermolen,Marcelo Fiori,Federico Larroca,Gonzalo Mateos*

Main category: stat.ML

TL;DR: 该论文扩展了随机点积图（RDPG）模型，提出了一种非参数加权（W）RDPG模型，支持权重异质分布的图分析，并提供了节点隐位置估计的统计保证和生成框架。


<details>
  <summary>Details</summary>
Motivation: 现有的网络模型无法充分捕捉权重的异质性分布，尤其是当不同权重分布具有相同的均值但高阶矩不同时。扩展RDPG模型以处理加权图的需求促成了本研究的动机。

Method: 通过为非参数加权RDPG模型分配节点隐位置序列，利用节点向量的内积定义边权重分布的矩生成函数，从而区分不同高阶矩的权重分布。

Result: 提出了WRDPG模型及其节点隐位置估计的统计保证（一致性和渐近正态性），并开发了一个生成框架以支持加权图的采样和分析。

Conclusion: WRDPG模型有效扩展了RDPG的应用范围，适用于多种网络分析场景，并通过实证案例验证了其有效性。

Abstract: Modeling of intricate relational patterns % through the analysis structures
of network data has become a cornerstone of contemporary statistical research
and related data science fields. Networks, represented as graphs, offer a
natural framework for this analysis. This paper extends the Random Dot Product
Graph (RDPG) model to accommodate weighted graphs, markedly broadening the
model's scope to scenarios where edges exhibit heterogeneous weight
distributions. We propose a nonparametric weighted (W)RDPG model that assigns a
sequence of latent positions to each node. Inner products of these nodal
vectors specify the moments of their incident edge weights' distribution via
moment-generating functions. In this way, and unlike prior art, the WRDPG can
discriminate between weight distributions that share the same mean but differ
in other higher-order moments. We derive statistical guarantees for an
estimator of the nodal's latent positions adapted from the workhorse adjacency
spectral embedding, establishing its consistency and asymptotic normality. We
also contribute a generative framework that enables sampling of graphs that
adhere to a (prescribed or data-fitted) WRDPG, facilitating, e.g., the analysis
and testing of observed graph metrics using judicious reference distributions.
The paper is organized to formalize the model's definition, the estimation (or
nodal embedding) process and its guarantees, as well as the methodologies for
generating weighted graphs, all complemented by illustrative and reproducible
examples showcasing the WRDPG's effectiveness in various network analytic
applications.

</details>


### [199] [Multi-modal cascade feature transfer for polymer property prediction](https://arxiv.org/abs/2505.03704)
*Kiichi Obuchi,Yuta Yahagi,Kiyohiko Toyama,Shukichi Tanaka,Kota Matsui*

Main category: stat.ML

TL;DR: 该论文提出了一种名为多模态级联模型的迁移学习方法，通过结合图卷积神经网络提取的化学结构特征与分子描述符等数据，提升聚合物物理性质的预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常单独使用不同类型的数据构建预测模型，无法充分利用聚合物多模态数据的潜力。作者希望通过结合多种数据特征，提高聚合物物理性质预测的准确性。

Method: 提出多模态级联模型，利用图卷积神经网络（GCN）提取化学结构特征，并将其与分子描述符、添加剂信息等其他特征结合，构建综合预测模型。

Result: 实验评估表明，该方法在多个聚合物数据集上表现优于仅使用单一特征的基线方法。

Conclusion: 该方法通过多模态特征融合有效提升了聚合物物理性质的预测性能，证明了其相比于传统单特征方法的优势。

Abstract: In this paper, we propose a novel transfer learning approach called
multi-modal cascade model with feature transfer for polymer property
prediction.Polymers are characterized by a composite of data in several
different formats, including molecular descriptors and additive information as
well as chemical structures. However, in conventional approaches, prediction
models were often constructed using each type of data separately. Our model
enables more accurate prediction of physical properties for polymers by
combining features extracted from the chemical structure by graph convolutional
neural networks (GCN) with features such as molecular descriptors and additive
information. The predictive performance of the proposed method is empirically
evaluated using several polymer datasets. We report that the proposed method
shows high predictive performance compared to the baseline conventional
approach using a single feature.

</details>


### [200] [Actor-Critics Can Achieve Optimal Sample Efficiency](https://arxiv.org/abs/2505.03710)
*Kevin Tan,Wei Fan,Yuting Wei*

Main category: stat.ML

TL;DR: 论文提出了一种新的actor-critic算法，解决了强化学习中样本复杂度难以达到O(1/ε²)的开放问题，同时在混合强化学习（Hybrid RL）中展示了离线数据的优势。


<details>
  <summary>Details</summary>
Motivation: 尽管actor-critic算法在强化学习中广泛应用，但现有方法无法在需要战略探索的通用函数逼近下实现O(1/ε²)的样本复杂度。本文旨在解决这一问题。

Method: 新算法结合了乐观探索、离策略critic估计和低频策略重置，同时在混合RL中利用离线数据提升效率。

Result: 算法实现了O(dH⁵log|A|/ε² + dH⁴log|F|/ε²)的样本复杂度和√T的遗憾，离线数据进一步降低了样本需求。

Conclusion: 该算法不仅填补了理论空白，还通过实验验证了其有效性，为混合RL提供了实用解决方案。

Abstract: Actor-critic algorithms have become a cornerstone in reinforcement learning
(RL), leveraging the strengths of both policy-based and value-based methods.
Despite recent progress in understanding their statistical efficiency, no
existing work has successfully learned an $\epsilon$-optimal policy with a
sample complexity of $O(1/\epsilon^2)$ trajectories with general function
approximation when strategic exploration is necessary.
  We address this open problem by introducing a novel actor-critic algorithm
that attains a sample-complexity of $O(dH^5 \log|\mathcal{A}|/\epsilon^2 + d
H^4 \log|\mathcal{F}|/ \epsilon^2)$ trajectories, and accompanying $\sqrt{T}$
regret when the Bellman eluder dimension $d$ does not increase with $T$ at more
than a $\log T$ rate.
  Here, $\mathcal{F}$ is the critic function class, $\mathcal{A}$ is the action
space, and $H$ is the horizon in the finite horizon MDP setting. Our algorithm
integrates optimism, off-policy critic estimation targeting the optimal
Q-function, and rare-switching policy resets.
  We extend this to the setting of Hybrid RL, showing that initializing the
critic with offline data yields sample efficiency gains compared to purely
offline or online RL. Further, utilizing access to offline data, we provide a
\textit{non-optimistic} provably efficient actor-critic algorithm that only
additionally requires $N_{\text{off}} \geq c_{\text{off}}^*dH^4/\epsilon^2$ in
exchange for omitting optimism, where $c_{\text{off}}^*$ is the single-policy
concentrability coefficient and $N_{\text{off}}$ is the number of offline
samples. This addresses another open problem in the literature. We further
provide numerical experiments to support our theoretical findings.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [201] [Physical foundations for trustworthy medical imaging: a review for artificial intelligence researchers](https://arxiv.org/abs/2505.02843)
*Miriam Cobo,David Corral Fontecha,Wilson Silva,Lara Lloret Iglesias*

Main category: eess.IV

TL;DR: 该论文探讨了医学影像中人工智能的发展，强调了物理知识在提升AI算法可信度和鲁棒性中的重要性，尤其是在数据有限的场景下。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI快速发展，但AI专业人士常缺乏对医学影像物理原理的全面理解，限制了算法潜力的发挥。

Method: 回顾医学影像物理基础及其对AI技术（如生成模型和重建算法）的影响，探索物理知识如何融入机器学习模型。

Result: 物理启发的机器学习模型能更好地学习医学影像特征，提升算法性能。

Conclusion: 物理知识的融入是增强医学影像AI可信度和鲁棒性的关键，尤其在数据有限的情况下。

Abstract: Artificial intelligence in medical imaging has seen unprecedented growth in
the last years, due to rapid advances in deep learning and computing resources.
Applications cover the full range of existing medical imaging modalities, with
unique characteristics driven by the physics of each technique. Yet, artificial
intelligence professionals entering the field, and even experienced developers,
often lack a comprehensive understanding of the physical principles underlying
medical image acquisition, which hinders their ability to fully leverage its
potential. The integration of physics knowledge into artificial intelligence
algorithms enhances their trustworthiness and robustness in medical imaging,
especially in scenarios with limited data availability. In this work, we review
the fundamentals of physics in medical images and their impact on the latest
advances in artificial intelligence, particularly, in generative models and
reconstruction algorithms. Finally, we explore the integration of physics
knowledge into physics-inspired machine learning models, which leverage
physics-based constraints to enhance the learning of medical imaging features.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [202] [Safer Prompts: Reducing IP Risk in Visual Generative AI](https://arxiv.org/abs/2505.03338)
*Lena Reissinger,Yuanyuan Li,Anna-Carolina Haensch,Neeraj Sarna*

Main category: math.NA

TL;DR: 该论文探讨了如何通过提示工程技术降低视觉生成AI模型在图像生成中的知识产权侵权风险，发现链式思维提示和任务指令提示能有效减少生成图像与训练数据的相似性。


<details>
  <summary>Details</summary>
Motivation: 由于视觉生成AI模型在多样化的图像数据上训练，存在记忆并重现特定内容的潜在风险，这可能引发知识产权侵权问题。

Method: 研究通过链式思维提示（Chain of Thought Prompting）和任务指令提示（Task Instruction Prompting）技术对扩散模型进行优化。

Result: 实验结果显示，这两种提示技术能显著降低生成图像与训练数据的相似性，从而减少知识产权侵权风险。

Conclusion: 提示工程是降低视觉生成AI模型知识产权侵权风险的有效方法，尤其是链式思维提示和任务指令提示。

Abstract: Visual Generative AI models have demonstrated remarkable capability in
generating high-quality images from simple inputs like text prompts. However,
because these models are trained on images from diverse sources, they risk
memorizing and reproducing specific content, raising concerns about
intellectual property (IP) infringement. Recent advances in prompt engineering
offer a cost-effective way to enhance generative AI performance. In this paper,
we evaluate the effectiveness of prompt engineering techniques in mitigating IP
infringement risks in image generation. Our findings show that Chain of Thought
Prompting and Task Instruction Prompting significantly reduce the similarity
between generated images and the training data of diffusion models, thereby
lowering the risk of IP infringement.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [203] [New affine invariant ensemble samplers and their dimensional scaling](https://arxiv.org/abs/2505.02987)
*Yifan Chen*

Main category: stat.CO

TL;DR: 论文提出了一种新的仿射不变集成采样器，改进了现有算法，尤其在处理高维问题时表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有采样器（如emcee包中的算法）在高维或偏态分布中表现不足，亟需更高效、适应性更强的采样方法。

Method: 1. 提出无需导数的集成侧移采样器；2. 开发基于导数的仿射不变集成HMC采样器，适用于偏态分布。

Result: 仿射不变集成HMC采样器在高斯目标分布中展现出优于标准HMC的维度扩展性，尤其在利用导数信息时效果显著。

Conclusion: 新采样器通过仿射不变性显著提升了高维和偏态分布的采样效率，为复杂概率模型提供了更优工具。

Abstract: We introduce some new affine invariant ensemble samplers that are easy to
construct and improve upon existing widely used algorithms, especially for
high-dimensional problems. Specifically, we propose a derivative-free ensemble
side move sampler that performs favorably compared to popular samplers in the
\texttt{emcee} package. Additionally, we develop a class of derivative-based
ensemble Hamiltonian Monte Carlo (HMC) samplers with affine invariance, which
outperform standard HMC without affine invariance when sampling highly skewed
distributions. We provide asymptotic scaling analysis for high-dimensional
Gaussian targets to further elucidate the properties of these affine invariant
ensemble samplers. In particular, with derivative information, the affine
invariant ensemble HMC can scale much better with dimension compared to
derivative-free ensemble samplers.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [204] [The Precautionary Principle and the Innovation Principle: Incompatible Guides for AI Innovation Governance?](https://arxiv.org/abs/2505.02846)
*Kim Kaivanto*

Main category: cs.CY

TL;DR: 论文探讨了AI治理中的预防原则与创新原则的兼容性，提出通过信号检测理论模型和监管沙盒实现两者的调和。


<details>
  <summary>Details</summary>
Motivation: 解决AI治理中预防原则（PP）与创新原则（IP）是否对立的问题，证明两者在弱形式下可协同。

Method: 引入信号检测理论（SDT）模型，分析类型I和II错误成本，提出监管沙盒作为中间策略。

Result: 监管沙盒工具能动态调整预期成本比例，避免极端红绿灯决策，实现优化监管。

Conclusion: 弱形式PP与IP可通过SDT模型和沙盒监管实现兼容，为AI治理提供灵活框架。

Abstract: In policy debates concerning the governance and regulation of Artificial
Intelligence (AI), both the Precautionary Principle (PP) and the Innovation
Principle (IP) are advocated by their respective interest groups. Do these
principles offer wholly incompatible and contradictory guidance? Does one
necessarily negate the other? I argue here that provided attention is
restricted to weak-form PP and IP, the answer to both of these questions is
"No." The essence of these weak formulations is the requirement to fully
account for type-I error costs arising from erroneously preventing the
innovation's diffusion through society (i.e. mistaken regulatory red-lighting)
as well as the type-II error costs arising from erroneously allowing the
innovation to diffuse through society (i.e. mistaken regulatory
green-lighting). Within the Signal Detection Theory (SDT) model developed here,
weak-PP red-light (weak-IP green-light) determinations are optimal for
sufficiently small (large) ratios of expected type-I to type-II error costs.
For intermediate expected cost ratios, an amber-light 'wait-and-monitor' policy
is optimal. Regulatory sandbox instruments allow AI testing and experimentation
to take place within a structured environment of limited duration and societal
scale, whereby the expected cost ratio falls within the 'wait-and-monitor'
range. Through sandboxing regulators and innovating firms learn more about the
expected cost ratio, and what respective adaptations -- of regulation, of
technical solution, of business model, or combination thereof, if any -- are
needed to keep the ratio out of the weak-PP red-light zone.

</details>


### [205] [Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration](https://arxiv.org/abs/2505.02848)
*Kexin Ding,Mu Zhou,Akshay Chaudhari,Shaoting Zhang,Dimitris N. Metaxas*

Main category: cs.CY

TL;DR: 总结: 本文探讨了如何在医疗健康领域实现大语言模型(LLM)与利益相关者需求的对接，强调人类专业人士在整个生命周期中的参与，以提升模型的实用性与可信度。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于确保LLM的输出与医疗健康领域的需求、知识和价值观对齐，以安全、高效地支持医疗工作流程。

Method: 方法包括医疗利益相关者在LLM的整个生命周期(如数据收集、模型训练和推理)中的参与，以及医疗知识整合、任务理解和人工引导技术的应用。

Result: 研究结果表明，通过适当的医疗知识整合和人工引导，LLM能更好地遵循人类价值观，增强模型与医疗需求的对齐。

Conclusion: 结论指出，未来需要加强人类与LLM之间的对齐，以开发更值得信赖的实际医疗应用。

Abstract: The wide exploration of large language models (LLMs) raises the awareness of
alignment between healthcare stakeholder preferences and model outputs. This
alignment becomes a crucial foundation to empower the healthcare workflow
effectively, safely, and responsibly. Yet the varying behaviors of LLMs may not
always match with healthcare stakeholders' knowledge, demands, and values. To
enable a human-AI alignment, healthcare stakeholders will need to perform
essential roles in guiding and enhancing the performance of LLMs. Human
professionals must participate in the entire life cycle of adopting LLM in
healthcare, including training data curation, model training, and inference. In
this review, we discuss the approaches, tools, and applications of alignments
between healthcare stakeholders and LLMs. We demonstrate that LLMs can better
follow human values by properly enhancing healthcare knowledge integration,
task understanding, and human guidance. We provide outlooks on enhancing the
alignment between humans and LLMs to build trustworthy real-world healthcare
applications.

</details>


### [206] [Enhancing tutoring systems by leveraging tailored promptings and domain knowledge with Large Language Models](https://arxiv.org/abs/2505.02849)
*Mohsen Balavar,Wenli Yang,David Herbert,Soonja Yeom*

Main category: cs.CY

TL;DR: 论文探讨了AI和机器学习在计算机辅助学习中的应用，通过结合Retrieval Augmented Generation（RAG）和大型语言模型（LLMs）开发个性化辅导系统，解决了多样学习风格和实时反馈的挑战。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决计算机辅助学习中多样学习风格和实时反馈的不足，通过AI技术提升个性化学习体验。

Method: 研究采用RAG结合LLMs的提示工程，开发了一个个性化辅导系统，并通过编程任务的定量指标进行测试。

Result: 系统成功将模拟学生按技能水平分类，并提供上下文感知反馈，比通用方法更具效果和适应性。

Conclusion: 结合RAG和LLMs的个性化辅导系统能有效提升计算机辅助学习的效果和适应性。

Abstract: Recent advancements in artificial intelligence (AI) and machine learning have
reignited interest in their impact on Computer-based Learning (CBL). AI-driven
tools like ChatGPT and Intelligent Tutoring Systems (ITS) have enhanced
learning experiences through personalisation and flexibility. ITSs can adapt to
individual learning needs and provide customised feedback based on a student's
performance, cognitive state, and learning path. Despite these advances,
challenges remain in accommodating diverse learning styles and delivering
real-time, context-aware feedback. Our research aims to address these gaps by
integrating skill-aligned feedback via Retrieval Augmented Generation (RAG)
into prompt engineering for Large Language Models (LLMs) and developing an
application to enhance learning through personalised tutoring in a computer
science programming context. The pilot study evaluated a proposed system using
three quantitative metrics: readability score, response time, and feedback
depth, across three programming tasks of varying complexity. The system
successfully sorted simulated students into three skill-level categories and
provided context-aware feedback. This targeted approach demonstrated better
effectiveness and adaptability compared to general methods.

</details>


### [207] [A Computational Model of Inclusive Pedagogy: From Understanding to Application](https://arxiv.org/abs/2505.02853)
*Francesco Balzan,Pedro P. Santos,Maurizio Gabbrielli,Mahault Albarracin,Manuel Lopes*

Main category: cs.CY

TL;DR: 论文提出了一个计算模型，模拟教师-学生互动（T-SI）中的共适应动态，旨在填补教育科学和机器学习在适应性学习支持方面的研究空白。


<details>
  <summary>Details</summary>
Motivation: 当前教育科学缺乏计算模型来测试和扩展教师-学生互动的情境洞察，同时机器学习系统难以模拟和适应性支持人类学习过程。

Method: 提出一个整合人类教育情境洞察的T-SI计算模型，并在合成课堂环境中评估不同T-SI策略，模拟不同感官信息访问能力的学生群体。

Result: 结果表明，采用共适应原则的策略（如双向主动性）优于单边策略（仅教师或学生主动），能提升所有学习类型的学习效果。

Conclusion: 该模型为教育科学的情境洞察提供了可测试和扩展的工具，同时为适应性教育AI系统奠定了基础，推动实现动态适应学习者需求的公平技术。

Abstract: Human education transcends mere knowledge transfer, it relies on
co-adaptation dynamics -- the mutual adjustment of teaching and learning
strategies between agents. Despite its centrality, computational models of
co-adaptive teacher-student interactions (T-SI) remain underdeveloped. We argue
that this gap impedes Educational Science in testing and scaling contextual
insights across diverse settings, and limits the potential of Machine Learning
systems, which struggle to emulate and adaptively support human learning
processes. To address this, we present a computational T-SI model that
integrates contextual insights on human education into a testable framework. We
use the model to evaluate diverse T-SI strategies in a realistic synthetic
classroom setting, simulating student groups with unequal access to sensory
information. Results show that strategies incorporating co-adaptation
principles (e.g., bidirectional agency) outperform unilateral approaches (i.e.,
where only the teacher or the student is active), improving the learning
outcomes for all learning types. Beyond the testing and scaling of
context-dependent educational insights, our model enables hypothesis generation
in controlled yet adaptable environments. This work bridges non-computational
theories of human education with scalable, inclusive AI in Education systems,
providing a foundation for equitable technologies that dynamically adapt to
learner needs.

</details>


### [208] [AI Education in a Mirror: Challenges Faced by Academic and Industry Experts](https://arxiv.org/abs/2505.02856)
*Mahir Akgun,Hadi Hosseini*

Main category: cs.CY

TL;DR: 本研究探讨了AI教育与实际工业挑战之间的差距，通过采访14位专家（8位来自工业界，6位来自学术界），识别了数据质量、模型可扩展性等关键挑战，并提出了改进AI课程的建议。


<details>
  <summary>Details</summary>
Motivation: 研究旨在弥合AI学术教育与工业实践之间的差距，以提升教育与实践的相关性和实用性。

Method: 采用半结构化访谈法，采访了14位AI专家（8位工业界，6位学术界），分析了他们在数据、模型等方面的挑战。

Result: 工业界更关注部署限制和资源问题，学术界更关注理论适应和标准化问题。两者均面临数据和模型适应的困难。

Conclusion: 建议AI课程应更多融入实际复杂性和跨学科学习，同时注重基础与伦理推理能力的培养。

Abstract: As Artificial Intelligence (AI) technologies continue to evolve, the gap
between academic AI education and real-world industry challenges remains an
important area of investigation. This study provides preliminary insights into
challenges AI professionals encounter in both academia and industry, based on
semi-structured interviews with 14 AI experts - eight from industry and six
from academia. We identify key challenges related to data quality and
availability, model scalability, practical constraints, user behavior, and
explainability. While both groups experience data and model adaptation
difficulties, industry professionals more frequently highlight deployment
constraints, resource limitations, and external dependencies, whereas academics
emphasize theoretical adaptation and standardization issues. These exploratory
findings suggest that AI curricula could better integrate real-world
complexities, software engineering principles, and interdisciplinary learning,
while recognizing the broader educational goals of building foundational and
ethical reasoning skills.

</details>


### [209] [Understanding University Students' Use of Generative AI: The Roles of Demographics and Personality Traits](https://arxiv.org/abs/2505.02863)
*Newnew Deng,Edward Jiusi Liu,Xiaoming Zhai*

Main category: cs.CY

TL;DR: 大学生使用生成式AI（GAI）的现象日益普遍，但相关实证研究较少。本研究调查了363名美国本科生和研究生的GAI使用情况及其影响因素（人口统计学和人格特质）。发现高年级、非英语母语者、亚裔和黑人对GAI接受度更高，且人格特质（如尽责性、宜人性等）显著影响使用态度。呼吁高校提供个性化指导。


<details>
  <summary>Details</summary>
Motivation: 填补关于大学生GAI使用及其影响因素的实证研究空白，帮助高校更好地引导学生的AI使用行为。

Method: 通过对363名美国本科生和研究生进行问卷调查，分析其GAI使用行为与人口统计学变量、大五人格特质之间的关系。

Result: 高年级、非英语母语者、亚裔和黑人对GAI接受度更高；人格特质（如尽责性低、宜人性低或外向性高等）显著影响GAI的使用态度和偏好。

Conclusion: 高校需根据学生个体差异（如人格、语言背景）提供针对性指导，以确保GAI在学术中的有效、道德和公平使用。

Abstract: The use of generative AI (GAI) among university students is rapidly
increasing, yet empirical research on students' GAI use and the factors
influencing it remains limited. To address this gap, we surveyed 363
undergraduate and graduate students in the United States, examining their GAI
usage and how it relates to demographic variables and personality traits based
on the Big Five model (i.e., extraversion, agreeableness, conscientiousness,
and emotional stability, and intellect/imagination). Our findings reveal: (a)
Students in higher academic years are more inclined to use GAI and prefer it
over traditional resources. (b) Non-native English speakers use and adopt GAI
more readily than native speakers. (c) Compared to White, Asian students report
higher GAI usage, perceive greater academic benefits, and express a stronger
preference for it. Similarly, Black students report a more positive impact of
GAI on their academic performance. Personality traits also play a significant
role in shaping perceptions and usage of GAI. After controlling demographic
factors, we found that personality still significantly predicts GAI use and
attitudes: (a) Students with higher conscientiousness use GAI less. (b)
Students who are higher in agreeableness perceive a less positive impact of GAI
on academic performance and express more ethical concerns about using it for
academic work. (c) Students with higher emotional stability report a more
positive impact of GAI on learning and fewer concerns about its academic use.
(d) Students with higher extraversion show a stronger preference for GAI over
traditional resources. (e) Students with higher intellect/imagination tend to
prefer traditional resources. These insights highlight the need for
universities to provide personalized guidance to ensure students use GAI
effectively, ethically, and equitably in their academic pursuits.

</details>


### [210] [The Cognitive Foundations of Economic Exchange: A Modular Framework Grounded in Behavioral Evidence](https://arxiv.org/abs/2505.02945)
*Egil Diau*

Main category: cs.CY

TL;DR: 论文提出了一种基于三种认知最小机制（个体识别、互相信任和成本回报敏感性）的概念框架，用于解决多智能体AI中社会合作的建模问题。


<details>
  <summary>Details</summary>
Motivation: 经济学和伦理学中许多基础概念（如“信任”或“道德”）通常定义模糊，缺乏操作标准或认知基础，限制了其在人工代理中的可测试性和实现。

Method: 通过整合灵长类行为、婴儿认知和经济人类学的实证证据，提出了一个包含三种认知最小机制的概念框架。

Result: 该框架将信任重新定义为一种分级认知预期，为人工代理中的互惠交换提供了可模拟的基础，并实现了自下而上涌现的可扩展合作和制度动态。

Conclusion: 提出的框架为多智能体AI中的社会合作建模提供了一种新的、认知基础扎实的方法。

Abstract: A key challenge in multi-agent AI is modeling social cooperation under
realistic behavioral constraints. Many foundational concepts in economics and
ethics such as "trust" or "morality" are often defined informally, without
operational criteria or cognitive grounding, which limits their testability and
implementation in artificial agents. Drawing on converging empirical evidence
from primate behavior, infant cognition, and economic anthropology, we propose
a conceptual framework composed of three cognitively minimal mechanisms:
individual recognition, reciprocal credence, and cost return sensitivity. This
framework reframes trust as a graded cognitive expectation, providing a
simulateable basis for reciprocal exchange in artificial agents, and enabling
the bottom-up emergence of scalable cooperation and institutional dynamics.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [211] [HMAE: Self-Supervised Few-Shot Learning for Quantum Spin Systems](https://arxiv.org/abs/2505.03140)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: quant-ph

TL;DR: HMAE是一种自监督框架，通过物理信息掩码预训练Transformer，显著提升量子系统的分类和预测性能，仅需少量标注数据。


<details>
  <summary>Details</summary>
Motivation: 解决量子机器学习中标注数据稀缺和计算模拟昂贵的问题。

Method: 提出Hamiltonian-Masked Autoencoding (HMAE)，基于量子信息理论选择性掩码哈密顿量项。

Result: 在12500个量子哈密顿量上，HMAE实现85.3%的相分类准确率和0.15 eV的基态能量预测误差，优于基线方法。

Conclusion: HMAE具有卓越的样本效率，但受限于小规模量子系统（12-20量子比特）。

Abstract: Quantum machine learning for spin and molecular systems faces critical
challenges of scarce labeled data and computationally expensive simulations. To
address these limitations, we introduce Hamiltonian-Masked Autoencoding (HMAE),
a novel self-supervised framework that pre-trains transformers on unlabeled
quantum Hamiltonians, enabling efficient few-shot transfer learning. Unlike
random masking approaches, HMAE employs a physics-informed strategy based on
quantum information theory to selectively mask Hamiltonian terms based on their
physical significance. Experiments on 12,500 quantum Hamiltonians (60%
real-world, 40% synthetic) demonstrate that HMAE achieves 85.3% $\pm$ 1.5%
accuracy in phase classification and 0.15 $\pm$ 0.02 eV MAE in ground state
energy prediction with merely 10 labeled examples - a statistically significant
improvement (p < 0.01) over classical graph neural networks (78.1% $\pm$ 2.1%)
and quantum neural networks (76.8% $\pm$ 2.3%). Our method's primary advantage
is exceptional sample efficiency - reducing required labeled examples by 3-5x
compared to baseline methods - though we emphasize that ground truth values for
fine-tuning and evaluation still require exact diagonalization or tensor
networks. We explicitly acknowledge that our current approach is limited to
small quantum systems (specifically limited to 12 qubits during training, with
limited extension to 16-20 qubits in testing) and that, while promising within
this regime, this size restriction prevents immediate application to larger
systems of practical interest in materials science and quantum chemistry.

</details>


### [212] [Quantum Feature Space of a Qubit Coupled to an Arbitrary Bath](https://arxiv.org/abs/2505.03397)
*Chris Wise,Akram Youssry,Alberto Peruzzo,Jo Plested,Matt Woolley*

Main category: quant-ph

TL;DR: 该论文提出了一种更高效的参数化方法（量子特征空间）来替代复杂的深度神经网络方法，用于分类和表征量子比特噪声过程，并展示了其在噪声分类和控制脉冲参数映射中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖复杂的深度神经网络与物理学编码层的结合，不仅结构复杂，还难以扩展和实时操作。作者旨在简化这一过程，提出更高效的参数化方法。

Method: 通过定义量子特征空间来参数化噪声操作符，利用欧几里得距离进行噪声过程分类，并采用随机森林等简单机器学习算法验证其分类效果。

Result: 量子特征空间能有效分类噪声的平稳性和类型，并能映射控制脉冲参数，展示了其在实际应用中的潜力。

Conclusion: 量子特征空间提供了一种高效、简化的噪声表征和分类方法，优于传统复杂的神经网络方法，且易于扩展和实时操作。

Abstract: Qubit control protocols have traditionally leveraged a characterisation of
the qubit-bath coupling via its power spectral density. Previous work proposed
the inference of noise operators that characterise the influence of a classical
bath using a grey-box approach that combines deep neural networks with
physics-encoded layers. This overall structure is complex and poses challenges
in scaling and real-time operations. Here, we show that no expensive neural
networks are needed and that this noise operator description admits an
efficient parameterisation. We refer to the resulting parameter space as the
\textit{quantum feature space} of the qubit dynamics resulting from the coupled
bath. We show that the Euclidean distance defined over the quantum feature
space provides an effective method for classifying noise processes in the
presence of a given set of controls. Using the quantum feature space as the
input space for a simple machine learning algorithm (random forest, in this
case), we demonstrate that it can effectively classify the stationarity and the
broad class of noise processes perturbing a qubit. Finally, we explore how
control pulse parameters map to the quantum feature space.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [213] [Soft Best-of-n Sampling for Model Alignment](https://arxiv.org/abs/2505.03156)
*Claudio Mayrink Verdun,Alex Oesterling,Himabindu Lakkaraju,Flavio P. Calmon*

Main category: cs.IT

TL;DR: Soft Best-of-$n$采样是一种通过温度参数平滑控制原始分布与奖励最大化分布之间过渡的方法，相比BoN采样在理论和实践中表现更优。


<details>
  <summary>Details</summary>
Motivation: BoN采样虽无需微调即可对齐语言模型与人类偏好，但其通过样本数量粗糙控制KL散度，导致效率不高且理论支持不足。

Method: 引入温度参数$λ$，提出Soft Best-of-$n$采样方法，理论上证明其以$O(1/n)$的KL和期望奖励速率收敛至最优倾斜分布。

Result: Soft Best-of-$n$在离散序列的加性奖励模型中表现出比BoN更低的采样偏差（$O(1/n)$），同时维持高奖励值。

Conclusion: Soft Best-of-$n$通过温度参数实现平滑过渡，理论收敛性更优，为无微调对齐任务提供了更高效的解决方案。

Abstract: Best-of-$n$ (BoN) sampling is a practical approach for aligning language
model outputs with human preferences without expensive fine-tuning. BoN
sampling is performed by generating $n$ responses to a prompt and then
selecting the sample that maximizes a reward function. BoN yields high reward
values in practice at a distortion cost, as measured by the KL-divergence
between the sampled and original distribution. This distortion is coarsely
controlled by varying the number of samples: larger $n$ yields a higher reward
at a higher distortion cost. We introduce Soft Best-of-$n$ sampling, a
generalization of BoN that allows for smooth interpolation between the original
distribution and reward-maximizing distribution through a temperature parameter
$\lambda$. We establish theoretical guarantees showing that Soft Best-of-$n$
sampling converges sharply to the optimal tilted distribution at a rate of
$O(1/n)$ in KL and the expected (relative) reward. For sequences of discrete
outputs, we analyze an additive reward model that reveals the fundamental
limitations of blockwise sampling.

</details>
