<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 63]
- [cs.LG](#cs.LG) [Total: 162]
- [cs.AI](#cs.AI) [Total: 44]
- [cs.RO](#cs.RO) [Total: 12]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.IR](#cs.IR) [Total: 6]
- [physics.data-an](#physics.data-an) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cs.SE](#cs.SE) [Total: 11]
- [eess.IV](#eess.IV) [Total: 8]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.MA](#cs.MA) [Total: 5]
- [math.OC](#math.OC) [Total: 4]
- [cs.CY](#cs.CY) [Total: 3]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [eess.SY](#eess.SY) [Total: 4]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CR](#cs.CR) [Total: 11]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.HC](#cs.HC) [Total: 4]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [math.NA](#math.NA) [Total: 2]
- [eess.AS](#eess.AS) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.CV](#cs.CV) [Total: 56]
- [stat.ML](#stat.ML) [Total: 15]
- [math.HO](#math.HO) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.SD](#cs.SD) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale](https://arxiv.org/abs/2507.08865)
*Javis AI Team,Amrendra Singh,Maulik Shah,Dharshan Sampath*

Key words: 表格提取, 键值对提取, 财务文档, Transformer, 空间嵌入

TL;DR: 提出了一种基于空间嵌入的Transformer模型Spatial ModernBERT，用于从复杂财务文档中提取表格和键值对，结合多任务分类和后处理方法提升准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 财务文档中的表格和键值对提取对审计、数据分析和自动化发票处理等业务流程至关重要，但现有方法在复杂场景下效果有限。

Method: 采用Spatial ModernBERT模型，通过三个分类头（标签、列索引、行区分）进行标记分类，并利用空间嵌入增强文本特征。预训练于PubTables-1M数据集，微调于财务文档数据集。后处理利用B-I-IB标记合并标记并重构表格布局。

Result: 模型在真实财务文档中表现出色，结合文本和空间线索实现了高精度的表格和键值对提取。

Conclusion: Spatial ModernBERT通过多任务分类和空间嵌入的有效结合，显著提升了财务文档中信息提取的准确性和实用性。

Abstract: Extracting tables and key-value pairs from financial documents is essential
for business workflows such as auditing, data analytics, and automated invoice
processing. In this work, we introduce Spatial ModernBERT-a transformer-based
model augmented with spatial embeddings-to accurately detect and extract
tabular data and key-value fields from complex financial documents. We cast the
extraction task as token classification across three heads: (1) Label Head,
classifying each token as a label (e.g., PO Number, PO Date, Item Description,
Quantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;
(3) Row Head, distinguishing the start of item rows and header rows. The model
is pretrained on the PubTables-1M dataset, then fine-tuned on a financial
document dataset, achieving robust performance through cross-entropy loss on
each classification head. We propose a post-processing method to merge tokens
using B-I-IB tagging, reconstruct the tabular layout, and extract key-value
pairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages
both textual and spatial cues, facilitating highly accurate table and key-value
extraction in real-world financial documents.

</details>


### [2] [SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems](https://arxiv.org/abs/2507.08898)
*Wenliang Shan,Michael Fu,Rui Yang,Chakkrit,Tantithamthavorn*

Key words: 多语言安全护栏, LLM, LoRA, SEALSBench, 防御成功率

TL;DR: SEALGuard提出了一种多语言安全护栏，用于提升LLM系统在多种语言中的安全对齐能力，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决现有护栏（如LlamaGuard）在低资源语言中检测不安全输入时的性能不足问题。

Method: 使用低秩适应（LoRA）将通用多语言模型转化为多语言护栏，并构建包含26万条提示的SEALSBench数据集。

Result: SEALGuard在检测多语言不安全提示时的防御成功率（DSR）比LlamaGuard提升48%，综合性能最佳。

Conclusion: SEALGuard通过有效的多语言护栏，提升了LLM系统的安全性。

Abstract: Safety alignment is critical for LLM-powered systems. While recent
LLM-powered guardrail approaches such as LlamaGuard achieve high detection
accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),
they struggle with multilingual unsafe inputs. This limitation leaves LLM
systems vulnerable to unsafe and jailbreak prompts written in low-resource
languages such as those in Southeast Asia. This paper introduces SEALGuard, a
multilingual guardrail designed to improve the safety alignment across diverse
languages. It aims to address the multilingual safety alignment gap of existing
guardrails and ensure effective filtering of unsafe and jailbreak prompts in
LLM-powered systems. We adapt a general-purpose multilingual language model
into a multilingual guardrail using low-rank adaptation (LoRA). We construct
SEALSBench, a large-scale multilingual safety alignment dataset containing over
260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.
We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on
this benchmark. Our findings show that multilingual unsafe and jailbreak
prompts substantially degrade the performance of the state-of-the-art
LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and
18%, respectively, compared to its performance on English-only prompts. In
contrast, SEALGuard outperforms existing guardrails in detecting multilingual
unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and
achieving the best DSR, precision, and F1-score. Our ablation study further
reveals the contributions of adaptation strategies and model size to the
overall performance of SEALGuard. SEALGuard advances the safety alignment of
LLM systems by introducing an effective multilingual guardrail.

</details>


### [3] [Evaluating LLMs in Medicine: A Call for Rigor, Transparency](https://arxiv.org/abs/2507.08916)
*Mahmoud Alwakeel,Aditya Nagori,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Key words: 大型语言模型,医学问答,数据集评估,临床真实性,标准化框架

TL;DR: 论文分析了当前用于评估大型语言模型（LLM）在医学问答领域表现的数据集的局限性，并提出了标准化框架的需求。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是评估现有医学问答数据集的质量，发现其在临床真实性、透明性和验证方面的不足，以推动更严谨的评估工具的开发。

Method: 通过回顾MedQA、MedMCQA、PubMedQA和MMLU等常用基准数据集，并分析医学期刊中的挑战性问题，评估其适用性。

Result: 结果显示现有数据集缺乏临床真实性和透明性，且验证不足；挑战性问题虽有一定价值，但规模小、范围窄且易被LLM训练数据覆盖。

Conclusion: 结论是需要建立标准化评估框架，并通过机构间合作开发更全面、安全的代表性数据集。

Abstract: Objectives: To evaluate the current limitations of large language models
(LLMs) in medical question answering, focusing on the quality of datasets used
for their evaluation. Materials and Methods: Widely-used benchmark datasets,
including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,
transparency, and relevance to clinical scenarios. Alternatives, such as
challenge questions in medical journals, were also analyzed to identify their
potential as unbiased evaluation tools. Results: Most existing datasets lack
clinical realism, transparency, and robust validation processes. Publicly
available challenge questions offer some benefits but are limited by their
small size, narrow scope, and exposure to LLM training. These gaps highlight
the need for secure, comprehensive, and representative datasets. Conclusion: A
standardized framework is critical for evaluating LLMs in medicine.
Collaborative efforts among institutions and policymakers are needed to ensure
datasets and methodologies are rigorous, unbiased, and reflective of clinical
complexities.

</details>


### [4] [From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation](https://arxiv.org/abs/2507.08924)
*Seokhee Hong,Sunkyoung Kim,Guijin Son,Soyeon Kim,Yeonjung Hong,Jinsik Lee*

Key words: 大型语言模型,基准测试,KMMLU-Redux,KMMLU-Pro,韩国工业知识

TL;DR: 本文介绍了两个韩国专家级基准测试KMMLU-Redux和KMMLU-Pro，用于全面评估大型语言模型在韩国工业领域的适用性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 开发能有效评估大型语言模型在现实场景中适用性的基准测试，尤其是在韩国工业领域。

Method: 重构KMMLU并清除关键错误创建KMMLU-Redux，基于韩国国家专业执照考试开发KMMLU-Pro。

Result: 实验证明这两个基准测试能全面代表韩国工业知识，数据集已公开。

Conclusion: KMMLU-Redux和KMMLU-Pro是评估大型语言模型在韩国工业领域应用的可靠工具。

Abstract: The development of Large Language Models (LLMs) requires robust benchmarks
that encompass not only academic domains but also industrial fields to
effectively evaluate their applicability in real-world scenarios. In this
paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,
reconstructed from the existing KMMLU, consists of questions from the Korean
National Technical Qualification exams, with critical errors removed to enhance
reliability. KMMLU-Pro is based on Korean National Professional Licensure exams
to reflect professional knowledge in Korea. Our experiments demonstrate that
these benchmarks comprehensively represent industrial knowledge in Korea. We
release our dataset publicly available.

</details>


### [5] [Self-Improving Model Steering](https://arxiv.org/abs/2507.08967)
*Rongyi Zhu,Yuhui Wang,Tanqiu Jiang,Jiacheng Liang,Ting Wang*

Key words: 模型引导,自改进框架,无监督学习,对比样本,LLM对齐

TL;DR: SIMS是一个自改进的模型引导框架，无需外部监督，通过自主生成和改进对比样本来实现动态模型引导。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统的模型引导方法依赖外部标注数据，限制了其适应性和效果，SIMS旨在解决这一问题。

Method: SIMS通过自改进循环生成和优化对比样本，并结合提示排名和对比采样策略。

Result: SIMS在多LLM和基准测试中表现优于现有方法，引导效果和适应性显著提升。

Conclusion: 自改进模型引导是未来LLM推理时对齐研究的潜在方向。

Abstract: Model steering represents a powerful technique that dynamically aligns large
language models (LLMs) with human preferences during inference. However,
conventional model-steering methods rely heavily on externally annotated data,
not only limiting their adaptability to varying contexts but also tethering
their effectiveness to annotation quality. In this paper, we present SIMS, the
first self-improving model-steering framework that operates without relying on
external supervision. At its core, SIMS autonomously generates and refines
contrastive samples through iterative self-improvement cycles, enabling
adaptive, context-specific steering. Additionally, SIMS employs novel
strategies, including prompt ranking and contrast sampling, to further enhance
steering efficacy. Extensive evaluation across diverse LLMs and benchmarks
demonstrates that SIMS substantially outperforms existing methods in steering
effectiveness and adaptability, highlighting self-improving model steering as a
promising direction for future research on inference-time LLM alignment.

</details>


### [6] [Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR](https://arxiv.org/abs/2507.08969)
*Drew Walker,Jennifer Love,Swati Rajwal,Isabel C Walker,Hannah LF Cooper,Abeed Sarker,Melvin Livingston III*

Key words: 电子健康记录（EHR）、污名化语言、怀疑标记、MIMIC-III、泊松回归

TL;DR: 研究发现，电子健康记录（EHR）中普遍存在对特定患者群体的污名化语言，尤其是黑人、低收入患者和精神疾病患者，且不同医疗团队成员（如护士和社会工作者）使用频率更高。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 通过分析EHR中的语言特征，揭示医疗团队中患者污名化的传播机制。

Method: 采用词汇匹配和监督学习分类器识别MIMIC-III EHR中的怀疑标记和污名标签，并通过泊松回归模型分析预测因素。

Result: 黑人、低收入患者和精神疾病患者的污名标签率更高，护士和社会工作者使用污名语言的频率较高。

Conclusion: 污名化语言在历史上被污名的患者群体中更为普遍，且由多种医疗提供者类型延续。

Abstract: Introduction: Electronic health records (EHR) are a critical medium through
which patient stigmatization is perpetuated among healthcare teams. Methods: We
identified linguistic features of doubt markers and stigmatizing labels in
MIMIC-III EHR via expanded lexicon matching and supervised learning
classifiers. Predictors of rates of linguistic features were assessed using
Poisson regression models. Results: We found higher rates of stigmatizing
labels per chart among patients who were Black or African American (RR: 1.16),
patients with Medicare/Medicaid or government-run insurance (RR: 2.46),
self-pay (RR: 2.12), and patients with a variety of stigmatizing disease and
mental health conditions. Patterns among doubt markers were similar, though
male patients had higher rates of doubt markers (RR: 1.25). We found increased
stigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),
with similar patterns of doubt markers. Discussion: Stigmatizing language
occurred at higher rates among historically stigmatized patients, perpetuated
by multiple provider types.

</details>


### [7] [Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery](https://arxiv.org/abs/2507.09011)
*Ana Chkhaidze,Reshanne R. Reeder,Connor Gag,Anastasia Kiyonaga,Seana Coulson*

Key words: Ganzflicker, 幻觉, 视觉想象, 自然语言处理

TL;DR: 研究使用自然语言处理工具分析Ganzflicker诱导幻觉的描述，发现强想象者与弱想象者在幻觉内容上存在显著差异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨不同视觉想象能力个体在Ganzflicker诱导幻觉中的体验差异。

Method: 通过分析4000多名参与者的自由文本描述，使用自然语言处理和视觉语言模型工具进行对比。

Result: 强想象者描述复杂自然内容，弱想象者报告简单几何模式；视觉语言模型能更好捕捉差异。

Conclusion: 结果可能反映早期视觉区与高阶区协调的个体差异。

Abstract: A rapidly alternating red and black display known as Ganzflicker induces
visual hallucinations that reflect the generative capacity of the visual
system. Recent proposals regarding the imagery spectrum, that is, differences
in the visual system of individuals with absent imagery, typical imagery, and
vivid imagery, suggest these differences should impact the complexity of other
internally generated visual experiences. Here, we used tools from natural
language processing to analyze free-text descriptions of hallucinations from
over 4,000 participants, asking whether people with different imagery
phenotypes see different things in their mind's eye during Ganzflicker-induced
hallucinations. Strong imagers described complex, naturalistic content, while
weak imagers reported simple geometric patterns. Embeddings from vision
language models better captured these differences than text-only language
models, and participants with stronger imagery used language with richer
sensorimotor associations. These findings may reflect individual variation in
coordination between early visual areas and higher-order regions relevant for
the imagery spectrum.

</details>


### [8] [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025)
*Chien Van Nguyen,Ruiyi Zhang,Hanieh Deilamsalehy,Puneet Mathur,Viet Dac Lai,Haoliang Wang,Jayakumar Subramanian,Ryan A. Rossi,Trung Bui,Nikos Vlassis,Franck Dernoncourt,Thien Huu Nguyen*

Key words: Lizard, Transformer, 线性化注意力, 门控机制, 长上下文生成

TL;DR: Lizard 是一个线性化框架，将预训练的 Transformer 大模型转化为支持无限上下文生成的次二次复杂度架构，解决了内存和计算瓶颈。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Transformer 大模型在长上下文场景下因二次复杂度注意力机制和不断增长的 KV 缓存导致性能受限，需要一种高效的方法来缓解这一问题。

Method: Lizard 通过引入次二次复杂度注意力机制和门控模块，结合全局上下文压缩与滑动窗口注意力，支持自适应内存控制和灵活模型设计。

Result: 实验表明，Lizard 在标准语言建模任务中几乎无损地恢复教师模型性能，并在 MMLU 和关联召回任务中显著优于此前方法。

Conclusion: Lizard 为长上下文生成提供了一种高效且灵活的解决方案，在性能和效率上均优于现有方法。

Abstract: We propose Lizard, a linearization framework that transforms pretrained
Transformer-based Large Language Models (LLMs) into flexible, subquadratic
architectures for infinite-context generation. Transformer-based LLMs face
significant memory and computational bottlenecks as context lengths increase,
due to the quadratic complexity of softmax attention and the growing key-value
(KV) cache. Lizard addresses these limitations by introducing a subquadratic
attention mechanism that closely approximates softmax attention while
preserving the output quality. Unlike previous linearization methods, which are
often limited by fixed model structures and therefore exclude gating
mechanisms, Lizard incorporates a gating module inspired by recent
state-of-the-art linear models. This enables adaptive memory control, supports
constant-memory inference, offers strong length generalization, and allows more
flexible model design. Lizard combines gated linear attention for global
context compression with sliding window attention enhanced by meta memory,
forming a hybrid mechanism that captures both long-range dependencies and
fine-grained local interactions. Moreover, we introduce a hardware-aware
algorithm that accelerates the training speed of our models. Extensive
experiments show that Lizard achieves near-lossless recovery of the teacher
model's performance across standard language modeling tasks, while
significantly outperforming previous linearization methods. On the 5-shot MMLU
benchmark, Lizard improves over prior models by 18 points and shows significant
improvements on associative recall tasks.

</details>


### [9] [ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making](https://arxiv.org/abs/2507.09037)
*Bharadwaj Ravichandran,David Joy,Paul Elliott,Brian Hu,Jadie Adams,Christopher Funk,Emily Veenhuis,Anthony Hoogs,Arslan Basharat*

Key words: 大语言模型,决策辅助,对齐,个性化,ALIGN系统

TL;DR: ALIGN系统通过提示对齐细粒度属性，动态个性化基于LLM的决策者，提供健壮的配置管理、结构化输出和可互换LLM算法，支持定性和定量分析。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: LLM作为决策辅助工具时，用户多样化的价值观和偏好需要新的对齐和个性化方法，现有工具主要关注基准测试任务，而ALIGN系统填补了这一空白。

Method: 通过提示对齐细粒度属性，ALIGN系统实现动态个性化，包括配置管理、结构化输出生成和多种算法实现，支持不同分析类型。

Result: ALIGN系统在公共意见调查和医疗分类决策两个领域进行了定量对齐分析，展示了其模块化后端和用户界面的有效性。

Conclusion: ALIGN框架开源，为可靠、负责和个性化的LLM决策者研究提供了新工具。

Abstract: Large language models (LLMs) are increasingly being used as decision aids.
However, users have diverse values and preferences that can affect their
decision-making, which requires novel methods for LLM alignment and
personalization. Existing LLM comparison tools largely focus on benchmarking
tasks, such as knowledge-based question answering. In contrast, our proposed
ALIGN system focuses on dynamic personalization of LLM-based decision-makers
through prompt-based alignment to a set of fine-grained attributes. Key
features of our system include robust configuration management, structured
output generation with reasoning, and several algorithm implementations with
swappable LLM backbones, enabling different types of analyses. Our user
interface enables a qualitative, side-by-side comparison of LLMs and their
alignment to various attributes, with a modular backend for easy algorithm
integration. Additionally, we perform a quantitative analysis comparing
alignment approaches in two different domains: demographic alignment for public
opinion surveys and value alignment for medical triage decision-making. The
entire ALIGN framework is open source and will enable new research on reliable,
responsible, and personalized LLM-based decision-makers.

</details>


### [10] [OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique](https://arxiv.org/abs/2507.09075)
*Wasi Uddin Ahmad,Somshubra Majumdar,Aleksander Ficek,Sean Narenthiran,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Vahid Noroozi,Boris Ginsburg*

Key words: Large Language Models, code generation, dataset, fine-tuning, code critique

TL;DR: 介绍了OpenCodeReasoning-II数据集，规模是目前最大的两倍，采用两阶段微调策略，提升了代码生成和评论的性能，并在LiveCodeBench中扩展了C++支持。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决代码生成和评论领域对大规模、高质量数据集的依赖问题。

Method: 使用两阶段监督微调策略，首阶段微调代码生成，第二阶段联合训练代码生成和评论模型。

Result: 微调后的模型在代码生成上达到或超过之前最佳开源模型，联合模型显著提升了竞赛编程性能。

Conclusion: OpenCodeReasoning-II数据集和微调策略成功提升了LLM在代码生成和评论方面的性能。

Abstract: Recent advancements in reasoning-based Large Language Models (LLMs),
particularly their potential through test-time scaling, have created
significant opportunities for distillation in code generation and critique.
However, progress in both areas fundamentally depends on large-scale,
high-quality datasets. In this work, we introduce OpenCodeReasoning-II, a
dataset consists of 2.5M question-solution-critique triples (approx. 35K unique
programming questions), making it nearly twice the size of the previous largest
publicly available code reasoning dataset. In this work, we employ a two-stage
supervised fine-tuning strategy. The first stage focuses on fine-tuning for
code generation, while the second stage involves the joint training of models
for both code generation and critique. Our resulting finetuned Qwen2.5-Instruct
models achieve performance in code generation that either exceeds or equals the
best prior open-weight distilled models. Notably, the integration of our code
generation and critique models leads to significant improvements in competitive
coding performance. Furthermore, we present an extension of the LiveCodeBench
benchmark to specifically support the C++ programming language, thereby
facilitating more comprehensive LLM evaluation using this benchmark.

</details>


### [11] [Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation](https://arxiv.org/abs/2507.09076)
*Jialong Mai,Xiaofen Xing,Yawei Li,Zhipeng Li,Jingyuan Xing,Xiangmin Xu*

Key words: 语音大语言模型,情感识别,动态参数记忆,上下文窗口,LoRA模块

TL;DR: 论文提出了一种动态参数记忆（DPM）机制，用于解决语音大语言模型（SLLM）在处理长音频序列时因高帧率导致的上下文窗口限制问题，通过逐句编码和临时LoRA模块存储情感上下文信息，显著提升了情感识别能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决语音大语言模型在处理高帧率语音数据时的上下文窗口受限问题，同时保留跨多轮对话的情感连续性和惯性。

Method: 提出DPM机制，结合上下文语义和句子级情感编码，通过临时LoRA模块在推理过程中逐步存储和更新情感信息。

Result: 在IEMOCAP数据集上，DPM显著提升了SLLM处理长音频的情感识别能力，达到最先进水平。

Conclusion: DPM机制有效克服了SLLM的上下文限制，为语音情感识别提供了新思路。

Abstract: Recent research has focused on applying speech large language model (SLLM) to
improve speech emotion recognition (SER). However, the inherently high frame
rate in speech modality severely limits the signal processing and understanding
capabilities of SLLM. For example, a SLLM with a 4K context window can only
process 80 seconds of audio at 50Hz feature sampling rate before reaching its
capacity limit. Input token compression methods used in SLLM overlook the
continuity and inertia of emotions across multiple conversation turns. This
paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual
semantics and sentence-level emotion encoding, enabling processing of
unlimited-length audio with limited context windows in SLLM. Specifically, DPM
progressively encodes sentence-level information and emotions into a temporary
LoRA module during inference to effectively "memorize" the contextual
information. We trained an emotion SLLM as a backbone and incorporated our DPM
into inference for emotion recognition in conversation (ERC). Experimental
results on the IEMOCAP dataset show that DPM significantly improves the emotion
recognition capabilities of SLLM when processing long audio sequences,
achieving state-of-the-art performance.

</details>


### [12] [CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards](https://arxiv.org/abs/2507.09104)
*Taolin Zhang,Maosong Cao,Alexander Lam,Songyang Zhang,Kai Chen*

Key words: LLM-as-judge, CompassJudger-2, 多领域数据, 拒绝采样, 边缘策略梯度, JudgerBenchV2

TL;DR: 提出了CompassJudger-2，一种新型通用评估模型，通过任务驱动的多领域数据策略解决现有模型的局限，并在多个基准测试中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有评估模型（LLM-as-judge）存在专业局限性强和鲁棒性不足的问题，无法进行全面评估，因此需要一种更通用和稳健的模型。

Method: 采用任务驱动的多领域数据策略，通过可验证奖励监督评估任务，利用拒绝采样引导内在批判性推理。此外，引入边缘策略梯度损失优化学习目标。

Result: CompassJudger-2在多个评估和奖励基准测试中表现优异，7B模型的评估精度与更大模型（如DeepSeek-V3等）竞争。提出了JudgerBenchV2标准化评估。

Conclusion: CompassJudger-2提升了LLM评估的鲁棒性和可扩展性，并为评估模型设立了新的性能和评估标准。

Abstract: Recently, the role of LLM-as-judge in evaluating large language models has
gained prominence. However, current judge models suffer from narrow
specialization and limited robustness, undermining their capacity for
comprehensive evaluations. In this work, we present CompassJudger-2, a novel
generalist judge model that overcomes these limitations via a task-driven,
multi-domain data curation strategy. Central to our approach is supervising
judgment tasks with verifiable rewards, guiding intrinsic critical reasoning
through rejection sampling to foster robust, generalizable judgment
capabilities. We introduce a refined learning objective with margin policy
gradient loss to enhance performance. Empirically, CompassJudger-2 achieves
superior results across multiple judge and reward benchmarks, and our 7B model
demonstrates competitive judgment accuracy with significantly larger models
like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a
comprehensive benchmark evaluating cross-domain judgment accuracy and rank
consistency to standardize judge model evaluation. These contributions advance
robust, scalable LLM judgment and establish new performance and evaluation
standards.

</details>


### [13] [OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering](https://arxiv.org/abs/2507.09155)
*Ali Vosoughi,Ayoub Shahnazari,Yufeng Xi,Zeliang Zhang,Griffin Hess,Chenliang Xu,Niaz Abdolrahim*

Key words: OPENXRD, 晶体学, GPT-4.5, 开卷问答, 小模型性能提升

TL;DR: OPENXRD是一个用于晶体学问题回答的开卷问答系统，通过GPT-4.5生成简洁支持内容提升小模型的性能。实验显示，其显著提高了模型准确性，尤其对晶体学知识有限的模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决传统扫描教材可能引发的版权问题，并通过AI生成内容填补小模型在晶体学领域的知识空白。

Method: 整合文本提示与GPT-4.5生成的简明支持内容，评估不同视觉语言模型在开卷和闭卷条件下的表现。

Result: 使用GPT-4.5生成内容的模型在准确率上有显著提升，特别是对晶体学知识有限的模型。

Conclusion: OPENXRD证明了开卷系统在材料科学中的实用性，并为科学领域NLP工具的开发奠定了基础。

Abstract: This work presents OPENXRD, an open-book pipeline designed for
crystallography question answering, which integrates textual prompts with
concise supporting content generated by GPT-4.5. Instead of using scanned
textbooks, which may lead to copyright issues, OPENXRD generates compact,
domain-specific references that help smaller models understand key concepts in
X-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217
expert-level XRD questions by comparing different vision-language models,
including GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,
under both closed-book (without supporting material) and open-book (with
supporting material) conditions. Our experimental results show significant
accuracy improvements in models that use the GPT-4.5-generated summaries,
particularly those with limited prior training in crystallography. OPENXRD uses
knowledge from larger models to fill knowledge gaps in crystallography and
shows that AI-generated texts can help smaller models reason more effectively
in scientific tasks. While the current version of OPENXRD focuses on text-based
inputs, we also explore future extensions such as adding real crystal diagrams
or diffraction patterns to improve interpretation in specialized materials
science contexts. Overall, OPENXRD shows that specialized open-book systems can
be useful in materials science and provides a foundation for broader natural
language processing (NLP) tools in critical scientific fields.

</details>


### [14] [PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning](https://arxiv.org/abs/2507.09157)
*Bhavinkumar Vinodbhai Kuwar,Bikrant Bikram Pratap Maurya,Priyanshu Gupta,Nitin Choudhury*

Key words: 

TL;DR: 该论文提出了一种轻量级模型PU-Lie，用于在Diplomacy数据集中检测欺骗性消息，解决了极端类别不平衡问题，使用PU学习目标，取得了新的最佳性能（宏F1分0.60）。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 在战略对话中检测欺骗性消息是一项高难度任务，因为语言微妙且欺骗性和真实性消息之间存在极端类别不平衡（欺骗性消息占比不到5%）。

Method: 结合冻结的BERT嵌入、可解释的语言特征和游戏特定特征以及PU学习目标，提出PU-Lie模型，适用于仅少量欺骗性消息被标注的情况。

Result: 模型在宏F1分上达到0.60的新最佳性能，同时减少了650倍以上的可训练参数。通过消融实验证明了PU学习、语言可解释性和发言者感知表示的价值。

Conclusion: 在欺骗检测任务中，准确捕捉欺骗性消息比识别真实性消息更重要，PU学习为此提供了有效解决方案。

Abstract: Detecting deception in strategic dialogues is a complex and high-stakes task
due to the subtlety of language and extreme class imbalance between deceptive
and truthful communications. In this work, we revisit deception detection in
the Diplomacy dataset, where less than 5% of messages are labeled deceptive. We
introduce a lightweight yet effective model combining frozen BERT embeddings,
interpretable linguistic and game-specific features, and a Positive-Unlabeled
(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is
tailored for situations where only a small portion of deceptive messages are
labeled, and the majority are unlabeled. Our model achieves a new best macro F1
of 0.60 while reducing trainable parameters by over 650x. Through comprehensive
evaluations and ablation studies across seven models, we demonstrate the value
of PU learning, linguistic interpretability, and speaker-aware representations.
Notably, we emphasize that in this problem setting, accurately detecting
deception is more critical than identifying truthful messages. This priority
guides our choice of PU learning, which explicitly models the rare but vital
deceptive class.

</details>


### [15] [RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking](https://arxiv.org/abs/2507.09174)
*Shuo Yang,Zijian Yu,Zhenzhe Ying,Yuqin Dai,Guoqing Wang,Jun Lan,Jinfeng Xu,Jinze Li,Edith C. H. Ngai*

Key words: 多模态虚假信息,检索增强,多智能体框架,事实核查,多媒体验证

TL;DR: 提出了一种检索增强的多智能体框架RAMA，用于验证多媒体虚假信息，通过精确查询、跨验证证据聚合和多智能体架构，显著提升性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决多模态虚假信息自动化验证的挑战，尤其是模糊或缺乏上下文的声明。

Method: RAMA整合了战略性查询生成、跨验证证据聚合和多智能体架构，利用多模态大型语言模型和提示变体。

Result: 在基准数据集上表现优异，尤其在模糊或不可能的声明验证中，通过基于检索的事实证据取得显著效果。

Conclusion: 集成网络证据和多智能体推理对可信的多媒体验证至关重要，为可扩展的事实核查解决方案铺平道路。

Abstract: The rapid proliferation of multimodal misinformation presents significant
challenges for automated fact-checking systems, especially when claims are
ambiguous or lack sufficient context. We introduce RAMA, a novel
retrieval-augmented multi-agent framework designed for verifying multimedia
misinformation. RAMA incorporates three core innovations: (1) strategic query
formulation that transforms multimodal claims into precise web search queries;
(2) cross-verification evidence aggregation from diverse, authoritative
sources; and (3) a multi-agent ensemble architecture that leverages the
complementary strengths of multiple multimodal large language models and prompt
variants. Extensive experiments demonstrate that RAMA achieves superior
performance on benchmark datasets, particularly excelling in resolving
ambiguous or improbable claims by grounding verification in retrieved factual
evidence. Our findings underscore the necessity of integrating web-based
evidence and multi-agent reasoning for trustworthy multimedia verification,
paving the way for more reliable and scalable fact-checking solutions. RAMA
will be publicly available at https://github.com/kalendsyang/RAMA.git.

</details>


### [16] [Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models](https://arxiv.org/abs/2507.09185)
*Ameen Ali,Shahar Katz,Lior Wolf,Ivan Titov*

Key words: 大规模语言模型, 剪枝, 泛化, 微调, Integrated Gradients

TL;DR: 提出了一种基于剪枝的微调方法，通过识别和剪枝与数据集特定机制相关的神经元，提升大规模语言模型的泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大规模语言模型（LLMs）通常会学习数据集特定的机制，导致在面对新任务或分布时性能下降，因此需要提升其泛化能力。

Method: 使用Integrated Gradients量化神经元对高置信度预测的影响，识别并剪枝与数据集特定机制相关的神经元，从而迫使模型依赖泛化性更强的表征。

Result: 在多项选择基准测试中，该方法显著提升了模型性能，优于之前的非剪枝适应方法。

Conclusion: 通过剪枝方法可以有效地提升LLMs的泛化能力，适应新任务或分布。

Abstract: Large language models (LLMs) often develop learned mechanisms specialized to
specific datasets, such as reliance on domain-specific correlations, which
yield high-confidence predictions without generalizable reasoning. While
beneficial in one setting, these dataset-specific mechanisms typically degrade
performance when models encounter novel tasks or distributions. In this work,
we introduce a fine-tuning approach designed to enhance generalization by
identifying and pruning neurons associated with dataset-specific mechanisms in
transformer-based LLMs. Our method employs Integrated Gradients to quantify
each neuron's influence on high-confidence predictions, pinpointing those that
disproportionately contribute to dataset-specific performance without
supporting robust, transferable reasoning. Selectively pruning these neurons
compels the model to depend on generalizable representations. Evaluated across
multiple-choice benchmarks, our pruning-based fine-tuning significantly
enhances performance, surpassing prior (non-pruning) adaptation methods.

</details>


### [17] [Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training](https://arxiv.org/abs/2507.09205)
*Leiyu Pan,Bojian Xiong,Lei Yang,Renren Jin,Shaowei Zhang,Yue Chen,Ling Shi,Jiang Zhou,Junru Wu,Zhen Wang,Jianxiang Peng,Juesi Xiao,Tianyu Dong,Zhuowen Han,Zhuo Chen,Sangjee Dondrub,Caizang Tai,Haixing Zhao,Huaque Cairang,Suonan Cairang,Rou Te,Lengben Zhaxi,Gazang Zhaxi,Zhonglin Ye,Yuhui Zheng,Chunyan Peng,Secha Jia,Pema Tashi,Cizhen Jiacuo,Pema Dorjee,Hongkai Liu,Pema Yanggon,Tsehang Dorjee,Jiaxin Han,Qiongying Hu,Jilin Man,Huanke You,Yuqi Ren,Duo La,Deyi Xiong*

Key words: 藏语, 低资源语言, 预训练语料库, 大语言模型, 生成式AI

TL;DR: 该论文通过构建最大的藏语预训练语料库，并训练多语言大模型Banzhida，显著提升了藏语的生成式AI能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 藏语作为低资源语言，现有模型中缺乏高质量训练语料，限制了其语言模型的性能。

Method: 通过整合多样数据源并设计专门的数据清洗流程，构建藏语预训练语料库；在此基础上预/后训练多语言基础模型，形成Banzhida模型。

Result: 实验表明，Banzhida在多种任务上优于开源模型和专为藏语设计的模型。

Conclusion: Banzhida显著提升了藏语的生成式AI能力，填补了低资源语言模型的空白。

Abstract: Large language models have achieved remarkable progress across many
languages. However, Tibetan, as a representative low-resource language, is
particularly underrepresented in existing models due to the scarcity of
high-quality training corpora. To address this gap, we curate the largest
Tibetan pre-training corpus to date, aggregating data from diverse sources and
applying a dedicated data cleaning and processing pipeline tailored for
Tibetan. With the curated data, we continue pre/post-training a multilingual
base model into Banzhida, a multilingual large language model that advances
generative AI for Tibetan. To evaluate the Tibetan capabilities of the model,
we create new high-quality Tibetan benchmarks, and complement them with
existing public benchmarks. Experimental results demonstrate that Banzhida
consistently and significantly outperforms both open-source models of similar
scale and Tibetan-tailored models across a wide range of tasks.

</details>


### [18] [MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis](https://arxiv.org/abs/2507.09225)
*Biagio Scalingi,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Key words: 视觉隐喻,气候变化,MetaClimage,认知负荷,美学体验

TL;DR: 研究分析了气候变化的视觉隐喻（如将冰川融化描绘为融化的手榴弹）在传播中的影响，建立了MetaClimage数据库，发现视觉隐喻虽更难理解但更具美学愉悦性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨视觉隐喻在气候变化传播中的效果，弥补过往研究材料的不足。

Method: 建立MetaClimage数据库，收集人类对图像的难度、效能、艺术质量和情感唤醒评分，并通过自然语言处理分析标签的语义和情感。

Result: 视觉隐喻比字面图像更难理解但更美观，未在效能和情感唤醒上显差异；高认知需求的参与者对隐喻的情感唤醒更高。

Conclusion: 视觉隐喻虽增加认知负荷，但可能促进更深认知加工和抽象思维，美学体验更积极。

Abstract: Visual metaphors of climate change (e.g., melting glaciers depicted as a
melting ice grenade) are regarded as valuable tools for addressing the
complexity of environmental challenges. However, few studies have examined
their impact on communication, also due to scattered availability of material.
Here, we present a novel database of Metaphors of Climate Change in Images
(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal
images and enriched with human ratings. For each image, we collected values of
difficulty, efficacy, artistic quality, and emotional arousal from human
rating, as well as number of tags generated by participants to summarize the
message. Semantic and emotion variables were further derived from the tags via
Natural Language Processing. Visual metaphors were rated as more difficult to
understand, yet more aesthetically pleasant than literal images, but did not
differ in efficacy and arousal. The latter for visual metaphors, however, was
higher in participants with higher Need For Cognition. Furthermore, visual
metaphors received more tags, often referring to entities not depicted in the
image, and elicited words with more positive valence and greater dominance than
literal images. These results evidence the greater cognitive load of visual
metaphors, which nevertheless might induce positive effects such as deeper
cognitive elaboration and abstraction compared to literal stimuli. Furthermore,
while they are not deemed as more effective and arousing, visual metaphors seem
to generate superior aesthetic appreciation and a more positively valenced
experience. Overall, this study contributes to understanding the impact of
visual metaphors of climate change both by offering a database for future
research and by elucidating a cost-benefit trade-off to take into account when
shaping environmental communication.

</details>


### [19] [Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources](https://arxiv.org/abs/2507.09245)
*Deshan Sumanathilaka,Sameera Perera,Sachithya Dharmasiri,Maneesha Athukorala,Anuja Dilrukshi Herath,Rukshan Dias,Pasindu Gamage,Ruvan Weerasinghe,Y. H. P. P. Priyadarshana*

Key words: Swa-bhasha, Romanized Sinhala, transliteration, NLP

TL;DR: Swa-bhasha Resource Hub 提供了一套全面的罗马化僧伽罗语到僧伽罗语的转写资源和算法，支持2020-2025年间的自然语言处理研究。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 推动僧伽罗语自然语言处理研究，尤其是罗马化僧伽罗语的转写模型和应用开发。

Method: 收集并公开数据资源和算法，进行比较分析。

Result: 提供了公开可用的数据集和工具，支持研究和应用开发。

Conclusion: 该资源中心对僧伽罗语NLP领域的研究和应用具有重要意义。

Abstract: The Swa-bhasha Resource Hub provides a comprehensive collection of data
resources and algorithms developed for Romanized Sinhala to Sinhala
transliteration between 2020 and 2025. These resources have played a
significant role in advancing research in Sinhala Natural Language Processing
(NLP), particularly in training transliteration models and developing
applications involving Romanized Sinhala. The current openly accessible data
sets and corresponding tools are made publicly available through this hub. This
paper presents a detailed overview of the resources contributed by the authors
and includes a comparative analysis of existing transliteration applications in
the domain.

</details>


### [20] [Psychology-Driven Enhancement of Humour Translation](https://arxiv.org/abs/2507.09259)
*Yuchen Su,Yonghua Zhu,Yang Chen,Diana Benavides-Prado,Michael Witbrock*

Key words: 幽默翻译、大语言模型、Chain-of-Thought、心理学

TL;DR: 提出了心理学启发的幽默分解机制（HDM），利用Chain-of-Thought模仿人类思维，改进LLMs的幽默翻译质量。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有大语言模型在幽默翻译中表现不佳，存在语言干扰和缺乏幽默感的问题。

Method: 通过HDM和Chain-of-Thought模仿人类思维，结合幽默理论优化翻译文本的幽默元素。

Result: 实验表明，该方法在幽默性、流畅性和连贯性上分别提升7.75%、2.81%和6.13%。

Conclusion: HDM有效提升了幽默翻译的质量。

Abstract: Humour translation plays a vital role as a bridge between different cultures,
fostering understanding and communication. Although most existing Large
Language Models (LLMs) are capable of general translation tasks, these models
still struggle with humour translation, which is especially reflected through
linguistic interference and lacking humour in translated text. In this paper,
we propose a psychology-inspired Humour Decomposition Mechanism (HDM) that
utilises Chain-of-Thought (CoT) to imitate the ability of the human thought
process, stimulating LLMs to optimise the readability of translated humorous
texts. Moreover, we integrate humour theory in HDM to further enhance the
humorous elements in the translated text. Our automatic evaluation experiments
on open-source humour datasets demonstrate that our method significantly
improves the quality of humour translation, yielding average gains of 7.75\% in
humour, 2.81\% in fluency, and 6.13\% in coherence of the generated text.

</details>


### [21] [ClaritySpeech: Dementia Obfuscation in Speech](https://arxiv.org/abs/2507.09282)
*Dominika Woszczyk,Ranya Aloufi,Soteris Demetriou*

Key words: 痴呆症,语音处理,ASR,零样本TTS,隐私保护

TL;DR: 提出了一种名为ClaritySpeech的新型框架，通过结合ASR、文本混淆和零样本TTS技术，在低数据环境中纠正痴呆症相关语音，同时保持说话者身份。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 痴呆症患者语音模式改变导致沟通障碍和隐私问题，现有语音技术难以应对。

Method: 整合ASR、文本混淆和零样本TTS，无需微调即可纠正语音并保护隐私。

Result: 在ADReSS和ADReSSo数据集上，F1分数分别下降16%和10%，WER显著改善，语音质量提升。

Conclusion: ClaritySpeech在隐私保护和语音可访问性方面表现优异。

Abstract: Dementia, a neurodegenerative disease, alters speech patterns, creating
communication barriers and raising privacy concerns. Current speech
technologies, such as automatic speech transcription (ASR), struggle with
dementia and atypical speech, further challenging accessibility. This paper
presents a novel dementia obfuscation in speech framework, ClaritySpeech,
integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to
correct dementia-affected speech while preserving speaker identity in low-data
environments without fine-tuning. Results show a 16% and 10% drop in mean F1
score across various adversarial settings and modalities (audio, text, fusion)
for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We
also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15
for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and
accessibility.

</details>


### [22] [DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models](https://arxiv.org/abs/2507.09424)
*Cathy Jiao,Yijun Pan,Emily Xiao,Daisy Sheng,Niket Jain,Hanzhang Zhao,Ishita Dasgupta,Jiaqi W. Ma,Chenyan Xiong*

Key words: 数据归因, LLM, 基准测试, 评估方法

TL;DR: DATE-LM是一个统一的基准测试，用于通过实际LLM应用评估数据归因方法，填补系统性LLM评估的空白。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 数据归因方法在LLM研究和应用中的重要性日益增加，但目前缺乏系统性的LLM评估基准。

Method: 引入DATE-LM基准，通过训练数据选择、毒性/偏见过滤和事实归因三项任务评估归因质量。

Result: 现有数据归因方法在不同任务中表现不一，存在与简单基线的权衡，且性能对任务设计敏感。

Conclusion: DATE-LM可作为未来LLM数据归因研究的基础，并发布了公共排行榜以促进社区参与。

Abstract: Data attribution methods quantify the influence of training data on model
outputs and are becoming increasingly relevant for a wide range of LLM research
and applications, including dataset curation, model interpretability, data
valuation. However, there remain critical gaps in systematic LLM-centric
evaluation of data attribution methods. To this end, we introduce DATE-LM (Data
Attribution Evaluation in Language Models), a unified benchmark for evaluating
data attribution methods through real-world LLM applications. DATE-LM measures
attribution quality through three key tasks -- training data selection,
toxicity/bias filtering, and factual attribution. Our benchmark is designed for
ease of use, enabling researchers to configure and run large-scale evaluations
across diverse tasks and LLM architectures. Furthermore, we use DATE-LM to
conduct a large-scale evaluation of existing data attribution methods. Our
findings show that no single method dominates across all tasks, data
attribution methods have trade-offs with simpler baselines, and method
performance is sensitive to task-specific evaluation design. Finally, we
release a public leaderboard for quick comparison of methods and to facilitate
community engagement. We hope DATE-LM serves as a foundation for future data
attribution research in LLMs.

</details>


### [23] [Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models](https://arxiv.org/abs/2507.09470)
*Mingchuan Yang,Ziyuan Huang*

Key words: DRAGON Longformer, 临床文本分类, 医学病例描述, 超参数优化, 自然语言处理

TL;DR: 该研究通过优化DRAGON Longformer基础模型，提升了对临床病例描述的二元分类性能，取得了显著的指标提升。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 目标是优化现有模型，以更好地处理临床文本分类任务，特别是医学病例描述的二元分类。

Method: 通过对预训练模型进行超参数调整、领域特定预处理和架构修改，包括增加序列长度、调整学习率、延长训练周期并引入医学术语。

Result: 优化后的模型准确率从72.0%提升至85.2%，其他指标（如精确率、召回率和F1分数）均有显著提升。

Conclusion: 优化后的模型在临床自然语言处理任务中表现优异，具有广泛的医疗应用潜力。

Abstract: This study explores the optimization of the DRAGON Longformer base model for
clinical text classification, specifically targeting the binary classification
of medical case descriptions. A dataset of 500 clinical cases containing
structured medical observations was used, with 400 cases for training and 100
for validation. Enhancements to the pre-trained
joeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter
tuning, domain-specific preprocessing, and architectural adjustments. Key
modifications involved increasing sequence length from 512 to 1024 tokens,
adjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5
to 8, and incorporating specialized medical terminology. The optimized model
achieved notable performance gains: accuracy improved from 72.0% to 85.2%,
precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from
71.0% to 85.2%. Statistical analysis confirmed the significance of these
improvements (p < .001). The model demonstrated enhanced capability in
interpreting medical terminology, anatomical measurements, and clinical
observations. These findings contribute to domain-specific language model
research and offer practical implications for clinical natural language
processing applications. The optimized model's strong performance across
diverse medical conditions underscores its potential for broad use in
healthcare settings.

</details>


### [24] [The CoNLL-2013 Shared Task on Grammatical Error Correction](https://arxiv.org/abs/2507.09474)
*Hwee Tou Ng,Siew Mei Wu,Yuanbin Wu,Christian Hadiwinoto,Joel Tetreault*

Key words: CoNLL-2013, 语法错误纠正, 共享任务, 评估指标

TL;DR: 该论文概述了CoNLL-2013共享任务的定义、数据集、评估指标及评分方法，总结了参与团队的多种方法，并展示了评估结果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 分析CoNLL-2013共享任务中的语法错误纠正问题，为相关研究提供参考。

Method: 介绍任务定义、数据集、评估指标及评分方法，总结参与团队的方法。

Result: 展示了共享任务的评估结果。

Conclusion: 论文总结了CoNLL-2013共享任务的各项内容和成果。

Abstract: The CoNLL-2013 shared task was devoted to grammatical error correction. In
this paper, we give the task definition, present the data sets, and describe
the evaluation metric and scorer used in the shared task. We also give an
overview of the various approaches adopted by the participating teams, and
present the evaluation results.

</details>


### [25] [Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs](https://arxiv.org/abs/2507.09477)
*Yangning Li,Weizhi Zhang,Yuyao Yang,Wei-Chieh Huang,Yaozu Wu,Junyu Luo,Yuanchen Bei,Henry Peng Zou,Xiao Luo,Yusheng Zhao,Chunkit Chan,Yankai Chen,Zhongfen Deng,Yinghui Li,Hai-Tao Zheng,Dongyuan Li,Renhe Jiang,Ming Zhang,Yangqiu Song,Philip S. Yu*

Key words: 检索增强生成（RAG）, 大语言模型（LLM）, 推理-检索, 协同框架, 知识密集型任务

TL;DR: 本文综述了将检索增强生成（RAG）与推理方法相结合的框架，提出了一种统一的“推理-检索”视角，旨在提升LLM的事实性和多步推理能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为了解决RAG在多步推理中的局限性以及纯推理方法中的幻觉和事实错误问题，本文探讨了如何将两者优势结合。

Method: 通过分析高级推理如何优化RAG的各个阶段（推理增强RAG），以及检索到的知识如何为复杂推理提供缺失的前提和扩展上下文（RAG增强推理）。

Result: 提出了一种协同的RAG-推理框架，其中LLMs通过迭代地交替搜索和推理，在知识密集型任务中实现了最先进的性能。

Conclusion: 本文总结了方法、数据集和开放挑战，并提出了朝着更有效、多模态适应、可信赖和以人为本的RAG-推理系统的研究方向。

Abstract: Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language
Models (LLMs) by injecting external knowledge, yet it falls short on problems
that demand multi-step inference; conversely, purely reasoning-oriented
approaches often hallucinate or mis-ground facts. This survey synthesizes both
strands under a unified reasoning-retrieval perspective. We first map how
advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,
we show how retrieved knowledge of different type supply missing premises and
expand context for complex inference (RAG-Enhanced Reasoning). Finally, we
spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs
iteratively interleave search and reasoning to achieve state-of-the-art
performance across knowledge-intensive benchmarks. We categorize methods,
datasets, and open challenges, and outline research avenues toward deeper
RAG-Reasoning systems that are more effective, multimodally-adaptive,
trustworthy, and human-centric. The collection is available at
https://github.com/DavidZWZ/Awesome-RAG-Reasoning.

</details>


### [26] [ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning](https://arxiv.org/abs/2507.09482)
*Changli Wang,Rui Wu,Fang Yin*

Key words: 讽刺生成、多模态、PPO、对比学习、ViSP

TL;DR: 本文介绍了M2SaG多模态讽刺生成数据集和ViSP生成框架，通过PPO和对比学习提升讽刺文本生成质量。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的讽刺生成研究过度依赖文本模态，忽略视觉线索，且数据集内容与讽刺意图不匹配。

Method: 提出ViSP框架，结合PPO和对比学习，利用DIP奖励分数引导讽刺文本生成。

Result: ViSP在五个指标集上超越基线模型，生成的文本具有更高的讽刺分数（0.898）和事实不一致性（0.768）。

Conclusion: ViSP能够生成更高质量的讽刺内容，为多模态讽刺生成研究提供了新方向。

Abstract: Human emotions are complex, with sarcasm being a subtle and distinctive form.
Despite progress in sarcasm research, sarcasm generation remains underexplored,
primarily due to the overreliance on textual modalities and the neglect of
visual cues, as well as the mismatch between image content and sarcastic intent
in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm
generation dataset with 4,970 samples, each containing an image, a sarcastic
text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation
framework that integrates Proximal Policy Optimization (PPO) and contrastive
learning. PPO utilizes reward scores from DIP to steer the generation of
sarcastic texts, while contrastive learning encourages the model to favor
outputs with higher reward scores. These strategies improve overall generation
quality and produce texts with more pronounced sarcastic intent. We evaluate
ViSP across five metric sets and find it surpasses all baselines, including
large language models, underscoring their limitations in sarcasm generation.
Furthermore, we analyze the distributions of Sarcasm Scores and Factual
Incongruity for both M2SaG and the texts generated by ViSP. The generated texts
exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity
(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic
content than the original dataset. % The dataset and code will be publicly
available. Our dataset and code will be released at
\textit{https://github.com/wclapply/ViSP}.

</details>


### [27] [Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.09485)
*Junjie Liu,Yuanhe Tian,Yan Song*

Key words: ABSA, 大型语言模型, 数据增强, 强化学习

TL;DR: 本文提出了一种基于大型语言模型（LLM）的ABSA方法，通过数据增强（DA）生成高质量的合成训练数据，以解决短文本和小规模不平衡数据的问题，并通过强化学习优化数据增强过程。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: ABSA任务在社交媒体场景中至关重要，但由于短文本和小规模不平衡数据的限制，现有方法难以充分学习上下文信息。数据增强是解决这一问题的可行策略，但如何保证增强数据的高质量是一个挑战。

Method: 提出了一种基于LLM的ABSA方法，利用LLM生成增强的训练数据以构建更大规模且标签分布平衡的数据集。同时，采用强化学习优化数据增强的LLM。

Result: 在英文ABSA基准数据集上的实验结果表明，该方法优于现有基线方法和大多数研究。

Conclusion: 通过LLM生成增强数据并结合强化学习方法，有效提升了ABSA模型的性能。

Abstract: Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in
social media scenarios to identify the sentiment polarity of specific aspect
terms in a sentence. Although many existing studies leverage large language
models (LLMs) to perform ABSA due to their strong context understanding
capabilities, they still face challenges to learn the context information in
the running text because of the short text, as well as the small and unbalanced
labeled training data, where most data are labeled with positive sentiment.
Data augmentation (DA) is a feasible strategy for providing richer contextual
information, especially when using LLMs to create synthetic training data, but
faces challenges in ensuring a high quality of the augmented data.In this
paper, we propose an LLM-based ABSA approach with training data
augmentation.Specifically, an LLM is prompted to generate augmented training
data based on the original training data, so as to construct a new training
data with larger size and balanced label distributions to better train an ABSA
model. Meanwhile, in order to improve the quality of the augmented data, we
propose a reinforcement learning approach to optimize the data augmentation.
LLM.Experiment results and further analyses on English benchmark datasets for
ABSA demonstrate the effectiveness of our approach, where superior performance
is observed over strong baselines and most existing studies.

</details>


### [28] [GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities](https://arxiv.org/abs/2507.09497)
*Siyi Wu,Zeyu Wang,Xinyuan Song,Zhengpeng Zhou,Lifan Sun,Tianyu Shi*

Key words: 多智能体系统, 协作框架, 记忆架构, 协议驱动, 经验重用

TL;DR: GoalfyMax是一种端到端多智能体协作的协议驱动框架，通过标准化的A2A通信层和XP架构解决了传统AI系统在协调、记忆重用和任务分解方面的不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现代企业环境需要能够处理复杂、动态和多方面任务的高自主性和适应性智能系统，而传统单用途AI系统缺乏协调和扩展性。

Method: GoalfyMax基于模型上下文协议（MCP）构建A2A通信层，使用XP架构作为分层记忆系统，结合多轮对话和长期记忆模块。

Result: 实验表明，GoalfyMax在复杂任务编排基准测试中表现出更高的适应性、协调性和经验重用能力。

Conclusion: GoalfyMax为多智能体智能系统提供了一个可扩展的未来基础。

Abstract: Modern enterprise environments demand intelligent systems capable of handling
complex, dynamic, and multi-faceted tasks with high levels of autonomy and
adaptability. However, traditional single-purpose AI systems often lack
sufficient coordination, memory reuse, and task decomposition capabilities,
limiting their scalability in realistic settings. To address these challenges,
we present \textbf{GoalfyMax}, a protocol-driven framework for end-to-end
multi-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent
(A2A) communication layer built on the Model Context Protocol (MCP), allowing
independent agents to coordinate through asynchronous, protocol-compliant
interactions. It incorporates the Experience Pack (XP) architecture, a layered
memory system that preserves both task rationales and execution traces,
enabling structured knowledge retention and continual learning. Moreover, our
system integrates advanced features including multi-turn contextual dialogue,
long-short term memory modules, and dynamic safety validation, supporting
robust, real-time strategy adaptation. Empirical results on complex task
orchestration benchmarks and case study demonstrate that GoalfyMax achieves
superior adaptability, coordination, and experience reuse compared to baseline
frameworks. These findings highlight its potential as a scalable, future-ready
foundation for multi-agent intelligent systems.

</details>


### [29] [Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models](https://arxiv.org/abs/2507.09506)
*Junjie Wu,Gefei Gu,Yanan Zheng,Dit-Yan Yeung,Arman Cohan*

Key words: 长上下文语言模型、引用任务、基准评估、GPT-4o、上下文关系

TL;DR: 本文提出了Ref-Long基准，用于评估长上下文语言模型（LCLMs）在长上下文引用任务中的表现，发现即使是GPT-4o等先进模型也存在显著不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 长上下文语言模型在长上下文理解任务中表现优异，但长上下文引用任务（将感兴趣的项与长上下文数据中的特定部分关联）尚未充分探索。

Method: 设计了Ref-Long基准，要求LCLMs识别引用特定关键字的文档索引，强调上下文关系而非简单检索。基准包含从合成到现实的三个子集。

Result: 实验结果显示13个LCLMs在长上下文引用任务中表现不佳。人类评估、任务调整、微调实验和错误分析揭示了关键问题。

Conclusion: Ref-Long基准揭示了LCLMs在长上下文引用任务中的挑战，为未来研究提供了方向。

Abstract: Long-context language models (LCLMs) have exhibited impressive capabilities
in long-context understanding tasks. Among these, long-context referencing -- a
crucial task that requires LCLMs to attribute items of interest to specific
parts of long-context data -- remains underexplored. To bridge this gap, this
paper proposes Referencing Evaluation for Long-context Language Models
(Ref-Long), a novel benchmark designed to assess the long-context referencing
capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the
indexes of documents that reference a specific key, emphasizing contextual
relationships between the key and the documents over simple retrieval. Based on
the task design, we construct three subsets ranging from synthetic to realistic
scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs
reveal significant shortcomings in long-context referencing, even among
advanced models like GPT-4o. To further investigate these challenges, we
conduct comprehensive analyses, including human evaluations, task format
adjustments, fine-tuning experiments, and error analyses, leading to several
key insights. Our data and code can be found in https://github.
com/wujunjie1998/Ref-Long.

</details>


### [30] [How Important is `Perfect' English for Machine Translation Prompts?](https://arxiv.org/abs/2507.09509)
*Patrícia Schmidtová,Niyati Bafna,Seth Aycock,Gianluca Vico,Wiktor Kamzela,Katharina Hämmerl,Vilém Zouhar*

Key words: 大语言模型、机器翻译、提示错误、噪声敏感性、指令遵循

TL;DR: 评估大语言模型（LLM）在机器翻译和翻译评估任务中对提示错误的敏感性，发现提示质量对翻译性能有显著影响。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究人类合理和合成错误如何影响LLM在机器翻译及相关任务中的表现。

Method: 系统地定量和定性分析模型对提示噪声的响应。

Result: 提示质量直接影响翻译性能，但不同类型噪声影响不同；LLM仍能在高噪声下翻译。

Conclusion: 提示质量主要影响指令遵循而非翻译质量本身，LLM对噪声有较强适应能力。

Abstract: Large language models (LLMs) have achieved top results in recent machine
translation evaluations, but they are also known to be sensitive to errors and
perturbations in their prompts. We systematically evaluate how both humanly
plausible and synthetic errors in user prompts affect LLMs' performance on two
related tasks: Machine translation and machine translation evaluation. We
provide both a quantitative analysis and qualitative insights into how the
models respond to increasing noise in the user prompt.
  The prompt quality strongly affects the translation performance: With many
errors, even a good prompt can underperform a minimal or poor prompt without
errors. However, different noise types impact translation quality differently,
with character-level and combined noisers degrading performance more than
phrasal perturbations. Qualitative analysis reveals that lower prompt quality
largely leads to poorer instruction following, rather than directly affecting
translation quality itself. Further, LLMs can still translate in scenarios with
overwhelming random noise that would make the prompt illegible to humans.

</details>


### [31] [Adapting Definition Modeling for New Languages: A Case Study on Belarusian](https://arxiv.org/abs/2507.09536)
*Daniela Kazakouskaya,Timothee Mickus,Janine Siewert*

Key words: 定义建模, 白俄罗斯语, 数据集, 自动评估指标

TL;DR: 该论文探讨了如何利用现有模型为白俄罗斯语生成定义，并提出了一个包含43,150条定义的新数据集。实验表明，适应定义建模系统需要的数据量较少，但自动指标存在不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 定义建模任务可以帮助词典编纂者记录更多语言和方言，但目前需评估如何利用现有模型支持尚未涵盖的语言。

Method: 研究聚焦于将现有模型适应于白俄罗斯语，并提出一个包含43,150条定义的新数据集。

Result: 实验表明，适应定义建模系统所需数据量较少，但自动指标未能完全捕捉模型表现。

Conclusion: 现有模型可以高效适应新语言，但需改进评估指标。

Abstract: Definition modeling, the task of generating new definitions for words in
context, holds great prospect as a means to assist the work of lexicographers
in documenting a broader variety of lects and languages, yet much remains to be
done in order to assess how we can leverage pre-existing models for as-of-yet
unsupported languages. In this work, we focus on adapting existing models to
Belarusian, for which we propose a novel dataset of 43,150 definitions. Our
experiments demonstrate that adapting a definition modeling systems requires
minimal amounts of data, but that there currently are gaps in what automatic
metrics do capture.

</details>


### [32] [NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance](https://arxiv.org/abs/2507.09601)
*Hanwool Lee,Sara Yu,Yewon Hwang,Jonghyun Choi,Heejae Ahn,Sungbum Jung,Youngjae Yu*

Key words: 金融嵌入模型, 低资源语言, 跨语言学习, 韩语金融语义

TL;DR: NMIXX是专为金融领域设计的跨语言嵌入模型，通过高质量的三元组数据微调，提升了金融语义的捕捉能力，尤其在韩语等低资源语言中表现突出。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 通用句子嵌入模型难以捕捉金融领域的专业语义，特别是在韩语等低资源语言中，存在领域术语、时间语义偏移和双语词汇不对齐等问题。

Method: 引入NMIXX模型，利用18.8K高质量三元组（包括领域内释义对、硬负样本和精确韩英翻译对）进行微调，并发布了KorFinSTS基准数据集。

Result: NMIXX在英语FinSTS和KorFinSTS上的性能显著提升（Spearman's rho分别提高0.10和0.22），同时发现韩语分词覆盖更广的模型适应性更强。

Conclusion: NMIXX和KorFinSTS为金融领域的多语言表示学习提供了高效工具，突显分词设计在低资源跨语言任务中的重要性。

Abstract: General-purpose sentence embedding models often struggle to capture
specialized financial semantics, especially in low-resource languages like
Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned
bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural
eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual
embedding models fine-tuned with 18.8K high-confidence triplets that pair
in-domain paraphrases, hard negatives derived from a semantic-shift typology,
and exact Korean-English translations. Concurrently, we release KorFinSTS, a
1,921-pair Korean financial STS benchmark spanning news, disclosures, research
reports, and regulations, designed to expose nuances that general benchmarks
miss.
  When evaluated against seven open-license baselines, NMIXX's multilingual
bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and
+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing
other models by the largest margin, while revealing a modest trade-off in
general STS performance. Our analysis further shows that models with richer
Korean token coverage adapt more effectively, underscoring the importance of
tokenizer design in low-resource, cross-lingual settings. By making both models
and the benchmark publicly available, we provide the community with robust
tools for domain-adapted, multilingual representation learning in finance.

</details>


### [33] [SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks](https://arxiv.org/abs/2507.09628)
*Salvatore Citraro,Edith Haim,Alessandra Carini,Cynthia S. Q. Siew,Giulio Rossetti,Massimo Stella*

Key words: SpreadPy, 激活传播, 认知网络, 数值模拟, 心理学, 神经科学

TL;DR: SpreadPy是一个Python库，用于模拟认知单层和多层网络中的激活传播，支持研究认知过程的结构-功能关系。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 通过模拟激活传播研究认知、心理和临床现象，验证理论模型与实际数据的关系。

Method: 利用数值模拟，基于实验或理论网络进行激活传播分析，并通过三个案例研究验证其有效性。

Result: 案例研究显示，SpreadPy能区分数学焦虑学生、揭示认知任务难度对激活轨迹的影响，并关联失语症患者的网络结构与错误类型。

Conclusion: SpreadPy提供了一个灵活、可复现的工具，为心理学、神经科学和教育研究提供机制性见解。

Abstract: We introduce SpreadPy as a Python library for simulating spreading activation
in cognitive single-layer and multiplex networks. Our tool is designed to
perform numerical simulations testing structure-function relationships in
cognitive processes. By comparing simulation results with grounded theories in
knowledge modelling, SpreadPy enables systematic investigations of how
activation dynamics reflect cognitive, psychological and clinical phenomena. We
demonstrate the library's utility through three case studies: (1) Spreading
activation on associative knowledge networks distinguishes students with high
versus low math anxiety, revealing anxiety-related structural differences in
conceptual organization; (2) Simulations of a creativity task show that
activation trajectories vary with task difficulty, exposing how cognitive load
modulates lexical access; (3) In individuals with aphasia, simulated activation
patterns on lexical networks correlate with empirical error types (semantic vs.
phonological) during picture-naming tasks, linking network structure to
clinical impairments. SpreadPy's flexible framework allows researchers to model
these processes using empirically derived or theoretical networks, providing
mechanistic insights into individual differences and cognitive impairments. The
library is openly available, supporting reproducible research in psychology,
neuroscience, and education research.

</details>


### [34] [An Exploration of Knowledge Editing for Arabic](https://arxiv.org/abs/2507.09629)
*Basel Mousi,Nadir Durrani,Fahim Dalvi*

Key words: 知识编辑（KE），阿拉伯语，多语言，跨语言，LTE

TL;DR: 本文首次研究了阿拉伯语的知识编辑（KE），评估了四种方法在阿拉伯语翻译数据集上的表现，发现基于参数的方法在跨语言泛化上表现不佳，而基于指令调整的方法更稳健。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探究知识编辑（KE）在阿拉伯语等形态丰富语言中的行为，填补这一领域的研究空白。

Method: 评估了四种KE方法（ROME、MEMIT、ICE和LTE）在阿拉伯语翻译数据集（ZsRE和Counterfact）上的表现，包括多语言和跨语言设置。

Result: 基于参数的方法在跨语言泛化上表现不佳，而指令调整方法（如LTE）表现更稳健。多语言训练提高了编辑和迁移性能。

Conclusion: 多语言训练的LTE方法在阿拉伯语KE中表现更优，为未来研究提供了阿拉伯语KE基准和多语言训练数据。

Abstract: While Knowledge Editing (KE) has been widely explored in English, its
behavior in morphologically rich languages like Arabic remains underexamined.
In this work, we present the first study of Arabic KE. We evaluate four methods
(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact
benchmarks, analyzing both multilingual and cross-lingual settings. Our
experiments on Llama-2-7B-chat show show that parameter-based methods struggle
with cross-lingual generalization, while instruction-tuned methods perform more
robustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show
that joint Arabic-English training improves both editability and transfer. We
release Arabic KE benchmarks and multilingual training for LTE data to support
future research.

</details>


### [35] [Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?](https://arxiv.org/abs/2507.09638)
*Pawitsapak Akarajaradwong,Chompakorn Chaksangchaichot,Pirat Pothavorn,Attapol Thamrongrattanarit-Rutherford,Ekapol Chuangsuwanich,Sarana Nutanong*

Key words: RAG, GRPO, 泰语法律问答, BGE-M3嵌入, 法律推理

TL;DR: 论文提出一种通过GRPO方法提升泰语法律问答系统性能的方案，显著提高了法律引用的准确性和回答质量。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 泰语法律问答系统中RAG的性能受限，特别是在需要复杂法律推理的问题上。

Method: 采用Group-Relative Policy Optimization (GRPO)并结合BGE-M3嵌入作为语义相似性奖励，显著降低计算成本。

Result: 在NitiBench测试中，GRPO比基础模型提高90%的引用准确性和31%的综合质量指标，且对复杂法律推理任务表现更稳健。

Conclusion: 该方法为提升泰语法律LLM提供了一种高效且资源节约的解决方案。

Abstract: The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal
question answering is still limited, especially for questions requiring
extensive, complex legal reasoning. To address these limitations, we introduce
an approach aligning LLMs toward improved law citation accuracy and better
response quality using Group-Relative Policy Optimization (GRPO). Our approach
leverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,
significantly reducing computational expenses up to 2.5x compared to large
language model judges. Experiments on the NitiBench benchmark demonstrate
substantial improvements: GRPO achieves up to 90% citation-F1 gains from the
base model and a 31% increase in joint quality metrics over instruction tuning.
Crucially, our method shows enhanced robustness on complex legal reasoning
tasks compared to instruction tuning, providing an effective and
resource-efficient solution for enhancing Thai legal LLMs.

</details>


### [36] [MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs](https://arxiv.org/abs/2507.09701)
*Shulin Huang,Linyi Yang,Yue Zhang*

Key words: 大型语言模型, 文化偏见, 跨文化理解, 多语言评估, MCEval

TL;DR: MCEval是一个多语言文化评估框架，通过动态文化问题构建和因果分析，评估语言模型的文化偏见和跨文化理解能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是评估大型语言模型在多语言和多元文化环境中的表现，揭示其文化偏见和跨文化理解能力的不足。

Method: 方法包括动态文化问题构建、反事实重述和混杂因素重述，覆盖13种文化和语言，生成大量评估实例。

Result: 结果显示不同语言场景下的表现差异，表明最佳文化表现与训练数据分布及语言-文化对齐相关，同时暴露了公平性问题。

Conclusion: MCEval是多语言文化评估的首次全面尝试，为语言模型的文化理解提供了深入见解。

Abstract: Large language models exhibit cultural biases and limited cross-cultural
understanding capabilities, particularly when serving diverse global user
populations. We propose MCEval, a novel multilingual evaluation framework that
employs dynamic cultural question construction and enables causal analysis
through Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive
evaluation spans 13 cultures and 13 languages, systematically assessing both
cultural awareness and cultural bias across different linguistic scenarios. The
framework provides 39,897 cultural awareness instances and 17,940 cultural bias
instances. Experimental results reveal performance disparities across different
linguistic scenarios, demonstrating that optimal cultural performance is not
only linked to training data distribution, but also is related to
language-culture alignment. The evaluation results also expose the fairness
issue, where approaches appearing successful in the English scenario create
substantial disadvantages. MCEval represents the first comprehensive
multilingual cultural evaluation framework that provides deeper insights into
LLMs' cultural understanding.

</details>


### [37] [Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces](https://arxiv.org/abs/2507.09709)
*Baturay Saglam,Paul Kassianik,Blaine Nelson,Sajana Weerawardhena,Yaron Singer,Amin Karbasi*

Key words: 大型语言模型,潜在空间,语义理解,线性可分性,推理模式,防御方法

TL;DR: 这篇论文研究了大型语言模型（LLMs）潜在空间的几何结构，发现高级语义信息存在于低维子空间中，且在不同领域中线性可分。这种可分性在深层和结构化推理提示下更明显，为直接操作潜在表示的工具开发提供了基础。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索LLMs内部如何组织与语义理解相关的表示，以解释其行为并改进对齐。

Method: 对11个基于Transformer的LLMs的隐藏状态进行大规模实证研究，分析6个科学主题和12层的表示。

Result: 发现高级语义信息在低维子空间中线性可分，且在深层和推理提示下更明显；证明可以通过简单向量方向捕捉推理模式。

Conclusion: 潜在空间的几何结构支持开发直接操作表示的工具，如检测和防御有害内容。

Abstract: Understanding the latent space geometry of large language models (LLMs) is
key to interpreting their behavior and improving alignment. \baturay{However,
it remains unclear to what extent LLMs internally organize representations
related to semantic understanding. To investigate this, we conduct a
large-scale empirical study of hidden states in transformer-based LLMs,
analyzing 11 decoder-only models across 6 scientific topics and 12 layers each.
We find that high-level semantic information consistently lies in
low-dimensional subspaces that form linearly separable representations across
distinct domains. This separability becomes more pronounced in deeper layers
and under prompts that trigger structured reasoning or alignment
behaviors$\unicode{x2013}$even when surface content is unchanged. This geometry
enables simple yet effective causal interventions in hidden space; for example,
reasoning patterns like chain-of-thought can be captured by a single vector
direction. Together, these findings support the development of geometry-aware
tools that operate directly on latent representations to detect and mitigate
harmful or adversarial content, using methods such as transport-based defenses
that leverage this separability. As a proof of concept, we demonstrate this
potential by training a simple MLP classifier as a lightweight latent-space
guardrail, which detects adversarial and malicious prompts with high precision.

</details>


### [38] [Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding](https://arxiv.org/abs/2507.09758)
*Qi Feng,Yihong Liu,Hinrich Schütze*

Key words: 课程学习, 自然语言处理, 预训练语言模型, 自适应学习, 微调

TL;DR: 论文提出了一种自适应的课程学习范式，通过预训练语言模型预测难度分数，优化示例的微调顺序，从而提升学习效率和性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有课程学习方法依赖手动定义的难度指标（如文本长度），可能无法准确反映模型的视角，因此需要一种自适应方法。

Method: 提出自适应的课程学习范式，利用预训练语言模型预测示例难度分数，并探索不同的示例排序策略（易到难、难到易、混合采样）。

Result: 在四个自然语言理解数据集上的实验表明，该方法比随机采样收敛更快且性能更优。

Conclusion: 自适应课程学习能有效提升模型训练效率和效果。

Abstract: Curriculum learning is a widely adopted training strategy in natural language
processing (NLP), where models are exposed to examples organized by increasing
difficulty to enhance learning efficiency and performance. However, most
existing approaches rely on manually defined difficulty metrics -- such as text
length -- which may not accurately reflect the model's own perspective. To
overcome this limitation, we present a self-adaptive curriculum learning
paradigm that prioritizes fine-tuning examples based on difficulty scores
predicted by pre-trained language models (PLMs) themselves. Building on these
scores, we explore various training strategies that differ in the ordering of
examples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed
sampling. We evaluate our method on four natural language understanding (NLU)
datasets covering both binary and multi-class classification tasks.
Experimental results show that our approach leads to faster convergence and
improved performance compared to standard random sampling.

</details>


### [39] [Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News](https://arxiv.org/abs/2507.09777)
*Gabriel Mordecki,Guillermo Moncecchi,Javier Couto*

Key words: clickbait, curiosity gap, dataset, Spanish, detection

TL;DR: 论文重新定义了点击诱饵（clickbait），提出其核心是通过制造好奇心缺口吸引读者点击，并发布了一个西班牙语的开源数据集TA1C。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前对点击诱饵的定义缺乏共识，作者希望通过明确其与类似现象的区别，提出更精确的定义。

Method: 通过细化概念边界和标注标准，创建一个新的数据集TA1C，并实现基线模型。

Result: TA1C包含3,500条标注数据，标注一致性为0.825，基线模型F1得分达0.84。

Conclusion: 论文为点击诱饵的识别提供了更清晰的定义和新工具，推动了相关研究。

Abstract: We revise the definition of clickbait, which lacks current consensus, and
argue that the creation of a curiosity gap is the key concept that
distinguishes clickbait from other related phenomena such as sensationalism and
headlines that do not deliver what they promise or diverge from the article.
Therefore, we propose a new definition: clickbait is a technique for generating
headlines and teasers that deliberately omit part of the information with the
goal of raising the readers' curiosity, capturing their attention and enticing
them to click. We introduce a new approach to clickbait detection datasets
creation, by refining the concept limits and annotations criteria, minimizing
the subjectivity in the decision as much as possible. Following it, we created
and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the
first open source dataset for clickbait detection in Spanish. It consists of
3,500 tweets coming from 18 well known media sources, manually annotated and
reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong
baselines that achieve 0.84 in F1-score.

</details>


### [40] [Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition](https://arxiv.org/abs/2507.09875)
*Qinyuan Ye,Robin Jia,Xiang Ren*

Key words: 语言模型, 上下文学习, 任务级泛化, 函数归纳, 路径修补

TL;DR: 本文通过研究语言模型在任务级泛化中的内部机制，发现了一种函数归纳机制，揭示了模型如何从未见的任务中学习，例如将标准加法泛化为“加一”加法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索语言模型如何通过上下文学习（in-context learning）执行未见任务，尤其是任务级泛化的内部机制。

Method: 使用电路风格的解释性技术（如路径修补）分析模型在‘加一’加法任务中的内部计算。

Result: 发现了函数归纳机制，表明模型通过多个注意力头并行诱导‘加一’函数，并将其重用于其他任务。

Conclusion: 语言模型通过可重用和可组合的内部结构实现任务级泛化，为理解其学习能力提供了新视角。

Abstract: Large language models demonstrate the intriguing ability to perform unseen
tasks via in-context learning. However, it remains unclear what mechanisms
inside the model drive such task-level generalization. In this work, we
approach this question through the lens of off-by-one addition (i.e., 1+1=3,
2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function
as a second step. Leveraging circuit-style interpretability techniques such as
path patching, we analyze the models' internal computations behind their
notable performance and present three key findings. First, we uncover a
function induction mechanism that explains the model's generalization from
standard addition to off-by-one addition. This mechanism resembles the
structure of the induction head mechanism found in prior work and elevates it
to a higher level of abstraction. Second, we show that the induction of the +1
function is governed by multiple attention heads in parallel, each of which
emits a distinct piece of the +1 function. Finally, we find that this function
induction mechanism is reused in a broader range of tasks, including synthetic
tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8
addition. Overall, our findings offer deeper insights into how reusable and
composable structures within language models enable task-level generalization.

</details>


### [41] [Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking](https://arxiv.org/abs/2507.09935)
*Hai Toan Nguyen,Tien Dat Nguyen,Viet Ha Nguyen*

Key words: 检索增强生成、语义分块、文本分割、聚类、信息检索

TL;DR: 提出一种增强RAG系统的新框架，通过分层文本分割和聚类生成更有语义意义的文本块，提升检索准确性和上下文相关性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统RAG系统的分块策略因忽略文本结构而难以捕获充分语义，需改进以生成更具语义连贯性的分块。

Method: 结合分层文本分割和聚类，生成语义更丰富的文本块，并在推理时利用分段和聚类级向量表示检索信息。

Result: 在NarrativeQA、QuALITY和QASPER数据集上的实验表明，该方法优于传统分块技术。

Conclusion: 该框架通过语义增强的分块策略，显著提升了RAG系统的信息检索精度和上下文相关性。

Abstract: Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies
for retrieval, which enhance large language models (LLMs) by enabling them to
access external knowledge, ensuring that the retrieved information is
up-to-date and domain-specific. However, traditional methods often fail to
create chunks that capture sufficient semantic meaning, as they do not account
for the underlying textual structure. This paper proposes a novel framework
that enhances RAG by integrating hierarchical text segmentation and clustering
to generate more meaningful and semantically coherent chunks. During inference,
the framework retrieves information by leveraging both segment-level and
cluster-level vector representations, thereby increasing the likelihood of
retrieving more precise and contextually relevant information. Evaluations on
the NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method
achieved improved results compared to traditional chunking techniques.

</details>


### [42] [Tiny Reward Models](https://arxiv.org/abs/2507.09973)
*Sarah Pan*

Key words: TinyRM, 奖励模型, 轻量级, 双向掩码语言模型, RLHF

TL;DR: TinyRM是一种小型双向掩码语言模型，性能媲美大模型，且资源消耗更低。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 减少奖励模型的推理成本，同时保持高性能。

Method: 结合FLAN提示、DoRA和层冻结技术，训练小规模模型。

Result: TinyRM在RewardBench上表现优异，参数仅为大模型的1/175。

Conclusion: 轻量级双向架构是高效、可扩展的偏好建模替代方案。

Abstract: Large decoder-based language models have become the dominant architecture for
reward modeling in reinforcement learning from human feedback (RLHF). However,
as reward models are increasingly deployed in test-time strategies, their
inference costs become a growing concern. We present TinyRM, a family of small,
bidirectional masked language models (MLMs) with as few as 400 million
parameters, that rival the capabilities of models over 175 times larger on
reasoning and safety preference modeling tasks. TinyRM combines FLAN-style
prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to
achieve strong performance on RewardBench, despite using significantly fewer
resources. Our experiments suggest that small models benefit from
domain-specific tuning strategies, particularly in reasoning, where lightweight
finetuning methods are especially effective. While challenges remain in
building generalist models and conversational preference modeling, our
preliminary results highlight the promise of lightweight bidirectional
architectures as efficient, scalable alternatives for preference modeling.

</details>


### [43] [TextOmics-Guided Diffusion for Hit-like Molecular Generation](https://arxiv.org/abs/2507.09982)
*Hang Yuan,Chen Li,Wenjun Ma,Yuncheng Jiang*

Key words: 分子生成, 组学, 文本描述, 药物发现, 扩散模型

TL;DR: TextOmics是一种开创性基准，通过建立组学表达与分子文本描述的一对一对应关系，促进分子生成。ToDi框架在此基础上，利用组学和文本描述生成生物相关、化学有效的候选分子，性能优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决药物发现中缺乏异质数据和统一框架的问题，实现靶向特异的候选分子生成。

Method: 提出TextOmics基准和ToDi生成框架，结合组学表达和分子文本描述，使用OmicsEn和TextEn编码器及DiffGen条件扩散模型。

Result: 实验证明TextOmics有效，ToDi性能优于现有方法，并在零样本治疗分子生成中展示潜力。

Conclusion: TextOmics和ToDi为靶向药物发现提供了新工具，具有显著的生物医学应用潜力。

Abstract: Hit-like molecular generation with therapeutic potential is essential for
target-specific drug discovery. However, the field lacks heterogeneous data and
unified frameworks for integrating diverse molecular representations. To bridge
this gap, we introduce TextOmics, a pioneering benchmark that establishes
one-to-one correspondences between omics expressions and molecular textual
descriptions. TextOmics provides a heterogeneous dataset that facilitates
molecular generation through representations alignment. Built upon this
foundation, we propose ToDi, a generative framework that jointly conditions on
omics expressions and molecular textual descriptions to produce biologically
relevant, chemically valid, hit-like molecules. ToDi leverages two encoders
(OmicsEn and TextEn) to capture multi-level biological and semantic
associations, and develops conditional diffusion (DiffGen) for controllable
generation. Extensive experiments confirm the effectiveness of TextOmics and
demonstrate ToDi outperforms existing state-of-the-art approaches, while also
showcasing remarkable potential in zero-shot therapeutic molecular generation.
Sources are available at: https://github.com/hala-ToDi.

</details>


### [44] [Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media](https://arxiv.org/abs/2507.10008)
*Jun Li,Xiangmeng Wang,Haoyang Li,Yifei Yan,Hong Va Leong,Ling Feng,Nancy Xiaonan Yu,Qing Li*

Key words: 自杀风险预测、社交媒体、保护因素、动态因素影响学习、Reddit

TL;DR: 该研究提出了一种新型框架，通过联合学习风险因素和保护因素对用户自杀风险动态变化的影响，预测未来的自杀风险。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 先前研究主要关注当前自杀风险的检测，而忽略了预测随时间变化的自杀风险，且忽视保护因素的作用。保护因素（如社会支持）可缓解风险因素的影响。

Method: 研究提出了一种动态因素影响学习方法，结合了风险和保护因素的作用，并构建了一个包含12年Reddit帖子的保护因素感知数据集。

Result: 实验表明，该模型显著优于现有先进模型和大型语言模型，且提供了可解释的权重，帮助临床医生更好地理解自杀模式。

Conclusion: 通过动态学习风险和保护因素的影响，该框架为自杀风险预测提供了更全面和可解释的方法，有助于精准干预。

Abstract: Suicide is a critical global health issue that requires urgent attention.
Even though prior work has revealed valuable insights into detecting current
suicide risk on social media, little attention has been paid to developing
models that can predict subsequent suicide risk over time, limiting their
ability to capture rapid fluctuations in individuals' mental state transitions.
In addition, existing work ignores protective factors that play a crucial role
in suicide risk prediction, focusing predominantly on risk factors alone.
Protective factors such as social support and coping strategies can mitigate
suicide risk by moderating the impact of risk factors. Therefore, this study
proposes a novel framework for predicting subsequent suicide risk by jointly
learning the dynamic influence of both risk factors and protective factors on
users' suicide risk transitions. We propose a novel Protective Factor-Aware
Dataset, which is built from 12 years of Reddit posts along with comprehensive
annotations of suicide risk and both risk and protective factors. We also
introduce a Dynamic Factors Influence Learning approach that captures the
varying impact of risk and protective factors on suicide risk transitions,
recognizing that suicide risk fluctuates over time according to established
psychological theories. Our thorough experiments demonstrate that the proposed
model significantly outperforms state-of-the-art models and large language
models across three datasets. In addition, the proposed Dynamic Factors
Influence Learning provides interpretable weights, helping clinicians better
understand suicidal patterns and enabling more targeted intervention
strategies.

</details>


### [45] [GeLaCo: An Evolutionary Approach to Layer Compression](https://arxiv.org/abs/2507.10059)
*David Ponce,Thierry Etchegoyhen,Javier Del Ser*

Key words: 大型语言模型, 模型压缩, 进化算法, 层折叠, 帕累托前沿

TL;DR: GeLaCo是一种基于进化算法的LLM压缩方法，通过层折叠高效探索压缩解空间，支持单目标和多目标搜索，并在性能和质量上优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型（LLM）因计算资源需求高而面临部署和使用障碍，需要高效的压缩方法来减少模型规模同时保持性能。

Method: GeLaCo采用进化算法进行层折叠，通过基于群体的搜索和模块相似性适应度函数（注意力、前馈网络和隐藏状态表示）探索压缩解空间。

Result: GeLaCo在困惑度和生成任务评估中优于现有方法，并首次建立压缩与质量之间的帕累托前沿。

Conclusion: GeLaCo为LLM压缩提供了一种高效且灵活的方法，显著提升了压缩模型的性能。

Abstract: Large Language Models (LLM) have achieved remarkable performance across a
large number of tasks, but face critical deployment and usage barriers due to
substantial computational requirements. Model compression methods, which aim to
reduce model size while preserving its capacity, are an important means to
mitigate these issues. Promising approaches along these lines, such as
structured pruning, typically require costly empirical search for optimal
variants and may run the risk of ignoring better solutions. In this work we
introduce GeLaCo, an evolutionary approach to LLM compression via layer
collapse. Our approach supports an efficient exploration of the compression
solution space via population-based search and a module-wise similarity fitness
function capturing attention, feed-forward, and hidden state representations.
GeLaCo also supports both single and multi-objective evolutionary compression
search, establishing the first Pareto frontier along compression and quality
axes. We evaluate GeLaCo solutions via both perplexity-based and generative
evaluations over foundational and instruction-tuned models, outperforming
state-of-the-art alternatives.

</details>


### [46] [Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires](https://arxiv.org/abs/2507.10073)
*Simon Münker*

Key words: AI对齐、大型语言模型、道德多样性、文化表征

TL;DR: 研究发现大型语言模型（LLMs）虽具备语言能力，但未能代表多样文化道德框架，系统性同质化道德多样性，挑战了其作为社会科学研究中合成人群的使用。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨AI系统是否能真实代表人类价值观，还是仅对其进行了平均化处理。

Method: 通过“道德基础问卷”在19种文化背景下对比LLMs与人类道德直觉的差异。

Result: LLMs系统性同质化道德多样性，模型规模的增加并未提高文化表征的准确性。

Conclusion: 当前AI对齐方法存在根本限制，需开发更接地气的对齐目标和评估指标。

Abstract: Are AI systems truly representing human values, or merely averaging across
them? Our study suggests a concerning reality: Large Language Models (LLMs)
fail to represent diverse cultural moral frameworks despite their linguistic
capabilities. We expose significant gaps between AI-generated and human moral
intuitions by applying the Moral Foundations Questionnaire across 19 cultural
contexts. Comparing multiple state-of-the-art LLMs' origins against human
baseline data, we find these models systematically homogenize moral diversity.
Surprisingly, increased model size doesn't consistently improve cultural
representation fidelity. Our findings challenge the growing use of LLMs as
synthetic populations in social science research and highlight a fundamental
limitation in current AI alignment approaches. Without data-driven alignment
beyond prompting, these systems cannot capture the nuanced, culturally-specific
moral intuitions. Our results call for more grounded alignment objectives and
evaluation metrics to ensure AI systems represent diverse human values rather
than flattening the moral landscape.

</details>


### [47] [Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning](https://arxiv.org/abs/2507.10085)
*Chenxi Huang,Shaotian Yan,Liang Xie,Binbin Lin,Sinan Fan,Yue Xin,Deng Cai,Chen Shen,Jieping Ye*

Key words: CRFT, ReFT, 复杂推理, 表征优化, 少样本学习

TL;DR: 提出了一种新的方法CRFT，通过分析信息流识别和优化关键表征，显著提升了复杂推理任务的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前ReFT方法在复杂推理任务中表现不佳，因为固定的表征修改位置对输出的影响不确定。研究发现存在关键表征对最终输出有显著影响，优化这些表征可以提升性能。

Method: 提出CRFT方法，通过信息流分析识别关键表征，并在低秩线性子空间中动态优化这些表征，同时冻结基础模型。

Result: 在八个算术和常识推理基准测试中验证了方法的有效性和高效性，适用于少样本设置，一次性准确率提升16.4%。

Conclusion: CRFT展示了表征级优化在复杂推理任务中的潜力，为传统PEFT方法提供了轻量且强大的替代方案。

Abstract: Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient
Fine-Tuning (PEFT) method, has attracted widespread attention for significantly
improving parameter efficiency by editing representation space alone. In this
work, we investigate applying ReFT to complex reasoning tasks. However,
directly using the native ReFT method, which modifies fixed representations at
the beginning and end of each layer, yields suboptimal performance, as these
fixed-position representations have uncertain impact on the outputs. We observe
that, in complex reasoning tasks, there often exist certain critical
representations. These representations either integrate significant information
from preceding layers or regulate subsequent layer representations. Through
layer-by-layer propagation, they exert a substantial influence on the final
output. Naturally, fine-tuning these critical representations has the potential
to greatly enhance reasoning performance. Building upon these insights, we
propose Critical Representation Fine-Tuning (CRFT), a novel method that
identifies and optimizes these critical representations through information
flow analysis. CRFT operates within a supervised learning framework,
dynamically optimizing critical representations in a low-rank linear subspace
while freezing the base model. The effectiveness and efficiency of our method
are validated across eight benchmarks for arithmetic and commonsense reasoning,
using LLaMA and Mistral model families. Furthermore, our method also adapts
effectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work
highlights the untapped potential of representation-level optimization for CoT
reasoning, offering a lightweight yet powerful alternative to traditional PEFT
methods.

</details>


### [48] [Fusing Large Language Models with Temporal Transformers for Time Series Forecasting](https://arxiv.org/abs/2507.10098)
*Chen Su,Yuanhe Tian,Qinyu Liu,Jun Zhang,Yan Song*

Key words: 时间序列预测, LLM, Transformer, 语义表示, 混合架构

TL;DR: 提出了一种结合LLM和Transformer的新架构，融合两者优势以提升时间序列预测的准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有LLM在时间序列预测中表现不佳，因其最初设计用于处理离散文本而非连续数值数据，而纯Transformer虽能直接处理时间序列，却难以学习高级语义模式。

Method: 设计了一种新型Transformer架构，将LLM的语义表示与时间序列Transformer的时序信息融合，形成混合表示。

Result: 实验表明，该模型在基准数据集上能更准确地预测未来值。

Conclusion: 通过融合LLM和Transformer的特性，新模型在时间序列预测任务中表现出色。

Abstract: Recently, large language models (LLMs) have demonstrated powerful
capabilities in performing various tasks and thus are applied by recent studies
to time series forecasting (TSF) tasks, which predict future values with the
given historical time series. Existing LLM-based approaches transfer knowledge
learned from text data to time series prediction using prompting or fine-tuning
strategies. However, LLMs are proficient at reasoning over discrete tokens and
semantic patterns but are not initially designed to model continuous numerical
time series data. The gaps between text and time series data lead LLMs to
achieve inferior performance to a vanilla Transformer model that is directly
trained on TSF data. However, the vanilla Transformers often struggle to learn
high-level semantic patterns. In this paper, we design a novel
Transformer-based architecture that complementarily leverages LLMs and vanilla
Transformers, so as to integrate the high-level semantic representations
learned by LLMs into the temporal information encoded by time series
Transformers, where a hybrid representation is obtained by fusing the
representations from the LLM and the Transformer. The resulting fused
representation contains both historical temporal dynamics and semantic
variation patterns, allowing our model to predict more accurate future values.
Experiments on benchmark datasets demonstrate the effectiveness of the proposed
approach.

</details>


### [49] [Task-Based Flexible Feature Distillation for LLMs](https://arxiv.org/abs/2507.10155)
*Khouloud Saadi,Di Wang*

Key words: 知识蒸馏、特征蒸馏、大语言模型、任务相关隐藏单元

TL;DR: 提出了一种基于任务的特征蒸馏方法，使教师和学生模型在不同隐藏层维度下也能进行知识迁移，无需引入新参数。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统特征蒸馏方法假设师生模型隐藏层尺寸相同，限制了学生模型的灵活性，而线性投影方法虽能对齐特征空间，但会引入额外参数并降低下游任务性能。

Method: 利用任务相关的隐藏单元，直接蒸馏教师的激活值到学生模型，无需线性投影。

Result: 在分类、指令跟随和摘要等任务上表现优于基线方法，性能提升达3%。

Conclusion: 该方法灵活且高效，显著提升了特征蒸馏的效果。

Abstract: Knowledge Distillation (KD) in general and feature distillation in particular
are promising techniques for reducing the high computational demand of large
language models (LLMs). However, traditional feature KD methods typically
assume that the teacher and the student share the same hidden size, limiting
the flexibility of the student's architecture. A common solution to this
problem involves training a linear projector to align their feature spaces, but
this introduces additional parameters that must be learned from scratch and
often degrades performance on downstream tasks, especially in generative
settings. To address this issue, in this work, we propose a novel task-based
feature distillation method that enables knowledge transfer between teacher and
student models with different hidden layer dimensions, without introducing any
new parameters. Leveraging the insight that only a subset of LLM components
contribute significantly to a specific downstream task, our approach identifies
the most task-relevant hidden units in the teacher and directly distills their
activations to the student. Our method is flexible and easily integrates with
other distillation frameworks. Empirical results show consistent improvements
over prior approaches across diverse tasks, including classification,
instruction-following, and summarization, achieving up to a 3\% performance
gain over the linear projection baseline.

</details>


### [50] [Abusive text transformation using LLMs](https://arxiv.org/abs/2507.10177)
*Rohitash Chandra,Jiyong Choi*

Key words: LLMs, 侮辱性文本, 文本转化, 情感分析, 语义分析

TL;DR: 本研究探索了大型语言模型（LLMs）在将侮辱性文本（如推文和评论）转化为非侮辱性版本时的效果，并比较了几种先进模型的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管LLMs在自然语言处理任务中表现优异，但在识别和转化侮辱性文本方面的能力仍需研究。

Method: 研究评估了Gemini、GPT-4o、DeepSeek和Groq等模型在识别侮辱性文本并将其转化为非侮辱性文本的能力，同时保留文本的情感和语义。

Result: 结果表明，Groq的表现与其他模型差异显著，而GPT-4o和DeepSeek-V3之间存在相似性。

Conclusion: LLMs在转化侮辱性文本方面具有潜力，但不同模型的表现差异值得进一步研究。

Abstract: Although Large Language Models (LLMs) have demonstrated significant
advancements in natural language processing tasks, their effectiveness in the
classification and transformation of abusive text into non-abusive versions
remains an area for exploration. In this study, we aim to use LLMs to transform
abusive text (tweets and reviews) featuring hate speech and swear words into
non-abusive text, while retaining the intent of the text. We evaluate the
performance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and
Groq, on their ability to identify abusive text. We them to transform and
obtain a text that is clean from abusive and inappropriate content but
maintains a similar level of sentiment and semantics, i.e. the transformed text
needs to maintain its message. Afterwards, we evaluate the raw and transformed
datasets with sentiment analysis and semantic analysis. Our results show Groq
provides vastly different results when compared with other LLMs. We have
identified similarities between GPT-4o and DeepSeek-V3.

</details>


### [51] [Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects](https://arxiv.org/abs/2507.10216)
*Renad Al-Monef,Hassan Alhuzali,Nora Alturayeif,Ashwag Alasmari*

Key words: LLMs, 阿拉伯语NLP, 沙特方言, 文化理解, 基准测试

TL;DR: 该论文介绍了针对沙特阿拉伯方言的全面基准测试	exttt{Absher}，评估了多语言和阿拉伯语特定LLMs在方言和文化任务中的表现，揭示了其在文化和上下文理解上的不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着LLMs在阿拉伯语NLP应用中的普及，评估其对区域方言和文化细微差别的理解尤为重要，尤其是在沙特阿拉伯这样的语言多样化环境中。

Method: 论文设计了一个包含18,000多个多选题的基准测试	exttt{Absher}，涵盖六大类别，任务基于沙特各地区的方言和文化内容。

Result: 研究发现，LLMs在需要文化推断或上下文理解的任务上表现明显不足。

Conclusion: 迫切需要方言感知训练和文化对齐的评估方法，以提升LLMs在真实阿拉伯语应用中的表现。

Abstract: As large language models (LLMs) become increasingly central to Arabic NLP
applications, evaluating their understanding of regional dialects and cultural
nuances is essential, particularly in linguistically diverse settings like
Saudi Arabia. This paper introduces \texttt{Absher}, a comprehensive benchmark
specifically designed to assess LLMs performance across major Saudi dialects.
\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six
distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,
Cultural Interpretation, and Location Recognition. These questions are derived
from a curated dataset of dialectal words, phrases, and proverbs sourced from
various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,
including multilingual and Arabic-specific models. We also provide detailed
insights into their capabilities and limitations. Our results reveal notable
performance gaps, particularly in tasks requiring cultural inference or
contextual understanding. Our findings highlight the urgent need for
dialect-aware training and culturally aligned evaluation methodologies to
improve LLMs performance in real-world Arabic applications.

</details>


### [52] [Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation](https://arxiv.org/abs/2507.10326)
*Muzhaffar Hazman,Minh-Khoi Pham,Shweta Soundararajan,Goncalo Mordido,Leonardo Custode,David Lynch,Giorgio Cruciata,Yucheng Shi,Hongmeng Song,Wang Chao,Pan Yue,Aleksandar Milenovic,Alexandros Agapitos*

Key words: 提示工程, 进化搜索, 遗传编程, 局部搜索, 语言模型

TL;DR: 该论文提出了一种基于进化搜索的自动化离散提示优化方法，通过两阶段搜索优化提示，显著提升了小型通用LLMs在复杂任务中的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前自动化提示工程方法在复杂任务和小型模型上表现不佳，需要一种更能适应多样化需求和模型敏感性的优化方法。

Method: 采用两阶段进化搜索：第一阶段使用语法引导的遗传编程合成提示创建程序，第二阶段通过局部搜索进一步优化性能。

Result: 该方法在三个小型通用LLMs和四个领域特定任务中优于PromptWizard、OPRO和RL-Prompt基准方法。

Conclusion: 进化搜索方法适用于复杂任务和小型模型，并显著减少性能退化。

Abstract: Prompt engineering has proven to be a crucial step in leveraging pretrained
large language models (LLMs) in solving various real-world tasks. Numerous
solutions have been proposed that seek to automate prompt engineering by using
the model itself to edit prompts. However, the majority of state-of-the-art
approaches are evaluated on tasks that require minimal prompt templates and on
very large and highly capable LLMs. In contrast, solving complex tasks that
require detailed information to be included in the prompt increases the amount
of text that needs to be optimised. Furthermore, smaller models have been shown
to be more sensitive to prompt design. To address these challenges, we propose
an evolutionary search approach to automated discrete prompt optimisation
consisting of two phases. In the first phase, grammar-guided genetic
programming is invoked to synthesise prompt-creating programmes by searching
the space of programmes populated by function compositions of syntactic,
dictionary-based and LLM-based prompt-editing functions. In the second phase,
local search is applied to explore the neighbourhoods of best-performing
programmes in an attempt to further fine-tune their performance. Our approach
outperforms three state-of-the-art prompt optimisation approaches,
PromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose
LLMs in four domain-specific challenging tasks. We also illustrate several
examples where these benchmark methods suffer relatively severe performance
degradation, while our approach improves performance in almost all task-model
combinations, only incurring minimal degradation when it does not.

</details>


### [53] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Key words: 对抗攻击、NLP、生长边界矩阵、LSTM、S4、CNN、鲁棒性

TL;DR: 该论文提出了一种基于生长边界矩阵（GBM）的新正则化技术，旨在提升NLP模型对对抗攻击的鲁棒性，特别是针对LSTM、S4和CNN架构，实验显示其方法比现有基线提升高达8.8%的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管NLP模型有所进步，但仍易受对抗攻击（如同义词替换）影响，而递归网络和现代状态空间模型（如S4）的鲁棒性研究不足，亟需新方法应对其独特的挑战。

Method: 论文提出基于生长边界矩阵（GBM）的正则化技术，通过减少输入扰动对输出的影响，提升模型的鲁棒性，并针对LSTM、S4和CNN架构进行系统性分析。

Result: 在多个架构和基准数据集上的实验表明，该方法将对抗鲁棒性提升高达8.8%，超越现有最先进方法。

Conclusion: GBM方法有效提升了NLP模型的对抗鲁棒性，特别是在LSTM、S4和CNN架构中表现优异，为SSM（S4）的鲁棒性研究提供了新视角。

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [54] [Using AI to replicate human experimental results: a motion study](https://arxiv.org/abs/2507.10342)
*Rosa Illan Castillo,Javier Valenzuela*

Key words: 大型语言模型, 语言学, 情感含义, 心理语言学, 实证研究

TL;DR: 论文探讨了大型语言模型（LLMs）在语言学研究中作为可靠分析工具的潜力，通过比较人类和LLMs在情感含义任务中的表现，发现两者结果高度一致。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究LLMs是否能复制人类的细微判断，以验证其在语言学研究中作为辅助工具的可靠性。

Method: 通过四项心理语言学实验（涉及情感含义、效价变化、情感语境中的动词选择及句子-表情符号关联），分别对人类和LLMs进行任务测试，并进行统计分析。

Result: 人类和LLMs的回答表现出显著一致性（如Spearman's rho = .73-.96），微小差异不影响整体解释结果。

Conclusion: LLMs可作为语言学研究中可信赖的补充工具，扩展研究规模且保持解释有效性。

Abstract: This paper explores the potential of large language models (LLMs) as reliable
analytical tools in linguistic research, focusing on the emergence of affective
meanings in temporal expressions involving manner-of-motion verbs. While LLMs
like GPT-4 have shown promise across a range of tasks, their ability to
replicate nuanced human judgements remains under scrutiny. We conducted four
psycholinguistic studies (on emergent meanings, valence shifts, verb choice in
emotional contexts, and sentence-emoji associations) first with human
participants and then replicated the same tasks using an LLM. Results across
all studies show a striking convergence between human and AI responses, with
statistical analyses (e.g., Spearman's rho = .73-.96) indicating strong
correlations in both rating patterns and categorical choices. While minor
divergences were observed in some cases, these did not alter the overall
interpretative outcomes. These findings offer compelling evidence that LLMs can
augment traditional human-based experimentation, enabling broader-scale studies
without compromising interpretative validity. This convergence not only
strengthens the empirical foundation of prior human-based findings but also
opens possibilities for hypothesis generation and data expansion through AI.
Ultimately, our study supports the use of LLMs as credible and informative
collaborators in linguistic inquiry.

</details>


### [55] [Meanings are like Onions: a Layered Approach to Metaphor Processing](https://arxiv.org/abs/2507.10354)
*Silvia Cappa,Anna Sofia Lippolis,Stefano Zoia*

Key words: 隐喻处理、分层模型、概念融合、语用意、计算系统

TL;DR: 本文提出了一种分层的隐喻处理模型，将意义比作洋葱，包含内容分析、概念融合和语用意三个层次，为计算系统提供更丰富的隐喻解释方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 隐喻意义并非概念的平面映射，而是复杂的认知现象。本文旨在通过分层模型更全面地解释隐喻的多层次结构。

Method: 1. 内容分析（基础概念标注）；2. 概念融合（组合与新兴意义关联）；3. 语用意（捕捉说话者意图与语境）。

Result: 提出了一种形式化框架，支持计算系统超越表面关联，实现更深层次的隐喻理解。

Conclusion: 该模型为计算隐喻解释提供了更认知化、语境敏感的框架基础。

Abstract: Metaphorical meaning is not a flat mapping between concepts, but a complex
cognitive phenomenon that integrates multiple levels of interpretation. In this
paper, we propose a stratified model of metaphor processing that treats meaning
as an onion: a multi-layered structure comprising (1) content analysis, (2)
conceptual blending, and (3) pragmatic intentionality. This three-dimensional
framework allows for a richer and more cognitively grounded approach to
metaphor interpretation in computational systems. At the first level, metaphors
are annotated through basic conceptual elements. At the second level, we model
conceptual combinations, linking components to emergent meanings. Finally, at
the third level, we introduce a pragmatic vocabulary to capture speaker intent,
communicative function, and contextual effects, aligning metaphor understanding
with pragmatic theories. By unifying these layers into a single formal
framework, our model lays the groundwork for computational methods capable of
representing metaphorical meaning beyond surface associations, toward deeper,
more context-sensitive reasoning.

</details>


### [56] [From Sequence to Structure: Uncovering Substructure Reasoning in Transformers](https://arxiv.org/abs/2507.10435)
*Xinnan Dai,Kai Yang,Jay Revolinsky,Kai Guo,Aoran Wang,Bohang Zhang,Jiliang Tang*

Key words: 大型语言模型, 图形推理, 子结构提取, Transformer, 诱导子结构过滤

TL;DR: 本文研究了仅解码器的Transformer架构如何理解图形结构，提出了诱导子结构过滤（ISF）视角，揭示了其在多层级中的子结构识别机制，并验证了其在大型语言模型中处理图形推理任务的能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索仅解码器的Transformer架构如何在文本描述的图形结构中执行子结构提取任务，以理解其内部机制。

Method: 通过实验和理论分析提出诱导子结构过滤（ISF）视角，并在不同图形类型中验证其有效性。

Result: 发现Transformer能够通过子结构提取高效处理复杂图形模式，包括分子图等属性图。

Conclusion: 序列化Transformer能够有效提取图形数据中的子结构，为图形推理任务提供了新视角。

Abstract: Recent studies suggest that large language models (LLMs) possess the
capability to solve graph reasoning tasks. Notably, even when graph structures
are embedded within textual descriptions, LLMs can still effectively answer
related questions. This raises a fundamental question: How can a decoder-only
Transformer architecture understand underlying graph structures? To address
this, we start with the substructure extraction task, interpreting the inner
mechanisms inside the transformers and analyzing the impact of the input
queries. Specifically, through both empirical results and theoretical analysis,
we present Induced Substructure Filtration (ISF), a perspective that captures
the substructure identification in the multi-layer transformers. We further
validate the ISF process in LLMs, revealing consistent internal dynamics across
layers. Building on these insights, we explore the broader capabilities of
Transformers in handling diverse graph types. Specifically, we introduce the
concept of thinking in substructures to efficiently extract complex composite
patterns, and demonstrate that decoder-only Transformers can successfully
extract substructures from attributed graphs, such as molecular graphs.
Together, our findings offer a new insight on how sequence-based Transformers
perform the substructure extraction task over graph data.

</details>


### [57] [Referential ambiguity and clarification requests: comparing human and LLM behaviour](https://arxiv.org/abs/2507.10445)
*Chris Madge,Matthew Purver,Massimo Poesio*

Key words: LLM, 任务导向对话, 澄清问题, 模糊性, 推理能力

TL;DR: 论文研究了LLM在异步任务导向对话中提出澄清问题的能力，发现人类与LLM在模糊性和澄清问题上的行为差异显著，并探讨了推理能力对LLM提问的影响。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨LLM在任务导向对话中处理模糊性和提出澄清问题的能力，并与人类行为进行对比。

Method: 构建了一个结合Minecraft对话语料库标注的新语料库，比较了LLM和人类在模糊性情况下的澄清提问行为，并测试了不同推理方法对LLM提问的影响。

Result: 人类很少为指代模糊提出澄清问题，而更多为任务不确定性提问；LLM则相反。推理能力能提升LLM提问的频率和相关性。

Conclusion: LLM提出澄清问题的能力可能与模拟推理能力相关，但其行为与人类存在明显差异。

Abstract: In this work we examine LLMs' ability to ask clarification questions in
task-oriented dialogues that follow the asynchronous
instruction-giver/instruction-follower format. We present a new corpus that
combines two existing annotations of the Minecraft Dialogue Corpus -- one for
reference and ambiguity in reference, and one for SDRT including clarifications
-- into a single common format providing the necessary information to
experiment with clarifications and their relation to ambiguity. With this
corpus we compare LLM actions with original human-generated clarification
questions, examining how both humans and LLMs act in the case of ambiguity. We
find that there is only a weak link between ambiguity and humans producing
clarification questions in these dialogues, and low correlation between humans
and LLMs. Humans hardly ever produce clarification questions for referential
ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce
more clarification questions for referential ambiguity, but less so for task
uncertainty. We question if LLMs' ability to ask clarification questions is
predicated on their recent ability to simulate reasoning, and test this with
different reasoning approaches, finding that reasoning does appear to increase
question frequency and relevancy.

</details>


### [58] [From BERT to Qwen: Hate Detection across architectures](https://arxiv.org/abs/2507.10468)
*Ariadna Mon,Saúl Fenollosa,Jon Lecumberri*

Key words: 仇恨言论检测；双向Transformer；自回归LLMs；在线平台；基准测试

TL;DR: 研究表明，超大规模自回归LLMs是否能比经典双向编码器更有效地检测真实世界中的仇恨言论，尚未明确验证。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 在线平台在遏制仇恨言论的同时，避免过度审查合法言论是一个挑战。研究旨在验证超大规模LLMs是否比经典编码器更优。

Method: 研究对双向Transformer编码器和超大规模自回归LLMs进行了基准测试，使用了在线交互的精选语料库（Hate or No Hate）。

Result: 论文未明确给出结果，但提出了对两种模型家族在实际仇恨言论检测中的效能进行比较。

Conclusion: 研究发现超大规模自回归LLMs在仇恨言论检测中的实际效能仍需验证。

Abstract: Online platforms struggle to curb hate speech without over-censoring
legitimate discourse. Early bidirectional transformer encoders made big
strides, but the arrival of ultra-large autoregressive LLMs promises deeper
context-awareness. Whether this extra scale actually improves practical
hate-speech detection on real-world text remains unverified. Our study puts
this question to the test by benchmarking both model families, classic encoders
and next-generation LLMs, on curated corpora of online interactions for
hate-speech detection (Hate or No Hate).

</details>


### [59] [MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking](https://arxiv.org/abs/2507.10472)
*Mohamed T. Younes,Omar Walid,Mai Hassan,Ali Hamdi*

Key words: MLAR, RPA, 招聘系统, 大型语言模型, 自动化

TL;DR: MLAR是一种基于大型语言模型和RPA的招聘跟踪系统，显著提升了简历处理的效率和准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统招聘流程因时间和资源限制存在瓶颈，需要更高效的解决方案。

Method: MLAR采用三层LLM架构：提取职位需求、解析简历、语义匹配；结合RPA自动化流程。

Result: 处理2400份简历时，MLAR平均耗时5.4秒/份，比主流RPA平台快16.9%-17.1%。

Conclusion: MLAR为招聘提供了高效、准确且可扩展的解决方案。

Abstract: This paper introduces an innovative Applicant Tracking System (ATS) enhanced
by a novel Robotic process automation (RPA) framework or as further referred to
as MLAR. Traditional recruitment processes often encounter bottlenecks in
resume screening and candidate shortlisting due to time and resource
constraints. MLAR addresses these challenges employing Large Language Models
(LLMs) in three distinct layers: extracting key characteristics from job
postings in the first layer, parsing applicant resume to identify education,
experience, skills in the second layer, and similarity matching in the third
layer. These features are then matched through advanced semantic algorithms to
identify the best candidates efficiently. Our approach integrates seamlessly
into existing RPA pipelines, automating resume parsing, job matching, and
candidate notifications. Extensive performance benchmarking shows that MLAR
outperforms the leading RPA platforms, including UiPath and Automation
Anywhere, in high-volume resume-processing tasks. When processing 2,400
resumes, MLAR achieved an average processing time of 5.4 seconds per resume,
reducing processing time by approximately 16.9% compared to Automation Anywhere
and 17.1% compared to UiPath. These results highlight the potential of MLAR to
transform recruitment workflows by providing an efficient, accurate, and
scalable solution tailored to modern hiring needs.

</details>


### [60] [Can You Detect the Difference?](https://arxiv.org/abs/2507.10475)
*İsmail Tarım,Aytuğ Onan*

Key words: 大语言模型、扩散模型、文本检测、困惑度、突发性

TL;DR: 本文比较了扩散模型(LLaDA)和自回归模型(LLaMA)生成的文本与人写作的区别，发现扩散模型更难检测，呼吁开发针对性检测方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着大语言模型的快速发展，检测AI生成文本的可靠性成为关注点。目前的方法对自回归模型有效，但对扩散模型的效果未知。

Method: 通过对2000个样本的系统比较，分析了困惑度、突发性、词汇多样性、可读性及BLEU/ROUGE分数。

Result: 扩散模型在困惑度和突发性上更接近人类文本，导致现有检测方法失效；自回归模型困惑度低但词汇保真度差。单一指标无法区分扩散输出和人类写作。

Conclusion: 需开发针对扩散模型的检测方法，如混合模型、特异性签名和鲁棒水印。

Abstract: The rapid advancement of large language models (LLMs) has raised concerns
about reliably detecting AI-generated text. Stylometric metrics work well on
autoregressive (AR) outputs, but their effectiveness on diffusion-based models
is unknown. We present the first systematic comparison of diffusion-generated
text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,
burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that
LLaDA closely mimics human text in perplexity and burstiness, yielding high
false-negative rates for AR-oriented detectors. LLaMA shows much lower
perplexity but reduced lexical fidelity. Relying on any single metric fails to
separate diffusion outputs from human writing. We highlight the need for
diffusion-aware detectors and outline directions such as hybrid models,
diffusion-specific stylometric signatures, and robust watermarking.

</details>


### [61] [Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation](https://arxiv.org/abs/2507.10524)
*Sangmin Bae,Yujin Kim,Reza Bayat,Sungnyun Kim,Jiyoun Ha,Tal Schuster,Adam Fisch,Hrayr Harutyunyan,Ziwei Ji,Aaron Courville,Se-Young Yun*

Key words: Mixture-of-Recursions, 语言模型, 参数共享, 自适应计算

TL;DR: Mixture-of-Recursions (MoR) 是一种统一框架，通过结合参数共享和自适应计算，提升语言模型的效率和性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为了解决语言模型在扩展时带来的高计算和内存开销问题。

Method: MoR 通过共享层和轻量级路由器动态分配递归深度，实现参数和内存访问的高效性。

Result: 在 135M 到 1.7B 参数范围内的模型上，MoR 显著降低了训练和推理成本，并提升了性能。

Conclusion: MoR 是一种高效的方法，可以在不增加成本的情况下提升大模型的性能。

Abstract: Scaling language models unlocks impressive capabilities, but the accompanying
computational and memory demands make both training and deployment expensive.
Existing efficiency efforts typically target either parameter sharing or
adaptive computation, leaving open the question of how to attain both
simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework
that combines the two axes of efficiency inside a single Recursive Transformer.
MoR reuses a shared stack of layers across recursion steps to achieve parameter
efficiency, while lightweight routers enable adaptive token-level thinking by
dynamically assigning different recursion depths to individual tokens. This
allows MoR to focus quadratic attention computation only among tokens still
active at a given recursion depth, further improving memory access efficiency
by selectively caching only their key-value pairs. Beyond these core
mechanisms, we also propose a KV sharing variant that reuses KV pairs from the
first recursion, specifically designed to decrease prefill latency and memory
footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms
a new Pareto frontier: at equal training FLOPs and smaller model sizes, it
significantly lowers validation perplexity and improves few-shot accuracy,
while delivering higher throughput compared with vanilla and existing recursive
baselines. These gains demonstrate that MoR is an effective path towards
large-model quality without incurring large-model cost.

</details>


### [62] [CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks](https://arxiv.org/abs/2507.10535)
*Hongchao Jiang,Yiming Chen,Yushi Cao,Hung-yi Lee,Robby T. Tan*

Key words: LLM, 代码评委, CodeJudgeBench, 编码任务, 随机性, 提示策略

TL;DR: 论文介绍了CodeJudgeBench基准，用于评估LLM在代码生成、修复和单元测试中的表现，发现思考模型表现更优，但存在随机性和敏感性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索LLM作为评委在编码任务中的有效性，弥补现有基准的不足。

Method: 引入CodeJudgeBench基准，评估26个LLM评委模型在三种编码任务中的表现。

Result: 思考模型优于非思考模型，但存在显著的随机性和敏感性；成对比较和保留完整响应可提升评委性能。

Conclusion: LLM评委在编码任务中有效性不足，需进一步优化提示策略和模型设计以提高可靠性。

Abstract: Large Language Models (LLMs) have significantly advanced the state-of-the-art
in various coding tasks. Beyond directly answering user queries, LLMs can also
serve as judges, assessing and comparing the quality of responses generated by
other models. Such an evaluation capability is crucial both for benchmarking
different LLMs and for improving response quality through response ranking.
However, despite the growing adoption of the LLM-as-a-Judge paradigm, its
effectiveness in coding scenarios remains underexplored due to the absence of
dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a
benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge
models across three critical coding tasks: code generation, code repair, and
unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge
models, we find that recent thinking models significantly outperform
non-thinking models on our carefully designed code judging tasks. Notably, even
relatively small thinking models, such as Qwen3-8B, can outperform specially
trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still
exhibit significant randomness in their judgment of coding tasks. For pairwise
judging tasks, simply changing the order in which responses are presented can
substantially impact accuracy. In addition, when judging code and unit tests
written by different LLMs, LLM-as-a-Judge models also show variance in
performance. This sensitivity raises concerns about the reliability and
consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal
prompting strategies for LLM-as-a-Judge. We find that using pair-wise
comparison outperforms scalar point-wise judging. Furthermore, retaining
comments and reasoning in the full, unprocessed LLM response leads to improved
judge performance.

</details>


### [63] [REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once](https://arxiv.org/abs/2507.10541)
*Zhuoshi Pan,Qizhi Pei,Yu Li,Qiyao Sun,Zinan Tang,H. Vicky Zhao,Conghui He,Lijun Wu*

Key words: 大型推理模型,REST,评测框架,多任务压力

TL;DR: REST框架通过同时测试多个问题评估大型推理模型，揭示现有单一问题评测的局限性，并展示了更强的区分能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有评测方法局限于单一问题推理，难以评估真实世界中的多任务压力需求。

Method: 提出REST框架，同时测试多个问题，评估模型在优先级分配、抗干扰和认知负载管理方面的能力。

Result: SOTA模型在REST下表现显著下降，REST区分能力更强。“长到短”训练模型表现更优。

Conclusion: REST是一种高效、可持续的评测范式，更贴近实际需求。

Abstract: Recent Large Reasoning Models (LRMs) have achieved remarkable progress on
task-specific benchmarks, yet their evaluation methods remain constrained by
isolated problem-solving paradigms. Existing benchmarks predominantly assess
single-question reasoning through sequential testing, resulting critical
limitations: (1) vulnerability to data contamination and less challenging
(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual
creation of new questions with large human efforts, (2) failure to evaluate
models under multi-context pressure, a key requirement for real-world
deployment. To bridge this gap, we present REST (Reasoning Evaluation through
Simultaneous Testing), a stress-testing framework that concurrently exposes
LRMs to multiple problems simultaneously. Beyond basic reasoning, REST
specifically evaluates several under-tested capabilities: contextual priority
allocation, cross-problem interference resistance, and dynamic cognitive load
management. Our evaluation reveals several striking findings: Even
state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance
degradation under stress testing. Crucially, REST demonstrates stronger
discriminative power than existing benchmarks, revealing pronounced performance
differences among models that exhibit similar, near-ceiling performance under
single-question evaluations. Some key mechanistic insights emerge from our
analysis: (1) the "overthinking trap" is a critical factor contributing to the
performance degradation; (2) the models trained with "long2short" technique
preserve more accuracy of their single-problem performance under REST,
outperforming standard-trained counterparts. These results establish REST as a
cost-efficient, future-proof evaluation paradigm that better reflects
real-world reasoning demands while reducing reliance on continuous human
annotation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [64] [Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning](https://arxiv.org/abs/2507.08828)
*Tarek Berghout*

Key words: Recurrent Expansion, Deep Learning, Self-evolving Models, Adaptive AI, Multiverse RE

TL;DR: 论文提出Recurrent Expansion（RE）作为超越传统机器学习和深度学习的新范式，通过模型自身行为的迭代改进实现自适应智能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统深度学习仅关注静态数据表示，而RE引入模型行为学习的维度，以实现更智能、自适应的系统。

Method: RE通过多轮映射分析模型内部特征与性能信号（如损失），支持迭代自改进。扩展版本包括Multiverse RE（多模型并行）和Heterogeneous MVRE（异构模型整合），最终提出Scalable HMVRE（选择性机制与规模多样性）。

Result: RE实现了从静态表示学习到行为感知自进化系统的转变，为可扩展、自适应的AI奠定基础。

Conclusion: RE开创了模型能够自我推理学习动态的新方向，推动智能系统的自适应性与可扩展性。

Abstract: This paper introduces Recurrent Expansion (RE) as a new learning paradigm
that advances beyond conventional Machine Learning (ML) and Deep Learning (DL).
While DL focuses on learning from static data representations, RE proposes an
additional dimension: learning from the evolving behavior of models themselves.
RE emphasizes multiple mappings of data through identical deep architectures
and analyzes their internal representations (i.e., feature maps) in conjunction
with observed performance signals such as loss. By incorporating these
behavioral traces, RE enables iterative self-improvement, allowing each model
version to gain insight from its predecessors. The framework is extended
through Multiverse RE (MVRE), which aggregates signals from parallel model
instances, and further through Heterogeneous MVRE (HMVRE), where models of
varying architectures contribute diverse perspectives. A scalable and adaptive
variant, Sc-HMVRE, introduces selective mechanisms and scale diversity for
real-world deployment. Altogether, RE presents a shift in DL: from purely
representational learning to behavior-aware, self-evolving systems. It lays the
groundwork for a new class of intelligent models capable of reasoning over
their own learning dynamics, offering a path toward scalable, introspective,
and adaptive artificial intelligence. A simple code example to support
beginners in running their own experiments is provided in Code Availability
Section of this paper.

</details>


### [65] [Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI](https://arxiv.org/abs/2507.08829)
*Kimia Soroush,Nastaran Shirazi,Mohsen Raji*

Key words: DNN, TMR, XAI, bit-flip faults, reliability

TL;DR: 该论文提出了一种基于可解释人工智能（XAI）的高效三重模块冗余（TMR）方法，通过LRP技术计算参数重要性，选择性保护关键权重，提升DNN在比特翻转故障下的可靠性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在安全关键领域，DNN的可靠性至关重要。传统TMR方法开销大，需选择性应用。XAI技术可为选择标准提供准确依据。

Method: 使用低成本的梯度XAI技术（LRP）计算DNN参数的重要性分数，选择性应用TMR保护关键权重。

Result: 在VGG16和AlexNet模型上验证，能在相同开销下显著提升可靠性（AlexNet在10-4误码率下可靠性提升60%+）。

Conclusion: 结合XAI的TMR方法高效、低成本，显著提升DNN可靠性。

Abstract: Deep Neural Networks (DNNs) are widely employed in safety-critical domains,
where ensuring their reliability is essential. Triple Modular Redundancy (TMR)
is an effective technique to enhance the reliability of DNNs in the presence of
bit-flip faults. In order to handle the significant overhead of TMR, it is
applied selectively on the parameters and components with the highest
contribution at the model output. Hence, the accuracy of the selection
criterion plays the key role on the efficiency of TMR. This paper presents an
efficient TMR approach to enhance the reliability of DNNs against bit-flip
faults using an Explainable Artificial Intelligence (XAI) method. Since XAI can
provide valuable insights about the importance of individual neurons and
weights in the performance of the network, they can be applied as the selection
metric in TMR techniques. The proposed method utilizes a low-cost,
gradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to
calculate importance scores for DNN parameters. These scores are then used to
enhance the reliability of the model, with the most critical weights being
protected by TMR. The proposed approach is evaluated on two DNN models, VGG16
and AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate
that the method can protect the AlexNet model at a bit error rate of 10-4,
achieving over 60% reliability improvement while maintaining the same overhead
as state-of-the-art methods.

</details>


### [66] [A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting](https://arxiv.org/abs/2507.08832)
*Niranjan Mallikarjun Sindhur,Pavithra C,Nivya Muchikel*

Key words: 决策支持系统，随机森林，LSTM，语音接口，农业经济

TL;DR: 该论文提出了一种结合机器学习和人机交互的新型决策支持系统，帮助印度卡纳塔克邦的低识字率农民应对市场和气候波动。系统通过语音接口提供经济优化的作物推荐。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决发展中国家农民面临的市场和气候波动风险，同时克服识字率低导致的数字鸿沟。

Method: 采用混合推荐引擎，结合随机森林分类器（评估作物适宜性）和LSTM网络（预测市场价格），并通过本地语言的语音接口提供服务。

Result: 随机森林模型准确率达98.5%，LSTM模型价格预测误差低。系统显著提升了农民的经济抗风险能力。

Conclusion: 该系统为边缘化农业社区提供了可扩展且高效的经济优化解决方案。

Abstract: Farmers in developing regions like Karnataka, India, face a dual challenge:
navigating extreme market and climate volatility while being excluded from the
digital revolution due to literacy barriers. This paper presents a novel
decision support system that addresses both challenges through a unique
synthesis of machine learning and human-computer interaction. We propose a
hybrid recommendation engine that integrates two predictive models: a Random
Forest classifier to assess agronomic suitability based on soil, climate, and
real-time weather data, and a Long Short-Term Memory (LSTM) network to forecast
market prices for agronomically viable crops. This integrated approach shifts
the paradigm from "what can grow?" to "what is most profitable to grow?",
providing a significant advantage in mitigating economic risk. The system is
delivered through an end-to-end, voice-based interface in the local Kannada
language, leveraging fine-tuned speech recognition and high-fidelity speech
synthesis models to ensure accessibility for low-literacy users. Our results
show that the Random Forest model achieves 98.5% accuracy in suitability
prediction, while the LSTM model forecasts harvest-time prices with a low
margin of error. By providing data-driven, economically optimized
recommendations through an inclusive interface, this work offers a scalable and
impactful solution to enhance the financial resilience of marginalized farming
communities.

</details>


### [67] [LoRA Is Slower Than You Think](https://arxiv.org/abs/2507.08833)
*Seokmin Ko*

Key words: 低秩适应（LoRA）、大型语言模型（LLM）、微调、计算效率、资源优化

TL;DR: 本文分析了低秩适应（LoRA）在大型语言模型（LLM）微调中的性能不一致问题，并提出更高效的微调方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: LoRA在减少参数更新和提升计算效率方面具有优势，但其速度提升在不同模型架构和训练设置中并不一致，需要深入分析。

Method: 进行全面的LoRA性能分析，研究限制其速度提升的潜在因素，并基于发现提出更高效的微调方法。

Result: 提出的方法在性能上与LoRA相当或更优，同时提供更一致的训练速度提升。

Conclusion: 本文为资源受限条件下优化LLM微调提供了有价值的见解和实用指南。

Abstract: Low-Rank Adaptation (LoRA) is one of the most widely used techniques for
fine-tuning large language models (LLMs). By introducing a small number of
trainable low-rank weight matrices, LoRA substantially reduces the number of
parameters that need to be updated, offering significant advantages in memory
consumption and computational efficiency compared to full fine-tuning. However,
we observed that LoRA does not consistently provide speed improvements across
all model architectures and training setups. Motivated by this inconsistency,
we conduct a comprehensive analysis of LoRA's performance and investigate the
underlying factors limiting its speedup. Based on our findings, we propose
several methods for more efficient fine-tuning of LLMs. We empirically evaluate
these methods and compare them to LoRA, demonstrating that our approach
achieves comparable or superior performance while delivering more consistent
training speed improvements. Our work offers valuable insights and practical
guidelines for practitioners seeking to optimize LLM fine-tuning under resource
constraints.

</details>


### [68] [Physical Informed Neural Networks for modeling ocean pollutant](https://arxiv.org/abs/2507.08834)
*Karishma Battina,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Key words: 物理信息神经网络(PINN)、污染物扩散、2D平流-扩散方程、Julia语言、混合损失函数

TL;DR: 传统数值方法难以处理复杂和大规模的海洋污染扩散模拟，本文提出一种基于物理信息的神经网络（PINN）框架，通过嵌入物理定律和噪声合成数据来解决2D平流-扩散方程的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统数值方法在处理大规模动态海洋污染物扩散时面临复杂性和计算挑战，亟需一种高效且物理一致的模拟方法。

Method: 采用PINN框架，结合2D平流-扩散方程物理定律和噪声合成数据训练神经网络，利用混合损失函数（包括PDE残差、边界/初始条件一致性和数据拟合项）。

Result: 模型成功实现了物理一致的预测，并通过Julia语言实现了高性能计算，提供了一种可扩展且灵活的替代方案。

Conclusion: PINN框架为解决复杂污染物扩散问题提供了一种高效、灵活且物理一致的方法，优于传统数值方法。

Abstract: Traditional numerical methods often struggle with the complexity and scale of
modeling pollutant transport across vast and dynamic oceanic domains. This
paper introduces a Physics-Informed Neural Network (PINN) framework to simulate
the dispersion of pollutants governed by the 2D advection-diffusion equation.
The model achieves physically consistent predictions by embedding physical laws
and fitting to noisy synthetic data, generated via a finite difference method
(FDM), directly into the neural network training process. This approach
addresses challenges such as non-linear dynamics and the enforcement of
boundary and initial conditions. Synthetic data sets, augmented with varying
noise levels, are used to capture real-world variability. The training
incorporates a hybrid loss function including PDE residuals, boundary/initial
condition conformity, and a weighted data fit term. The approach takes
advantage of the Julia language scientific computing ecosystem for
high-performance simulations, offering a scalable and flexible alternative to
traditional solvers

</details>


### [69] [Representation learning with a transformer by contrastive learning for money laundering detection](https://arxiv.org/abs/2507.08835)
*Harold Guéneau,Alain Celisse,Pascal Delange*

Key words: 洗钱检测, Transformer, 对比学习, 时间序列, 假阳性率控制

TL;DR: 该论文提出一种利用Transformer神经网络处理结构化时间序列数据的新方法，通过对比学习生成表征并实现洗钱检测，控制假阳性率，优于基于规则或LSTM的方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决洗钱检测问题，减少对领域专家标注的依赖，并提升对非欺诈者和欺诈者的检测能力。

Method: 提出两阶段方法：1）通过对比学习学习时间序列表征；2）利用表征生成洗钱评分，并使用Benjamini-Hochberg程序控制假阳性率。

Result: 实验表明Transformer能有效学习通用表征，并在最小专家监督下成功捕捉洗钱模式，同时控制假阳性率。

Conclusion: 新方法在洗钱检测中优于规则或LSTM方法，且能有效平衡检测精度与假阳性率。

Abstract: The present work tackles the money laundering detection problem. A new
procedure is introduced which exploits structured time series of both
qualitative and quantitative data by means of a transformer neural network. The
first step of this procedure aims at learning representations of time series
through contrastive learning (without any labels). The second step leverages
these representations to generate a money laundering scoring of all
observations. A two-thresholds approach is then introduced, which ensures a
controlled false-positive rate by means of the Benjamini-Hochberg (BH)
procedure. Experiments confirm that the transformer is able to produce general
representations that succeed in exploiting money laundering patterns with
minimal supervision from domain experts. It also illustrates the higher ability
of the new procedure for detecting nonfraudsters as well as fraudsters, while
keeping the false positive rate under control. This greatly contrasts with
rule-based procedures or the ones based on LSTM architectures.

</details>


### [70] [Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing](https://arxiv.org/abs/2507.08836)
*Damien Fovet,Shashank Chamoli,Sarah Oury,Srishti Singhal*

Key words: CompactifAI, Llama 3.1 8B, 压缩, 效率, 准确性

TL;DR: CompactifAI压缩方法显著减少Llama 3.1 8B模型的资源消耗，同时保持准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 评估CompactifAI压缩方法在大语言模型Llama 3.1 8B上的性能，关注效率和准确性。

Method: 使用Codecarbon评估能耗，Ragas评估准确性，对比压缩与完整模型。

Result: 压缩模型资源消耗显著减少，准确性未受影响。

Conclusion: CompactifAI使模型更高效、可扩展且经济。

Abstract: This study evaluates the performance of a compression method, called
CompactifAI, developed by Multiverse Computing, applied to the large language
model Llama 3.1 8B\cite{llama}. The evaluation focused on model efficiency (in
terms of energy consumption) and accuracy using respectively the frameworks
Codecarbon\cite{codecarbon} and Ragas\cite{ragas}. A comparison was performed
between the model compressed with
CompactifAI\cite{compactifai}\cite{compactifai2} and its full-size version. Our
findings reveal that the compressed model using CompactifAI not only
significantly reduced the computational resources but also maintained the model
accuracy, making the model more efficient, scalable and cost-effective.

</details>


### [71] [wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models](https://arxiv.org/abs/2507.08838)
*Xiaohang Tang,Rares Dolga,Sangwoong Yoon,Ilija Bogunovic*

Key words: 扩散型大语言模型，强化学习，加权似然，推理能力，策略优化

TL;DR: 论文提出了一种名为$	exttt{wd1}$的新方法，通过加权似然优化扩散型大语言模型（dLLMs）的推理能力，避免了传统强化学习中多次近似计算的问题，显著提高了效率和准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前通过强化学习提升扩散型大语言模型（dLLMs）推理能力的方法存在计算复杂度高和潜在偏差大的问题，尤其是在重要性采样中需要多次近似计算。

Method: 论文提出了$	exttt{wd1}$方法，将优化目标重新表述为加权似然，仅需对当前策略似然进行一次近似，减少了计算负担和偏差。

Result: 在没有监督微调或监督数据的情况下，$	exttt{wd1}$在广泛使用的推理基准测试中比现有RL方法准确率提高了16%，同时减少了训练时间和每次梯度步的函数评估次数（NFEs）。

Conclusion: $	exttt{wd1}$作为一种更高效和有效的方法，为扩散型大语言模型的强化学习应用提供了新的方向。

Abstract: Improving the reasoning capabilities of diffusion-based large language models
(dLLMs) through reinforcement learning (RL) remains an open problem. The
intractability of dLLMs likelihood function necessitates approximating the
current, old, and reference policy likelihoods at each policy optimization
step. This reliance introduces additional computational overhead and lead to
potentially large bias -- particularly when approximation errors occur in the
denominator of policy ratios used for importance sampling. To mitigate these
issues, we introduce $\mathtt{wd1}$, a novel policy optimization approach that
reformulates the objective as a weighted likelihood, requiring only a single
approximation for the current parametrized policy likelihood. Experiments on
widely used reasoning benchmarks demonstrate that $\mathtt{wd1}$, without
supervised fine-tuning (SFT) or any supervised data, outperforms existing RL
methods for dLLMs, achieving up to 16% higher accuracy. $\mathtt{wd1}$ delivers
additional computational gains, including reduced training time and fewer
function evaluations (NFEs) per gradient step. These findings, combined with
the simplicity of method's implementation and R1-Zero-like training (no SFT),
position $\mathtt{wd1}$ as a more effective and efficient method for applying
RL to dLLMs reasoning.

</details>


### [72] [Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer](https://arxiv.org/abs/2507.08839)
*Xiaowei Yu,Jing Zhang,Tong Chen,Yan Zhuang,Minheng Chen,Chao Cao,Yanjun Lyu,Lu Zhang,Li Su,Tianming Liu,Dajiang Zhu*

Key words: 路易体痴呆,阿尔茨海默病,迁移学习,领域自适应,注意力机制

TL;DR: 提出了一种基于注意力机制的迁移学习方法（TAT），利用阿尔茨海默病（AD）的丰富数据增强路易体痴呆（LBD）的诊断能力，解决了数据稀缺和领域偏移问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 路易体痴呆（LBD）是一种常见但研究不足的痴呆症，诊断数据稀缺且存在领域偏移，而AD数据丰富，适合迁移学习。

Method: 设计了Transferability Aware Transformer（TAT），通过注意力机制自适应地分配权重，抑制领域特异性特征，减少领域偏移。

Result: 实验证明TAT能有效提升LBD诊断准确性，为罕见疾病的领域自适应诊断提供了可行框架。

Conclusion: 本研究首次探索了在数据稀缺和领域偏移条件下从AD到LBD的迁移学习，为罕见病诊断提供了新思路。

Abstract: Lewy Body Disease (LBD) is a common yet understudied form of dementia that
imposes a significant burden on public health. It shares clinical similarities
with Alzheimer's disease (AD), as both progress through stages of normal
cognition, mild cognitive impairment, and dementia. A major obstacle in LBD
diagnosis is data scarcity, which limits the effectiveness of deep learning. In
contrast, AD datasets are more abundant, offering potential for knowledge
transfer. However, LBD and AD data are typically collected from different sites
using different machines and protocols, resulting in a distinct domain shift.
To effectively leverage AD data while mitigating domain shift, we propose a
Transferability Aware Transformer (TAT) that adapts knowledge from AD to
enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived
from structural MRI as training data. Built on the attention mechanism, TAT
adaptively assigns greater weights to disease-transferable features while
suppressing domain-specific ones, thereby reducing domain shift and improving
diagnostic accuracy with limited LBD data. The experimental results demonstrate
the effectiveness of TAT. To the best of our knowledge, this is the first study
to explore domain adaptation from AD to LBD under conditions of data scarcity
and domain shift, providing a promising framework for domain-adaptive diagnosis
of rare diseases.

</details>


### [73] [Zero-Shot Neural Architecture Search with Weighted Response Correlation](https://arxiv.org/abs/2507.08841)
*Kun Jing,Luoyu Chen,Jungang Xu,Jianwei Tai,Yiyu Wang,Shuaimin Li*

Key words: 神经架构搜索,零样本NAS,训练免费代理,WRCor,ImageNet-1k

TL;DR: 论文提出了一种名为WRCor的训练免费估计方法，通过响应相关性矩阵评估神经架构的表达性和泛化能力，实验表明其比现有代理更高效，且在NAS任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统神经架构搜索（NAS）方法因需从零训练多个架构而计算成本高，现有零样本NAS方法虽快但效果、稳定性和通用性不足，因此提出更高效的训练免费代理WRCor。

Method: 提出WRCor方法，利用不同输入样本的响应相关性矩阵计算架构代理分数，评估其表达性和泛化能力，并结合多种搜索策略应用于NAS。

Result: WRCor及其投票代理在代理评估中表现优于现有方法，搜索到的架构在ImageNet-1k上仅需4 GPU小时即达到22.1%测试错误率。

Conclusion: WRCor是一种高效稳定的零样本NAS估计代理，显著提升搜索效率并在多个搜索空间中优于现有NAS算法。

Abstract: Neural architecture search (NAS) is a promising approach for automatically
designing neural network architectures. However, the architecture estimation of
NAS is computationally expensive and time-consuming because of training
multiple architectures from scratch. Although existing zero-shot NAS methods
use training-free proxies to accelerate the architecture estimation, their
effectiveness, stability, and generality are still lacking. We present a novel
training-free estimation proxy called weighted response correlation (WRCor).
WRCor utilizes correlation coefficient matrices of responses across different
input samples to calculate the proxy scores of estimated architectures, which
can measure their expressivity and generalizability. Experimental results on
proxy evaluation demonstrate that WRCor and its voting proxies are more
efficient estimation strategies than existing proxies. We also apply them with
different search strategies in architecture search. Experimental results on
architecture search show that our zero-shot NAS algorithm outperforms most
existing NAS algorithms in different search spaces. Our NAS algorithm can
discover an architecture with a 22.1% test error on the ImageNet-1k dataset
within 4 GPU hours. All codes are publicly available at
https://github.com/kunjing96/ZSNAS-WRCor.git.

</details>


### [74] [Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing](https://arxiv.org/abs/2507.08842)
*Zhufeng Lu,Chentao Jia,Ming Hu,Xiaofei Xie,Mingsong Chen*

Key words: 联邦学习、推荐系统、通信效率、动作共享、自适应聚类

TL;DR: FedRAS是一个通信高效的联邦推荐系统框架，通过动作共享策略和自适应聚类机制，显著减少通信开销并保持推荐性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦推荐系统（FedRecs）存在通信开销高和训练效率低的问题，现有压缩技术会降低模型性能。

Method: 提出FedRAS框架，采用动作共享策略聚类梯度为少量模型更新动作，并通过自适应聚类机制适应异构环境。

Result: 实验结果表明，FedRAS能减少96.88%的通信负载，且不影响推荐性能。

Conclusion: FedRAS有效解决了FedRecs的通信和效率问题，适用于异构场景。

Abstract: As a promising privacy-aware collaborative model training paradigm, Federated
Learning (FL) is becoming popular in the design of distributed recommender
systems. However, Federated Recommender Systems (FedRecs) greatly suffer from
two major problems: i) extremely high communication overhead due to massive
item embeddings involved in recommendation systems, and ii) intolerably low
training efficiency caused by the entanglement of both heterogeneous network
environments and client devices. Although existing methods attempt to employ
various compression techniques to reduce communication overhead, due to the
parameter errors introduced by model compression, they inevitably suffer from
model performance degradation. To simultaneously address the above problems,
this paper presents a communication-efficient FedRec framework named FedRAS,
which adopts an action-sharing strategy to cluster the gradients of item
embedding into a specific number of model updating actions for communication
rather than directly compressing the item embeddings. In this way, the cloud
server can use the limited actions from clients to update all the items. Since
gradient values are significantly smaller than item embeddings, constraining
the directions of gradients (i.e., the action space) introduces smaller errors
compared to compressing the entire item embedding matrix into a reduced space.
To accommodate heterogeneous devices and network environments, FedRAS
incorporates an adaptive clustering mechanism that dynamically adjusts the
number of actions. Comprehensive experiments on well-known datasets demonstrate
that FedRAS can reduce the size of communication payloads by up to 96.88%,
while not sacrificing recommendation performance within various heterogeneous
scenarios. We have open-sourced FedRAS at
https://github.com/mastlab-T3S/FedRAS.

</details>


### [75] [Can We Predict Your Next Move Without Breaking Your Privacy?](https://arxiv.org/abs/2507.08843)
*Arpita Soni,Sahil Tripathi,Gautam Siddharth Kashyap,Manaswi Kulahara,Mohammad Anas Azeez,Zohaib Hasan Siddiqui,Nipun Joshi,Jiechao Gao*

Key words: 联邦学习、大型语言模型、隐私保护、下一位置预测

TL;DR: FLLL3M是一个基于大型语言模型的联邦学习框架，用于隐私保护的下一位置预测，通过本地数据处理和高效机制实现高准确率和低资源需求。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 开发一种隐私保护的下一位置预测方法，解决传统方法中数据隐私和资源消耗的问题。

Method: 采用联邦学习框架，结合大型语言模型和高效的外积机制，本地处理用户数据，减少参数和内存使用。

Result: 在多个数据集上达到SOTA性能（如Gowalla、WeePlace等），同时参数减少45.6%，内存使用降低52.7%。

Conclusion: FLLL3M在隐私保护和资源效率方面表现出色，适用于下一位置预测任务。

Abstract: We propose FLLL3M--Federated Learning with Large Language Models for Mobility
Modeling--a privacy-preserving framework for Next-Location Prediction (NxLP).
By retaining user data locally and leveraging LLMs through an efficient outer
product mechanism, FLLL3M ensures high accuracy with low resource demands. It
achieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,
0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while
reducing parameters by up to 45.6% and memory usage by 52.7%.

</details>


### [76] [DAFOS: Dynamic Adaptive Fanout Optimization Sampler](https://arxiv.org/abs/2507.08845)
*Irfan Ullah,Young-Koo Lee*

Key words: GNN, 动态采样, 训练效率, 节点评分, 早停机制

TL;DR: DAFOS提出了一种动态调整GNN训练中邻居采样数量的方法，通过节点评分和动态调整采样数来提高训练效率和准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的GNN训练中，邻居采样数量和静态采样数限制了模型的扩展性和效率，需要一种更灵活的方法来优化训练过程。

Method: DAFOS利用节点度数评分动态调整采样数量，并根据模型表现逐步增加采样数，同时集成早停机制。

Result: 在三个基准数据集上，DAFOS显著提升了训练速度和准确性，最高速度提升达12.6倍，F1分数最高提升约3分。

Conclusion: DAFOS是一种高效且可扩展的GNN训练解决方案，适用于大规模图数据。

Abstract: Graph Neural Networks (GNNs) are becoming an essential tool for learning from
graph-structured data, however uniform neighbor sampling and static fanout
settings frequently limit GNNs' scalability and efficiency. In this paper, we
propose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel
approach that dynamically adjusts the fanout based on model performance and
prioritizes important nodes during training. Our approach leverages node
scoring based on node degree to focus computational resources on structurally
important nodes, incrementing the fanout as the model training progresses.
DAFOS also integrates an early stopping mechanism to halt training when
performance gains diminish. Experiments conducted on three benchmark datasets,
ogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach
significantly improves training speed and accuracy compared to a
state-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv
dataset and a 12.6x speedup on the Reddit dataset while improving the F1 score
from 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the
ogbn-products dataset, respectively. These results highlight the potential of
DAFOS as an efficient and scalable solution for large-scale GNN training.

</details>


### [77] [Assuring the Safety of Reinforcement Learning Components: AMLAS-RL](https://arxiv.org/abs/2507.08848)
*Calum Corrie Imrie,Ioannis Stefanakos,Sepeedeh Shahbeigi,Richard Hawkins,Simon Burton*

Key words: 强化学习, 安全保障, 信息物理系统, AMLAS, 安全关键应用

TL;DR: 本文提出了一种名为AMLAS-RL的框架，用于在强化学习（RL）生命周期中系统地保障安全性，填补了现有方法在安全关键应用中的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 将机器学习（尤其是强化学习）集成到信息物理系统（CPS）中带来了显著的安全和保障挑战，现有方法无法提供系统化的安全保障。

Method: 通过将AMLAS方法适应于强化学习场景，提出了AMLAS-RL框架，以迭代方式生成RL系统的安全保障论证。

Result: AMLAS-RL通过一个轮式车辆避障任务的示例展示了其有效性。

Conclusion: AMLAS-RL为RL在安全关键应用中的系统性安全保障提供了可行方案。

Abstract: The rapid advancement of machine learning (ML) has led to its increasing
integration into cyber-physical systems (CPS) across diverse domains. While CPS
offer powerful capabilities, incorporating ML components introduces significant
safety and assurance challenges. Among ML techniques, reinforcement learning
(RL) is particularly suited for CPS due to its capacity to handle complex,
dynamic environments where explicit models of interaction between system and
environment are unavailable or difficult to construct. However, in
safety-critical applications, this learning process must not only be effective
but demonstrably safe. Safe-RL methods aim to address this by incorporating
safety constraints during learning, yet they fall short in providing systematic
assurance across the RL lifecycle. The AMLAS methodology offers structured
guidance for assuring the safety of supervised learning components, but it does
not directly apply to the unique challenges posed by RL. In this paper, we
adapt AMLAS to provide a framework for generating assurance arguments for an
RL-enabled system through an iterative process; AMLAS-RL. We demonstrate
AMLAS-RL using a running example of a wheeled vehicle tasked with reaching a
target goal without collision.

</details>


### [78] [Foundation models for time series forecasting: Application in conformal prediction](https://arxiv.org/abs/2507.08858)
*Sami Achour,Yassine Bouher,Duong Nguyen,Nicolas Chesneau*

Key words: 基础模型, 时间序列, 共形预测, 零样本学习

TL;DR: 基于基础模型（FMs）的时间序列零样本能力在共形预测中表现出潜力，与传统方法相比表现出更高可靠性和稳定性，尤其在数据有限的情况下。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨基础模型在时间序列共形预测中的表现，尤其是在数据受限时的可靠性。

Method: 比较时间序列基础模型（TSFMs）与统计模型和梯度提升在共形预测设置中的表现。

Result: TSFMs在数据有限时提供更可靠的预测区间，且校准过程更稳定。

Conclusion: 基础模型在数据受限时提升共形预测的可靠性，潜力显著。

Abstract: The zero-shot capabilities of foundation models (FMs) for time series
forecasting offer promising potentials in conformal prediction, as most of the
available data can be allocated to calibration. This study compares the
performance of Time Series Foundation Models (TSFMs) with traditional methods,
including statistical models and gradient boosting, within a conformal
prediction setting. Our findings highlight two key advantages of TSFMs. First,
when the volume of data is limited, TSFMs provide more reliable conformalized
prediction intervals than classic models, thanks to their superior predictive
accuracy. Second, the calibration process is more stable because more data are
used for calibration. Morever, the fewer data available, the more pronounced
these benefits become, as classic models require a substantial amount of data
for effective training. These results underscore the potential of foundation
models in improving conformal prediction reliability in time series
applications, particularly in data-constrained cases. All the code to reproduce
the experiments is available.

</details>


### [79] [e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction](https://arxiv.org/abs/2507.08860)
*Awais Manzoor,M. Atif Qureshi,Etain Kidney,Luca Longo*

Key words: 客户关系管理,流失预测,e-Profits,Kaplan-Meier,财务指标

TL;DR: 论文提出了一种名为e-Profits的新型商业对齐评估指标，用于量化基于客户价值、留存概率和干预成本的模型性能，优于传统指标如AUC和F1-score。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统客户流失预测模型的评估指标（如AUC和F1-score）无法反映财务结果，可能导致战略决策失误。

Method: 使用Kaplan-Meier生存分析估计个性化留存率，提出e-Profits指标支持基于客户粒度的评估。

Result: 在电信数据集上测试六种分类器，e-Profits重塑了模型排名，揭示了传统指标忽视的财务优势。

Conclusion: e-Profits可作为事后工具支持业务决策，尤其在营销和分析团队中优先考虑利润驱动决策。

Abstract: Retention campaigns in customer relationship management often rely on churn
prediction models evaluated using traditional metrics such as AUC and F1-score.
However, these metrics fail to reflect financial outcomes and may mislead
strategic decisions. We introduce e-Profits, a novel business-aligned
evaluation metric that quantifies model performance based on customer-specific
value, retention probability, and intervention costs. Unlike existing
profit-based metrics such as Expected Maximum Profit, which assume fixed
population-level parameters, e-Profits uses Kaplan-Meier survival analysis to
estimate personalised retention rates and supports granular, per customer
evaluation. We benchmark six classifiers across two telecom datasets (IBM Telco
and Maven Telecom) and demonstrate that e-Profits reshapes model rankings
compared to traditional metrics, revealing financial advantages in models
previously overlooked by AUC or F1-score. The metric also enables segment-level
insight into which models maximise return on investment for high-value
customers. e-Profits is designed as an understandable, post hoc tool to support
model evaluation in business contexts, particularly for marketing and analytics
teams prioritising profit-driven decisions. All source code is available at:
https://github.com/matifq/eprofits.

</details>


### [80] [On the under-reaching phenomenon in message-passing neural PDE solvers: revisiting the CFL condition](https://arxiv.org/abs/2507.08861)
*Lucas Tesan,Mikel M. Iparraguirre,David Gonzalez,Pedro Martins,Elias Cueto*

Key words: 图神经网络(GNN), 偏微分方程(PDE), 消息传递, 迭代次数下限, 物理特性

TL;DR: 该论文为图神经网络（GNN）求解偏微分方程（PDE）时所需的消息传递迭代次数提出了清晰的下界，显著减少了超参数调优的需求。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究旨在通过将问题的物理特性与GNN的消息传递要求关联，为三类基本PDE（双曲、抛物和椭圆）推导出迭代次数的下限，从而优化GNN在PDE求解中的效率。

Method: 通过分析PDE的物理常数、时空离散化与GNN消息传递机制的关系，推导出不同类别PDE的迭代次数下限。

Result: 实验证明，当迭代次数低于下限时，GNN无法有效传播信息，求解效果差；而满足下限时，GNN能准确捕捉物理现象，提供高精度解。

Conclusion: 论文提出的下限有效且必要，显著提升了GNN在PDE求解中的表现，并通过四个方程示例验证了其准确性。

Abstract: This paper proposes sharp lower bounds for the number of message passing
iterations required in graph neural networks (GNNs) when solving partial
differential equations (PDE). This significantly reduces the need for
exhaustive hyperparameter tuning. Bounds are derived for the three fundamental
classes of PDEs (hyperbolic, parabolic and elliptic) by relating the physical
characteristics of the problem in question to the message-passing requirement
of GNNs. In particular, we investigate the relationship between the physical
constants of the equations governing the problem, the spatial and temporal
discretisation and the message passing mechanisms in GNNs.
  When the number of message passing iterations is below these proposed limits,
information does not propagate efficiently through the network, resulting in
poor solutions, even for deep GNN architectures. In contrast, when the
suggested lower bound is satisfied, the GNN parameterisation allows the model
to accurately capture the underlying phenomenology, resulting in solvers of
adequate accuracy.
  Examples are provided for four different examples of equations that show the
sharpness of the proposed lower bounds.

</details>


### [81] [Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond](https://arxiv.org/abs/2507.08866)
*Marina Ceccon,Giandomenico Cornacchia,Davide Dalle Pezze,Alessandro Fabris,Gian Antonio Susto*

Key words: 数据偏见, 算法歧视, 公平性, 数据偏见配置文件, 反歧视

TL;DR: 该研究探讨了数据偏见对算法歧视的影响，并提出了数据偏见配置文件（DBP）来检测和缓解偏见，通过案例研究验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 数据偏见是算法歧视的主要原因之一，但研究不足，阻碍了检测和缓解偏见的计算方法的开发。

Method: 研究了三种常见数据偏见及其对算法歧视的影响，开发了DBP机制检测特定偏见类型，并结合案例验证其效果。

Result: 发现弱势群体在训练集中的代表性不足对歧视的影响较小，而代理和标签偏见的组合更为关键。DBP能有效预测歧视风险。

Conclusion: DBP为系统记录偏见信号提供了初步框架，连接了算法公平研究和反歧视政策。

Abstract: Undesirable biases encoded in the data are key drivers of algorithmic
discrimination. Their importance is widely recognized in the algorithmic
fairness literature, as well as legislation and standards on
anti-discrimination in AI. Despite this recognition, data biases remain
understudied, hindering the development of computational best practices for
their detection and mitigation. In this work, we present three common data
biases and study their individual and joint effect on algorithmic
discrimination across a variety of datasets, models, and fairness measures. We
find that underrepresentation of vulnerable populations in training sets is
less conducive to discrimination than conventionally affirmed, while
combinations of proxies and label bias can be far more critical. Consequently,
we develop dedicated mechanisms to detect specific types of bias, and combine
them into a preliminary construct we refer to as the Data Bias Profile (DBP).
This initial formulation serves as a proof of concept for how different bias
signals can be systematically documented. Through a case study with popular
fairness datasets, we demonstrate the effectiveness of the DBP in predicting
the risk of discriminatory outcomes and the utility of fairness-enhancing
interventions. Overall, this article bridges algorithmic fairness research and
anti-discrimination policy through a data-centric lens.

</details>


### [82] [GUIDE: Towards Scalable Advising for Research Ideas](https://arxiv.org/abs/2507.08870)
*Yaowenqi Liu,BingXu Meng,Rui Pan,Jerry Huang,Tong Zhang*

Key words: AI, 建议系统, 模型规模, 置信度, 结构化推理

TL;DR: 论文探讨了如何开发可扩展的AI建议系统，通过分析模型规模、上下文长度等因素，发现小型模型在特定条件下可以优于大型通用模型，显著提升假设生成和实验设计的质量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有AI研究缺乏高质量的自动化建议系统，论文旨在填补这一空白。

Method: 研究了模型规模、上下文长度、置信度估计和结构化推理等关键因素。

Result: 小型模型在特定条件下表现优于Deepseek-R1，高置信度预测的接受率超过90%。

Conclusion: 结构化推理和小型模型的结合可以显著提升建议系统的效果。

Abstract: The field of AI research is advancing at an unprecedented pace, enabling
automated hypothesis generation and experimental design across diverse domains
such as biology, mathematics, and artificial intelligence. Despite these
advancements, there remains a significant gap in the availability of scalable
advising systems capable of providing high-quality, well-reasoned feedback to
refine proposed hypotheses and experimental designs. To address this challenge,
we explore key factors that underlie the development of robust advising
systems, including model size, context length, confidence estimation, and
structured reasoning processes. Our findings reveal that a relatively small
model, when equipped with a well-compressed literature database and a
structured reasoning framework, can outperform powerful general-purpose
language models such as Deepseek-R1 in terms of acceptance rates for
self-ranked top-30% submissions to ICLR 2025. Moreover, when limited to
high-confidence predictions, our system achieves an acceptance rate exceeding
90% on the ICLR 2025 test set, underscoring its potential to significantly
enhance the quality and efficiency of hypothesis generation and experimental
design. The code is released at
https://github.com/HowardLiu0830/GUIDE-Research-Idea-Evaluation.

</details>


### [83] [Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination](https://arxiv.org/abs/2507.08871)
*Xishun Liao,Haoxuan Ma,Yifan Liu,Yuxiang Wei,Brian Yueshuai He,Chris Stanford,Jiaqi Ma*

Key words: 出行需求模型、数据驱动、生成式设计、活动模式、微观仿真

TL;DR: 该论文提出了一种基于学习方法的出行需求建模框架，通过数据驱动和生成式设计，解决了传统活动模型（ABMs）成本高、适应性差的问题。模型在洛杉矶的全面实施和验证中表现出色。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统活动模型（ABMs）虽然基于行为理论，但依赖简化假设且开发成本高，难以跨区域推广。本文旨在通过数据驱动的学习框架，提供更高效、可扩展的解决方案。

Method: 提出的框架整合了人口合成、协调活动生成、位置分配和大规模微观交通仿真，形成一个统一的生成式系统。

Result: 模型在洛杉矶的10百万人口实施中，与真实出行模式高度吻合，性能接近传统ABMs但成本更低。评估指标如余弦相似度0.97和JSD 0.006均表现优异。

Conclusion: 该学习框架显著降低了建模成本，提升了可扩展性，适用于不同区域的出行需求预测。

Abstract: Travel demand models are critical tools for planning, policy, and mobility
system design. Traditional activity-based models (ABMs), although grounded in
behavioral theories, often rely on simplified rules and assumptions, and are
costly to develop and difficult to adapt across different regions. This paper
presents a learning-based travel demand modeling framework that synthesizes
household-coordinated daily activity patterns based on a household's
socio-demographic profiles. The whole framework integrates population
synthesis, coordinated activity generation, location assignment, and
large-scale microscopic traffic simulation into a unified system. It is fully
generative, data-driven, scalable, and transferable to other regions. A
full-pipeline implementation is conducted in Los Angeles with a 10 million
population. Comprehensive validation shows that the model closely replicates
real-world mobility patterns and matches the performance of legacy ABMs with
significantly reduced modeling cost and greater scalability. With respect to
the SCAG ABM benchmark, the origin-destination matrix achieves a cosine
similarity of 0.97, and the daily vehicle miles traveled (VMT) in the network
yields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute
percentage error (MAPE). When compared to real-world observations from Caltrans
PeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001
JSD and a 6.11% MAPE.

</details>


### [84] [Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization](https://arxiv.org/abs/2507.08873)
*Shaoran Yang,Dongyu Wei,Hanzhi Yu,Zhaohui Yang,Yuchen Liu,Mingzhe Chen*

Key words: 语义通信, CLIP模型, PPO算法, 无线网络, 资源分配

TL;DR: 提出了一种基于CLIP模型的语义通信框架，无需训练即可提取数据含义，并通过PPO算法优化CLIP模型和无线资源分配，提升性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统语义编码器需要联合训练，而CLIP模型无需训练即可提取语义信息；无线噪声和频谱限制对语义通信的影响需要优化解决。

Method: 设计基于CLIP模型的语义通信框架，联合优化CLIP模型架构和资源块分配，使用PPO算法学习最优配置。

Result: 仿真表明，该方法收敛速度提升40%，累积奖励是软演员-评论家的4倍。

Conclusion: CLIP模型和PPO算法的结合有效提升了语义通信性能，适用于噪声环境。

Abstract: In this paper, a novel contrastive language-image pre-training (CLIP) model
based semantic communication framework is designed. Compared to standard neural
network (e.g.,convolutional neural network) based semantic encoders and
decoders that require joint training over a common dataset, our CLIP model
based method does not require any training procedures thus enabling a
transmitter to extract data meanings of the original data without neural
network model training, and the receiver to train a neural network for
follow-up task implementation without the communications with the transmitter.
Next, we investigate the deployment of the CLIP model based semantic framework
over a noisy wireless network. Since the semantic information generated by the
CLIP model is susceptible to wireless noise and the spectrum used for semantic
information transmission is limited, it is necessary to jointly optimize CLIP
model architecture and spectrum resource block (RB) allocation to maximize
semantic communication performance while considering wireless noise, the delay
and energy used for semantic communication. To achieve this goal, we use a
proximal policy optimization (PPO) based reinforcement learning (RL) algorithm
to learn how wireless noise affect the semantic communication performance thus
finding optimal CLIP model and RB for each user. Simulation results show that
our proposed method improves the convergence rate by up to 40%, and the
accumulated reward by 4x compared to soft actor-critic.

</details>


### [85] [An Automated Classifier of Harmful Brain Activities for Clinical Usage Based on a Vision-Inspired Pre-trained Framework](https://arxiv.org/abs/2507.08874)
*Yulin Sun,Xiaopeng Si,Runnan He,Xiao Hu,Peter Smielewski,Wenlong Wang,Xiaoguang Tong,Wei Yue,Meijun Pang,Kuo Zhang,Xizi Song,Dong Ming,Xiuyun Liu*

Key words: 脑电图、卷积神经网络、VIPEEGNet、脑疾病、人工智能

TL;DR: VIPEEGNet是一种针对脑电图（EEG）设计的人工智能模型，用于高效识别有害脑活动，并通过大规模数据集验证其性能，其准确性与人类专家相当。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统脑电图分析存在专家间差异大、资源限制和AI模型泛化性差的问题，需要开发更高效的AI模型。

Method: 开发并验证了一种名为VIPEEGNet的卷积神经网络模型，使用两个独立的数据集（2006-2020年）进行训练和测试。

Result: VIPEEGNet在二分类和多分类任务中表现出色，AUROC高达0.972，灵敏度在36.8%至88.2%之间，性能与人类专家相近。

Conclusion: VIPEEGNet在脑电图分析中表现出高性能和高效性，优于现有大多数算法。

Abstract: Timely identification of harmful brain activities via electroencephalography
(EEG) is critical for brain disease diagnosis and treatment, which remains
limited application due to inter-rater variability, resource constraints, and
poor generalizability of existing artificial intelligence (AI) models. In this
study, a convolutional neural network model, VIPEEGNet, was developed and
validated using EEGs recorded from Massachusetts General Hospital/Harvard
Medical School. The VIPEEGNet was developed and validated using two independent
datasets, collected between 2006 and 2020. The development cohort included EEG
recordings from 1950 patients, with 106,800 EEG segments annotated by at least
one experts (ranging from 1 to 28). The online testing cohort consisted of EEG
segments from a subset of an additional 1,532 patients, each annotated by at
least 10 experts. For the development cohort (n=1950), the VIPEEGNet achieved
high accuracy, with an AUROC for binary classification of seizure, LPD, GPD,
LRDA, GRDA, and "other" categories at 0.972 (95% CI, 0.957-0.988), 0.962 (95%
CI, 0.954-0.970), 0.972 (95% CI, 0.960-0.984), 0.938 (95% CI, 0.917-0.959),
0.949 (95% CI, 0.941-0.957), and 0.930 (95% CI, 0.926-0.935). For multi
classification, the sensitivity of VIPEEGNET for the six categories ranges from
36.8% to 88.2% and the precision ranges from 55.6% to 80.4%, and performance
similar to human experts. Notably, the external validation showed
Kullback-Leibler Divergence (KLD)of 0.223 and 0.273, ranking top 2 among the
existing 2,767 competing algorithms, while we only used 2.8% of the parameters
of the first-ranked algorithm.

</details>


### [86] [ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling](https://arxiv.org/abs/2507.08877)
*Hanlong Zhang,Jingsheng Yang,Hao Li,Yuhao He,Franck Gong*

Key words: 函数调用, 大型语言模型, 延迟优化, 定向蒸馏, 在线交互数据

TL;DR: 提出了一种名为ODIA的新方法，通过在线用户交互数据加速LLM的函数调用，显著降低延迟并保持准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大型语言模型（LLM）的函数调用存在高延迟问题，影响用户体验，亟需一种高效解决方案。

Method: 采用定向蒸馏（ODIA）方法，自动从生产流量中识别简单查询，并将大模型的知识蒸馏到小模型，以减少延迟。

Result: 实验显示，该方法预期减少45%的延迟，中位数减少78%，同时在音乐应用中处理60%的流量且精度损失可忽略。

Conclusion: ODIA是一种自动化、高效的解决方案，适用于生产环境，能够持续改进模型性能。

Abstract: Function Calling is a crucial technique that enables Large Language Models
(LLMs) to interact with external systems through APIs. However, the high
latency associated with LLM-based Function Calling significantly impacts user
experience. This paper presents a novel approach called Oriented Distillation
for Inline Acceleration (ODIA) that leverages online user interaction data to
accelerate Function Calling. By automatically identifying "simple queries" from
production traffic and distilling knowledge from larger models to smaller ones,
our method reduces response latency by 45% (expected) and 78% (median) while
maintaining accuracy. We demonstrate the effectiveness of our approach through
real-world deployment in a music application, where the smaller model
successfully handles 60% of traffic with negligible accuracy loss. Our method
requires minimal human intervention and continuously improves through automated
data collection and model updating, making it a practical solution for
production environments.

</details>


### [87] [Last Layer Hamiltonian Monte Carlo](https://arxiv.org/abs/2507.08905)
*Koen Vellenga,H. Joe Steinhauer,Göran Falkman,Jonas Andersson,Anders Sjögren*

Key words: 哈密顿蒙特卡洛, 深度神经网络, 不确定性估计, 最后一层采样, 离群检测

TL;DR: 论文探讨了限制哈密顿蒙特卡洛采样于深度神经网络的最后一层（LL-HMC），以降低计算成本，并在真实视频数据集上与其他概率深度学习方法比较性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 哈密顿蒙特卡洛（HMC）是估计不确定性的黄金标准，但其高计算成本限制了在大规模数据和大网络中的应用。研究旨在通过限制采样到最后一层，降低计算需求。

Method: 提出LL-HMC方法，将HMC采样限制在DNN的最后一层，比较了五种概率深度学习方法和三种真实视频数据集的表现。

Result: LL-HMC在分布内分类和离群检测中表现竞争力，更多采样参数未改善分类性能但提升离群检测，多链或多起点未显著改进效果。

Conclusion: LL-HMC在计算资源有限的数据密集型场景中表现良好，但采样参数增加仅对离群检测有帮助。

Abstract: We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a
probabilistic last layer approach for deep neural networks (DNNs). While HMC is
widely regarded as a gold standard for uncertainty estimation, the
computational demands limit its application to large-scale datasets and large
DNN architectures. Although the predictions from the sampled DNN parameters can
be parallelized, the computational cost still scales linearly with the number
of samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the
required computations by restricting the HMC sampling to the final layer of a
DNN, making it applicable to more data-intensive scenarios with limited
computational resources. In this paper, we compare LL-HMC against five last
layer probabilistic deep learning (LL-PDL) methods across three real-world
video datasets for driver action and intention. We evaluate the in-distribution
classification performance, calibration, and out-of-distribution (OOD)
detection. Due to the stochastic nature of the probabilistic evaluations, we
performed five grid searches for different random seeds to avoid being reliant
on a single initialization for the hyperparameter configurations. The results
show that LL--HMC achieves competitive in-distribution classification and OOD
detection performance. Additional sampled last layer parameters do not improve
the classification performance, but can improve the OOD detection. Multiple
chains or starting positions did not yield consistent improvements.

</details>


### [88] [Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising](https://arxiv.org/abs/2507.08912)
*Tomasz Szandala,Fatima Ezzeddine,Natalia Rusin,Silvia Giordano,Omran Ayoub*

Key words: 深度伪造检测, 公平性, Fair-FLIP, 后处理

TL;DR: 论文提出了一种名为Fair-FLIP的后处理方法，用于减少深度伪造检测中的偏见，同时保持高检测性能。实验显示该方法能提升公平性指标达30%，且仅降低准确率0.25%。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决深度伪造检测方法中存在的跨人口统计学属性（如种族和性别）的偏见问题，同时保持检测性能。

Method: 提出Fair-FLIP方法，通过重新加权训练模型的最终层输入，减少子群体差异，优先处理低变异性输入。

Result: Fair-FLIP将公平性指标提升高达30%，同时准确率仅降低0.25%。

Conclusion: Fair-FLIP是一种有效的后处理方法，能够在保持检测性能的同时显著提升公平性。

Abstract: Artificial Intelligence-generated content has become increasingly popular,
yet its malicious use, particularly the deepfakes, poses a serious threat to
public trust and discourse. While deepfake detection methods achieve high
predictive performance, they often exhibit biases across demographic attributes
such as ethnicity and gender. In this work, we tackle the challenge of fair
deepfake detection, aiming to mitigate these biases while maintaining robust
detection capabilities. To this end, we propose a novel post-processing
approach, referred to as Fairness-Oriented Final Layer Input Prioritising
(Fair-FLIP), that reweights a trained model's final-layer inputs to reduce
subgroup disparities, prioritising those with low variability while demoting
highly variable ones. Experimental results comparing Fair-FLIP to both the
baseline (without fairness-oriented de-biasing) and state-of-the-art approaches
show that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining
baseline accuracy, with only a negligible reduction of 0.25%.
  Code is available on Github:
https://github.com/szandala/fair-deepfake-detection-toolbox

</details>


### [89] [Revisiting Convergence: Shuffling Complexity Beyond Lipschitz Smoothness](https://arxiv.org/abs/2507.08913)
*Qi He,Peiran Yu,Ziyi Chen,Heng Huang*

Key words: 混洗梯度方法、收敛性分析、Lipschitz平滑、步长策略、机器学习

TL;DR: 论文重新审视了不依赖于Lipschitz平滑条件的混洗梯度方法的收敛性，提出了新的步长策略，证明了在非凸、强凸和非强凸情况下的收敛速率，并通过实验验证了其实际效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的混洗梯度方法大多依赖于Lipschitz平滑条件，而许多机器学习模型并不满足这一条件，限制了方法的适用性。论文旨在填补这一理论空白。

Method: 作者重新设计了步长策略，在不假设Lipschitz平滑的情况下，分析了混洗梯度方法在随机洗牌和任意洗牌方案下的收敛性，并考虑了有界方差条件。

Result: 新方法不仅收敛于更弱的假设下，且收敛速率与当前最佳方法匹配，实验进一步验证了其实际性能。

Conclusion: 论文拓展了混洗梯度方法的理论适用性，证明其在更广泛条件下仍能高效收敛，为实际应用提供了更强支持。

Abstract: Shuffling-type gradient methods are favored in practice for their simplicity
and rapid empirical performance. Despite extensive development of convergence
guarantees under various assumptions in recent years, most require the
Lipschitz smoothness condition, which is often not met in common machine
learning models. We highlight this issue with specific counterexamples. To
address this gap, we revisit the convergence rates of shuffling-type gradient
methods without assuming Lipschitz smoothness. Using our stepsize strategy, the
shuffling-type gradient algorithm not only converges under weaker assumptions
but also match the current best-known convergence rates, thereby broadening its
applicability. We prove the convergence rates for nonconvex, strongly convex,
and non-strongly convex cases, each under both random reshuffling and arbitrary
shuffling schemes, under a general bounded variance condition. Numerical
experiments further validate the performance of our shuffling-type gradient
algorithm, underscoring its practical efficacy.

</details>


### [90] [Beyond Scores: Proximal Diffusion Models](https://arxiv.org/abs/2507.08956)
*Zhenghan Fang,Mateo Díaz,Sam Buchanan,Jeremias Sulam*

Key words: 扩散模型,反向离散化,近端映射,分数匹配,ProxDM

TL;DR: 论文提出了一种基于反向离散化的扩散模型（ProxDM），通过使用近端映射替代分数匹配，理论上和实际上都表现出优势。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 扩散模型在处理高维数据时具有强大生成能力，但传统方法依赖于分数匹配，计算成本较高。本文旨在寻找更高效的替代方法。

Method: 采用反向离散化的SDE，利用近端映射替代分数匹配，并基于近端匹配学习对数密度的近端算子，提出ProxDM。

Result: 理论证明ProxDM仅需较少步骤即可生成高精度分布，实验显示其收敛速度明显快于传统分数匹配方法。

Conclusion: ProxDM在理论和实践中均优于传统方法，为扩散模型提供了更高效的实现途径。

Abstract: Diffusion models have quickly become some of the most popular and powerful
generative models for high-dimensional data. The key insight that enabled their
development was the realization that access to the score -- the gradient of the
log-density at different noise levels -- allows for sampling from data
distributions by solving a reverse-time stochastic differential equation (SDE)
via forward discretization, and that popular denoisers allow for unbiased
estimators of this score. In this paper, we demonstrate that an alternative,
backward discretization of these SDEs, using proximal maps in place of the
score, leads to theoretical and practical benefits. We leverage recent results
in proximal matching to learn proximal operators of the log-density and, with
them, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that
$\widetilde{O}(d/\sqrt{\varepsilon})$ steps suffice for the resulting
discretization to generate an $\varepsilon$-accurate distribution w.r.t. the KL
divergence. Empirically, we show that two variants of ProxDM achieve
significantly faster convergence within just a few sampling steps compared to
conventional score-matching methods.

</details>


### [91] [Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign](https://arxiv.org/abs/2507.08959)
*Xiang Li,Xinyu Wang,Yifan Lin*

Key words: 图神经网络, 广告推荐, 跨平台, 多维度建模

TL;DR: 提出了一种基于图神经网络（GNN）的跨平台广告推荐方法，通过多维度建模提升推荐准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了提高跨平台广告推荐的准确性，需要捕捉用户兴趣迁移的潜在路径。

Method: 通过多维度建模（用户行为数据、广告内容和平台特征）构建GNN模型，优化超参数以适应异构数据。

Result: 实验结果显示，Platform B的AUC值达到0.937，表现最佳，但其他平台的精确度和召回率略有下降。

Conclusion: 该GNN方法在跨平台广告推荐中表现出色，但需针对数据分布不均进一步优化。

Abstract: In order to improve the accuracy of cross-platform advertisement
recommendation, a graph neural network (GNN)- based advertisement
recommendation method is analyzed. Through multi-dimensional modeling, user
behavior data (e.g., click frequency, active duration) reveal temporal patterns
of interest evolution, ad content (e.g., type, tag, duration) influences
semantic preferences, and platform features (e.g., device type, usage context)
shape the environment where interest transitions occur. These factors jointly
enable the GNN to capture the latent pathways of user interest migration across
platforms. The experimental results are based on the datasets of three
platforms, and Platform B reaches 0.937 in AUC value, which is the best
performance. Platform A and Platform C showed a slight decrease in precision
and recall with uneven distribution of ad labels. By adjusting the
hyperparameters such as learning rate, batch size and embedding dimension, the
adaptability and robustness of the model in heterogeneous data are further
improved.

</details>


### [92] [Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models](https://arxiv.org/abs/2507.08965)
*Kevin Rojas,Ye He,Chieh-Hsin Lai,Yuta Takida,Yuki Mitsufuji,Molei Tao*

Key words: Classifier-Free Guidance（CFG）, 离散扩散, 指导计划, 生成质量, 掩码离散扩散

TL;DR: 本文通过理论分析，探讨了离散扩散模型中Classifier-Free Guidance（CFG）的作用，指出了早期高指导对生成质量的负面影响，并提出了一种简单高效的改进方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究目的是为了深入理解CFG在离散扩散模型中的作用机制，尤其是指导计划（guidance schedule）的影响，并提出改进方法以提升生成质量。

Method: 论文通过理论分析了CFG在掩码离散扩散中的作用，发现了当前实现中的不足（如不平衡的过渡），并提出了一种新的CFG机制，通过平滑数据分布与初始分布之间的传输来改进生成质量。

Result: 实验结果表明，提出的方法在ImageNet（掩码离散扩散）和QM9（均匀离散扩散）上均有效提升了样本质量。

Conclusion: 研究不仅解释了现有经验观察的理论基础，还提出了一种简单易行的改进方法，为离散扩散模型的优化提供了新的思路。

Abstract: Classifier-Free Guidance (CFG) is a widely used technique for conditional
generation and improving sample quality in continuous diffusion models, and
recent works have extended it to discrete diffusion. This paper theoretically
analyzes CFG in the context of masked discrete diffusion, focusing on the role
of guidance schedules. Our analysis shows that high guidance early in sampling
(when inputs are heavily masked) harms generation quality, while late-stage
guidance has a larger effect. These findings provide a theoretical explanation
for empirical observations in recent studies on guidance schedules. The
analysis also reveals an imperfection of the current CFG implementations. These
implementations can unintentionally cause imbalanced transitions, such as
unmasking too rapidly during the early stages of generation, which degrades the
quality of the resulting samples. To address this, we draw insight from the
analysis and propose a novel classifier-free guidance mechanism empirically
applicable to any discrete diffusion. Intuitively, our method smoothens the
transport between the data distribution and the initial (masked/uniform)
distribution, which results in improved sample quality. Remarkably, our method
is achievable via a simple one-line code change. The efficacy of our method is
empirically demonstrated with experiments on ImageNet (masked discrete
diffusion) and QM9 (uniform discrete diffusion).

</details>


### [93] [ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha](https://arxiv.org/abs/2507.08966)
*Meng Liu,Karl Leswing,Simon K. S. Chu,Farhad Ramezanghorbani,Griffin Young,Gabriel Marques,Prerna Das,Anjali Panikar,Esther Jamir,Mohammed Sulaiman Shamsudeen,K. Shawn Watts,Ananya Sen,Hari Priya Devannagari,Edward B. Miller,Muyun Lihan,Howook Hwang,Janet Paulsen,Xin Yu,Kyle Gion,Timur Rvachov,Emine Kucukbenli,Saee Gopal Paliwal*

Key words: Protein-ligand binding affinity, machine learning, AB-FEP, ToxBench, DualBind

TL;DR: 研究了蛋白质-配体结合亲和力预测，提出了ToxBench数据集和DualBind模型，填补了机器学习和物理方法之间的空白。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 药物发现和毒性评估需要准确的结合亲和力预测，但现有机器学习方法数据不足，物理方法计算成本高。

Method: 提出ToxBench数据集，包含8,770个ERα-配体复合物结构，结合自由能通过AB-FEP计算，并开发DualBind模型。

Result: DualBind表现最佳，展示机器学习能以低成本近似AB-FEP。

Conclusion: ToxBench和DualBind为药物发现提供了高效工具，缓解了计算成本问题。

Abstract: Protein-ligand binding affinity prediction is essential for drug discovery
and toxicity assessment. While machine learning (ML) promises fast and accurate
predictions, its progress is constrained by the availability of reliable data.
In contrast, physics-based methods such as absolute binding free energy
perturbation (AB-FEP) deliver high accuracy but are computationally prohibitive
for high-throughput applications. To bridge this gap, we introduce ToxBench,
the first large-scale AB-FEP dataset designed for ML development and focused on
a single pharmaceutically critical target, Human Estrogen Receptor Alpha
(ER$\alpha$). ToxBench contains 8,770 ER$\alpha$-ligand complex structures with
binding free energies computed via AB-FEP with a subset validated against
experimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping
ligand splits to assess model generalizability. Using ToxBench, we further
benchmark state-of-the-art ML methods, and notably, our proposed DualBind
model, which employs a dual-loss framework to effectively learn the binding
energy function. The benchmark results demonstrate the superior performance of
DualBind and the potential of ML to approximate AB-FEP at a fraction of the
computational cost.

</details>


### [94] [Simulating Three-dimensional Turbulence with Physics-informed Neural Networks](https://arxiv.org/abs/2507.08972)
*Sifan Wang,Shyam Sankaran,Panos Stinis,Paris Perdikaris*

Key words: 湍流模拟、物理信息神经网络、混沌系统、无网格计算

TL;DR: PINNs通过物理方程直接训练神经网络，无需传统计算网格或训练数据，成功模拟了二维和三维湍流，解决了复杂混沌系统的挑战。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统湍流模拟计算成本高昂，而PINNs提供了一种连续、无网格的解决方案，有望突破传统计算限制。

Method: 结合自适应网络架构、因果训练和高级优化方法，直接学习流体动力学方程的解。

Result: 在多维度湍流问题中，PINNs能准确复现能量谱、动能、涡度和雷诺应力等关键流统计量。

Conclusion: PINNs证明能够处理复杂混沌系统，为连续湍流建模开辟了新途径。

Abstract: Turbulent fluid flows are among the most computationally demanding problems
in science, requiring enormous computational resources that become prohibitive
at high flow speeds. Physics-informed neural networks (PINNs) represent a
radically different approach that trains neural networks directly from physical
equations rather than data, offering the potential for continuous, mesh-free
solutions. Here we show that appropriately designed PINNs can successfully
simulate fully turbulent flows in both two and three dimensions, directly
learning solutions to the fundamental fluid equations without traditional
computational grids or training data. Our approach combines several algorithmic
innovations including adaptive network architectures, causal training, and
advanced optimization methods to overcome the inherent challenges of learning
chaotic dynamics. Through rigorous validation on challenging turbulence
problems, we demonstrate that PINNs accurately reproduce key flow statistics
including energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our
results demonstrate that neural equation solvers can handle complex chaotic
systems, opening new possibilities for continuous turbulence modeling that
transcends traditional computational limitations.

</details>


### [95] [Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery](https://arxiv.org/abs/2507.08977)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Marisa Eisenberg*

Key words: Simulation-Grounded Neural Networks, 深度学习, 科学建模, 可解释性, 模拟数据

TL;DR: SGNNs结合科学模拟与深度学习，提升模型预测和推断能力，同时在缺乏真实数据时提供可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决科学建模中机械模型可解释性强但适应性差，机器学习模型灵活但依赖大数据且不可解释的问题。

Method: 通过机械模拟生成训练数据，训练神经网络（SGNNs），支持多样化的模型结构和参数。

Result: 在COVID-19预测、化学反应产率及生态预测等任务中表现优异，推断任务如信息传播源分类也准确。

Conclusion: SGNNs将科学理论与深度学习结合，开创了新的建模范式，即使缺乏真实数据也能实现稳健、可解释的推断。

Abstract: Scientific modeling faces a core limitation: mechanistic models offer
interpretability but collapse under real-world complexity, while machine
learning models are flexible but require large labeled datasets, cannot infer
unobservable quantities, and operate as black boxes. We introduce
Simulation-Grounded Neural Networks (SGNNs), a general framework that uses
mechanistic simulations as training data for neural networks. SGNNs are
pretrained on synthetic corpora spanning diverse model structures, parameter
regimes, stochasticity, and observational artifacts. We evaluated SGNNs across
scientific disciplines and modeling tasks, and found that SGNNs achieved
state-of-the-art results across settings: for prediction tasks, they nearly
tripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield
prediction error by one third, and maintained accuracy in ecological
forecasting where task specific models failed. For inference tasks, SGNNs also
accurately classified the source of information spread in simulated social
networks and enabled supervised learning for unobservable targets, such as
estimating COVID-19 transmissibility more accurately than traditional methods
even in early outbreaks. Finally, SGNNs enable back-to-simulation attribution,
a new form of mechanistic interpretability. Given real world input, SGNNs
retrieve simulations based on what the model has learned to see as most
similar, revealing which underlying dynamics the model believes are active.
This provides process-level insight -- what the model thinks is happening --
not just which features mattered. SGNNs unify scientific theory with deep
learning flexibility and unlock a new modeling paradigm -- transforming
simulations from rigid, post hoc tools into flexible sources of supervision,
enabling robust, interpretable inference even when ground truth is missing.

</details>


### [96] [Learning Diffusion Models with Flexible Representation Guidance](https://arxiv.org/abs/2507.08980)
*Chenyu Wang,Cai Zhou,Sharut Gupta,Zongyu Lin,Stefanie Jegelka,Stephen Bates,Tommi Jaakkola*

Key words: 扩散模型,表征对齐,多模态学习,训练课程

TL;DR: 论文提出了一种系统框架，通过在扩散模型中引入表征对齐指导，提升了生成质量和训练速度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 通过将扩散模型的内部表征与预训练模型对齐，提高生成质量。

Method: 提出了两种新策略：一是学习多模态配对的联合模型；二是设计平衡表征学习和数据生成的最优训练课程。

Result: 在图像、蛋白质序列和分子生成任务中表现出优越性能，训练速度显著提升。

Conclusion: 表征指导可以显著提升扩散模型的性能和训练效率。

Abstract: Diffusion models can be improved with additional guidance towards more
effective representations of input. Indeed, prior empirical work has already
shown that aligning internal representations of the diffusion model with those
of pre-trained models improves generation quality. In this paper, we present a
systematic framework for incorporating representation guidance into diffusion
models. We provide alternative decompositions of denoising models along with
their associated training criteria, where the decompositions determine when and
how the auxiliary representations are incorporated. Guided by our theoretical
insights, we introduce two new strategies for enhancing representation
alignment in diffusion models. First, we pair examples with target
representations either derived from themselves or arisen from different
synthetic modalities, and subsequently learn a joint model over the multimodal
pairs. Second, we design an optimal training curriculum that balances
representation learning and data generation. Our experiments across image,
protein sequence, and molecule generation tasks demonstrate superior
performance as well as accelerated training. In particular, on the
class-conditional ImageNet $256\times 256$ benchmark, our guidance results in
$23.3$ times faster training than the original SiT-XL as well as four times
speedup over the state-of-the-art method REPA. The code is available at
https://github.com/ChenyuWang-Monica/REED.

</details>


### [97] [Exploiting Leaderboards for Large-Scale Distribution of Malicious Models](https://arxiv.org/abs/2507.08983)
*Anshuman Suri,Harsh Chaudhari,Yuefeng Peng,Ali Naseh,Amir Houmansadr,Alina Oprea*

Key words: 中毒攻击, 模型排行榜, TrojanClimb框架, 安全漏洞, 机器学习

TL;DR: 论文探讨了模型排行榜如何成为分发有毒模型的隐秘渠道，提出了TrojanClimb框架，展示了其在多种模态中的有效性，揭示了机器学习生态系统的漏洞。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究动机在于探索敌手如何通过模型排行榜大规模分发有毒模型，填补了这一领域的空白。

Method: 提出了TrojanClimb框架，允许注入恶意行为同时保持竞争力的排行榜表现，并在四种模态中验证其有效性。

Result: 结果显示敌手能成功在排行榜中排名靠前，同时嵌入任意有害功能，如后门或偏见注入。

Conclusion: 结论指出了机器学习生态系统中的重大漏洞，呼吁重新设计排行榜评估机制以检测恶意模型。

Abstract: While poisoning attacks on machine learning models have been extensively
studied, the mechanisms by which adversaries can distribute poisoned models at
scale remain largely unexplored. In this paper, we shed light on how model
leaderboards -- ranked platforms for model discovery and evaluation -- can
serve as a powerful channel for adversaries for stealthy large-scale
distribution of poisoned models. We present TrojanClimb, a general framework
that enables injection of malicious behaviors while maintaining competitive
leaderboard performance. We demonstrate its effectiveness across four diverse
modalities: text-embedding, text-generation, text-to-speech and text-to-image,
showing that adversaries can successfully achieve high leaderboard rankings
while embedding arbitrary harmful functionalities, from backdoors to bias
injection. Our findings reveal a significant vulnerability in the machine
learning ecosystem, highlighting the urgent need to redesign leaderboard
evaluation mechanisms to detect and filter malicious (e.g., poisoned) models,
while exposing broader security implications for the machine learning community
regarding the risks of adopting models from unverified sources.

</details>


### [98] [Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography](https://arxiv.org/abs/2507.09009)
*Zhengxiao He,Huayu Li,Geng Yuan,William D. S. Killgore,Stuart F. Quan,Chen X. Chen,Ao Li*

Key words: 自监督学习,多模态信号,CVD风险预测,深度学习

TL;DR: 开发了一种自监督深度学习模型，从多模态信号中提取有意义的模式，并在独立队列中验证其预测性能，结果显示结合Framingham风险评分可显著提升预测效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 通过多模态信号（EEG、ECG和呼吸信号）提取模式，以个性化评估心血管疾病（CVD）风险。

Method: 采用自监督深度学习模型训练4398名参与者的数据，通过对比有无CVD结果的嵌入生成投影分数，并在1093名独立队列中进行外部验证。

Result: 投影分数揭示了各模态的临床意义模式，结合Framingham风险评分后，AUC值达0.607至0.965，结果在外部队列中稳定复现。

Conclusion: 该框架可直接从PSG数据生成个性化CVD风险评分，有望整合到临床实践中。

Abstract: Methods: We developed a self-supervised deep learning model that extracts
meaningful patterns from multi-modal signals (Electroencephalography (EEG),
Electrocardiography (ECG), and respiratory signals). The model was trained on
data from 4,398 participants. Projection scores were derived by contrasting
embeddings from individuals with and without CVD outcomes. External validation
was conducted in an independent cohort with 1,093 participants. The source code
is available on https://github.com/miraclehetech/sleep-ssl. Results: The
projection scores revealed distinct and clinically meaningful patterns across
modalities. ECG-derived features were predictive of both prevalent and incident
cardiac conditions, particularly CVD mortality. EEG-derived features were
predictive of incident hypertension and CVD mortality. Respiratory signals
added complementary predictive value. Combining these projection scores with
the Framingham Risk Score consistently improved predictive performance,
achieving area under the curve values ranging from 0.607 to 0.965 across
different outcomes. Findings were robustly replicated and validated in the
external testing cohort. Conclusion: Our findings demonstrate that the proposed
framework can generate individualized CVD risk scores directly from PSG data.
The resulting projection scores have the potential to be integrated into
clinical practice, enhancing risk assessment and supporting personalized care.

</details>


### [99] [Enhancing RLHF with Human Gaze Modeling](https://arxiv.org/abs/2507.09016)
*Karim Galliamov,Ivan Titov,Ilya Pershin*

Key words: RLHF, 视线建模, 奖励模型, 计算效率

TL;DR: RLHF通过人类反馈对齐语言模型偏好，但计算成本高。研究利用人类视线建模，提出视线感知奖励模型和基于视线稀疏奖励分发方法。实验显示该方法能更快收敛且保持或略微提升性能，降低计算成本。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: RLHF虽有效但计算成本高，人类视线模型可能提供未充分利用的有价值信号，以提升RLHF效率。

Method: 提出两种方法：(1)视线感知奖励模型；(2)基于视线稀疏奖励分发。

Result: 方法实现更快收敛，性能保持或略升，显著降低计算成本。

Conclusion: 人类视线是优化RLHF的有效信号，未来有望进一步改进效率。

Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns language models with
human preferences but is computationally expensive. We explore two approaches
that leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models
and (2) gaze-based distribution of sparse rewards at token level. Our
experiments demonstate that gaze-informed RLHF achieves faster convergence
while maintaining or slightly improving performance, thus, reducing
computational costs during policy optimization. These results show that human
gaze provides a valuable and underused signal for policy optimization, pointing
to a promising direction for improving RLHF efficiency.

</details>


### [100] [On Evaluating Performance of LLM Inference Serving Systems](https://arxiv.org/abs/2507.09019)
*Amey Agrawal,Nitin Kedia,Anmol Agarwal,Jayashree Mohan,Nipun Kwatra,Souvik Kundu,Ramachandran Ramjee,Alexey Tumanov*

Key words: 大型语言模型, 评估方法, 性能优化, 反模式

TL;DR: 本文分析了当前大型语言模型（LLM）推理系统评估方法中的常见问题，提出了改进框架以避免误导性结论。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有评估方法存在缺陷，导致性能特征被掩盖，阻碍科学发展。

Method: 通过系统分析，识别了基线公平性、评估设置和指标设计三个维度的常见反模式。

Result: 制定了一份全面的检查清单，帮助避免误导性评估。

Conclusion: 提出了一种更严谨的评估框架，以促进LLM推理系统的科学进步。

Abstract: The rapid evolution of Large Language Model (LLM) inference systems has
yielded significant efficiency improvements. However, our systematic analysis
reveals that current evaluation methodologies frequently exhibit fundamental
flaws, often manifesting as common evaluation anti-patterns that obscure true
performance characteristics and impede scientific progress. Through a
comprehensive examination of recent systems, we identify recurring
anti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,
and Metric Design. These anti-patterns are uniquely problematic for LLM
inference due to its dual-phase nature combining distinct prefill and decode
operations, its handling of highly heterogeneous workloads, and its strict
temporal requirements for interactive use. We demonstrate how common
anti-patterns -- such as inadequate baseline comparisons that conflate
engineering effort with algorithmic novelty, workload selections that fail to
represent production scenarios, and metric normalizations that hide substantial
performance variability like generation stalls-lead to misleading conclusions.
To address these challenges, we provide a comprehensive checklist derived from
our analysis, establishing a framework for recognizing and avoiding these
anti-patterns in favor of robust LLM inference evaluation. To demonstrate the
practical application of our framework, we present a case study analyzing
speculative decoding, a technique whose bursty, non-uniform token generation is
easily misinterpreted when evaluated using approaches characteristic of these
anti-patterns. Our work establishes a rigorous foundation for evaluation
methodology, enabling meaningful comparisons, ensuring reproducible results,
and ultimately accelerating genuine progress in LLM inference systems by moving
beyond common anti-patterns to align evaluation with real-world requirements.

</details>


### [101] [Model Parallelism With Subnetwork Data Parallelism](https://arxiv.org/abs/2507.09029)
*Vaibhav Singh,Zafir Khalid,Edouard Oyallon,Eugene Belilovsky*

Key words: 分布式预训练、子网络、内存优化、通信成本

TL;DR: 提出了一种分布式预训练新方法，通过训练小型结构化子网络降低内存需求和节点间通信成本，性能不降。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决大规模模型预训练中内存需求高和节点间通信成本高的问题。

Method: 采用子网络构造策略，避免节点间激活通信，保持带宽需求低于标准数据并行方案。

Result: 随机块丢弃技术优于宽度子网络构造，内存使用减少20-40%，性能无损失。

Conclusion: 新方法有效降低了内存和通信成本，性能表现优异。

Abstract: Distributed pre-training of large models at scale often imposes heavy memory
demands on individual nodes and incurs significant intra-node communication
costs. We propose a novel alternative approach that reduces the memory
requirements by training small, structured subnetworks of the model on separate
workers. Unlike pipelining, our method avoids inter-node activation
communication and maintains bandwidth requirements that are comparable to or
lower than standard data parallel communication schemes based on all-reduce. We
evaluate two subnetwork construction strategies guided by the principle of
ensuring uniform representation of each parameter across the distributed
training setup. Our results show that the stochastic block dropping technique
consistently outperforms the width-wise subnetwork construction previously
explored in federated learning. We empirically attribute this superior
performance to stronger gradient alignment in subnetworks that retain blocks
having skip connections. Preliminary experiments highlight the promise of our
approach, achieving a 20-40% reduction in memory usage without any loss in
performance.

</details>


### [102] [Confounder-Free Continual Learning via Recursive Feature Normalization](https://arxiv.org/abs/2507.09031)
*Yash Shah,Camila Gonzalez,Mohammad H. Abbasi,Qingyu Zhao,Kilian M. Pohl,Ehsan Adeli*

Key words: 混杂变量、持续学习、元数据归一化、递归最小二乘、公平预测

TL;DR: 该论文提出了一种名为递归元数据归一化（R-MDN）的层，用于在持续学习中消除混杂变量对特征表示的影响，从而提高模型预测的公平性并减少灾难性遗忘。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 混杂变量会影响输入和目标，导致虚假相关性和偏差预测。传统模型已有方法如元数据归一化（MDN）来处理混杂变量，但在持续学习背景下，学习对混杂变量不变的特征表示仍具挑战性。

Method: 引入R-MDN层，可集成到任何深度学习架构中，通过递归最小二乘算法持续更新内部模型状态以应对数据和混杂变量分布的变化。

Result: 实验表明，R-MDN在静态学习和持续学习的不同阶段均能促进跨群体公平预测，减少因混杂变量随时间变化引起的灾难性遗忘。

Conclusion: R-MDN有效提升了模型在持续学习中处理混杂变量的能力，增强了预测公平性。

Abstract: Confounders are extraneous variables that affect both the input and the
target, resulting in spurious correlations and biased predictions. There are
recent advances in dealing with or removing confounders in traditional models,
such as metadata normalization (MDN), where the distribution of the learned
features is adjusted based on the study confounders. However, in the context of
continual learning, where a model learns continuously from new data over time
without forgetting, learning feature representations that are invariant to
confounders remains a significant challenge. To remove their influence from
intermediate feature representations, we introduce the Recursive MDN (R-MDN)
layer, which can be integrated into any deep learning architecture, including
vision transformers, and at any model stage. R-MDN performs statistical
regression via the recursive least squares algorithm to maintain and
continually update an internal model state with respect to changing
distributions of data and confounding variables. Our experiments demonstrate
that R-MDN promotes equitable predictions across population groups, both within
static learning and across different stages of continual learning, by reducing
catastrophic forgetting caused by confounder effects changing over time.

</details>


### [103] [Behavioral Exploration: Learning to Explore via In-Context Adaptation](https://arxiv.org/abs/2507.09041)
*Andrew Wagenmaker,Zhiyuan Zhou,Sergey Levine*

Key words: 自主智能体, 行为探索, 在线适应, 专家行为, 机器人

TL;DR: 论文提出了一种名为‘行为探索’的方法，通过训练长上下文生成模型来模仿专家行为并实现快速在线适应和探索。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法依赖随机探索和缓慢的梯度更新，无法与人类快速适应和探索的能力相比。论文旨在开发能够像人类一样快速在线探索和适应的自主智能体。

Method: 利用专家演示数据集，训练一个长上下文生成模型，预测专家动作，并根据上下文和探索性度量来选择不同的专家行为。

Result: 方法在模拟和现实机器人任务中证明了其有效性，能够学习适应性和探索性行为。

Conclusion: 行为探索方法为自主智能体提供了快速在线适应和专家级别的探索能力。

Abstract: Developing autonomous agents that quickly explore an environment and adapt
their behavior online is a canonical challenge in robotics and machine
learning. While humans are able to achieve such fast online exploration and
adaptation, often acquiring new information and skills in only a handful of
interactions, existing algorithmic approaches tend to rely on random
exploration and slow, gradient-based behavior updates. How can we endow
autonomous agents with such capabilities on par with humans? Taking inspiration
from recent progress on both in-context learning and large-scale behavioral
cloning, in this work we propose behavioral exploration: training agents to
internalize what it means to explore and adapt in-context over the space of
``expert'' behaviors. To achieve this, given access to a dataset of expert
demonstrations, we train a long-context generative model to predict expert
actions conditioned on a context of past observations and a measure of how
``exploratory'' the expert's behaviors are relative to this context. This
enables the model to not only mimic the behavior of an expert, but also, by
feeding its past history of interactions into its context, to select different
expert behaviors than what have been previously selected, thereby allowing for
fast online adaptation and targeted, ``expert-like'' exploration. We
demonstrate the effectiveness of our method in both simulated locomotion and
manipulation settings, as well as on real-world robotic manipulation tasks,
illustrating its ability to learn adaptive, exploratory behavior.

</details>


### [104] [Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation](https://arxiv.org/abs/2507.09043)
*Jingxiang Qu,Wenhan Gao,Yi Liu*

Key words: GPGMs, 高斯噪声, 计算效率, 生成模型

TL;DR: 本文提出了一种改进高斯概率生成模型（GPGMs）效率的框架，通过分析数据在噪声过程中的高斯化特征，优化生成轨迹，提升计算效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 高斯概率生成模型在训练和采样时的高计算成本限制了其实际应用，本文旨在解决这一问题。

Method: 通过分析数据在噪声过程中的高斯化特征，识别出其特征步骤，并用封闭式高斯近似替换剩余的生成轨迹。

Result: 实验结果显示，该方法在多种数据类型上都显著提高了样本质量和计算效率。

Conclusion: 该方法在不牺牲训练粒度和推理保真度的情况下，有效提升了生成效率。

Abstract: Gaussian-based Probabilistic Generative Models (GPGMs) generate data by
reversing a stochastic process that progressively corrupts samples with
Gaussian noise. While these models have achieved state-of-the-art performance
across diverse domains, their practical deployment remains constrained by the
high computational cost of long generative trajectories, which often involve
hundreds to thousands of steps during training and sampling. In this work, we
introduce a theoretically grounded and empirically validated framework that
improves generation efficiency without sacrificing training granularity or
inference fidelity. Our key insight is that for certain data modalities, the
noising process causes data to rapidly lose its identity and converge toward a
Gaussian distribution. We analytically identify a characteristic step at which
the data has acquired sufficient Gaussianity, and then replace the remaining
generation trajectory with a closed-form Gaussian approximation. Unlike
existing acceleration techniques that coarsening the trajectories by skipping
steps, our method preserves the full resolution of learning dynamics while
avoiding redundant stochastic perturbations between `Gaussian-like'
distributions. Empirical results across multiple data modalities demonstrate
substantial improvements in both sample quality and computational efficiency.

</details>


### [105] [Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction](https://arxiv.org/abs/2507.09061)
*Thomas T. Zhang,Daniel Pfrommer,Nikolai Matni,Max Simchowitz*

Key words: 模仿学习, 复合误差, 动作分块, 噪声注入, 控制理论, 强化学习

TL;DR: 论文研究了在连续状态和动作动态系统中模仿专家演示的问题，提出了两种最小干预方法——“动作分块”和“噪声注入”，以解决模仿学习中的复合误差问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 模仿学习在离散环境中取得巨大成功，但在物理系统（如自动驾驶和机器人学习）中由于复合误差问题表现较差。本研究旨在探索更有效的策略参数化或数据增强方法。

Method: 提出两种干预方法：1）在开环稳定系统中使用“动作分块”，即预测并在开环中执行动作序列；2）在可能不稳定的系统中使用“噪声注入”，即在专家演示中增加噪声。

Result: 通过控制理论和强化学习的结合，研究证明这两种方法能有效缓解复合误差，且其优势与现有文献的设计目标不同。

Conclusion: 研究揭示了从控制理论和强化学习中独立考虑时未自然出现的新见解，为连续系统中的模仿学习提供了简单而有效的方法。

Abstract: We study the problem of imitating an expert demonstrator in a continuous
state-and-action dynamical system. While imitation learning in discrete
settings such as autoregressive language modeling has seen immense success and
popularity in recent years, imitation in physical settings such as autonomous
driving and robot learning has proven comparably more complex due to the
compounding errors problem, often requiring elaborate set-ups to perform
stably. Recent work has demonstrated that even in benign settings, exponential
compounding errors are unavoidable when learning solely from expert-controlled
trajectories, suggesting the need for more advanced policy parameterizations or
data augmentation. To this end, we present minimal interventions that provably
mitigate compounding errors in continuous state-and-action imitation learning.
When the system is open-loop stable, we prescribe "action chunking," i.e.,
predicting and playing sequences of actions in open-loop; when the system is
possibly unstable, we prescribe "noise injection," i.e., adding noise during
expert demonstrations. These interventions align with popular choices in modern
robot learning, though the benefits we derive are distinct from the effects
they were designed to target. Our results draw insights and tools from both
control theory and reinforcement learning; however, our analysis reveals novel
considerations that do not naturally arise when either literature is considered
in isolation.

</details>


### [106] [Queue up for takeoff: a transferable deep learning framework for flight delay prediction](https://arxiv.org/abs/2507.09084)
*Nnamdi Daniel Aghanya,Ta Duong Vu,Amaëlle Diop,Charlotte Deville,Nour Imane Kerroumi,Irene Moulitsas,Jun Li,Desmond Bisandu*

Key words: 航班延误预测,排队论,注意力模型,泛化能力,QT-SimAM

TL;DR: 论文提出了一种结合排队论和注意力模型（QT-SimAM）的新方法，用于高精度预测航班延误，并在不同网络中表现出强泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 航班延误对航空业造成重大经济和运营损失，提升预测模型的精确性和泛化能力是改善乘客体验和减少收入损失的关键。

Method: 采用排队论与简单注意力模型（SimAM）结合的方法，开发了QT-SimAM（双向）模型。

Result: 在美国交通统计局数据上，模型准确率0.927，F1得分0.932；在EUROCONTROL数据集上，准确率0.826，F1得分0.791。

Conclusion: QT-SimAM是一种高效的端到端航班延误预测方法，能高精度预测不同网络的延误，有助于减少乘客焦虑和优化运营决策。

Abstract: Flight delays are a significant challenge in the aviation industry, causing
major financial and operational disruptions. To improve passenger experience
and reduce revenue loss, flight delay prediction models must be both precise
and generalizable across different networks. This paper introduces a novel
approach that combines Queue-Theory with a simple attention model, referred to
as the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from
the US Bureau of Transportation Statistics, where our proposed QT-SimAM
(Bidirectional) model outperformed existing methods with an accuracy of 0.927
and an F1 score of 0.932. To assess transferability, we tested the model on the
EUROCONTROL dataset. The results demonstrated strong performance, achieving an
accuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an
effective, end-to-end methodology for predicting flight delays. The proposed
model's ability to forecast delays with high accuracy across different networks
can help reduce passenger anxiety and improve operational decision-making

</details>


### [107] [Deep Reinforcement Learning with Gradient Eligibility Traces](https://arxiv.org/abs/2507.09087)
*Esraa Elelimy,Brett Daley,Andrew Patterson,Marlos C. Machado,Adam White,Martha White*

Key words: 深度强化学习，离策略学习，GTD方法，信用分配，λ-return

TL;DR: 论文提出了将GPBE目标扩展到多步信用分配的方法，并基于λ-return推导了三种梯度优化算法，提高了深度强化学习的效率和稳定性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的半梯度TD方法简单高效但易发散，而GTD方法虽有强收敛性但多用于非线性函数逼近。本文旨在解决单步方法的信用分配慢和样本需求大的问题。

Method: 将GPBE目标扩展至多步信用分配，基于λ-return推导了三种梯度优化算法，提供了兼容经验回放的前向视角和兼容流式算法的后向视角公式。

Result: 在MuJoCo和MinAtar环境中，提出的算法性能优于PPO和StreamQ。

Conclusion: 多步GPBE方法在深度强化学习中实现了更高效和稳定的离策略学习。

Abstract: Achieving fast and stable off-policy learning in deep reinforcement learning
(RL) is challenging. Most existing methods rely on semi-gradient
temporal-difference (TD) methods for their simplicity and efficiency, but are
consequently susceptible to divergence. While more principled approaches like
Gradient TD (GTD) methods have strong convergence guarantees, they have rarely
been used in deep RL. Recent work introduced the Generalized Projected Bellman
Error ($\GPBE$), enabling GTD methods to work efficiently with nonlinear
function approximation. However, this work is only limited to one-step methods,
which are slow at credit assignment and require a large number of samples. In
this paper, we extend the $\GPBE$ objective to support multistep credit
assignment based on the $\lambda$-return and derive three gradient-based
methods that optimize this new objective. We provide both a forward-view
formulation compatible with experience replay and a backward-view formulation
compatible with streaming algorithms. Finally, we evaluate the proposed
algorithms and show that they outperform both PPO and StreamQ in MuJoCo and
MinAtar environments, respectively. Code available at
https://github.com/esraaelelimy/gtd\_algos

</details>


### [108] [Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA](https://arxiv.org/abs/2507.09091)
*Shayan K. Azmoodeh,Krishna Subramani,Paris Smaragdis*

Key words: 低秩分解,主成分分析,独立成分分析,连续时间信号,隐式神经表示

TL;DR: 论文提出了一种模型无关的隐式神经信号表示框架，用于解决连续时间向量值信号的低秩分解问题，如主成分分析和独立成分分析（PCA、ICA）。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统PCA和ICA方法在处理点云或不规则采样信号时存在局限性，需要一种能在连续域中统一解决这些问题的新方法。

Method: 通过将信号建模为连续时间随机过程，并在网络损失中引入对比函数项，以确保分解后的源信号具有所需的统计特性（如去相关、独立性）。

Result: 该方法扩展了PCA和ICA的应用范围，使其适用于标准技术无法处理的点云和不规则采样信号。

Conclusion: 提出的框架为连续域中的信号分解提供了统一的解决方案，并展示了其在非传统信号处理任务中的潜力。

Abstract: We generalize the low-rank decomposition problem, such as principal and
independent component analysis (PCA, ICA) for continuous-time vector-valued
signals and provide a model-agnostic implicit neural signal representation
framework to learn numerical approximations to solve the problem. Modeling
signals as continuous-time stochastic processes, we unify the approaches to
both the PCA and ICA problems in the continuous setting through a contrast
function term in the network loss, enforcing the desired statistical properties
of the source signals (decorrelation, independence) learned in the
decomposition. This extension to a continuous domain allows the application of
such decompositions to point clouds and irregularly sampled signals where
standard techniques are not applicable.

</details>


### [109] [Principled Foundations for Preference Optimization](https://arxiv.org/abs/2507.07855)
*Wenxuan Zhou,Shujian Zhang,Brice Magdalou,John Lambert,Ehsan Amid,Richard Nock,Andrew Hard*

Key words: 直接偏好优化（DPO），损失函数，随机选择理论，机器学习

TL;DR: 本文揭示了直接偏好优化（DPO）与ML中偏好学习的两种主要理论之间的特定联系，覆盖广泛且具有实际意义。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 理解DPO的工作原理及其与其他理论的联系，以适应多样化的应用场景并避免潜在陷阱。

Method: 通过理论分析建立DPO与损失函数（Savage）和随机选择理论的联系，支持非凸目标和扩展设定。

Result: 建立了DPO与多种理论的普适性联系，支持了选择性放弃和非凸目标等特性。

Conclusion: 从理论角度理解DPO有助于优化应用和避免陷阱，其扩展性为未来研究提供了方向。

Abstract: In this paper, we show that direct preference optimization (DPO) is a very
specific form of a connection between two major theories in the ML context of
learning from preferences: loss functions (Savage) and stochastic choice
(Doignon-Falmagne and Machina). The connection is established for all of
Savage's losses and at this level of generality, (i) it includes support for
abstention on the choice theory side, (ii) it includes support for non-convex
objectives on the ML side, and (iii) it allows to frame for free some notable
extensions of the DPO setting, including margins and corrections for length.
Getting to understand how DPO operates from a general principled perspective is
crucial because of the huge and diverse application landscape of models,
because of the current momentum around DPO, but also -- and importantly --
because many state of the art variations on DPO definitely occupy a small
region of the map that we cover. It also helps to understand the pitfalls of
departing from this map, and figure out workarounds.

</details>


### [110] [On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving](https://arxiv.org/abs/2507.09095)
*Md Hasan Shahriar,Md Mohaimin Al Barat,Harshavardhan Sundar,Naren Ramakrishnan,Y. Thomas Hou,Wenjing Lou*

Key words: 多模态融合,自动驾驶,时间错位攻击,防御方案

TL;DR: DejaVu攻击利用网络延迟导致传感器流时间错位，严重降低多模态融合（MMF）感知性能；AION防御方案通过跨模态时间一致性检测此类攻击。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有MMF依赖时间同步，易受网络延迟攻击，影响自动驾驶感知任务。

Method: 提出DejaVu攻击和AION防御方案，后者利用多模态共享表示学习和动态时间规整检测时间错位。

Result: DejaVu分别使目标检测和目标跟踪性能下降88.5%和73%；AION的AUROC达0.92-0.98。

Conclusion: AION是高效且通用的时间错位攻击防御方案。

Abstract: Multimodal fusion (MMF) plays a critical role in the perception of autonomous
driving, which primarily fuses camera and LiDAR streams for a comprehensive and
efficient scene understanding. However, its strict reliance on precise temporal
synchronization exposes it to new vulnerabilities. In this paper, we introduce
DejaVu, a novel attack that exploits network-induced delays to create subtle
temporal misalignments across sensor streams, severely degrading downstream
MMF-based perception tasks. Our comprehensive attack analysis across different
models and datasets reveals these sensors' task-specific imbalanced
sensitivities: object detection is overly dependent on LiDAR inputs while
object tracking is highly reliant on the camera inputs. Consequently, with a
single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to
88.5%, while with a three-frame camera delay, multiple object tracking accuracy
(MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense
patch that can work alongside the existing perception model to monitor temporal
alignment through cross-modal temporal consistency. AION leverages multimodal
shared representation learning and dynamic time warping to determine the path
of temporal alignment and calculate anomaly scores based on the alignment. Our
thorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with
low false positives across datasets and model architectures, demonstrating it
as a robust and generalized defense against the temporal misalignment attacks.

</details>


### [111] [S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe](https://arxiv.org/abs/2507.09101)
*Yanan Cao,Omid Memarrast,Shiqin Cai,Sinduja Subramaniam,Evren Korpeoglu,Kannan Achan*

Key words: 电商、食谱推荐、多任务学习、Set Transformer

TL;DR: 该论文提出了S2SRec2框架，用于解决电商中基于食谱的食材推荐问题，通过多任务学习同时预测缺失食材和评估购物篮完整性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统方法仅能预测单一缺失食材，且忽略食材间关系，无法满足现实需求。

Method: 采用基于Set Transformer的集合到集合（S2S）推荐框架，通过多任务学习优化预测和完整性评估。

Result: 在大规模数据集实验中，S2SRec2显著优于单目标基线方法。

Conclusion: S2SRec2为提升购物体验和激发烹饪创意提供了有效解决方案。

Abstract: In grocery e-commerce, customers often build ingredient baskets guided by
dietary preferences but lack the expertise to create complete meals. Leveraging
recipe knowledge to recommend complementary ingredients based on a partial
basket is essential for improving the culinary experience. Traditional recipe
completion methods typically predict a single missing ingredient using a
leave-one-out strategy. However, they fall short in two key aspects: (i) they
do not reflect real-world scenarios where multiple ingredients are often
needed, and (ii) they overlook relationships among the missing ingredients
themselves. To address these limitations, we reformulate basket completion as a
set-to-set (S2S) recommendation problem, where an incomplete basket is input
into a system that predicts a set of complementary ingredients. We introduce
S2SRec2, a set-to-set ingredient recommendation framework based on a Set
Transformer and trained in a multitask learning paradigm. S2SRec2 jointly
learns to (i) retrieve missing ingredients from the representation of existing
ones and (ii) assess basket completeness after prediction. These tasks are
optimized together, enforcing accurate retrieval and coherent basket
completion. Experiments on large-scale recipe datasets and qualitative analyses
show that S2SRec2 significantly outperforms single-target baselines, offering a
promising approach to enhance grocery shopping and inspire culinary creativity.

</details>


### [112] [A Study of Value-Aware Eigenoptions](https://arxiv.org/abs/2507.09127)
*Harshil Kotamreddy,Marlos C. Machado*

Key words: 强化学习, 信用分配, eigenoptions, 探索, 深度强化学习

TL;DR: 本文探讨了eigenoptions在模型无关强化学习（RL）信用分配中的作用，发现预定义的eigenoptions不仅能帮助探索，还能加速信用分配，而在线发现则可能阻碍学习。文中还提出了一种在非线性函数逼近下学习option-values的方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究eigenoptions在RL中的作用，尤其是它们在加速信用分配方面的潜力，以及如何通过预定义或在线发现的方式优化学习过程。

Method: 在表格和基于像素的网格世界中评估预定义和在线发现的eigenoptions，提出了一种在深度RL中学习option-values的方法。

Result: 预定义的eigenoptions有助于探索和信用分配，而在线发现可能因过于强烈的经验偏差而阻碍学习。提出的option-values学习方法在非线函数逼近下有效。

Conclusion: eigenoptions及其更广泛的options框架在同时支持信用分配和探索方面具有潜力，但也带来复杂性。

Abstract: Options, which impose an inductive bias toward temporal and hierarchical
structure, offer a powerful framework for reinforcement learning (RL). While
effective in sequential decision-making, they are often handcrafted rather than
learned. Among approaches for discovering options, eigenoptions have shown
strong performance in exploration, but their role in credit assignment remains
underexplored. In this paper, we investigate whether eigenoptions can
accelerate credit assignment in model-free RL, evaluating them in tabular and
pixel-based gridworlds. We find that pre-specified eigenoptions aid not only
exploration but also credit assignment, whereas online discovery can bias the
agent's experience too strongly and hinder learning. In the context of deep RL,
we also propose a method for learning option-values under non-linear function
approximation, highlighting the impact of termination conditions on
performance. Our findings reveal both the promise and complexity of using
eigenoptions, and options more broadly, to simultaneously support credit
assignment and exploration in reinforcement learning.

</details>


### [113] [Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning](https://arxiv.org/abs/2507.09132)
*Chu-Yuan Wei,Shun-Yao Liu,Sheng-Da Zhuo,Chang-Dong Wang,Shu-Qiang Huang,Mohsen Guizani*

Key words: 图神经网络, 图提示, 权重剪枝, 节点分类

TL;DR: 本文提出了一种结合图提示与权重剪枝的新框架GPAWP，旨在通过减少提示数量提升图神经网络的性能和效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管图神经网络（GNNs）在图相关任务中表现优异，但仍面临训练推理时间长、难以捕捉复杂关系以及特征提取不足等问题。图预训练和提示方法虽受关注，但其在优化模型和稳定性方面的潜力未被充分挖掘。

Method: 提出GPAWP框架，通过重要性评估函数确定不同粒度下的正负提示权重，并采用分层结构化剪枝去除负提示标签，生成更高效的提示。

Result: 在三个基准数据集上的实验表明，GPAWP显著减少了节点分类任务中的参数量。

Conclusion: GPAWP通过优化图提示的权重和剪枝，提升了模型性能和效率，为GNNs的进一步优化提供了新思路。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in various
graph-based tasks (e.g., node classification or link prediction). Despite their
triumphs, GNNs still face challenges such as long training and inference times,
difficulty in capturing complex relationships, and insufficient feature
extraction. To tackle these issues, graph pre-training and graph prompt methods
have garnered increasing attention for their ability to leverage large-scale
datasets for initial learning and task-specific adaptation, offering potential
improvements in GNN performance. However, previous research has overlooked the
potential of graph prompts in optimizing models, as well as the impact of both
positive and negative graph prompts on model stability and efficiency. To
bridge this gap, we propose a novel framework combining graph prompts with
weight pruning, called GPAWP, which aims to enhance the performance and
efficiency of graph prompts by using fewer of them. We evaluate the importance
of graph prompts using an importance assessment function to determine positive
and negative weights at different granularities. Through hierarchically
structured pruning, we eliminate negative prompt labels, resulting in more
parameter-efficient and competitively performing prompts. Extensive experiments
on three benchmark datasets demonstrate the superiority of GPAWP, leading to a
significant reduction in parameters in node classification tasks.

</details>


### [114] [POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution](https://arxiv.org/abs/2507.09137)
*Nripsuta Ani Saxena,Shang-Ling Hsu,Mehul Shetty,Omar Alkhadra,Cyrus Shahabi,Abigail L. Horn*

Key words: POI attribution, Transformer, mobility analytics, GPS误差, 自注意力机制

TL;DR: 本文提出了一个名为POIFormer的新型Transformer框架，用于准确识别用户访问的兴趣点（POI）。它通过整合多种特征（如空间邻近性、访问时间、上下文和行为模式），解决了GPS不准确和POI密集分布的挑战。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的POI归属方法主要依赖有限的时空或行为特征，无法有效应对GPS误差和城市环境中POIs高密度分布的复杂性。

Method: POIFormer利用Transformer的自注意力机制，联合建模空间邻近性、访问时间、上下文和行为特征，并结合用户历史与未来访问模式及群体行为模式（通过预计算的KDE）。

Result: 在真实世界移动数据集上的实验表明，POIFormer在空间噪声和POI密集分布的场景中表现显著优于现有基线方法。

Conclusion: POIFormer提供了一种高效且准确的方法，适用于大规模、噪声多的移动数据，且无需依赖难以获取的数据层。

Abstract: Accurately attributing user visits to specific Points of Interest (POIs) is a
foundational task for mobility analytics, personalized services, marketing and
urban planning. However, POI attribution remains challenging due to GPS
inaccuracies, typically ranging from 2 to 20 meters in real-world settings, and
the high spatial density of POIs in urban environments, where multiple venues
can coexist within a small radius (e.g., over 50 POIs within a 100-meter radius
in dense city centers). Relying on proximity is therefore often insufficient
for determining which POI was actually visited. We introduce
\textsf{POIFormer}, a novel Transformer-based framework for accurate and
efficient POI attribution. Unlike prior approaches that rely on limited
spatiotemporal, contextual, or behavioral features, \textsf{POIFormer} jointly
models a rich set of signals, including spatial proximity, visit timing and
duration, contextual features from POI semantics, and behavioral features from
user mobility and aggregated crowd behavior patterns--using the Transformer's
self-attention mechanism to jointly model complex interactions across these
dimensions. By leveraging the Transformer to model a user's past and future
visits (with the current visit masked) and incorporating crowd-level behavioral
patterns through pre-computed KDEs, \textsf{POIFormer} enables accurate,
efficient attribution in large, noisy mobility datasets. Its architecture
supports generalization across diverse data sources and geographic contexts
while avoiding reliance on hard-to-access or unavailable data layers, making it
practical for real-world deployment. Extensive experiments on real-world
mobility datasets demonstrate significant improvements over existing baselines,
particularly in challenging real-world settings characterized by spatial noise
and dense POI clustering.

</details>


### [115] [Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations](https://arxiv.org/abs/2507.09173)
*Mengjie Chen,Ming Zhang,Cunquan Qu*

Key words: 药物相互作用, 图神经网络, 分子结构, 生物医学知识, 可解释性

TL;DR: MolecBioNet是一个基于图的新框架，整合分子和生物医学知识以预测药物相互作用（DDI），通过统一建模和多尺度知识集成，提高了预测性能和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有图方法忽视了药物对的复杂上下文依赖性，且难以整合生物和分子层面的信息，导致预测机制缺乏解释性。

Method: MolecBioNet通过统一建模药物对，提取生物医学知识图谱的局部子图，并结合分子表示构建层次交互图，引入CASPool和AGIPool两种池化策略，以及互信息最小化正则化。

Result: 实验结果优于现有方法，消融研究和嵌入可视化验证了统一建模和多尺度知识集成的优势。

Conclusion: MolecBioNet为DDI预测提供了全面且可解释的解决方案，强调了多尺度建模的重要性。

Abstract: Drug-drug interactions (DDIs) represent a critical challenge in pharmacology,
often leading to adverse drug reactions with significant implications for
patient safety and healthcare outcomes. While graph-based methods have achieved
strong predictive performance, most approaches treat drug pairs independently,
overlooking the complex, context-dependent interactions unique to drug pairs.
Additionally, these models struggle to integrate biological interaction
networks and molecular-level structures to provide meaningful mechanistic
insights. In this study, we propose MolecBioNet, a novel graph-based framework
that integrates molecular and biomedical knowledge for robust and interpretable
DDI prediction. By modeling drug pairs as unified entities, MolecBioNet
captures both macro-level biological interactions and micro-level molecular
influences, offering a comprehensive perspective on DDIs. The framework
extracts local subgraphs from biomedical knowledge graphs and constructs
hierarchical interaction graphs from molecular representations, leveraging
classical graph neural network methods to learn multi-scale representations of
drug pairs. To enhance accuracy and interpretability, MolecBioNet introduces
two domain-specific pooling strategies: context-aware subgraph pooling
(CASPool), which emphasizes biologically relevant entities, and
attention-guided influence pooling (AGIPool), which prioritizes influential
molecular substructures. The framework further employs mutual information
minimization regularization to enhance information diversity during embedding
fusion. Experimental results demonstrate that MolecBioNet outperforms
state-of-the-art methods in DDI prediction, while ablation studies and
embedding visualizations further validate the advantages of unified drug pair
modeling and multi-scale knowledge integration.

</details>


### [116] [Continual Reinforcement Learning by Planning with Online World Models](https://arxiv.org/abs/2507.09177)
*Zichen Liu,Guoji Fu,Chao Du,Wee Sun Lee,Min Lin*

Key words: 持续强化学习,灾难性遗忘,在线世界模型,模型预测控制

TL;DR: 论文提出了一种基于在线世界模型的规划方法（FTL OA），通过模型预测控制解决持续强化学习中的灾难性遗忘问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 持续强化学习中，代理在序列任务学习中容易遗忘旧任务（灾难性遗忘）。本文旨在通过在线世界模型解决这一问题。

Method: 提出一种在线学习的Follow-The-Leader浅层模型，用于捕捉世界动态，并通过模型预测控制规划任务。模型具有免遗忘特性，且支持增量更新。

Result: 在设计的Continual Bench环境中，FTL OA表现优于基于深度世界模型的多类持续学习基线，能持续学习新任务且不遗忘旧技能。

Conclusion: 在线世界模型和规划方法在持续强化学习中有效解决了灾难性遗忘问题，并在实验中显示出优越性。

Abstract: Continual reinforcement learning (CRL) refers to a naturalistic setting where
an agent needs to endlessly evolve, by trial and error, to solve multiple tasks
that are presented sequentially. One of the largest obstacles to CRL is that
the agent may forget how to solve previous tasks when learning a new task,
known as catastrophic forgetting. In this paper, we propose to address this
challenge by planning with online world models. Specifically, we learn a
Follow-The-Leader shallow model online to capture the world dynamics, in which
we plan using model predictive control to solve a set of tasks specified by any
reward functions. The online world model is immune to forgetting by
construction with a proven regret bound of $\mathcal{O}(\sqrt{K^2D\log(T)})$
under mild assumptions. The planner searches actions solely based on the latest
online model, thus forming a FTL Online Agent (OA) that updates incrementally.
To assess OA, we further design Continual Bench, a dedicated environment for
CRL, and compare with several strong baselines under the same model-planning
algorithmic framework. The empirical results show that OA learns continuously
to solve new tasks while not forgetting old skills, outperforming agents built
on deep world models with various continual learning techniques.

</details>


### [117] [XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge](https://arxiv.org/abs/2507.09202)
*Wuxin Wang,Weicheng Ni,Lilan Huang,Tao Hao,Ben Fei,Shuo Ma,Taikang Yuan,Yanlai Zhao,Kefeng Deng,Xiaoyong Li,Boheng Duan,Lei Bai,Kaijun Ren*

Key words: AI, 天气预报, 数据同化, 全球预报系统, 数值天气预报

TL;DR: XiChen是一种完全由AI驱动的全球天气预报系统，能够在17秒内完成从数据同化到中期预报的整个流程，准确性与传统数值天气预报系统相当。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统AI驱动的天气预报模型依赖数值天气预报系统准备初始条件，耗时且效率低。XiChen旨在实现完全由AI驱动的独立天气预报。

Method: XiChen基于预训练的天气预报基础模型，通过微调作为观测算子和数据同化模型，可扩展地同化常规和卫星原始观测数据，并整合四维变分知识。

Result: XiChen的预报准确性与传统数值天气预报系统相当，预报技能时间超过8.25天。

Conclusion: XiChen展示了完全由AI驱动的天气预报独立于数值天气预报系统的潜力。

Abstract: Recent advancements in Artificial Intelligence (AI) demonstrate significant
potential to revolutionize weather forecasting. However, most AI-driven models
rely on Numerical Weather Prediction (NWP) systems for initial condition
preparation, which often consumes hours on supercomputers. Here we introduce
XiChen, the first observation-scalable fully AI-driven global weather
forecasting system, whose entire pipeline, from Data Assimilation (DA) to
medium-range forecasting, can be accomplished within only 17 seconds. XiChen is
built upon a foundation model that is pre-trained for weather forecasting.
Meanwhile, this model is subsequently fine-tuned to serve as both observation
operators and DA models, thereby scalably assimilating conventional and raw
satellite observations. Furthermore, the integration of four-dimensional
variational knowledge ensures that XiChen's DA and medium-range forecasting
accuracy rivals that of operational NWP systems, amazingly achieving a skillful
forecasting lead time exceeding 8.25 days. These findings demonstrate that
XiChen holds strong potential toward fully AI-driven weather forecasting
independent of NWP systems.

</details>


### [118] [Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling](https://arxiv.org/abs/2507.09211)
*Xinyue Liu,Xiao Peng,Shuyue Yan,Yuntian Chen,Dongxiao Zhang,Zhixiao Niu,Hui-Min Wang,Xiaogang He*

Key words: 气候极端事件, DeepX-GAN, 空间结构, 潜在风险, 中东和北非

TL;DR: 论文提出了DeepX-GAN模型，用于捕捉罕见极端气候事件的空间结构，模拟超出历史经验的极端事件，揭示脆弱系统中的潜在风险。应用该模型于中东和北非地区，发现这些事件对高脆弱性、低社会经济准备的地区影响更大。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有气候极端事件记录不完整，忽视空间依赖性低估了同步灾害的风险。

Method: 开发了知识引导的深度生成模型DeepX-GAN，模拟超出历史经验但统计上合理的极端事件。

Result: 模型在中东和北非地区的应用揭示了对高脆弱性地区的非比例影响，未来变暖可能扩展和重新分布这些风险。

Conclusion: 研究强调了传统灾害规划中的盲点，提出了需要开发适应性强、预见性的空间政策。

Abstract: Observed records of climate extremes provide an incomplete picture of risk,
missing "unseen" extremes that exceed historical bounds. In parallel,
neglecting spatial dependence undervalues the risk of synchronized hazards that
amplify impacts. To address these challenges, we develop DeepX-GAN
(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial
Network), a knowledge-informed deep generative model designed to better capture
the spatial structure of rare extremes. The zero-shot generalizability of
DeepX-GAN enables simulation of unseen extremes that fall outside historical
experience yet remain statistically plausible. We define two types of unseen
extremes: "checkmate" extremes that directly hit targets, and "stalemate"
extremes that narrowly miss. These unrealized scenarios expose latent risks in
fragile systems and may reinforce a false sense of resilience if overlooked.
Near misses, in particular, can prompt either proactive adaptation or dangerous
complacency, depending on how they are interpreted. Applying DeepX-GAN to the
Middle East and North Africa (MENA), we find that these unseen extremes
disproportionately affect regions with high vulnerability and low socioeconomic
readiness, but differ in urgency and interpretation. Future warming could
expand and redistribute these unseen extremes, with emerging exposure hotspots
in Indo-Pakistan and Central Africa. This distributional shift highlights
critical blind spots in conventional hazard planning and underscores the need
to develop spatially adaptive policies that anticipate emergent risk hotspots
rather than simply extrapolating from historical patterns.

</details>


### [119] [Warm Starts Accelerate Generative Modelling](https://arxiv.org/abs/2507.09212)
*Jonas Scholz,Richard E. Turner*

Key words: 生成模型, 条件生成, 高效采样, warm-start模型

TL;DR: 提出了一种基于条件生成的高效方法——warm-start模型，通过预测条件化的先验分布显著加速生成过程。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 迭代生成模型（如扩散模型和流匹配）生成高质量样本需要大量计算，效率低下。本文旨在通过提供更好的初始点加速生成过程。

Method: 设计了一个确定性的warm-start模型，预测条件化的高斯先验分布（N(mu, sigma)），取代传统的无信息先验（N(0, I)），大幅减少生成距离。

Result: 在图像修复任务上，仅用11次函数评估（1次warm-start，10次生成）即可达到与1000步DDPM基线相当的效果。

Conclusion: warm-start方法显著提升生成效率，且兼容现有生成模型和采样器，可与其他加速技术结合。

Abstract: Iterative generative models, like diffusion and flow-matching, create
high-fidelity samples by progressively refining a noise vector into data.
However, this process is notoriously slow, often requiring hundreds of function
evaluations. We introduce the warm-start model, a simple, deterministic model
that dramatically accelerates conditional generation by providing a better
starting point. Instead of starting generation from an uninformed N(0, I)
prior, our warm-start model predicts an informed prior N(mu, sigma), whose
moments are conditioned on the input context. This "warm start" substantially
reduces the distance the generative process must traverse, particularly when
the conditioning information is strongly informative. On tasks like image
inpainting, our method achieves results competitive with a 1000-step DDPM
baseline using only 11 total function evaluations (1 for the warm start, 10 for
generation). A simple conditional normalization trick makes our method
compatible with any standard generative model and sampler without modification,
allowing it to be combined with other efficient sampling techniques for further
acceleration. Our implementation is available at
https://github.com/jonas-scholz123/warm-start-model.

</details>


### [120] [Optimizing Basis Function Selection in Constructive Wavelet Neural Networks and Its Applications](https://arxiv.org/abs/2507.09213)
*Dunsheng Huang,Dong Shen,Lei Lu,Ying Tan*

Key words: 小波神经网络,信号处理,时间序列分析,计算效率

TL;DR: 提出了一种构造性小波神经网络（WNN），通过分析非线性函数频率成分选择初始小波基，提升计算效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统WNN在构建精确小波基和高计算成本方面存在挑战，限制了其应用。

Method: 引入频率估计器和基增加机制，选择高频能量部分的小波基。

Result: 显著提高计算效率，并通过多个案例展示了框架的广泛适用性。

Conclusion: 该方法为WNN的高效应用提供了新的理论基础和实用工具。

Abstract: Wavelet neural network (WNN), which learns an unknown nonlinear mapping from
the data, has been widely used in signal processing, and time-series analysis.
However, challenges in constructing accurate wavelet bases and high
computational costs limit their application. This study introduces a
constructive WNN that selects initial bases and trains functions by introducing
new bases for predefined accuracy while reducing computational costs. For the
first time, we analyze the frequency of unknown nonlinear functions and select
appropriate initial wavelets based on their primary frequency components by
estimating the energy of the spatial frequency component. This leads to a novel
constructive framework consisting of a frequency estimator and a wavelet-basis
increase mechanism to prioritize high-energy bases, significantly improving
computational efficiency. The theoretical foundation defines the necessary
time-frequency range for high-dimensional wavelets at a given accuracy. The
framework's versatility is demonstrated through four examples: estimating
unknown static mappings from offline data, combining two offline datasets,
identifying time-varying mappings from time-series data, and capturing
nonlinear dependencies in real time-series data. These examples showcase the
framework's broad applicability and practicality. All the code will be released
at https://github.com/dshuangdd/CWNN.

</details>


### [121] [TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding](https://arxiv.org/abs/2507.09252)
*Shukai Gong,Yiyang Fu,Fengyuan Ran,Feng Zhou*

Key words: Transformer, 时间点过程, 推测解码, 高效采样, 并行验证

TL;DR: TPP-SD是一种新颖的方法，通过结合Transformer时间点过程和推测解码技术，加速事件采样。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决Transformer时间点过程采样效率低的问题，提高实际应用中的序列采样速度。

Method: 利用较小的草稿模型生成候选事件，并通过较大的目标模型并行验证。

Result: 在保持相同输出分布的同时，实现2-6倍的速度提升。

Conclusion: TPP-SD在高效采样与实际需求之间架起桥梁。

Abstract: We propose TPP-SD, a novel approach that accelerates Transformer temporal
point process (TPP) sampling by adapting speculative decoding (SD) techniques
from language models. By identifying the structural similarities between
thinning algorithms for TPPs and speculative decoding for language models, we
develop an efficient sampling framework that leverages a smaller draft model to
generate multiple candidate events, which are then verified by the larger
target model in parallel. TPP-SD maintains the same output distribution as
autoregressive sampling while achieving significant acceleration. Experiments
on both synthetic and real datasets demonstrate that our approach produces
samples from identical distributions as standard methods, but with 2-6$\times$
speedup. Our ablation studies analyze the impact of hyperparameters such as
draft length and draft model size on sampling efficiency. TPP-SD bridges the
gap between powerful Transformer TPP models and the practical need for rapid
sequence sampling.

</details>


### [122] [Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations](https://arxiv.org/abs/2507.09264)
*Payel Mukhopadhyay,Michael McCabe,Ruben Ohana,Miles Cranmer*

Key words: 补丁变换器、动态补丁大小、CKM、CSM、PDE替代模型

TL;DR: 本文提出两种轻量级、架构无关的模块CKM和CSM，用于动态调整基于补丁的模型中补丁的大小，无需重新训练且不损失精度，从而提高预测任务的长期稳定性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对基于补丁的变换器模型中固定补丁大小限制了其在实际部署中的预算效率问题，提出动态调整补丁大小的解决方案。

Method: 引入两种模块CKM和CSM，通过循环补丁大小调整技术，动态控制补丁大小，无需重新训练。

Result: 在多种2D和3D PDE基准测试中，提高了预测的保真度和运行时效率。

Conclusion: 这是第一个在基于补丁的PDE替代模型中实现推理时补丁大小可调的方法，其即插即用的设计使其广泛适用于各种架构。

Abstract: Patch-based transformer surrogates have become increasingly effective for
modeling spatiotemporal dynamics, but the fixed patch size is a major
limitation for budget-conscience deployment in production. We introduce two
lightweight, architecture-agnostic modules-the Convolutional Kernel Modulator
(CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size
control at inference in patch based models, without retraining or accuracy
loss. Combined with a cyclic patch-size rollout, our method mitigates patch
artifacts and improves long-term stability for video-like prediction tasks.
Applied to a range of challenging 2D and 3D PDE benchmarks, our approach
improves rollout fidelity and runtime efficiency. To our knowledge, this is the
first framework to enable inference-time patch-size tunability in patch-based
PDE surrogates. Its plug-and-play design makes it broadly applicable across
architectures-establishing a general foundation for compute-adaptive modeling
in PDE surrogate tasks.

</details>


### [123] [Impute With Confidence: A Framework for Uncertainty Aware Multivariate Time Series Imputation](https://arxiv.org/abs/2507.09353)
*Addison Weatherhead,Anna Goldenberg*

Key words: missing data, uncertainty, time series, healthcare, imputation

TL;DR: 论文提出了一个框架，用于量化时间序列数据插补中的不确定性，并选择性插补高置信度值，以减少错误并提升下游任务性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 医疗领域中因传感器长时间断开导致的数据缺失问题，现有方法常忽视模型不确定性或缺乏估计机制。

Method: 引入通用框架，量化并利用不确定性实现选择性插补，避免低可靠性插补。

Result: 实验证明，选择性插补减少了插补错误，并在24小时死亡率预测任务中取得性能提升。

Conclusion: 将不确定性纳入时间序列插补具有实际价值，能显著改善数据质量与下游任务表现。

Abstract: Time series data with missing values is common across many domains.
Healthcare presents special challenges due to prolonged periods of sensor
disconnection. In such cases, having a confidence measure for imputed values is
critical. Most existing methods either overlook model uncertainty or lack
mechanisms to estimate it. To address this gap, we introduce a general
framework that quantifies and leverages uncertainty for selective imputation.
By focusing on values the model is most confident in, highly unreliable
imputations are avoided. Our experiments on multiple EHR datasets, covering
diverse types of missingness, demonstrate that selectively imputing
less-uncertain values not only reduces imputation errors but also improves
downstream tasks. Specifically, we show performance gains in a 24-hour
mortality prediction task, underscoring the practical benefit of incorporating
uncertainty into time series imputation.

</details>


### [124] [Meta-autoencoders: An approach to discovery and representation of relationships between dynamically evolving classes](https://arxiv.org/abs/2507.09362)
*Assaf Marron,Smadar Szekely,Irun Cohen,David Harel*

Key words: 元自动编码器, 自动编码器, 自监督学习, 机器学习, 生物学

TL;DR: 论文介绍了“元自动编码器”（MAE）的概念，它是一种针对多个自动编码器的自动编码器，通过学习紧凑表示来捕捉多个类别的共同特征，应用于动态进化的物种研究等领域。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究MAE的动机在于为多个参数化类别的自动编码器提供统一的紧凑表示，特别适用于自然进化研究中捕捉物种间的共同和区分特征。

Method: 通过构造性的定义和初始示例，提出了一种神经网络的训练方法，能够为一系列类别特定的自动编码器进行编码和解码。

Result: 初步结果表明，MAE能够有效捕捉多个类别间的共同特征，为机器学习和生物学研究提供了新方向。

Conclusion: MAE为动态进化的多类别数据提供了一种统一的紧凑表示方法，具有广泛的应用潜力。

Abstract: An autoencoder (AE) is a neural network that, using self-supervised training,
learns a succinct parameterized representation, and a corresponding encoding
and decoding process, for all instances in a given class. Here, we introduce
the concept of a meta-autoencoder (MAE): an AE for a collection of
autoencoders. Given a family of classes that differ from each other by the
values of some parameters, and a trained AE for each class, an MAE for the
family is a neural net that has learned a compact representation and associated
encoder and decoder for the class-specific AEs. One application of this general
concept is in research and modeling of natural evolution -- capturing the
defining and the distinguishing properties across multiple species that are
dynamically evolving from each other and from common ancestors. In this interim
report we provide a constructive definition of MAEs, initial examples, and the
motivating research directions in machine learning and biology.

</details>


### [125] [Fair CCA for Fair Representation Learning: An ADNI Study](https://arxiv.org/abs/2507.09382)
*Bojian Hou,Zhanliang Wang,Zhuoping Zhou,Boning Tong,Zexuan Wang,Jingxuan Bao,Duy Duong-Tran,Qi Long,Li Shen*

Key words: 公平CCA，表示学习，公平性，分类任务，神经影像

TL;DR: 提出了一种新的公平CCA方法，用于公平表示学习，确保投影特征与敏感属性无关，从而在不影响准确性的情况下提高公平性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着公平性在机器学习中变得重要，现有公平CCA方法往往忽略对下游分类任务的影响，限制了其适用性，因此需要一种新方法。

Method: 采用公平CCA方法进行公平表示学习，确保投影特征与敏感属性无关。在合成数据和ADNI真实数据上进行验证。

Result: 方法在保持高相关性分析性能的同时，提高了分类任务的公平性。

Conclusion: 该方法适用于需要无偏分析的神经影像研究，实现了公平机器学习。

Abstract: Canonical correlation analysis (CCA) is a technique for finding correlations
between different data modalities and learning low-dimensional representations.
As fairness becomes crucial in machine learning, fair CCA has gained attention.
However, previous approaches often overlook the impact on downstream
classification tasks, limiting applicability. We propose a novel fair CCA
method for fair representation learning, ensuring the projected features are
independent of sensitive attributes, thus enhancing fairness without
compromising accuracy. We validate our method on synthetic data and real-world
data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating
its ability to maintain high correlation analysis performance while improving
fairness in classification tasks. Our work enables fair machine learning in
neuroimaging studies where unbiased analysis is essential.

</details>


### [126] [Geometric Generative Modeling with Noise-Conditioned Graph Networks](https://arxiv.org/abs/2507.09391)
*Peter Pao-Huang,Mitchell Black,Xiaojie Qiu*

Key words: 噪声条件图网络、动态消息传递、生成式模型、图神经网络

TL;DR: 提出了一种噪声条件图网络（NCGNs），通过动态调整架构以适应噪声水平，提升了生成式图模型的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有图神经网络在噪声水平变化时架构固定，限制了模型的表达力，需改进。

Method: 设计NCGNs，结合动态消息传递（DMP），根据噪声水平调整消息传递的范围和分辨率。

Result: 在3D点云、时空转录组学和图像等任务中表现优于噪声无关架构。

Conclusion: NCGNs通过动态适应噪声水平，显著提升了生成式图模型的性能。

Abstract: Generative modeling of graphs with spatial structure is essential across many
applications from computer graphics to spatial genomics. Recent flow-based
generative models have achieved impressive results by gradually adding and then
learning to remove noise from these graphs. Existing models, however, use graph
neural network architectures that are independent of the noise level, limiting
their expressiveness. To address this issue, we introduce
\textit{Noise-Conditioned Graph Networks} (NCGNs), a class of graph neural
networks that dynamically modify their architecture according to the noise
level during generation. Our theoretical and empirical analysis reveals that as
noise increases, (1) graphs require information from increasingly distant
neighbors and (2) graphs can be effectively represented at lower resolutions.
Based on these insights, we develop Dynamic Message Passing (DMP), a specific
instantiation of NCGNs that adapts both the range and resolution of message
passing to the noise level. DMP consistently outperforms noise-independent
architectures on a variety of domains including $3$D point clouds,
spatiotemporal transcriptomics, and images. Code is available at
https://github.com/peterpaohuang/ncgn.

</details>


### [127] [A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention](https://arxiv.org/abs/2507.09394)
*Nandan Kumar Jha,Brandon Reagen*

Key words: 多头潜在注意力、Transformer、旋转嵌入、频谱分析、容量瓶颈

TL;DR: 本文研究了多头潜在注意力（MLA）在预训练过程中对Transformer内部容量的影响，发现旋转嵌入的应用方式对防止容量瓶颈和保持表达能力至关重要。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨多头潜在注意力（MLA）如何影响Transformer预训练时的内部容量，特别是通过旋转嵌入的不同应用方式对其频谱特性的影响。

Method: 使用Marchenko-Pastur（MP）诊断工具分析$W_{Q}W_{K}^\top$格拉姆矩阵的频谱，比较标准多头注意力（MHA）、MLA-PreRoPE和MLA-Decoupled三种变体。

Result: 发现容量瓶颈在特定层中局部出现，导致表达性集中在狭窄子空间；仅MLA-Decoupled变体能防止这种问题，保持广泛的频谱支持。

Conclusion: 旋转嵌入的应用方式与压缩位置同样重要，共享旋转组件可以减轻频谱碎片化并保持表达能力。

Abstract: In this work, we study how multi-head latent attention (MLA), a popular
strategy for compressing key/value memory, affects a transformer's internal
capacity during pretraining. Using a lightweight suite of Marchenko-Pastur (MP)
diagnostics, we analyze the spectrum of the $W_{Q}W_{K}^\top$ gram matrix
throughout training, comparing three variants: the standard multi-head
attention (MHA) baseline, MLA-PreRoPE with rotary applied before compression,
and MLA-Decoupled, which shares a single rotary sub-vector across all heads.
Our random matrix analysis reveals \textbf{three key findings:} \textbf{ i)}
capacity bottlenecks emerge locally: both MHA and MLA-PreRoPE exhibit sharp,
early spikes in specific layers that persist and propagate, disrupting the
balance between bulk and outlier directions; \textbf{ ii)} these spikes
coincide with rank collapse, concentrating the model's expressivity into narrow
subspaces; \textbf{ iii)} only the decoupled variant prevents this cascade,
maintaining broad spectral support and suppressing outlier formation across
layers. These results underscore that \emph{how} rotary embeddings are applied
is just as critical as \emph{where} compression occurs. Sharing rotary
components across heads mitigates spectral fragmentation and preserves
representational capacity.

</details>


### [128] [Scaling Laws for Optimal Data Mixtures](https://arxiv.org/abs/2507.09404)
*Mustafa Shukor,Louis Bethune,Dan Busbridge,David Grangier,Enrico Fini,Alaaeldin El-Nouby,Pierre Ablin*

Key words: 扩展法则, 数据混合, 大规模预训练, 最优权重, 语言模型, 多模态模型, 视觉模型

TL;DR: 该论文提出了一种系统性方法，利用扩展法则确定目标领域的最佳数据混合比例，避免了传统试错法的成本和复杂性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的数据混合比例选择方法依赖试错，但大规模预训练中这种方法不切实际。因此，作者希望通过扩展法则提供一种更高效、可预测的方法。

Method: 通过扩展法则预测模型在不同数据混合比例下的损失，并在大型语言模型、多模态模型和视觉模型的预训练中验证其普适性。小规模训练即可估算参数，用于预测更大规模和未见过的数据混合比例下的性能。

Result: 扩展法则在不同领域和规模的模型预训练中表现出强大的预测能力，能够准确估算最佳数据混合比例，为资源分配提供了理论依据。

Conclusion: 该方法为大规模预训练中的数据混合比例选择提供了理论支持，显著优于传统试错法。

Abstract: Large foundation models are typically trained on data from multiple domains,
with the data mixture--the proportion of each domain used--playing a critical
role in model performance. The standard approach to selecting this mixture
relies on trial and error, which becomes impractical for large-scale
pretraining. We propose a systematic method to determine the optimal data
mixture for any target domain using scaling laws. Our approach accurately
predicts the loss of a model of size $N$ trained with $D$ tokens and a specific
domain weight vector $h$. We validate the universality of these scaling laws by
demonstrating their predictive power in three distinct and large-scale
settings: large language model (LLM), native multimodal model (NMM), and large
vision models (LVM) pretraining. We further show that these scaling laws can
extrapolate to new data mixtures and across scales: their parameters can be
accurately estimated using a few small-scale training runs, and used to
estimate the performance at larger scales and unseen domain weights. The
scaling laws allow to derive the optimal domain weights for any target domain
under a given training budget ($N$,$D$), providing a principled alternative to
costly trial-and-error methods.

</details>


### [129] [Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers](https://arxiv.org/abs/2507.09406)
*Santhosh Kumar Ravindran*

Key words: 大型语言模型, 对抗性修补, 欺骗行为, AI安全, 机制可解释性

TL;DR: 本文提出了一种名为对抗性激活修补的新机制解释框架，用于检测和减轻大型语言模型中的欺骗行为，通过实验验证了其效果，并提出了未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大型语言模型（LLMs）虽然通过人类反馈强化学习（RLHF）等技术对齐安全性，但仍存在隐蔽的欺骗行为，需要一种方法来检测和缓解这些问题。

Method: 通过对抗性激活修补技术，在特定层级补丁欺骗性提示的激活，模拟漏洞并量化欺骗率，结合玩具神经网络实验验证假设。

Result: 实验显示，对抗性修补将欺骗性输出从0%提升至23.9%，并揭示了层级特异性影响，支持了提出的六项假设。

Conclusion: 该研究为AI安全提供了新工具，强调了修补技术的双重用途潜力，并为大规模模型的实证研究提供了路线图。

Abstract: Large language models (LLMs) aligned for safety through techniques like
reinforcement learning from human feedback (RLHF) often exhibit emergent
deceptive behaviors, where outputs appear compliant but subtly mislead or omit
critical information. This paper introduces adversarial activation patching, a
novel mechanistic interpretability framework that leverages activation patching
as an adversarial tool to induce, detect, and mitigate such deception in
transformer-based models. By sourcing activations from "deceptive" prompts and
patching them into safe forward passes at specific layers, we simulate
vulnerabilities and quantify deception rates. Through toy neural network
simulations across multiple scenarios (e.g., 1000 trials per setup), we
demonstrate that adversarial patching increases deceptive outputs to 23.9% from
a 0% baseline, with layer-specific variations supporting our hypotheses. We
propose six hypotheses, including transferability across models, exacerbation
in multimodal settings, and scaling effects. An expanded literature review
synthesizes over 20 key works in interpretability, deception, and adversarial
attacks. Mitigation strategies, such as activation anomaly detection and robust
fine-tuning, are detailed, alongside ethical considerations and future research
directions. This work advances AI safety by highlighting patching's dual-use
potential and provides a roadmap for empirical studies on large-scale models.

</details>


### [130] [On Information Geometry and Iterative Optimization in Model Compression: Operator Factorization](https://arxiv.org/abs/2507.09428)
*Zakhar Shumaylov,Vasileios Tsiaras,Yannis Stylianou*

Key words: 深度学习, 模型压缩, 信息几何, 算子分解, 奇异值阈值

TL;DR: 该论文探讨了信息几何在深度模型压缩中的应用，分析了如何通过低计算子流形和投影实现有效压缩，并强调了信息散度在预训练模型压缩中的重要性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于深度学习模型参数量不断增加，需在资源受限设备上部署有效压缩技术，信息几何提供了一种新的分析视角。

Method: 采用信息几何方法分析模型压缩，重点关注算子分解，并提出了通过软秩约束和迭代奇异值阈值训练神经网络。

Result: 研究表明，信息散度在预训练模型压缩中至关重要，而在微调场景中，训练性更为重要。通过软秩改进现有方法，能在固定压缩率下提升性能。

Conclusion: 信息几何为模型压缩提供了新思路，软秩约束和迭代方法有助于在压缩与性能间取得更好平衡。

Abstract: The ever-increasing parameter counts of deep learning models necessitate
effective compression techniques for deployment on resource-constrained
devices. This paper explores the application of information geometry, the study
of density-induced metrics on parameter spaces, to analyze existing methods
within the space of model compression, primarily focusing on operator
factorization. Adopting this perspective highlights the core challenge:
defining an optimal low-compute submanifold (or subset) and projecting onto it.
We argue that many successful model compression approaches can be understood as
implicitly approximating information divergences for this projection. We
highlight that when compressing a pre-trained model, using information
divergences is paramount for achieving improved zero-shot accuracy, yet this
may no longer be the case when the model is fine-tuned. In such scenarios,
trainability of bottlenecked models turns out to be far more important for
achieving high compression ratios with minimal performance degradation,
necessitating adoption of iterative methods. In this context, we prove
convergence of iterative singular value thresholding for training neural
networks subject to a soft rank constraint. To further illustrate the utility
of this perspective, we showcase how simple modifications to existing methods
through softer rank reduction result in improved performance under fixed
compression rates.

</details>


### [131] [Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series](https://arxiv.org/abs/2507.09439)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Key words: 多变量时间序列、因果发现、动态稀疏注意力、扩张卷积、可解释性

TL;DR: 提出了DyCAST-Net模型，用于多变量时间序列中的因果发现，通过动态稀疏注意力和扩张卷积提升精度和可解释性，在金融和营销数据集上表现优于现有模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多变量时间序列中的因果关系理解对金融和营销等领域的决策至关重要，但复杂依赖和滞后效应使传统方法难以应对。

Method: 结合扩张时间卷积和动态稀疏注意力机制，通过自适应阈值策略消除虚假连接，并利用统计shuffle测试增强稳健性。

Result: 在金融和营销数据上优于TCDF、GCFormer和CausalFormer，准确估计因果延迟，减少虚假发现，并提供可解释的注意力热图。

Conclusion: DyCAST-Net在高维动态环境中能有效检测潜在中介和滞后因素，架构设计和稳定性增强使其具有广泛适用性。

Abstract: Understanding causal relationships in multivariate time series (MTS) is
essential for effective decision-making in fields such as finance and
marketing, where complex dependencies and lagged effects challenge conventional
analytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal
Networks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel
architecture designed to enhance causal discovery by integrating dilated
temporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net
effectively captures multiscale temporal dependencies through dilated
convolutions while leveraging an adaptive thresholding strategy in its
attention mechanism to eliminate spurious connections, ensuring both accuracy
and interpretability. A statistical shuffle test validation further strengthens
robustness by filtering false positives and improving causal inference
reliability. Extensive evaluations on financial and marketing datasets
demonstrate that DyCAST-Net consistently outperforms existing models such as
TCDF, GCFormer, and CausalFormer. The model provides a more precise estimation
of causal delays and significantly reduces false discoveries, particularly in
noisy environments. Moreover, attention heatmaps offer interpretable insights,
uncovering hidden causal patterns such as the mediated effects of advertising
on consumer behavior and the influence of macroeconomic indicators on financial
markets. Case studies illustrate DyCAST-Net's ability to detect latent
mediators and lagged causal factors, making it particularly effective in
high-dimensional, dynamic settings. The model's architecture enhanced by
RMSNorm stabilization and causal masking ensures scalability and adaptability
across diverse application domains

</details>


### [132] [Transformers Don't In-Context Learn Least Squares Regression](https://arxiv.org/abs/2507.09440)
*Joshua Hill,Benjamin Eyre,Elliot Creager*

Key words: in-context learning, transformers, out-of-distribution generalization, pretraining corpus, spectral analysis

TL;DR: 论文研究了大型预训练变换器中的上下文学习（ICL）机制，发现其泛化能力受提示分布影响，且预训练语料对ICL行为有重要影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索上下文学习（ICL）在大型预训练变换器中的工作机制，尤其是在分布变化时的表现，以揭示其学习原理。

Method: 通过合成线性回归任务和分布外泛化实验，分析变换器的学习行为及其与预训练语料的关系。

Result: 变换器在分布外提示下泛化能力差，且预训练语料通过残差流中的谱特性影响ICL行为。

Conclusion: ICL的表现受预训练语料分布影响，其工作机制并非简单实现传统学习算法。

Abstract: In-context learning (ICL) has emerged as a powerful capability of large
pretrained transformers, enabling them to solve new tasks implicit in example
input-output pairs without any gradient updates. Despite its practical success,
the mechanisms underlying ICL remain largely mysterious. In this work we study
synthetic linear regression to probe how transformers implement learning at
inference time. Previous works have demonstrated that transformers match the
performance of learning rules such as Ordinary Least Squares (OLS) regression
or gradient descent and have suggested ICL is facilitated in transformers
through the learned implementation of one of these techniques. In this work, we
demonstrate through a suite of out-of-distribution generalization experiments
that transformers trained for ICL fail to generalize after shifts in the prompt
distribution, a behaviour that is inconsistent with the notion of transformers
implementing algorithms such as OLS. Finally, we highlight the role of the
pretraining corpus in shaping ICL behaviour through a spectral analysis of the
learned representations in the residual stream. Inputs from the same
distribution as the training data produce representations with a unique
spectral signature: inputs from this distribution tend to have the same top two
singular vectors. This spectral signature is not shared by out-of-distribution
inputs, and a metric characterizing the presence of this signature is highly
correlated with low loss.

</details>


### [133] [Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components](https://arxiv.org/abs/2507.09443)
*Luiz Aldeia Machado,Victor Coppo Leite,Elia Merzari,Arthur Motta,Roberto Ponciroli,Lander Ibarra,Lise Charlot*

Key words: 预测性维护, 卷积神经网络, 核电站, 热机械模型, 燃料棒

TL;DR: 论文探讨了结合卷积神经网络（CNN）和热机械模型预测核电站燃料棒的温度、应力和应变，以支持预测性维护策略。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 减少核电站因部件故障导致的意外停机时间，提升运行效率。

Method: 使用CNN架构结合热机械模型，基于燃料棒外壳有限温度测量数据，通过BISON和MOOSE-THM仿真生成训练、验证和测试数据集。

Result: CNN经过1000次训练未出现过拟合，能高精度预测温度分布，并用于热机械模型计算应力和应变分布。

Conclusion: 该方法有望用于核反应堆实时监测，支持预测性维护工具开发。

Abstract: Proactive maintenance strategies, such as Predictive Maintenance (PdM), play
an important role in the operation of Nuclear Power Plants (NPPs), particularly
due to their capacity to reduce offline time by preventing unexpected shutdowns
caused by component failures.
  In this work, we explore the use of a Convolutional Neural Network (CNN)
architecture combined with a computational thermomechanical model to calculate
the temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel
rod during operation. This estimation relies on a limited number of temperature
measurements from the cladding's outer surface. This methodology can
potentially aid in developing PdM tools for nuclear reactors by enabling
real-time monitoring of such systems.
  The training, validation, and testing datasets were generated through coupled
simulations involving BISON, a finite element-based nuclear fuel performance
code, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven
simulations, varying the peak linear heat generation rates. Of these, eight
were used for training, two for validation, and one for testing.
  The CNN was trained for over 1,000 epochs without signs of overfitting,
achieving highly accurate temperature distribution predictions. These were then
used in a thermomechanical model to determine the stress and strain
distribution within the fuel rod.

</details>


### [134] [Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting](https://arxiv.org/abs/2507.09445)
*Runze Yang,Longbing Cao,Xin You,Kun Fang,Jianxun Li,Jie Yang*

Key words: 傅里叶变换, 时间序列预测, 深度学习, 时间-频率特征, 傅里叶基映射

TL;DR: 论文提出了基于傅里叶变换与深度学习的时间序列预测方法，通过傅里叶基映射（FBM）解决了现有方法中的频率解释不精确和时间信息忽视问题，并结合多种神经网络模型提升性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有基于傅里叶变换的时间序列预测方法存在频率解释不精确和忽视时间信息的问题，需要一种新方法整合时间-频率特征。

Method: 提出傅里叶基映射（FBM）方法，通过傅里叶基展开和时间-频率空间映射整合特征，并设计了FBM-L、FBM-NL、FBM-NP和FBM-S等模型。

Result: 在多样化的真实数据集上验证了方法的有效性，在长短期预测任务中达到了SOTA性能。

Conclusion: FBM方法通过精确的频率特征提取和时间信息保留，显著提升了时间序列预测性能，且能灵活集成到多种神经网络中。

Abstract: The integration of Fourier transform and deep learning opens new avenues for
time series forecasting. We reconsider the Fourier transform from a basis
functions perspective. Specifically, the real and imaginary parts of the
frequency components can be regarded as the coefficients of cosine and sine
basis functions at tiered frequency levels, respectively. We find that existing
Fourier-based methods face inconsistent starting cycles and inconsistent series
length issues. They fail to interpret frequency components precisely and
overlook temporal information. Accordingly, the novel Fourier Basis Mapping
(FBM) method addresses these issues by integrating time-frequency features
through Fourier basis expansion and mapping in the time-frequency space. Our
approach extracts explicit frequency features while preserving temporal
characteristics. FBM supports plug-and-play integration with various types of
neural networks by only adjusting the first initial projection layer for better
performance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear,
MLP-based, and Transformer-based models, respectively, demonstrating the
effectiveness of time-frequency features. Next, we propose a synergetic model
architecture, termed FBM-S, which decomposes the seasonal, trend, and
interaction effects into three separate blocks, each designed to model
time-frequency features in a specialized manner. Finally, we introduce several
techniques tailored for time-frequency features, including interaction masking,
centralization, patching, rolling window projection, and multi-scale
down-sampling. The results are validated on diverse real-world datasets for
both long-term and short-term forecasting tasks with SOTA performance.

</details>


### [135] [Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores Estimated from Ambient Home Health Monitoring](https://arxiv.org/abs/2507.09460)
*Noah Marchal,William E. Janes,Mihail Popescu,Xing Song*

Key words: ALS, 功能衰退, 半监督学习, 迁移学习, 传感器监测

TL;DR: 论文通过半监督回归模型，结合传感器数据，比较不同学习方法和插值技术，以预测ALS患者功能衰退率，发现特定学习方法和功能域匹配可提高预测准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有临床监测可能遗漏ALS患者功能衰退的关键变化，需开发更连续、精准的预测方法。

Method: 比较三种模型范式（个体批量学习、群体批量学习、增量迁移学习）及三种插值技术（线性、三次多项式、自注意力插值），评估其对ALSFRS-R量表轨迹的预测效果。

Result: 迁移学习在多数子量表预测中优于其他方法（28/32），自注意力插值在子量表模型中表现最佳，线性插值在复合量表中更稳定。不同功能域存在异质性-同质性特征。

Conclusion: 匹配学习方法和功能域特征可提升预测准确性，自适应模型选择有望在多中心研究中实现及时干预。

Abstract: Clinical monitoring of functional decline in ALS relies on periodic
assessments that may miss critical changes occurring between visits. To address
this gap, semi-supervised regression models were developed to estimate rates of
decline in a case series cohort by targeting ALSFRS- R scale trajectories with
continuous in-home sensor monitoring data. Our analysis compared three model
paradigms (individual batch learning and cohort-level batch versus incremental
fine-tuned transfer learning) across linear slope, cubic polynomial, and
ensembled self-attention pseudo-label interpolations. Results revealed cohort
homogeneity across functional domains responding to learning methods, with
transfer learning improving prediction error for ALSFRS-R subscales in 28 of 32
contrasts (mean RMSE=0.20(0.04)), and individual batch learning for predicting
the composite scale (mean RMSE=3.15(1.25)) in 2 of 3. Self-attention
interpolation achieved the lowest prediction error for subscale-level models
(mean RMSE=0.19(0.06)), capturing complex nonlinear progression patterns,
outperforming linear and cubic interpolations in 20 of 32 contrasts, though
linear interpolation proved more stable in all ALSFRS-R composite scale models
(mean RMSE=0.23(0.10)). We identified distinct homogeneity-heterogeneity
profiles across functional domains with respiratory and speech exhibiting
patient-specific patterns benefiting from personalized incremental adaptation,
while swallowing and dressing functions followed cohort-level trajectories
suitable for transfer models. These findings suggest that matching learning and
pseudo-labeling techniques to functional domain-specific
homogeneity-heterogeneity profiles enhances predictive accuracy in ALS
progression tracking. Integrating adaptive model selection within sensor
monitoring platforms could enable timely interventions and scalable deployment
in future multi-center studies.

</details>


### [136] [La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching](https://arxiv.org/abs/2507.09466)
*Tomas Geffner,Kieran Didi,Zhonglin Cao,Danny Reidenbach,Zuobai Zhang,Christian Dallago,Emine Kucukbenli,Karsten Kreis,Arash Vahdat*

Key words: 全原子蛋白质设计、潜在表示、流匹配、氨基酸序列生成、原子基序支架

TL;DR: La-Proteina是一种基于部分潜在蛋白质表示的全原子蛋白质设计模型，通过固定维度的残基潜在变量捕获序列和原子细节，实现了高效的序列与全原子结构的联合生成。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的生成模型大多难以直接生成全原子结构与氨基酸序列，特别是需要处理侧链变化的问题。

Method: La-Proteina采用部分潜在表示，显式建模粗粒度主干结构，通过流匹配在潜在空间中建模序列与全原子结构的联合分布。

Result: La-Proteina在多种生成基准测试中表现优异，特别是在原子基序支架任务中，能够生成长达800残基的可设计蛋白质。

Conclusion: La-Proteina展示了强大的可扩展性和鲁棒性，解决了全原子结构设计中的挑战。

Abstract: Recently, many generative models for de novo protein structure design have
emerged. Yet, only few tackle the difficult task of directly generating fully
atomistic structures jointly with the underlying amino acid sequence. This is
challenging, for instance, because the model must reason over side chains that
change in length during generation. We introduce La-Proteina for atomistic
protein design based on a novel partially latent protein representation: coarse
backbone structure is modeled explicitly, while sequence and atomistic details
are captured via per-residue latent variables of fixed dimensionality, thereby
effectively side-stepping challenges of explicit side-chain representations.
Flow matching in this partially latent space then models the joint distribution
over sequences and full-atom structures. La-Proteina achieves state-of-the-art
performance on multiple generation benchmarks, including all-atom
co-designability, diversity, and structural validity, as confirmed through
detailed structural analyses and evaluations. Notably, La-Proteina also
surpasses previous models in atomistic motif scaffolding performance, unlocking
critical atomistic structure-conditioned protein design tasks. Moreover,
La-Proteina is able to generate co-designable proteins of up to 800 residues, a
regime where most baselines collapse and fail to produce valid samples,
demonstrating La-Proteina's scalability and robustness.

</details>


### [137] [Discrete Differential Principle for Continuous Smooth Function Representation](https://arxiv.org/abs/2507.09480)
*Guoyou Wang,Yihua Tan,Shiqi Liu*

Key words: 泰勒公式,离散微分算子,范德蒙德矩阵,误差传播,维度灾难

TL;DR: 本文提出了一种新的离散微分算子，通过范德蒙德系数矩阵估计导数并局部表示连续平滑函数，解决了泰勒公式的维度灾难和误差传播问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 泰勒公式在处理离散情况下的导数计算时存在维度灾难和误差传播的问题，限制了其在视觉感知、流体力学等领域的应用。

Method: 利用截断泰勒级数导出的范德蒙德系数矩阵，均匀采样点同时计算所有低阶导数，减少误差传播并提高精度。

Result: 数学证明了导数和函数表示的严格误差界限，二维扩展支持多元导数计算，实验显示优于有限差分法和插值方法。

Conclusion: 该方法在视觉表示、特征提取等领域具有广泛适用性。

Abstract: Taylor's formula holds significant importance in function representation,
such as solving differential difference equations, ordinary differential
equations, partial differential equations, and further promotes applications in
visual perception, complex control, fluid mechanics, weather forecasting and
thermodynamics. However, the Taylor's formula suffers from the curse of
dimensionality and error propagation during derivative computation in discrete
situations. In this paper, we propose a new discrete differential operator to
estimate derivatives and to represent continuous smooth function locally using
the Vandermonde coefficient matrix derived from truncated Taylor series. Our
method simultaneously computes all derivatives of orders less than the number
of sample points, inherently mitigating error propagation. Utilizing
equidistant uniform sampling, it achieves high-order accuracy while alleviating
the curse of dimensionality. We mathematically establish rigorous error bounds
for both derivative estimation and function representation, demonstrating
tighter bounds for lower-order derivatives. We extend our method to the
two-dimensional case, enabling its use for multivariate derivative
calculations. Experiments demonstrate the effectiveness and superiority of the
proposed method compared to the finite forward difference method for derivative
estimation and cubic spline and linear interpolation for function
representation. Consequently, our technique offers broad applicability across
domains such as vision representation, feature extraction, fluid mechanics, and
cross-media imaging.

</details>


### [138] [An Analysis of Action-Value Temporal-Difference Methods That Learn State Values](https://arxiv.org/abs/2507.09523)
*Brett Daley,Prabhat Nagarajan,Martha White,Marlos C. Machado*

Key words: TD学习, AV-learning, QV-learning, 收敛性, 样本效率, RDQ

TL;DR: 该论文分析了基于两种不对称价值函数的TD学习方法，发现AV-learning在控制问题上优于Q-learning，并提出了新的RDQ算法，效果优于Dueling DQN。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨在TD学习中，使用两种不对称价值函数是否比单一价值函数更优，以及这些方法的理论合理性。

Method: 分析QV-learning和AV-learning的收敛性和样本效率，并引入新的AV-learning算法RDQ。

Result: AV-learning在控制问题上表现优于Q-learning；RDQ算法在MinAtar基准测试中性能显著优于Dueling DQN。

Conclusion: 虽然两种方法在预测任务中均优于Expected Sarsa，但只有AV-learning在控制任务中表现突出。

Abstract: The hallmark feature of temporal-difference (TD) learning is bootstrapping:
using value predictions to generate new value predictions. The vast majority of
TD methods for control learn a policy by bootstrapping from a single
action-value function (e.g., Q-learning and Sarsa). Significantly less
attention has been given to methods that bootstrap from two asymmetric value
functions: i.e., methods that learn state values as an intermediate step in
learning action values. Existing algorithms in this vein can be categorized as
either QV-learning or AV-learning. Though these algorithms have been
investigated to some degree in prior work, it remains unclear if and when it is
advantageous to learn two value functions instead of just one -- and whether
such approaches are theoretically sound in general. In this paper, we analyze
these algorithmic families in terms of convergence and sample efficiency. We
find that while both families are more efficient than Expected Sarsa in the
prediction setting, only AV-learning methods offer any major benefit over
Q-learning in the control setting. Finally, we introduce a new AV-learning
algorithm called Regularized Dueling Q-learning (RDQ), which significantly
outperforms Dueling DQN in the MinAtar benchmark.

</details>


### [139] [Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events](https://arxiv.org/abs/2507.09545)
*Ilaria Vascotto,Valentina Blasone,Alex Rodriguez,Alessandro Bonaita,Luca Bortolussi*

Key words: XAI, 数据不平衡, 解释稳健性, 少数类, 霜冻事件

TL;DR: 该研究探讨了在数据不平衡情况下评估XAI方法解释的可靠性，提出了一种针对少数类的评估方法，并通过一个霜冻事件的案例进行分析。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于AI模型的广泛应用和近年来的立法要求，XAI方法的可靠性变得尤为重要。然而，解释的稳健性这一关键属性常被忽视，尤其是在数据不平衡的情况下。

Method: 研究提出了一种针对少数类的评估方法，包括基于流形的邻居生成、解释聚合和一致性测试指标。

Result: 研究通过一个基于数值特征的霜冻事件数据集展示了该方法的应用。

Conclusion: 该研究为XAI方法在高风险且数据不平衡的场景中的可靠性评估提供了初步见解。

Abstract: The usage of eXplainable Artificial Intelligence (XAI) methods has become
essential in practical applications, given the increasing deployment of
Artificial Intelligence (AI) models and the legislative requirements put
forward in the latest years. A fundamental but often underestimated aspect of
the explanations is their robustness, a key property that should be satisfied
in order to trust the explanations. In this study, we provide some preliminary
insights on evaluating the reliability of explanations in the specific case of
unbalanced datasets, which are very frequent in high-risk use-cases, but at the
same time considerably challenging for both AI models and XAI methods. We
propose a simple evaluation focused on the minority class (i.e. the less
frequent one) that leverages on-manifold generation of neighbours, explanation
aggregation and a metric to test explanation consistency. We present a use-case
based on a tabular dataset with numerical features focusing on the occurrence
of frost events.

</details>


### [140] [Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health Narratives](https://arxiv.org/abs/2507.09565)
*Heeba Shakeel,Tanvir Ahmad,Chandni Saxena*

Key words: 社交媒体,健康分类,机器学习,Transformer,数据集

TL;DR: 介绍了一个用于分类社交媒体用户帖子中六个关键维度（身体、情感、社交、智力、精神和职业）的数据集，并评估了传统和基于Transformer的模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 支持社交媒体中的个性化健康评估和早期心理健康干预。

Method: 开发了全面的注释框架，并评估了传统机器学习和Transformer模型。

Result: 使用精度、召回率和F1分数评估模型性能，并通过后解释确保透明性。

Conclusion: 数据集支持地区特定健康评估，促进心理健康干预，并公开符合伦理。

Abstract: We introduce a dataset for classifying wellness dimensions in social media
user posts, covering six key aspects: physical, emotional, social,
intellectual, spiritual, and vocational. The dataset is designed to capture
these dimensions in user-generated content, with a comprehensive annotation
framework developed under the guidance of domain experts. This framework allows
for the classification of text spans into the appropriate wellness categories.
We evaluate both traditional machine learning models and advanced
transformer-based models for this multi-class classification task, with
performance assessed using precision, recall, and F1-score, averaged over
10-fold cross-validation. Post-hoc explanations are applied to ensure the
transparency and interpretability of model decisions. The proposed dataset
contributes to region-specific wellness assessments in social media and paves
the way for personalized well-being evaluations and early intervention
strategies in mental health. We adhere to ethical considerations for
constructing and releasing our experiments and dataset publicly on Github.

</details>


### [141] [DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences](https://arxiv.org/abs/2507.09602)
*Bocheng Ju,Junchao Fan,Jiaqi Liu,Xiaolin Chang*

Key words: 联邦学习, 去学习, 隐私攻击, 梯度差异, 数据重构

TL;DR: 论文提出了DRAGD和DRAGDP两种攻击方法，利用联邦去学习过程中的梯度差异重构被删除的数据，揭示了隐私漏洞。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究联邦去学习中因梯度交换导致的数据隐私泄露问题。

Method: 通过分析去学习前后的梯度差异，设计了DRAGD攻击方法，并结合公开数据提出了增强版DRAGDP。

Result: 实验证明DRAGD和DRAGDP在数据重构上显著优于现有方法。

Conclusion: 研究揭示了联邦去学习的隐私漏洞，并提供了实际解决方案以增强安全性。

Abstract: Federated learning enables collaborative machine learning while preserving
data privacy. However, the rise of federated unlearning, designed to allow
clients to erase their data from the global model, introduces new privacy
concerns. Specifically, the gradient exchanges during the unlearning process
can leak sensitive information about deleted data. In this paper, we introduce
DRAGD, a novel attack that exploits gradient discrepancies before and after
unlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced
version of DRAGD that leverages publicly available prior data to improve
reconstruction accuracy, particularly for complex datasets like facial images.
Extensive experiments across multiple datasets demonstrate that DRAGD and
DRAGDP significantly outperform existing methods in data reconstruction.Our
work highlights a critical privacy vulnerability in federated unlearning and
offers a practical solution, advancing the security of federated unlearning
systems in real-world applications.

</details>


### [142] [MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression](https://arxiv.org/abs/2507.09616)
*Ofir Gordon,Ariel Lapid,Elad Cohen,Yarden Yagil,Arnon Netzer,Hai Victor Habi*

Key words: Transformer, 低秩近似, 混合精度量化, 边缘设备, 优化

TL;DR: MLoRQ是一种结合低秩近似和混合精度量化的新方法，通过两阶段优化过程确定每层的最佳比特宽度和秩分配，以满足内存约束，并在多种任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在资源受限的边缘设备上部署基于Transformer的神经网络是一个重大挑战，需通过低秩近似和混合精度量化等技术来解决。

Method: MLoRQ采用两阶段优化：内层优化选择最佳压缩方案，外层优化分配比特宽度和秩，并可选步骤缓解压缩误差。

Result: 在图像分类、目标检测和实例分割任务中，MLoRQ表现优异，性能提升高达15%。

Conclusion: MLoRQ是一种高效的方法，可与现有量化算法无缝集成，在边缘设备上部署Transformer网络时表现卓越。

Abstract: Deploying transformer-based neural networks on resource-constrained edge
devices presents a significant challenge. This challenge is often addressed
through various techniques, such as low-rank approximation and mixed-precision
quantization. In this work, we introduce Mixed Low-Rank and Quantization
(MLoRQ), a novel method that integrates both techniques. MLoRQ employs a
two-stage optimization process to determine optimal bit-width and rank
assignments for each layer, adhering to predefined memory constraints. This
process includes: (i) an intra-layer optimization that identifies potentially
optimal compression solutions out of all low-rank and quantization
combinations; (ii) an inter-layer optimization that assigns bit-width precision
and rank to each layer while ensuring the memory constraint is met. An optional
final step applies a sequential optimization process using a modified adaptive
rounding technique to mitigate compression-induced errors in joint low-rank
approximation and quantization. The method is compatible and can be seamlessly
integrated with most existing quantization algorithms. MLoRQ shows
state-of-the-art results with up to 15\% performance improvement, evaluated on
Vision Transformers for image classification, object detection, and instance
segmentation tasks.

</details>


### [143] [Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset](https://arxiv.org/abs/2507.09650)
*Lily Hong Zhang,Smitha Milli,Karen Jusko,Jonathan Smith,Brandon Amos,Wassim,Bouaziz,Manon Revel,Jack Kussman,Lisa Titus,Bhaktipriya Radharapu,Jane Yu,Vidya Sarma,Kris Rose,Maximilian Nickel*

Key words: 大型语言模型, 用户偏好, 多样性与对齐, 负相关采样, 多语言数据集

TL;DR: 研究表明人类偏好多样性远超主流语言模型，现有数据收集方法不足，需负相关采样改进，并开源大型多语言偏好数据集Community Alignment。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索如何让大型语言模型（LLMs）适应不同文化、政治等多维度的用户偏好冲突。

Method: 通过多国大规模人类研究比较LLMs响应多样性，提出负相关采样方法优化候选集，构建新数据集Community Alignment。

Result: 人类偏好多样性显著高于模型响应；负相关采样显著提升对齐方法性能；开源最大多语言多轮偏好数据集。

Conclusion: 负相关采样和多样化数据集（Community Alignment）能有效提升LLMs对全球多样化偏好的适应能力。

Abstract: How can large language models (LLMs) serve users with varying preferences
that may conflict across cultural, political, or other dimensions? To advance
this challenge, this paper establishes four key results. First, we demonstrate,
through a large-scale multilingual human study with representative samples from
five countries (N=15,000), that humans exhibit significantly more variation in
preferences than the responses of 21 state-of-the-art LLMs. Second, we show
that existing methods for preference dataset collection are insufficient for
learning the diversity of human preferences even along two of the most salient
dimensions of variability in global values, due to the underlying homogeneity
of candidate responses. Third, we argue that this motivates the need for
negatively-correlated sampling when generating candidate sets, and we show that
simple prompt-based techniques for doing so significantly enhance the
performance of alignment methods in learning heterogeneous preferences. Fourth,
based on this novel candidate sampling approach, we collect and open-source
Community Alignment, the largest and most representative multilingual and
multi-turn preference dataset to date, featuring almost 200,000 comparisons
from annotators spanning five countries. We hope that the Community Alignment
dataset will be a valuable resource for improving the effectiveness of LLMs for
a diverse global population.

</details>


### [144] [Conformal Prediction for Privacy-Preserving Machine Learning](https://arxiv.org/abs/2507.09678)
*Alexander David Balinsky,Dominik Krzeminski,Alexander Balinsky*

Key words: Conformal Prediction, 加密数据, 不确定性量化, 隐私保护

TL;DR: 研究将Conformal Prediction (CP)与加密数据监督学习结合的可行性，展示在加密域中CP方法的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 填补不确定性量化与隐私保护机器学习之间的鸿沟。

Method: 使用AES加密的MNIST数据集，测试基于$p$值和$e$值的CP方法。

Result: 加密数据模型测试准确率36.88%，$e$值CP覆盖率达60%以上。

Conclusion: CP在加密数据中具有潜力，但需权衡预测集紧凑性与可靠性。

Abstract: We investigate the integration of Conformal Prediction (CP) with supervised
learning on deterministically encrypted data, aiming to bridge the gap between
rigorous uncertainty quantification and privacy-preserving machine learning.
Using AES-encrypted variants of the MNIST dataset, we demonstrate that CP
methods remain effective even when applied directly in the encrypted domain,
owing to the preservation of data exchangeability under fixed-key encryption.
We test traditional $p$-value-based against $e$-value-based conformal
predictors. Our empirical evaluation reveals that models trained on
deterministically encrypted data retain the ability to extract meaningful
structure, achieving 36.88\% test accuracy -- significantly above random
guessing (9.56\%) observed with per-instance encryption. Moreover,
$e$-value-based CP achieves predictive set coverage of over 60\% with 4.3
loss-threshold calibration, correctly capturing the true label in 4888 out of
5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive
sets but with reduced coverage accuracy. These findings highlight both the
promise and limitations of CP in encrypted data settings and underscore
critical trade-offs between prediction set compactness and reliability. %Our
work sets a foundation for principled uncertainty quantification in secure,
privacy-aware learning systems.

</details>


### [145] [Networked Information Aggregation via Machine Learning](https://arxiv.org/abs/2507.09683)
*Michael Kearns,Aaron Roth,Emily Ryu*

Key words: 分布式学习，有向无环图，信息聚合，特征子集，线性假设

TL;DR: 这篇论文研究了在DAG（有向无环图）中的分布式学习问题，探讨了通过观察父节点的预测和局部特征是否能实现信息聚合，即某个节点是否能够学习到与直接访问所有特征时最佳模型性能相媲美的模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究动机在于理解在分布式学习中，如何通过DAG结构实现信息的高效聚合，尤其是在每个节点只能观察到部分特征的情况下。

Method: 方法包括在DAG中按拓扑顺序训练模型，每个节点利用自身观察到的特征和父节点的预测作为额外输入。通过理论分析和实验验证，探讨了DAG深度和信息聚合的关系。

Result: 结果表明，DAG的深度是实现信息聚合的关键因素。在足够深的DAG中，信息可以聚合，但在某些分布下（如线性和浅层DAG），聚合无法实现。

Conclusion: 结论指出，DAG的结构和深度对信息聚合至关重要，同时需要通过实验验证理论结果的普适性。

Abstract: We study a distributed learning problem in which learning agents are embedded
in a directed acyclic graph (DAG). There is a fixed and arbitrary distribution
over feature/label pairs, and each agent or vertex in the graph is able to
directly observe only a subset of the features -- potentially a different
subset for every agent. The agents learn sequentially in some order consistent
with a topological sort of the DAG, committing to a model mapping observations
to predictions of the real-valued label. Each agent observes the predictions of
their parents in the DAG, and trains their model using both the features of the
instance that they directly observe, and the predictions of their parents as
additional features. We ask when this process is sufficient to achieve
\emph{information aggregation}, in the sense that some agent in the DAG is able
to learn a model whose error is competitive with the best model that could have
been learned (in some hypothesis class) with direct access to \emph{all}
features, despite the fact that no single agent in the network has such access.
We give upper and lower bounds for this problem for both linear and general
hypothesis classes. Our results identify the \emph{depth} of the DAG as the key
parameter: information aggregation can occur over sufficiently long paths in
the DAG, assuming that all of the relevant features are well represented along
the path, and there are distributions over which information aggregation cannot
occur even in the linear case, and even in arbitrarily large DAGs that do not
have sufficient depth (such as a hub-and-spokes topology in which the spoke
vertices collectively see all the features). We complement our theoretical
results with a comprehensive set of experiments.

</details>


### [146] [Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness](https://arxiv.org/abs/2507.09687)
*Md Mushfiqur Rahaman,Elliot Chang,Tasmiah Haque,Srinjoy Das*

Key words: 文本分类, 边缘计算, 生成式分类器, 判别式分类器, PTQ, Brevitas, LSTM

TL;DR: 本文比较了生成式和判别式LSTM文本分类模型在后训练量化（PTQ）下的表现，发现生成式模型对位宽、校准数据和输入噪声更敏感，且类别不平衡的校准数据会降低其性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 边缘计算中文本分类需要低延迟和高准确性，生成式分类器对噪声和分布外数据具有鲁棒性，但部署时面临计算和内存限制。PTQ是一种无需重新训练的模型压缩方法，适合边缘部署。

Method: 使用Brevitas量化库对生成式和判别式LSTM模型进行PTQ，评估不同位宽和噪声输入条件下的性能，并研究校准数据的类别不平衡对权重调整和激活分布的影响。

Result: 判别式分类器更鲁棒，而生成式分类器对位宽、校准数据和输入噪声更敏感。类别不平衡的校准数据会导致生成式分类器在低位宽下性能下降。

Conclusion: 校准数据的质量对PTQ至关重要，生成式分类器在噪声下的表现取决于校准数据的分布，这对边缘部署具有指导意义。

Abstract: Text classification plays a pivotal role in edge computing applications like
industrial monitoring, health diagnostics, and smart assistants, where low
latency and high accuracy are both key requirements. Generative classifiers, in
particular, have been shown to exhibit robustness to out-of-distribution and
noisy data, which is an extremely critical consideration for deployment in such
real-time edge environments. However, deploying such models on edge devices
faces computational and memory constraints. Post Training Quantization (PTQ)
reduces model size and compute costs without retraining, making it ideal for
edge deployment. In this work, we present a comprehensive comparative study of
generative and discriminative Long Short Term Memory (LSTM)-based text
classification models with PTQ using the Brevitas quantization library. We
evaluate both types of classifier models across multiple bitwidths and assess
their robustness under regular and noisy input conditions. We find that while
discriminative classifiers remain robust, generative ones are more sensitive to
bitwidth, calibration data used during PTQ, and input noise during quantized
inference. We study the influence of class imbalance in calibration data for
both types of classifiers, comparing scenarios with evenly and unevenly
distributed class samples including their effect on weight adjustments and
activation profiles during PTQ. Using test statistics derived from
nonparametric hypothesis testing, we identify that using class imbalanced data
during calibration introduces insufficient weight adaptation at lower bitwidths
for generative LSTM classifiers, thereby leading to degraded performance. This
study underscores the role of calibration data in PTQ and when generative
classifiers succeed or fail under noise, aiding deployment in edge
environments.

</details>


### [147] [Frequency-aware Surrogate Modeling With SMT Kernels For Advanced Data Forecasting](https://arxiv.org/abs/2507.09694)
*Nicolas Gonel,Paul Saves,Joseph Morlier*

Key words: 开源框架, 核函数, 替代建模, 频率感知, SMT 2.0

TL;DR: 该论文介绍了一个开源框架，专注于用户自定义和组合核函数，用于替代建模，扩展了传统核函数范围，并整合到SMT 2.0中。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 通过改进基于核的建模技术，捕捉复杂机械行为和时频动态，提升飞机系统建模能力。

Method: 引入频率感知元素，扩展核函数类型（如指数平方正弦核、有理二次核及其导数），并在实际案例中验证。

Result: 框架成功应用于预测Mauna-Loa CO₂浓度和航空客流量，整合到SMT 2.0中，提供灵活建模工具。

Conclusion: 该框架为工程师和研究人员提供了一个多功能工具，适用于频率敏感领域的复杂建模。

Abstract: This paper introduces a comprehensive open-source framework for developing
correlation kernels, with a particular focus on user-defined and composition of
kernels for surrogate modeling. By advancing kernel-based modeling techniques,
we incorporate frequency-aware elements that effectively capture complex
mechanical behaviors and timefrequency dynamics intrinsic to aircraft systems.
Traditional kernel functions, often limited to exponential-based methods, are
extended to include a wider range of kernels such as exponential squared sine
and rational quadratic kernels, along with their respective firstand
second-order derivatives. The proposed methodologies are first validated on a
sinus cardinal test case and then applied to forecasting Mauna-Loa Carbon
Dioxide (CO 2 ) concentrations and airline passenger traffic. All these
advancements are integrated into the open-source Surrogate Modeling Toolbox
(SMT 2.0), providing a versatile platform for both standard and customizable
kernel configurations. Furthermore, the framework enables the combination of
various kernels to leverage their unique strengths into composite models
tailored to specific problems. The resulting framework offers a flexible
toolset for engineers and researchers, paving the way for numerous future
applications in metamodeling for complex, frequency-sensitive domains.

</details>


### [148] [EPT-2 Technical Report](https://arxiv.org/abs/2507.09703)
*Roberto Molinaro,Niall Siegenheim,Niels Poulsen,Jordan Dane Daubinet,Henry Martin,Mark Frey,Kevin Thiart,Alexander Jakob Dautel,Andreas Schlueter,Alex Grigoryev,Bogdan Danciu,Nikoo Ekhtiari,Bas Steunebrink,Leonie Wagner,Marvin Vincent Gabler*

Key words: 地球物理模型, AI天气预报, Transformer, 概率预测

TL;DR: EPT-2是Earth Physics Transformer家族的最新AI模型，显著提升了对地球系统能量相关变量的预测性能，并在计算成本更低的情况下超越了主流天气预报模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 提升地球系统预测的准确性和效率，超越现有的AI和传统天气预报模型。

Method: 采用Transformer架构，并引入扰动集成模型EPT-2e进行概率预测。

Result: EPT-2在0-240h的预测范围内表现优于Microsoft Aurora和ECMWF IFS HRES，EPT-2e超越了ECMWF ENS。

Conclusion: EPT-2及其集成模型EPT-2e在地球系统预测中设立了新的标杆，且计算效率更高。

Abstract: We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT)
family of foundation AI models for Earth system forecasting. EPT-2 delivers
substantial improvements over its predecessor, EPT-1.5, and sets a new state of
the art in predicting energy-relevant variables-including 10m and 100m wind
speed, 2m temperature, and surface solar radiation-across the full 0-240h
forecast horizon. It consistently outperforms leading AI weather models such as
Microsoft Aurora, as well as the operational numerical forecast system IFS HRES
from the European Centre for Medium-Range Weather Forecasts (ECMWF). In
parallel, we introduce a perturbation-based ensemble model of EPT-2 for
probabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly
surpasses the ECMWF ENS mean-long considered the gold standard for medium- to
longrange forecasting-while operating at a fraction of the computational cost.
EPT models, as well as third-party forecasts, are accessible via the app.jua.ai
platform.

</details>


### [149] [Continental scale habitat modelling with artificial intelligence and multimodal earth observation](https://arxiv.org/abs/2507.09732)
*Sara Si-Moussi,Stephan Hennekens,Sander Mucher,Stan Los,Wilfried Thuiller*

Key words: 生境分类, 遥感数据, 人工智能, 多源数据集成, 机器学习

TL;DR: 该研究探讨了如何利用高分辨率遥感数据和AI工具提高大范围精细生境分类的准确性，提出了分层分类、多源数据集成和机器学习等方法，取得了显著的效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 面对人类活动对生境的日益加剧的压力，高分辨率生境地图对保护和恢复至关重要。现有地图在主题或空间分辨率上表现不足，因此需要改进方法。

Method: 利用欧洲植被档案的植被样地数据，结合高分辨率遥感数据和AI工具，采用分层分类、多光谱与雷达影像集成以及集成机器学习等方法进行分类。

Result: 分层分类解决了分类模糊问题，多源数据集成提升了分类性能，集成机器学习进一步提高了准确性。方法框架具有普适性和可扩展性。

Conclusion: 该研究为生境分类提供了创新方法，未来可扩展至动态生境建模、生境分割和质量评估，并利用新一代数据和更高质量实地观测。

Abstract: Habitats integrate the abiotic conditions and biophysical structures that
support biodiversity and sustain nature's contributions to people. As these
ecosystems face mounting pressure from human activities, accurate,
high-resolution habitat maps are essential for effective conservation and
restoration. Yet current maps often fall short in thematic or spatial
resolution because they must (1) model several mutually exclusive habitat types
that co-occur across landscapes and (2) cope with severe class imbalance that
complicate multi-class training. Here, we evaluated how high-resolution remote
sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat
classification over large geographic extents at fine thematic resolution. Using
vegetation plots from the European Vegetation Archive, we modelled Level 3
EUNIS habitats across Europe and assessed multiple modelling strategies against
independent validation datasets. Strategies that exploited the hierarchical
nature of habitat nomenclatures resolved classification ambiguities, especially
in fragmented landscapes. Integrating multi-spectral (MSI) and synthetic
aperture radar (SAR) imagery, particularly through Earth Observation Foundation
models, enhanced within-formation discrimination and overall performance.
Finally, ensemble machine learning that corrects class imbalance boosted
accuracy further. Our methodological framework is transferable beyond Europe
and adaptable to other classification systems. Future research should advance
temporal modelling of dynamic habitats, extend to habitat segmentation and
quality assessment, and exploit next-generation EO data paired with
higher-quality in-situ observations.

</details>


### [150] [Universal Physics Simulation: A Foundational Diffusion Approach](https://arxiv.org/abs/2507.09733)
*Bradley Camburn*

Key words: AI模型,物理仿真,扩散变换器,边界条件,物理发现

TL;DR: 首次提出无需预编码物理方程的通用物理仿真AI基础模型，通过边界条件数据直接学习物理定律。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统方法如PINNs和有限差分法需明确数学方程，限制了其通用性和发现潜力。

Method: 采用素描引导的扩散变换器，将仿真视为条件生成问题，利用空间边界条件合成稳态解。

Result: 模型直接映射边界至稳态，通用性强，稳态解SSIM>0.8，边界精度达亚像素级。

Conclusion: 该研究实现了从AI加速物理到AI发现物理的范式转变，建立了首个通用物理仿真框架。

Abstract: We present the first foundational AI model for universal physics simulation
that learns physical laws directly from boundary-condition data without
requiring a priori equation encoding. Traditional physics-informed neural
networks (PINNs) and finite-difference methods necessitate explicit
mathematical formulation of governing equations, fundamentally limiting their
generalizability and discovery potential. Our sketch-guided diffusion
transformer approach reimagines computational physics by treating simulation as
a conditional generation problem, where spatial boundary conditions guide the
synthesis of physically accurate steady-state solutions.
  By leveraging enhanced diffusion transformer architectures with novel spatial
relationship encoding, our model achieves direct boundary-to-equilibrium
mapping and is generalizable to diverse physics domains. Unlike sequential
time-stepping methods that accumulate errors over iterations, our approach
bypasses temporal integration entirely, directly generating steady-state
solutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our
data-informed approach enables physics discovery through learned
representations analyzable via Layer-wise Relevance Propagation (LRP),
revealing emergent physical relationships without predetermined mathematical
constraints. This work represents a paradigm shift from AI-accelerated physics
to AI-discovered physics, establishing the first truly universal physics
simulation framework.

</details>


### [151] [Do we need equivariant models for molecule generation?](https://arxiv.org/abs/2507.09753)
*Ewa M. Nowara,Joshua Rackers,Patricia Suriana,Pan Kessel,Max Shen,Andrew Martin Watkins,Michael Maser*

Key words: 生成模型, 等变性, 卷积神经网络, 分子生成, 噪声消除

TL;DR: 研究探讨非等变卷积神经网络（CNN）通过旋转增强是否能学会等变性，并与等变图神经网络（GNN）性能相当。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前等变GNN模型复杂、训练困难且扩展性差，提出非等变CNN是否也能达到类似性能的问题。

Method: 通过损失分解分离预测误差与等变误差，评估模型大小、数据集规模及训练时长对去噪、分子生成和性质预测的影响。

Result: 研究发现非等变CNN通过旋转增强可以学习等变性，性能与等变模型相当。

Conclusion: 非等变CNN在生成任务中可以通过训练学习等变性，为分子发现提供更简单的替代方案。

Abstract: Deep generative models are increasingly used for molecular discovery, with
most recent approaches relying on equivariant graph neural networks (GNNs)
under the assumption that explicit equivariance is essential for generating
high-quality 3D molecules. However, these models are complex, difficult to
train, and scale poorly.
  We investigate whether non-equivariant convolutional neural networks (CNNs)
trained with rotation augmentations can learn equivariance and match the
performance of equivariant models. We derive a loss decomposition that
separates prediction error from equivariance error, and evaluate how model
size, dataset size, and training duration affect performance across denoising,
molecule generation, and property prediction. To our knowledge, this is the
first study to analyze learned equivariance in generative tasks.

</details>


### [152] [Explainable AI in Genomics: Transcription Factor Binding Site Prediction with Mixture of Experts](https://arxiv.org/abs/2507.09754)
*Aakash Tripathi,Ian E. Nielsen,Muhammad Umer,Ravi P. Ramachandran,Ghulam Rasool*

Key words: 转录因子结合位点（TFBS）, 混合专家（MoE）, 卷积神经网络（CNN）, 解释性分析, 基因组生物学

TL;DR: 该研究提出了一种新的混合专家（MoE）方法，用于转录因子结合位点（TFBS）预测，结合了多个预训练的CNN模型，并在不同数据集上表现出优越性能，尤其在OOD场景下。同时，提出了新的ShiftSmooth解释性技术。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 转录因子结合位点（TFBS）预测对于理解基因调控和多种生物过程至关重要，但现有方法在泛化性和解释性方面存在不足。

Method: 采用了混合专家（MoE）方法，整合多个预训练的CNN模型，并引入ShiftSmooth技术以提高模型解释性。

Result: MoE模型在不同TF结合位点上表现出竞争性或优越性能，尤其在OOD场景下，且ShiftSmooth提供了更鲁棒的解释性分析。

Conclusion: 该方法为TFBS预测提供了一种高效、可泛化且可解释的解决方案，有助于基因组生物学的新发现和转录调控的理解。

Abstract: Transcription Factor Binding Site (TFBS) prediction is crucial for
understanding gene regulation and various biological processes. This study
introduces a novel Mixture of Experts (MoE) approach for TFBS prediction,
integrating multiple pre-trained Convolutional Neural Network (CNN) models,
each specializing in different TFBS patterns. We evaluate the performance of
our MoE model against individual expert models on both in-distribution and
out-of-distribution (OOD) datasets, using six randomly selected transcription
factors (TFs) for OOD testing. Our results demonstrate that the MoE model
achieves competitive or superior performance across diverse TF binding sites,
particularly excelling in OOD scenarios. The Analysis of Variance (ANOVA)
statistical test confirms the significance of these performance differences.
Additionally, we introduce ShiftSmooth, a novel attribution mapping technique
that provides more robust model interpretability by considering small shifts in
input sequences. Through comprehensive explainability analysis, we show that
ShiftSmooth offers superior attribution for motif discovery and localization
compared to traditional Vanilla Gradient methods. Our work presents an
efficient, generalizable, and interpretable solution for TFBS prediction,
potentially enabling new discoveries in genome biology and advancing our
understanding of transcriptional regulation.

</details>


### [153] [Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights](https://arxiv.org/abs/2507.09766)
*Mohamadreza Akbari Pour,Ali Ghasemzadeh,MohamadAli Bijarchi,Mohammad Behshad Shafii*

Key words: 剩余使用寿命, 健康状态, 图神经网络, 注意力机制, 强化学习

TL;DR: 提出了一种结合物理监督与时空学习的RGPD框架，用于剩余使用寿命（RUL）和健康状态（SOH）的精确估计。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在工业应用中准确预测RUL和SOH对设备健康管理至关重要，需要一种能够动态结合物理约束的方法。

Method: RGPD框架结合了图卷积循环网络（GCRN）、图注意力卷积（GATConv）和软性行动-批判（SAC）模块，动态加权物理约束以提高学习效果。

Result: 在RUL和SOH估计任务中，RGPD在多个工业数据集上表现优于现有方法，具有强的鲁棒性和预测准确性。

Conclusion: RGPD通过动态加权物理约束和时空学习，显著提升了预测性能，适用于多种工业场景。

Abstract: Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH)
is essential for Prognostics and Health Management (PHM) across a wide range of
industrial applications. We propose a novel framework -- Reinforced Graph-Based
Physics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that
combines physics-based supervision with advanced spatio-temporal learning.
Graph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional
filters within recurrent units to capture how node representations evolve over
time. Graph Attention Convolution (GATConv) leverages a self-attention
mechanism to compute learnable, edge-wise attention coefficients, dynamically
weighting neighbor contributions for adaptive spatial aggregation. A Soft
Actor-Critic (SAC) module is positioned between the Temporal Attention Unit
(TAU) and GCRN to further improve the spatio-temporal learning. This module
improves attention and prediction accuracy by dynamically scaling hidden
representations to minimize noise and highlight informative features. To
identify the most relevant physical constraints in each area, Q-learning agents
dynamically assign weights to physics-informed loss terms, improving
generalization across real-time industrial systems and reducing the need for
manual tuning. In both RUL and SOH estimation tasks, the proposed method
consistently outperforms state-of-the-art models, demonstrating strong
robustness and predictive accuracy across varied degradation patterns across
three diverse industrial benchmark datasets.

</details>


### [154] [Multiple Choice Learning of Low Rank Adapters for Language Modeling](https://arxiv.org/abs/2507.10419)
*Victor Letzelter,Hugo Malard,Mathieu Fontaine,Gaël Richard,Slim Essid,Andrei Bursuc,Patrick Pérez*

Key words: LoRA-MCL, 多选择学习, 低秩适应, 语言模型, 多样性生成

TL;DR: LoRA-MCL通过结合多选择学习和低秩适应，扩展了语言模型的训练方法，以生成多样且合理的句子延续。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统语言模型在给定上下文时存在多解问题，LoRA-MCL旨在解决这一问题。

Method: 结合多选择学习(MCL)、Winner-Takes-All损失和低秩适应(LoRA)来训练模型。

Result: 实验表明，该方法在视觉和音频字幕任务中生成的结果多样且相关。

Conclusion: LoRA-MCL有效解决了语言模型中的歧义问题，提升了生成质量。

Abstract: We propose LoRA-MCL, a training scheme that extends next-token prediction in
language models with a method designed to decode diverse, plausible sentence
continuations at inference time. Traditional language modeling is an
intrinsically ill-posed problem: given a context, multiple futures may be
equally plausible. Our approach leverages Multiple Choice Learning (MCL) and
the Winner-Takes-All (WTA) loss to efficiently handle ambiguity through
Low-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying
Multiple Choice Learning to Language Modeling, assuming the data is generated
from a mixture of distributions. To illustrate the proposed approach, we use
data sampled from mixtures of Markov chains. We then demonstrate with extensive
experiments on real-world visual and audio captioning tasks that our method
achieves high diversity and relevance in generated outputs.

</details>


### [155] [Knowing When to Quit: Probabilistic Early Exits for Speech Separation](https://arxiv.org/abs/2507.09768)
*Kenny Falkær Olsen. Mads Østergaard,Karl Ulbæk,Søren Føns Nielsen,Rasmus Malik Høegh Lindrup,Bjørn Sand Jensen,Morten Mørup*

Key words: 语音分离，提前退出，概率框架，动态计算

TL;DR: 该论文提出了一种支持提前退出的神经网络架构和概率框架，用于动态调整计算资源的语音分离任务，同时保持高性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决现有固定计算和参数预算的语音分离模型无法适应嵌入式设备和移动设备资源需求的问题。

Method: 设计了一个支持提前退出的神经网络架构，并提出基于不确定性的概率框架，用于联合建模干净语音信号和误差方差，以确定提前退出条件。

Result: 实验表明，单一提前退出模型在多个计算和参数预算下能媲美最先进的模型。

Conclusion: 该框架实现了动态计算资源调整的语音分离，同时保持高性能和可解释的退出条件。

Abstract: In recent years, deep learning-based single-channel speech separation has
improved considerably, in large part driven by increasingly compute- and
parameter-efficient neural network architectures. Most such architectures are,
however, designed with a fixed compute and parameter budget, and consequently
cannot scale to varying compute demands or resources, which limits their use in
embedded and heterogeneous devices such as mobile phones and hearables. To
enable such use-cases we design a neural network architecture for speech
separation capable of early-exit, and we propose an uncertainty-aware
probabilistic framework to jointly model the clean speech signal and error
variance which we use to derive probabilistic early-exit conditions in terms of
desired signal-to-noise ratios. We evaluate our methods on both speech
separation and enhancement tasks, and we show that a single early-exit model
can be competitive with state-of-the-art models trained at many compute and
parameter budgets. Our framework enables fine-grained dynamic compute-scaling
of speech separation networks while achieving state-of-the-art performance and
interpretable exit conditions.

</details>


### [156] [Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow](https://arxiv.org/abs/2507.09785)
*Zhonglin Cao,Mario Geiger,Allan dos Santos Costa,Danny Reidenbach,Karsten Kreis,Tomas Geffner,Franco Pellegrini,Guoqing Zhou,Emine Kucukbenli*

Key words: 分子构象生成, 流匹配, SO(3)-Averaged Flow, 重流, 蒸馏

TL;DR: 该论文提出了一种基于流匹配的方法，通过SO(3)-Averaged Flow训练目标和重流/蒸馏技术，实现了快速且高质量的分子构象生成。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前基于扩散或流的分子构象生成模型需要大量计算资源。本研究旨在加速训练和推理过程，以满足计算化学和药物发现的需求。

Method: 提出了SO(3)-Averaged Flow训练目标，以及重流和蒸馏技术，用于加速训练和实现少步甚至一步推理。

Result: 模型训练更快且生成质量更高，达到了最先进的分子构象生成水平。

Conclusion: 该技术为高效分子构象生成提供了可能。

Abstract: Fast and accurate generation of molecular conformers is desired for
downstream computational chemistry and drug discovery tasks. Currently,
training and sampling state-of-the-art diffusion or flow-based models for
conformer generation require significant computational resources. In this work,
we build upon flow-matching and propose two mechanisms for accelerating
training and inference of generative models for 3D molecular conformer
generation. For fast training, we introduce the SO(3)-Averaged Flow training
objective, which leads to faster convergence to better generation quality
compared to conditional optimal transport flow or Kabsch-aligned flow. We
demonstrate that models trained using SO(3)-Averaged Flow can reach
state-of-the-art conformer generation quality. For fast inference, we show that
the reflow and distillation methods of flow-based models enable few-steps or
even one-step molecular conformer generation with high quality. The training
techniques proposed in this work show a path towards highly efficient molecular
conformer generation with flow-based models.

</details>


### [157] [Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination](https://arxiv.org/abs/2507.10532)
*Mingqi Wu,Zhihao Zhang,Qiaole Dong,Zhiheng Xi,Jun Zhao,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Qin Liu,Songyang Zhang,Qi Zhang*

Key words: 大型语言模型,强化学习,数据污染,推理能力,合成数据

TL;DR: 该论文探讨了大型语言模型（LLMs）的推理能力，指出现有研究在Qwen2.5模型上取得的突破可能受数据污染影响，并提出了基于合成数据的干净评估方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究目的是揭示现有强化学习（RL）方法在LLM推理能力提升中的局限性，特别是数据污染对结果的影响，并提出更可靠的评估方法。

Method: 使用合成数据生成器RandomCalculation创建无泄漏的算术问题数据集，对比分析不同奖励信号对模型性能的影响。

Result: 实验表明，仅准确的奖励信号能稳定提升性能，而噪声或错误信号无效，且现有基准如MATH-500可能因数据污染导致结果不可靠。

Conclusion: 建议在无污染基准和多样化模型家族上评估RL方法，以确保结论的可信度和普适性。

Abstract: The reasoning capabilities of large language models (LLMs) have been a
longstanding focus of research. Recent works have further enhanced these
capabilities using reinforcement learning (RL), with many new methods claiming
significant improvements with minimal or no external supervision. Surprisingly,
some studies even suggest that random or incorrect reward signals can enhance
reasoning performance. However, these breakthroughs are mostly reported on the
Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,
AMC, and AIME, while failing to achieve similar gains on other models like
Llama, which warrants further investigation. Our analysis shows that although
Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on
large-scale web corpora makes it vulnerable to data contamination in popular
benchmarks. As a result, results derived from these benchmarks may be
unreliable. To address this, we introduce a generator that produces fully
synthetic arithmetic problems of arbitrary length and difficulty, yielding a
clean dataset we call RandomCalculation. Using these leakage-free datasets, we
show that only accurate reward signals consistently improve performance, while
noisy or incorrect signals do not. We advocate for evaluating RL methods on
uncontaminated benchmarks and across diverse model families to ensure
trustworthy conclusions.

</details>


### [158] [Leveraging Distribution Matching to Make Approximate Machine Unlearning Faster](https://arxiv.org/abs/2507.09786)
*Junaid Iqbal Khan*

Key words: 近似机器遗忘, 数据集压缩, 损失函数优化, 计算效率

TL;DR: 提出两种互补方法加速分类导向的近似机器遗忘（AMU）：Blend（数据集压缩）和A-AMU（损失函数优化），显著减少计算时间。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有AMU方法处理保留数据集的耗时问题及减少训练轮次的挑战。

Method: 1. Blend通过视觉相似的图像合并减少数据集大小；2. A-AMU通过增强损失函数加速收敛。

Result: 实验表明该方法大幅减少遗忘延迟，同时保持模型效用和隐私。

Conclusion: 首次系统解决遗忘效率问题，结合数据集压缩和损失函数优化。

Abstract: Approximate machine unlearning (AMU) enables models to `forget' specific
training data through specialized fine-tuning on a retained dataset subset.
However, processing this retained subset still dominates computational runtime,
while reductions of epochs also remain a challenge. We propose two
complementary methods to accelerate classification-oriented AMU. First,
\textbf{Blend}, a novel distribution-matching dataset condensation (DC), merges
visually similar images with shared blend-weights to significantly reduce the
retained set size. It operates with minimal pre-processing overhead and is
orders of magnitude faster than state-of-the-art DC methods. Second, our
loss-centric method, \textbf{Accelerated-AMU (A-AMU)}, augments the unlearning
objective to quicken convergence. A-AMU achieves this by combining a steepened
primary loss to expedite forgetting with a novel, differentiable regularizer
that matches the loss distributions of forgotten and in-distribution unseen
data. Our extensive experiments demonstrate that this dual approach of data and
loss-centric optimization dramatically reduces end-to-end unlearning latency
across both single and multi-round scenarios, all while preserving model
utility and privacy. To our knowledge, this is the first work to systematically
tackle unlearning efficiency by jointly designing a specialized dataset
condensation technique with a dedicated accelerated loss function. Code is
available at https://github.com/algebraicdianuj/DC_Unlearning.

</details>


### [159] [A Scalable and Efficient Signal Integration System for Job Matching](https://arxiv.org/abs/2507.09797)
*Ping Liu,Rajat Arora,Xiao Shi,Benjamin Le,Qianqi Shen,Jianqiang Shen,Chengming Jiang,Nikita Zhiltsov,Priya Bannur,Yidan Zhu,Liming Dong,Haichao Wei,Qi Guo,Luke Simon,Liangjie Hong,Wenjing Zhang*

Key words: LinkedIn, 推荐系统, 冷启动, LLM, GNN, STAR

TL;DR: LinkedIn开发了STAR系统，结合LLM和GNN解决推荐系统中的冷启动、过滤泡沫和偏见问题，提供了端到端的嵌入解决方案。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: LinkedIn面临冷启动、过滤泡沫和偏见等建模挑战，需要高效且可扩展的推荐系统。

Method: STAR系统整合了LLM（理解文本数据）和GNN（捕捉复杂关系），结合自适应采样和版本管理等工业规模范式。

Result: STAR提供了高性能的推荐方法，并为工业应用开发嵌入提供了稳健的方法论。

Conclusion: STAR系统成功解决了推荐系统中的关键问题，并提供了实用的模型部署见解。

Abstract: LinkedIn, one of the world's largest platforms for professional networking
and job seeking, encounters various modeling challenges in building
recommendation systems for its job matching product, including cold-start,
filter bubbles, and biases affecting candidate-job matching. To address these,
we developed the STAR (Signal Integration for Talent And Recruiters) system,
leveraging the combined strengths of Large Language Models (LLMs) and Graph
Neural Networks (GNNs). LLMs excel at understanding textual data, such as
member profiles and job postings, while GNNs capture intricate relationships
and mitigate cold-start issues through network effects. STAR integrates diverse
signals by uniting LLM and GNN capabilities with industrial-scale paradigms
including adaptive sampling and version management. It provides an end-to-end
solution for developing and deploying embeddings in large-scale recommender
systems. Our key contributions include a robust methodology for building
embeddings in industrial applications, a scalable GNN-LLM integration for
high-performing recommendations, and practical insights for real-world model
deployment.

</details>


### [160] [Federated Learning with Graph-Based Aggregation for Traffic Forecasting](https://arxiv.org/abs/2507.09805)
*Audri Banik,Glaucio Haroldo Silva de Carvalho,Renata Dividino*

Key words: 交通预测、联邦学习、图学习、邻域聚合、计算效率

TL;DR: 提出了一种轻量级的图感知联邦学习方法，结合FedAvg的简单性和图学习的关键思想，有效捕捉空间关系且计算高效。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在交通预测中，联邦学习可以保护数据隐私，但标准方法忽略空间关系，而图学习方法计算开销大。

Method: 采用基于邻域聚合的参数更新策略，根据图连接性加权客户端模型。

Result: 在METR-LA和PEMS-BAY数据集上表现优于标准基线和图学习联邦方法。

Conclusion: 提出的方法在保持计算效率的同时，有效捕捉空间关系，性能优异。

Abstract: In traffic prediction, the goal is to estimate traffic speed or flow in
specific regions or road segments using historical data collected by devices
deployed in each area. Each region or road segment can be viewed as an
individual client that measures local traffic flow, making Federated Learning
(FL) a suitable approach for collaboratively training models without sharing
raw data. In centralized FL, a central server collects and aggregates model
updates from multiple clients to build a shared model while preserving each
client's data privacy. Standard FL methods, such as Federated Averaging
(FedAvg), assume that clients are independent, which can limit performance in
traffic prediction tasks where spatial relationships between clients are
important. Federated Graph Learning methods can capture these dependencies
during server-side aggregation, but they often introduce significant
computational overhead. In this paper, we propose a lightweight graph-aware FL
approach that blends the simplicity of FedAvg with key ideas from graph
learning. Rather than training full models, our method applies basic
neighbourhood aggregation principles to guide parameter updates, weighting
client models based on graph connectivity. This approach captures spatial
relationships effectively while remaining computationally efficient. We
evaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY,
and show that it achieves competitive performance compared to standard
baselines and recent graph-based federated learning techniques.

</details>


### [161] [Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem](https://arxiv.org/abs/2507.09816)
*Adam Newgas*

Key words: 神经网络, 压缩计算, Universal-AND, 高效电路, 可解释性

TL;DR: 研究探讨了神经网络在低维度下实现高效计算的可能性，发现实际学习到的解决方案与理论构造不同，具有高效、鲁棒性强的特点。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索神经网络是否能在实践中学习到高效的压缩计算电路，特别是针对Universal-AND问题。

Method: 使用一个受限的隐藏维度模型，通过训练寻找高效的计算电路，研究其性质和性能。

Result: 训练过程找到了一种简单且高效的解决方案，不同于理论构造，具有扩展性和鲁棒性。

Conclusion: 实际学习到的电路更高效，为神经网络电路设计和可解释性提供了新见解。

Abstract: Neural networks are capable of superposition -- representing more features
than there are dimensions. Recent work considers the analogous concept for
computation instead of storage, proposing theoretical constructions. But there
has been little investigation into whether these circuits can be learned in
practice. In this work, we investigate a toy model for the Universal-AND
problem which computes the AND of all $m\choose 2$ pairs of $m$ sparse inputs.
The hidden dimension that determines the number of non-linear activations is
restricted to pressure the model to find a compute-efficient circuit, called
compressed computation. We find that the training process finds a simple
solution that does not correspond to theoretical constructions. It is fully
dense -- every neuron contributes to every output. The solution circuit
naturally scales with dimension, trading off error rates for neuron efficiency.
It is similarly robust to changes in sparsity and other key parameters, and
extends naturally to other boolean operations and boolean circuits. We explain
the found solution in detail and compute why it is more efficient than the
theoretical constructions at low sparsity. Our findings shed light on the types
of circuits that models like to form and the flexibility of the superposition
representation. This contributes to a broader understanding of network
circuitry and interpretability.

</details>


### [162] [Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification](https://arxiv.org/abs/2507.09826)
*Jintao Qu,Zichong Wang,Chenhao Wu,Wenbin Zhang*

Key words: 动态时间规整（DTW）、神经网络、时间序列分类、可解释性、冷启动

TL;DR: 本文提出了一种结合动态时间规整（DTW）和神经网络的模型，解决了冷启动场景下数据不足的问题，同时保持了模型的解释性和可训练性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 神经网络的性能依赖于大量标注数据，缺乏解释性；而DTW在少数据场景下效果好但无法利用大数据。本文旨在开发一个兼顾二者优势的模型。

Method: 提出动态长度缩短算法，将时间序列转换为原型，并将DTW的递推关系转化为等效的循环神经网络，构建可训练的模型。

Result: 模型在少数据场景下显著优于现有方法，在大数据场景下仍保持竞争力。

Conclusion: 该模型成功结合了DTW的简单性和神经网络的可训练性，适用于多种时间序列分类任务。

Abstract: Neural networks have achieved remarkable success in time series
classification, but their reliance on large amounts of labeled data for
training limits their applicability in cold-start scenarios. Moreover, they
lack interpretability, reducing transparency in decision-making. In contrast,
dynamic time warping (DTW) combined with a nearest neighbor classifier is
widely used for its effectiveness in limited-data settings and its inherent
interpretability. However, as a non-parametric method, it is not trainable and
cannot leverage large amounts of labeled data, making it less effective than
neural networks in rich-resource scenarios. In this work, we aim to develop a
versatile model that adapts to cold-start conditions and becomes trainable with
labeled data, while maintaining interpretability. We propose a dynamic
length-shortening algorithm that transforms time series into prototypes while
preserving key structural patterns, thereby enabling the reformulation of the
DTW recurrence relation into an equivalent recurrent neural network. Based on
this, we construct a trainable model that mimics DTW's alignment behavior. As a
neural network, it becomes trainable when sufficient labeled data is available,
while still retaining DTW's inherent interpretability. We apply the model to
several benchmark time series classification tasks and observe that it
significantly outperforms previous approaches in low-resource settings and
remains competitive in rich-resource settings.

</details>


### [163] [Generative Cognitive Diagnosis](https://arxiv.org/abs/2507.09831)
*Jiatong Li,Qi Liu,Mengxiao Zhu*

Key words: 认知诊断,生成式建模,G-IRT,G-NCDM,教育评估

TL;DR: 该研究提出了一种新的生成式认知诊断范式，通过生成建模实现对学习者认知状态的推断，无需重新优化参数，显著提升了诊断速度和可靠性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统认知诊断模型存在对新学习者诊断需重训练和可靠性不足的问题，研究旨在通过生成式方法解决这些限制。

Method: 提出了生成式诊断范式，并实例化为G-IRT和G-NCDM模型，通过设计生成过程实现认知状态与响应预测的解耦。

Result: 实验表明，该方法在真实数据集上表现优异，尤其是对新学习者的诊断速度提升了100倍。

Conclusion: 生成式方法为认知诊断在智能评估和教育系统中的应用开辟了新途径。

Abstract: Cognitive diagnosis (CD) models latent cognitive states of human learners by
analyzing their response patterns on diagnostic tests, serving as a crucial
machine learning technique for educational assessment and evaluation.
Traditional cognitive diagnosis models typically follow a transductive
prediction paradigm that optimizes parameters to fit response scores and
extract learner abilities. These approaches face significant limitations as
they cannot perform instant diagnosis for new learners without computationally
expensive retraining and produce diagnostic outputs with limited reliability.
In this study, we introduces a novel generative diagnosis paradigm that
fundamentally shifts CD from predictive to generative modeling, enabling
inductive inference of cognitive states without parameter re-optimization. We
propose two simple yet effective instantiations of this paradigm: Generative
Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model
(G-NCDM), which achieve excellent performance improvements over traditional
methods. The generative approach disentangles cognitive state inference from
response prediction through a well-designed generation process that
incorporates identifiability and monotonicity conditions. Extensive experiments
on real-world datasets demonstrate the effectiveness of our methodology in
addressing scalability and reliability challenges, especially $\times 100$
speedup for the diagnosis of new learners. Our framework opens new avenues for
cognitive diagnosis applications in artificial intelligence, particularly for
intelligent model evaluation and intelligent education systems. The code is
available at https://github.com/CSLiJT/Generative-CD.git.

</details>


### [164] [A Pre-training Framework for Relational Data with Information-theoretic Principles](https://arxiv.org/abs/2507.09837)
*Quang Truong,Zhikai Chen,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Key words: 关系数据库,预训练框架,任务异构性,TVE,RelBench

TL;DR: 论文提出了一种新的预训练框架Task Vector Estimation (TVE)，用于解决关系数据库中的任务异构性问题，并通过实验验证其优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 关系数据库在多个领域支撑关键基础设施，但由于任务异构性，如何设计通用的预训练策略仍是一个挑战。

Method: 提出TVE框架，通过基于集合的聚合和显式建模关系动态，构建预测性监督信号。

Result: 在RelBench基准测试中，TVE表现优于传统预训练基线。

Conclusion: 研究支持将任务异构性和时间结构编码为预训练目标的设计原则。

Abstract: Relational databases underpin critical infrastructure across a wide range of
domains, yet the design of generalizable pre-training strategies for learning
from relational databases remains an open challenge due to task heterogeneity.
Specifically, there exist infinitely many possible downstream tasks, as tasks
are defined based on relational schema graphs, temporal dependencies, and
SQL-defined label logics. An effective pre-training framework is desired to
take these factors into account in order to obtain task-aware representations.
By incorporating knowledge of the underlying distribution that drives label
generation, downstream tasks can benefit from relevant side-channel
information. To bridge this gap, we introduce Task Vector Estimation (TVE), a
novel pre-training framework that constructs predictive supervisory signals via
set-based aggregation over schema traversal graphs, explicitly modeling
next-window relational dynamics. We formalize our approach through an
information-theoretic lens, demonstrating that task-informed representations
retain more relevant signals than those obtained without task priors. Extensive
experiments on the RelBench benchmark show that TVE consistently outperforms
traditional pre-training baselines. Our findings advocate for pre-training
objectives that encode task heterogeneity and temporal structure as design
principles for predictive modeling on relational databases.

</details>


### [165] [Rethinking Prompt Optimization: Reinforcement, Diversification, and Migration in Blackbox LLMs](https://arxiv.org/abs/2507.09839)
*MohammadReza Davari,Utkarsh Garg,Weixin Cai,Eugene Belilovsky*

Key words: 自动提示优化（APO），反馈多样化，持续提示优化（CPO），大型语言模型（LLM），迁移学习

TL;DR: 该论文提出了一种新的自动提示优化（APO）框架，通过增强反馈机制和引入正负强化，显著提高了提示优化的效果和效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有APO方法主要关注错误修正，忽视了成功预测中的有益信息，限制了其效果和效率。

Method: 重新定义文本梯度为负强化，引入正强化保护有益提示部分，并提出反馈多样化技术以减少噪声。同时提出持续提示优化（CPO）以应对模型迁移挑战。

Result: 实验表明，该方法在准确率、收敛速度和计算成本方面均优于基线，尤其是在模型迁移场景中表现突出。

Conclusion: 通过增强反馈机制和优化迁移策略，新框架显著提升了提示优化的性能和适用性。

Abstract: An increasing number of NLP applications interact with large language models
(LLMs) through black-box APIs, making prompt engineering critical for
controlling model outputs. While recent Automatic Prompt Optimization (APO)
methods iteratively refine prompts using model-generated feedback, textual
gradients, they primarily focus on error correction and neglect valuable
insights from correct predictions. This limits both their effectiveness and
efficiency. In this paper, we propose a novel APO framework centered on
enhancing the feedback mechanism. We reinterpret the textual gradient as a form
of negative reinforcement and introduce the complementary positive
reinforcement to explicitly preserve beneficial prompt components identified
through successful predictions. To mitigate the noise inherent in LLM-generated
feedback, we introduce a technique called feedback diversification, which
aggregates multiple feedback signals, emphasizing consistent, actionable advice
while filtering out outliers. Motivated by the rapid evolution and diversity of
available LLMs, we also formalize Continual Prompt Optimization (CPO),
addressing the practical challenge of efficiently migrating optimized prompts
between different model versions or API providers. Our experiments reveal that
naive prompt migration often degrades performance due to loss of critical
instructions. In contrast, our approach consistently outperforms strong
baselines, achieving significant accuracy improvements, faster convergence, and
lower computational costs in both standard and migration scenarios.

</details>


### [166] [Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training](https://arxiv.org/abs/2507.09846)
*Minhak Song,Beomhan Baek,Kwangjun Ahn,Chulhee Yun*

Key words: 大规模训练, Schedule-Free方法, SF-AdamW, 学习率调度, 语言模型

TL;DR: 本文探讨了大规模模型训练的调度方法，介绍了Schedule-Free (SF)方法的优势及其改进版本，证明了其在语言模型训练中的实用性和可扩展性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着模型和数据集的规模迅速增长，传统的预训练策略（如固定计算预算）已不足以应对大规模训练需求，需要更灵活和高效的方法。

Method: 重新审视了Schedule-Free (SF)方法，并通过理论和实证分析揭示了其动态特性，提出了改进版本以增强鲁棒性和大批量训练性能。

Result: SF-AdamW在无需额外内存开销的情况下，有效优化了损失函数，并在大规模训练中表现出色。

Conclusion: SF方法为语言模型训练提供了一种实用、可扩展且理论支持的新选择。

Abstract: As both model and dataset sizes continue to scale rapidly, conventional
pretraining strategies with fixed compute budgets-such as cosine learning rate
schedules-are increasingly inadequate for large-scale training. Recent
alternatives, including warmup-stable-decay (WSD) schedules and weight
averaging, offer greater flexibility. However, WSD relies on explicit decay
phases to track progress, while weight averaging addresses this limitation at
the cost of additional memory. In search of a more principled and scalable
alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],
which has shown strong empirical performance across diverse settings. We show
that SF-AdamW effectively navigates the "river" structure of the loss landscape
without decay phases or auxiliary averaging, making it particularly suitable
for continuously scaling training workloads. To understand this behavior, we
conduct a theoretical and empirical analysis of SF dynamics, revealing that it
implicitly performs weight averaging without memory overhead. Guided by this
analysis, we propose a refined variant of SF that improves robustness to
momentum and performs better under large batch sizes, addressing key
limitations of the original method. Together, these results establish SF as a
practical, scalable, and theoretically grounded approach for language model
training.

</details>


### [167] [Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks](https://arxiv.org/abs/2507.09871)
*Niket Patel,Randall Balestriero*

Key words: AI研究、自监督学习、任务评估、任务先验、概率空间

TL;DR: 论文提出了一种新的评估框架，通过定义任务先验和任务分布，解决了当前AI研究中评测方法的局限性，使模型能在所有可能的任务上进行评估。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前AI研究中的评估方法依赖于固定的下游基准测试，限制了研究的广度和深度。论文旨在解决这一瓶颈，通过任务先验和任务分布，实现更全面的模型评估。

Method: 提出了一种基于概率空间的任务评估框架，包括任务分布和任务先验的定义，从而能够评估模型在所有可能下游任务上的性能。

Result: 该框架首次提供了对模型在所有可能任务上的平均性能和性能方差的量化评估方法。

Conclusion: 任务先验框架为AI研究（尤其是自监督学习）提供了更灵活和全面的评估标准，有望加速研究进展。

Abstract: The grand goal of AI research, and particularly Self Supervised Learning
(SSL), is to produce systems that can successfully solve any possible task. In
contrast, current evaluation methods available to AI researchers typically rely
on a fixed collection of hand-picked downstream benchmarks. Hence, a large
amount of effort is put into designing and searching for large collection of
evaluation tasks that can serve as a proxy of our grand goal. We argue that
such a rigid evaluation protocol creates a silent bottleneck in AI research. To
remedy that, we define a probabilistic space of downstream tasks obtained by
adopting a distribution of tasks and by defining Task Priors. Under this view,
one can evaluate a model's performance over the set of all possible downstream
tasks. Our framework is the first to provide answers to key questions such as
(i) what is the average performance of my model over all possible downstream
tasks weighted by the probability to encounter each task? or (ii) what is the
variance of my model's performance across all downstream tasks under the
defined Task Priors? Beyond establishing a new standard for evaluation, we
believe that Task Priors will accelerate the pace of research in SSL - where
downstream task evaluation is the sole qualitative signal that researchers have
access to.

</details>


### [168] [AdaBrain-Bench: Benchmarking Brain Foundation Models for Brain-Computer Interface Applications](https://arxiv.org/abs/2507.09882)
*Jiamin Wu,Zichen Ren,Junyu Wang,Pengyu Zhu,Yonghao Song,Mianxin Liu,Qihao Zheng,Lei Bai,Wanli Ouyang,Chunfeng Song*

Key words: 非侵入式脑机接口,脑基础模型,基准评估,神经解码,电生理信号

TL;DR: AdaBrain-Bench是一个用于评估脑基础模型在非侵入式BCI任务中的通用性的标准化基准，涵盖多种应用场景并提供适应性工具。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 非侵入式脑机接口（BCI）的噪声大且任务数据有限，当前缺乏全面评估脑基础模型的基准，阻碍其广泛应用。

Method: 引入AdaBrain-Bench，一个包含7种关键应用的代表性数据集、多维度评估指标和适应性工具的标准化基准。

Result: 通过评估公开的脑基础模型，提供了在不同场景中选择合适模型的实践建议。

Conclusion: AdaBrain-Bench为促进稳健和通用的神经解码研究提供了一个可扩展且实用的平台。

Abstract: Non-invasive Brain-Computer Interfaces (BCI) offer a safe and accessible
means of connecting the human brain to external devices, with broad
applications in home and clinical settings to enhance human capabilities.
However, the high noise level and limited task-specific data in non-invasive
signals constrain decoding capabilities. Recently, the adoption of
self-supervised pre-training is transforming the landscape of non-invasive BCI
research, enabling the development of brain foundation models to capture
generic neural representations from large-scale unlabeled
electroencephalography (EEG) signals with substantial noises. However, despite
these advances, the field currently lacks comprehensive, practical and
extensible benchmarks to assess the utility of the public foundation models
across diverse BCI tasks, hindering their widespread adoption. To address this
challenge, we present AdaBrain-Bench, a large-scale standardized benchmark to
systematically evaluate brain foundation models in widespread non-invasive BCI
tasks. AdaBrain-Bench encompasses a diverse collection of representative BCI
decoding datasets spanning 7 key applications. It introduces a streamlined task
adaptation pipeline integrated with multi-dimensional evaluation metrics and a
set of adaptation tools. The benchmark delivers an inclusive framework for
assessing generalizability of brain foundation models across key transfer
settings, including cross-subject, multi-subject, and few-shot scenarios. We
leverage AdaBrain-Bench to evaluate a suite of publicly available brain
foundation models and offer insights into practices for selecting appropriate
models in various scenarios. We make our benchmark pipeline available to enable
reproducible research and external use, offering a continuously evolving
platform to foster progress toward robust and generalized neural decoding
solutions.

</details>


### [169] [TolerantECG: A Foundation Model for Imperfect Electrocardiogram](https://arxiv.org/abs/2507.09887)
*Huynh Nguyen Dang,Thang Pham,Ngan Le,Van Nguyen*

Key words: ECG, 噪声鲁棒性, 导联缺失, 对比学习, 自监督学习

TL;DR: TolerantECG是一种抗噪声且支持12导联部分缺失的心电信号基础模型，表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 心电图（ECG）诊断心脏疾病的效力可能因噪声或部分导联缺失而受限，需提升其鲁棒性。

Method: 结合对比学习和自监督学习框架，训练模型学习ECG信号及其文本报告描述，同时处理噪声或导联缺失信号。

Result: 在PTB-XL数据集和MIT-BIH心律失常数据库中，TolerantECG表现最佳或接近最佳。

Conclusion: TolerantECG能有效提升ECG在噪声或导联缺失情况下的诊断准确性，具有广泛应用潜力。

Abstract: The electrocardiogram (ECG) is an essential and effective tool for diagnosing
heart diseases. However, its effectiveness can be compromised by noise or
unavailability of one or more leads of the standard 12-lead recordings,
resulting in diagnostic errors or uncertainty. To address these challenges, we
propose TolerantECG, a foundation model for ECG signals that is robust to noise
and capable of functioning with arbitrary subsets of the standard 12-lead ECG.
TolerantECG training combines contrastive and self-supervised learning
frameworks to jointly learn ECG signal representations alongside their
corresponding knowledge-retrieval-based text report descriptions and corrupted
or lead-missing signals. Comprehensive benchmarking results demonstrate that
TolerantECG consistently ranks as the best or second-best performer across
various ECG signal conditions and class levels in the PTB-XL dataset, and
achieves the highest performance on the MIT-BIH Arrhythmia Database.

</details>


### [170] [NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting](https://arxiv.org/abs/2507.09888)
*Huibo Xu,Likang Wu,Xianquan Wang,Haoning Dang,Chun-Wun Cheng,Angelica I Aviles-Rivero,Qi Liu*

Key words: 时间序列预测, 连续函数家族, 神经操作器, 流匹配, NeuTSFlow

TL;DR: 该论文提出一种新方法NeuTSFlow，利用神经操作器学习连续函数家族之间的转换路径，超越了传统离散时间序列分析方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统方法将时间序列视为离散序列，忽略了其作为连续过程的噪声样本的本质。论文提出从连续函数家族的角度重新定义预测任务。

Method: 提出NeuTSFlow框架，利用神经操作器进行流匹配，学习历史与未来函数家族之间的路径，直接建模函数级特征。

Result: 实验表明，NeuTSFlow在多类预测任务中具有更高的准确性和鲁棒性。

Conclusion: 从函数家族的视角出发能更有效地解决时间序列预测问题，NeuTSFlow展示了这一方法的优越性。

Abstract: Time series forecasting is a fundamental task with broad applications, yet
conventional methods often treat data as discrete sequences, overlooking their
origin as noisy samples of continuous processes. Crucially, discrete noisy
observations cannot uniquely determine a continuous function; instead, they
correspond to a family of plausible functions. Mathematically, time series can
be viewed as noisy observations of a continuous function family governed by a
shared probability measure. Thus, the forecasting task can be framed as
learning the transition from the historical function family to the future
function family. This reframing introduces two key challenges: (1) How can we
leverage discrete historical and future observations to learn the relationships
between their underlying continuous functions? (2) How can we model the
transition path in function space from the historical function family to the
future function family? To address these challenges, we propose NeuTSFlow, a
novel framework that leverages Neural Operators to facilitate flow matching for
learning path of measure between historical and future function families. By
parameterizing the velocity field of the flow in infinite-dimensional function
spaces, NeuTSFlow moves beyond traditional methods that focus on dependencies
at discrete points, directly modeling function-level features instead.
Experiments on diverse forecasting tasks demonstrate NeuTSFlow's superior
accuracy and robustness, validating the effectiveness of the function-family
perspective.

</details>


### [171] [Soft Graph Clustering for single-cell RNA Sequencing Data](https://arxiv.org/abs/2507.09890)
*Ping Xu,Pengfei Wang,Zhiyuan Ning,Meng Xiao,Min Wu,Yuanchun Zhou*

Key words: 单细胞RNA测序, 软图聚类, GNN, 最优传输, 细胞异质性

TL;DR: 论文提出了一种名为scSGC的软图聚类方法，通过非二值化的边权重更准确地表征细胞间的连续相似性，克服了硬图构建的限制，并在实验中表现出优于现有方法的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的基于GNN的scRNA-seq聚类方法依赖于硬图构建，导致细胞间关系简化为二值化边和显著的信息丢失。scSGC旨在解决这些问题，提升聚类准确性。

Method: scSGC包含三个核心组件：基于ZINB的特征自编码器、双通道切分信息的软图嵌入模块和基于最优传输的聚类优化模块。

Result: 在十个数据集上的实验表明，scSGC在聚类准确性、细胞类型标注和计算效率上优于13种现有方法。

Conclusion: scSGC在scRNA-seq数据分析中具有显著潜力，可以进一步加深对细胞异质性的理解。

Abstract: Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq)
data analysis for elucidating cellular heterogeneity and diversity. Recent
graph-based scRNA-seq clustering methods, particularly graph neural networks
(GNNs), have significantly improved in tackling the challenges of
high-dimension, high-sparsity, and frequent dropout events that lead to
ambiguous cell population boundaries. However, their reliance on hard graph
constructions derived from thresholded similarity matrices presents
challenges:(i) The simplification of intercellular relationships into binary
edges (0 or 1) by applying thresholds, which restricts the capture of
continuous similarity features among cells and leads to significant information
loss.(ii) The presence of significant inter-cluster connections within hard
graphs, which can confuse GNN methods that rely heavily on graph structures,
potentially causing erroneous message propagation and biased clustering
outcomes. To tackle these challenges, we introduce scSGC, a Soft Graph
Clustering for single-cell RNA sequencing data, which aims to more accurately
characterize continuous similarities among cells through non-binary edge
weights, thereby mitigating the limitations of rigid data structures. The scSGC
framework comprises three core components: (i) a zero-inflated negative
binomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed
soft graph embedding module; and (iii) an optimal transport-based clustering
optimization module. Extensive experiments across ten datasets demonstrate that
scSGC outperforms 13 state-of-the-art clustering models in clustering accuracy,
cell type annotation, and computational efficiency. These results highlight its
substantial potential to advance scRNA-seq data analysis and deepen our
understanding of cellular heterogeneity.

</details>


### [172] [Algorithm Development in Neural Networks: Insights from the Streaming Parity Task](https://arxiv.org/abs/2507.09897)
*Loek van Rossem,Andrew M. Saxe*

Key words: 深度神经网络, 无限泛化, 循环神经网络, 流式奇偶校验, 相变

TL;DR: 尽管深度神经网络在极度过参数化时仍能出色地泛化，本文通过研究循环神经网络（RNN）在流式奇偶校验任务中的学习动态，揭示了其无限泛化的机制。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨神经网络在训练数据分布外如何实现无限泛化的机制。

Method: 通过训练RNN在流式奇偶校验任务（一种非线性任务）上的表现，分析其学习动态和相变现象。

Result: 发现RNN在经过有限训练后会出现相变，实现完美的无限泛化，揭示了隐含表示的合并效应。

Conclusion: 本研究为神经网络如何从有限经验中实现无限泛化提供了一种机制解释。

Abstract: Even when massively overparameterized, deep neural networks show a remarkable
ability to generalize. Research on this phenomenon has focused on
generalization within distribution, via smooth interpolation. Yet in some
settings neural networks also learn to extrapolate to data far beyond the
bounds of the original training set, sometimes even allowing for infinite
generalization, implying that an algorithm capable of solving the task has been
learned. Here we undertake a case study of the learning dynamics of recurrent
neural networks (RNNs) trained on the streaming parity task in order to develop
an effective theory of algorithm development. The streaming parity task is a
simple but nonlinear task defined on sequences up to arbitrary length. We show
that, with sufficient finite training experience, RNNs exhibit a phase
transition to perfect infinite generalization. Using an effective theory for
the representational dynamics, we find an implicit representational merger
effect which can be interpreted as the construction of a finite automaton that
reproduces the task. Overall, our results disclose one mechanism by which
neural networks can generalize infinitely from finite training experience.

</details>


### [173] [Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer Model](https://arxiv.org/abs/2507.09925)
*Md Ahsanul Kabir,Abrar Jahin,Mohammad Al Hasan*

Key words: 因果短语提取，依赖树，Transformer，NLP

TL;DR: 论文提出DepBERT模型，将依赖树整合到Transformer框架中，用于句子中的因果短语提取，实验表明其优于现有监督方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有监督方法在因果短语提取任务中未充分利用依赖树等语言学工具，限制了性能。

Method: 提出DepBERT，将句子的依赖树信息集成到基于Transformer的模型中。

Result: 在三个数据集上的实验表明，DepBERT优于现有的先进监督方法。

Conclusion: DepBERT通过整合依赖树信息，显著提升了因果短语提取的效果。

Abstract: Extracting cause and effect phrases from a sentence is an important NLP task,
with numerous applications in various domains, including legal, medical,
education, and scientific research. There are many unsupervised and supervised
methods proposed for solving this task. Among these, unsupervised methods
utilize various linguistic tools, including syntactic patterns, dependency
tree, dependency relations, etc. among different sentential units for
extracting the cause and effect phrases. On the other hand, the contemporary
supervised methods use various deep learning based mask language models
equipped with a token classification layer for extracting cause and effect
phrases. Linguistic tools, specifically, dependency tree, which organizes a
sentence into different semantic units have been shown to be very effective for
extracting semantic pairs from a sentence, but existing supervised methods do
not have any provision for utilizing such tools within their model framework.
In this work, we propose DepBERT, which extends a transformer-based model by
incorporating dependency tree of a sentence within the model framework.
Extensive experiments over three datasets show that DepBERT is better than
various state-of-the art supervised causality extraction methods.

</details>


### [174] [Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications](https://arxiv.org/abs/2507.09931)
*Yoon Pyo Lee*

Key words: 大语言模型（LLM）,核工程,低秩适应,神经元激活,模型透明性

TL;DR: 论文提出了一种新型方法，用于解释大语言模型（LLM）如何编码和利用领域特定知识，以沸水反应堆系统为例，通过参数高效微调技术和神经元激活模式分析，增强了模型透明性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 将大语言模型引入安全关键领域（如核工程）时，需理解其内部推理过程，以满足核监管框架（如10CFR50附录B）的验证和验证要求。

Method: 采用低秩适应（Low-RankAdaptation）对通用LLM（Gemma-3-1b-it）进行微调，比较基模型和微调模型的神经元激活模式，识别关键神经元组，并通过神经元静默技术验证其因果作用。

Result: 静默单个关键神经元无显著影响，但静默整组神经元会导致任务性能显著下降，且模型生成详细技术信息的能力受损。

Conclusion: 该方法增强了黑箱模型的透明性，为核级人工智能验证提供了可行路径，有助于解决安全关键领域AI部署的挑战。

Abstract: The integration of Large Language Models (LLMs) into safety-critical domains,
such as nuclear engineering, necessitates a deep understanding of their
internal reasoning processes. This paper presents a novel methodology for
interpreting how an LLM encodes and utilizes domain-specific knowledge, using a
Boiling Water Reactor system as a case study. We adapted a general-purpose LLM
(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning
technique known as Low-Rank Adaptation. By comparing the neuron activation
patterns of the base model to those of the fine-tuned model, we identified a
sparse set of neurons whose behavior was significantly altered during the
adaptation process. To probe the causal role of these specialized neurons, we
employed a neuron silencing technique. Our results demonstrate that while
silencing most of these specialized neurons individually did not produce a
statistically significant effect, deactivating the entire group collectively
led to a statistically significant degradation in task performance. Qualitative
analysis further revealed that silencing these neurons impaired the model's
ability to generate detailed, contextually accurate technical information. This
paper provides a concrete methodology for enhancing the transparency of an
opaque black-box model, allowing domain expertise to be traced to verifiable
neural circuits. This offers a pathway towards achieving nuclear-grade
artificial intelligence (AI) assurance, addressing the verification and
validation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR
50 Appendix B), which have limited AI deployment in safety-critical nuclear
operations.

</details>


### [175] [Memorization Sinks: Isolating Memorization during LLM Training](https://arxiv.org/abs/2507.09937)
*Gaurav R. Ghosal,Pratyush Maini,Aditi Raghunathan*

Key words: 大语言模型,记忆隔离,隐私保护,MemSinks,泛化

TL;DR: 提出MemSinks新范式，设计隔离记忆机制，以解决大语言模型对重复序列的记忆问题，同时保持通用语言能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大语言模型容易记忆重复序列，引发隐私和版权问题，现有后处理方法效果有限。

Method: 引入MemSinks范式，通过序列标识符激活特定记忆神经元，促进记忆内容的隔离。

Result: 在十亿级参数和标记规模下实现有效隔离和强泛化，证明隔离与泛化可兼顾。

Conclusion: MemSinks首次在真实数据上证明记忆隔离与通用语言能力可同时实现。

Abstract: Large language models are susceptible to memorizing repeated sequences,
posing privacy and copyright concerns. A popular mitigation strategy is to
remove memorized information from specific neurons post-hoc. However, such
approaches have shown limited success so far. In a controlled setting, we show
that the memorization of natural sequences (those that resemble linguistically
plausible text) become mechanistically entangled with general language
abilities, thereby becoming challenging to remove post-hoc. In this work, we
put forward a new paradigm of MemSinks that promotes isolation of memorization
by design. We leverage a sequence identifier that activates a unique set of
memorization neurons for each sequence across repetitions. By analyzing the
dynamics of learning and forgetting, we argue that MemSinks facilitates
isolation of memorized content, making it easier to remove without compromising
general language capabilities. We implement MemSinks at the billion-parameter
and billion-token scale, and observe both effective isolation and strong
generalization. To our knowledge, this is the first proof-of-concept on real
data demonstrating that simultaneous generalization and isolation is
achievable. We open-source our code at http://github.com/grghosal/MemSinks.

</details>


### [176] [Long-Tailed Data Classification by Increasing and Decreasing Neurons During Training](https://arxiv.org/abs/2507.09940)
*Taigo Sakai,Kazuhiro Hotta*

Key words: 动态神经元调整, 类别不平衡, 生物启发, 深度学习

TL;DR: 论文提出了一种动态调整神经元数量的方法，通过训练过程中定期增减神经元，提升少数类别的表征能力，最终网络大小和结构保持不变，显著优于固定大小网络。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 受到生物学启发，人脑在学习过程中不断生成和修剪神经元，表明灵活的容量分配可以提升性能。现实数据集常存在类别不平衡问题，固定大小的网络可能导致少数类别识别精度显著下降。

Method: 在训练过程中定期增减神经元，动态调整网络容量，同时保留多数类别的关键特征，并增加少数类别的神经元，最终保持网络大小和结构不变。

Result: 在三个数据集和五种代表性模型上的实验表明，该方法优于固定大小网络，并结合其他不平衡处理技术时表现更佳。

Conclusion: 动态生物启发网络设计能有效提升类别不平衡数据的性能。

Abstract: In conventional deep learning, the number of neurons typically remains fixed
during training. However, insights from biology suggest that the human
hippocampus undergoes continuous neuron generation and pruning of neurons over
the course of learning, implying that a flexible allocation of capacity can
contribute to enhance performance. Real-world datasets often exhibit class
imbalance situations where certain classes have far fewer samples than others,
leading to significantly reduce recognition accuracy for minority classes when
relying on fixed size networks.To address the challenge, we propose a method
that periodically adds and removes neurons during training, thereby boosting
representational power for minority classes. By retaining critical features
learned from majority classes while selectively increasing neurons for
underrepresented classes, our approach dynamically adjusts capacity during
training. Importantly, while the number of neurons changes throughout training,
the final network size and structure remain unchanged, ensuring efficiency and
compatibility with deployment.Furthermore, by experiments on three different
datasets and five representative models, we demonstrate that the proposed
method outperforms fixed size networks and shows even greater accuracy when
combined with other imbalance-handling techniques. Our results underscore the
effectiveness of dynamic, biologically inspired network designs in improving
performance on class-imbalanced data.

</details>


### [177] [Iceberg: Enhancing HLS Modeling with Synthetic Data](https://arxiv.org/abs/2507.09948)
*Zijian Ding,Tung Nguyen,Weikai Li,Aditya Grover,Yizhou Sun,Jason Cong*

Key words: 深度学习, 高层次综合, 数据增强, 弱标签, 泛化能力

TL;DR: 论文提出了一种名为Iceberg的合成数据增强方法，通过预训练和弱标签生成，显著提升了深度学习模型在高层次综合（HLS）硬件设计中的泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统基于深度学习的HLS预测模型在泛化性方面表现不佳，因此需要一种能够通过合成数据增强来弥补这一不足的方法。

Method: 引入了Iceberg，结合LLM生成程序和弱标签扩展，通过上下文化模型架构实现元学习。

Result: Iceberg将几何平均建模精度提高了86.4%，并在少样本场景下在六个实际应用中表现优异，同时在两个测试数据集上的离线DSE性能分别提升了2.47倍和1.12倍。

Conclusion: Iceberg通过合成数据增强和弱标签生成，显著提升了HLS预测模型的泛化能力，为实际应用提供了有效解决方案。

Abstract: Deep learning-based prediction models for High-Level Synthesis (HLS) of
hardware designs often struggle to generalize. In this paper, we study how to
close the generalizability gap of these models through pretraining on synthetic
data and introduce Iceberg, a synthetic data augmentation approach that expands
both large language model (LLM)-generated programs and weak labels of unseen
design configurations. Our weak label generation method is integrated with an
in-context model architecture, enabling meta-learning from actual and proximate
labels. Iceberg improves the geometric mean modeling accuracy by $86.4\%$ when
adapt to six real-world applications with few-shot examples and achieves a
$2.47\times$ and a $1.12\times$ better offline DSE performance when adapting to
two different test datasets. Our open-sourced code is here:
\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}

</details>


### [178] [Hierarchical Job Classification with Similarity Graph Integration](https://arxiv.org/abs/2507.09949)
*Md Ahsanul Kabir,Kareem Abdelfatah,Mohammed Korayem,Mohammad Al Hasan*

Key words: 职位分类, 表示学习, 层次化分类, 潜在嵌入空间, 招聘优化

TL;DR: 该论文提出了一种新颖的表示学习和分类模型，用于改进在线招聘中的职位分类，通过嵌入层次化的行业类别到一个潜在的嵌入空间中，解决了传统方法的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在动态的在线招聘领域，准确的职位分类对于优化职位推荐系统、搜索排名和劳动力市场分析至关重要。传统文本分类方法由于无法充分利用行业类别的层次结构而效果不佳。

Method: 论文提出了一种结合标准职业分类（SOC）系统和内部层次化分类法Carotene的表示学习模型，将职位和层次化行业类别嵌入到潜在空间中，以捕捉图和层次关系。

Result: 在大规模职位发布数据集上的实验表明，该模型能够有效利用层次结构和丰富的语义特征，显著优于现有方法。

Conclusion: 该研究为提升职位分类准确性提供了一个鲁棒的框架，支持招聘行业更明智的决策。

Abstract: In the dynamic realm of online recruitment, accurate job classification is
paramount for optimizing job recommendation systems, search rankings, and labor
market analyses. As job markets evolve, the increasing complexity of job titles
and descriptions necessitates sophisticated models that can effectively
leverage intricate relationships within job data. Traditional text
classification methods often fall short, particularly due to their inability to
fully utilize the hierarchical nature of industry categories. To address these
limitations, we propose a novel representation learning and classification
model that embeds jobs and hierarchical industry categories into a latent
embedding space. Our model integrates the Standard Occupational Classification
(SOC) system and an in-house hierarchical taxonomy, Carotene, to capture both
graph and hierarchical relationships, thereby improving classification
accuracy. By embedding hierarchical industry categories into a shared latent
space, we tackle cold start issues and enhance the dynamic matching of
candidates to job opportunities. Extensive experimentation on a large-scale
dataset of job postings demonstrates the model's superior ability to leverage
hierarchical structures and rich semantic features, significantly outperforming
existing methods. This research provides a robust framework for improving job
classification accuracy, supporting more informed decision-making in the
recruitment industry.

</details>


### [179] [Radial Neighborhood Smoothing Recommender System](https://arxiv.org/abs/2507.09952)
*Zerui Zhang,Yumou Qiu*

Key words: 推荐系统；潜在空间；距离估计；RNE；邻域构造；冷启动

TL;DR: 该论文提出了一种基于潜在空间中距离估计的新方法Radial Neighborhood Estimator (RNE)，通过行和列距离的系统近似和噪声校正，改进了邻域构造和填补精度，并在理论和实验上验证了其优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 推荐系统在潜在空间中具有低秩结构，但如何在该空间中定义可测量的距离以捕捉用户-用户、项目-项目、用户-项目关系是一个关键挑战。

Method: 引入基于经验方差估计器的校正方法以减少噪声影响，提出径向邻域估计器（RNE），通过包括重叠和部分重叠的用户-项目对构造邻域，并使用局部核回归平滑提高填补精度。

Result: 在模拟和真实数据集上的实验表明，RNE在性能上优于现有的协同过滤和矩阵分解方法，还能缓解冷启动问题。

Conclusion: 该方法不仅改进了潜在空间中的距离估计，还通过RNE提供了更结构化的邻域构造方式，显著提升了推荐系统的性能。

Abstract: Recommender systems inherently exhibit a low-rank structure in latent space.
A key challenge is to define meaningful and measurable distances in the latent
space to capture user-user, item-item, user-item relationships effectively. In
this work, we establish that distances in the latent space can be
systematically approximated using row-wise and column-wise distances in the
observed matrix, providing a novel perspective on distance estimation. To
refine the distance estimation, we introduce the correction based on empirical
variance estimator to account for noise-induced non-centrality. The novel
distance estimation enables a more structured approach to constructing
neighborhoods, leading to the Radial Neighborhood Estimator (RNE), which
constructs neighborhoods by including both overlapped and partially overlapped
user-item pairs and employs neighborhood smoothing via localized kernel
regression to improve imputation accuracy. We provide the theoretical
asymptotic analysis for the proposed estimator. We perform evaluations on both
simulated and real-world datasets, demonstrating that RNE achieves superior
performance compared to existing collaborative filtering and matrix
factorization methods. While our primary focus is on distance estimation in
latent space, we find that RNE also mitigates the ``cold-start'' problem.

</details>


### [180] [Rethinking Inductive Bias in Geographically Neural Network Weighted Regression](https://arxiv.org/abs/2507.09958)
*Zhenyuan Chen*

Key words: 归纳偏差, GNNWR, 空间回归, 非平稳性

TL;DR: 该论文探讨了空间回归模型中归纳偏差的重要性，提出了改进GNNWR模型的方法，并展示了其在模拟数据上的优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究旨在提升GNNWR模型捕捉空间非平稳性的能力，通过引入更强的归纳偏差来解决现有方法的局限性。

Method: 通过结合卷积神经网络、循环神经网络和Transformer的概念，改进GNNWR模型，引入局部感受野、序列上下文和自注意力机制。

Result: 改进后的GNNWR在捕捉非线性复杂空间关系上优于经典方法，且性能受数据特征影响显著。

Conclusion: 归纳偏差对空间建模至关重要，未来研究方向包括可学习的空间加权函数、混合神经架构和提升模型可解释性。

Abstract: Inductive bias is a key factor in spatial regression models, determining how
well a model can learn from limited data and capture spatial patterns. This
work revisits the inductive biases in Geographically Neural Network Weighted
Regression (GNNWR) and identifies limitations in current approaches for
modeling spatial non-stationarity. While GNNWR extends traditional
Geographically Weighted Regression by using neural networks to learn spatial
weighting functions, existing implementations are often restricted by fixed
distance-based schemes and limited inductive bias. We propose to generalize
GNNWR by incorporating concepts from convolutional neural networks, recurrent
neural networks, and transformers, introducing local receptive fields,
sequential context, and self-attention into spatial regression. Through
extensive benchmarking on synthetic spatial datasets with varying
heterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic
methods in capturing nonlinear and complex spatial relationships. Our results
also reveal that model performance depends strongly on data characteristics,
with local models excelling in highly heterogeneous or small-sample scenarios,
and global models performing better with larger, more homogeneous data. These
findings highlight the importance of inductive bias in spatial modeling and
suggest future directions, including learnable spatial weighting functions,
hybrid neural architectures, and improved interpretability for models handling
non-stationary spatial data.

</details>


### [181] [Text-Driven Causal Representation Learning for Source-Free Domain Generalization](https://arxiv.org/abs/2507.09961)
*Lihua Zhou,Mao Ye,Nianxin Li,Shuaifeng Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Key words: 无源域泛化、因果推理、文本驱动、域不变特征

TL;DR: 提出了一种结合因果推理的文本驱动表示学习方法（TDCRL），用于无源域泛化（SFDG），通过数据增强和因果干预网络提取域不变特征，实验证明其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统域泛化方法需要多源域数据，成本高且不实用；现有SFDG方法因域特定混杂因素而泛化能力有限。

Method: 两步法：1）通过数据增强生成风格词向量与类别信息结合生成文本嵌入；2）训练带混杂因子字典的因果干预网络提取域不变特征。

Result: 在PACS、VLCS、OfficeHome和DomainNet上实现了最先进的性能。

Conclusion: TDCRL通过因果学习机制提取域不变特征，实现了稳健的泛化能力。

Abstract: Deep learning often struggles when training and test data distributions
differ. Traditional domain generalization (DG) tackles this by including data
from multiple source domains, which is impractical due to expensive data
collection and annotation. Recent vision-language models like CLIP enable
source-free domain generalization (SFDG) by using text prompts to simulate
visual representations, reducing data demands. However, existing SFDG methods
struggle with domain-specific confounders, limiting their generalization
capabilities. To address this issue, we propose TDCRL
(\textbf{T}ext-\textbf{D}riven \textbf{C}ausal \textbf{R}epresentation
\textbf{L}earning), the first method to integrate causal inference into the
SFDG setting. TDCRL operates in two steps: first, it employs data augmentation
to generate style word vectors, combining them with class information to
generate text embeddings to simulate visual representations; second, it trains
a causal intervention network with a confounder dictionary to extract
domain-invariant features. Grounded in causal learning, our approach offers a
clear and effective mechanism to achieve robust, domain-invariant features,
ensuring robust generalization. Extensive experiments on PACS, VLCS,
OfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL
effectiveness in SFDG.

</details>


### [182] [Compliance Minimization via Physics-Informed Gaussian Processes](https://arxiv.org/abs/2507.09968)
*Xiangyu Sun,Amin Yousefpour,Shirin Hosseinmardi,Ramin Bostanabad*

Key words: 机器学, 合规性最小化, 高斯过程, 网格自由, PGCANs

TL;DR: 提出了一种基于物理信息高斯过程的网格自由框架，解决了传统机器学习方法在合规性最小化问题中的局限性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统机器学习方法在解决合规性最小化问题时存在边界不清晰、计算成本高和缺乏设计复杂度控制机制的问题。

Method: 使用共享多输出神经网络的高斯过程先验参数化设计和状态变量，结合PGCANs架构和损失函数优化。

Result: 框架实现了超分辨率拓扑、更快的收敛速度、更低的合规性和灰度区域占比，优于传统数值方法和竞争性机器学习方法。

Conclusion: 该框架有效解决了合规性最小化问题，提供了设计复杂度控制和高效率的计算方案。

Abstract: Machine learning (ML) techniques have recently gained significant attention
for solving compliance minimization (CM) problems. However, these methods
typically provide poor feature boundaries, are very expensive, and lack a
systematic mechanism to control the design complexity. Herein, we address these
limitations by proposing a mesh-free and simultaneous framework based on
physics-informed Gaussian processes (GPs). In our approach, we parameterize the
design and state variables with GP priors which have independent kernels but
share a multi-output neural network (NN) as their mean function. The
architecture of this NN is based on Parametric Grid Convolutional Attention
Networks (PGCANs) which not only mitigate spectral bias issues, but also
provide an interpretable mechanism to control design complexity. We estimate
all the parameters of our GP-based representations by simultaneously minimizing
the compliance, total potential energy, and residual of volume fraction
constraint. Importantly, our loss function exclude all data-based residuals as
GPs automatically satisfy them. We also develop computational schemes based on
curriculum training and numerical integration to increase the efficiency and
robustness of our approach which is shown to (1) produce super-resolution
topologies with fast convergence, (2) achieve smaller compliance and less gray
area fraction compared to traditional numerical methods, (3) provide control
over fine-scale features, and (4) outperform competing ML-based methods.

</details>


### [183] [Effects of structural properties of neural networks on machine learning performance](https://arxiv.org/abs/2507.10005)
*Yash Arya,Sang Hoon Lee*

Key words: 图神经网络、社区结构、生物神经网络、机器学习、网络科学

TL;DR: 论文探讨了图结构（特别是社区结构）对神经网络性能的影响，发现密集互连的社区结构能提升学习能力，并与生物神经网络对比验证了其现实相关性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有研究对图结构与神经网络性能关系的探索有限，缺乏对真实网络特征（如异质度分布和社区结构）的全面分析，本研究旨在填补这一空白。

Method: 使用随机和无标度网络模型，结合生物神经网络及其子集，分析结构属性对图像分类任务性能的影响。

Result: 结构特征确实影响性能，具有密集互连社区的网络表现出更强的学习能力；与生物神经网络的对比验证了结果的现实意义。

Conclusion: 研究为网络科学和机器学习提供了重要洞见，为设计更贴近生物学的神经网络提供了启发。

Abstract: In recent years, graph-based machine learning techniques, such as
reinforcement learning and graph neural networks, have garnered significant
attention. While some recent studies have started to explore the relationship
between the graph structure of neural networks and their predictive
performance, they often limit themselves to a narrow range of model networks,
particularly lacking mesoscale structures such as communities. Our work
advances this area by conducting a more comprehensive investigation,
incorporating realistic network structures characterized by heterogeneous
degree distributions and community structures, which are typical
characteristics of many real networks. These community structures offer a
nuanced perspective on network architecture. Our analysis employs model
networks such as random and scale-free networks, alongside a comparison with a
biological neural network and its subsets for more detailed analysis. We
examine the impact of these structural attributes on the performance of image
classification tasks. Our findings reveal that structural properties do affect
performance to some extent. Specifically, networks featuring coherent, densely
interconnected communities demonstrate enhanced learning capabilities. The
comparison with the biological neural network emphasizes the relevance of our
findings to real-world structures, suggesting an intriguing connection worth
further exploration. This study contributes meaningfully to network science and
machine learning, providing insights that could inspire the design of more
biologically informed neural networks.

</details>


### [184] [Forecasting Coccidioidomycosis (Valley Fever) in Arizona: A Graph Neural Network Approach](https://arxiv.org/abs/2507.10014)
*Ali Sarabi,Arash Sarabi,Hao Yan,Beckett Sterner,Petar Jevtić*

Key words: 山谷热, 图神经网络, 环境预测因子, 疾病预测

TL;DR: 本研究开发了首个用于预测亚利桑那州山谷热发病率的图神经网络模型，结合环境预测因子和病例数据，展示了模型的预测能力和对环境因素的洞察。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 山谷热是西南美国地区的公共卫生问题，需要有效的预测工具来支持疾病防控。

Method: 利用图神经网络（GNN）整合环境数据和病例数据，通过延迟效应捕捉疾病传播的复杂时间依赖性。

Result: 模型成功预测了山谷热的趋势，并揭示了关键的环境驱动因素。

Conclusion: 该模型可为高风险地区的早期预警系统和资源分配提供支持。

Abstract: Coccidioidomycosis, commonly known as Valley Fever, remains a significant
public health concern in endemic regions of the southwestern United States.
This study develops the first graph neural network (GNN) model for forecasting
Valley Fever incidence in Arizona. The model integrates surveillance case data
with environmental predictors using graph structures, including soil
conditions, atmospheric variables, agricultural indicators, and air quality
metrics. Our approach explores correlation-based relationships among variables
influencing disease transmission. The model captures critical delays in disease
progression through lagged effects, enhancing its capacity to reflect complex
temporal dependencies in disease ecology. Results demonstrate that the GNN
architecture effectively models Valley Fever trends and provides insights into
key environmental drivers of disease incidence. These findings can inform early
warning systems and guide resource allocation for disease prevention efforts in
high-risk areas.

</details>


### [185] [Towards Applying Large Language Models to Complement Single-Cell Foundation Models](https://arxiv.org/abs/2507.10039)
*Steven Palayew,Bo Wang,Gary Bader*

Key words: 单细胞分析, scGPT, LLMs, scMPT, 模型融合

TL;DR: 该论文探讨了LLMs（大型语言模型）在单细胞数据分析中的作用，提出了一种结合scGPT和LLMs的模型scMPT，旨在提升单细胞分析的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 单细胞基础模型（如scGPT）无法利用生物学中的文本信息，而LLMs在这一领域表现优异，但缺乏对其性能驱动因素的理解。研究旨在探索如何将两者结合以优化单细胞数据分析。

Method: 通过分析LLMs在单细胞数据中的表现驱动因素，提出scMPT模型，结合scGPT和LLMs的优势，并尝试不同的融合方法。

Result: scMPT表现优于单一模型，且在不同数据集上的性能更加稳定。实验还展示了结合专用推理模型与scGPT的潜力。

Conclusion: 研究表明LLMs可以补充单细胞基础模型，推动单细胞分析技术的发展。

Abstract: Single-cell foundation models such as scGPT represent a significant
advancement in single-cell omics, with an ability to achieve state-of-the-art
performance on various downstream biological tasks. However, these models are
inherently limited in that a vast amount of information in biology exists as
text, which they are unable to leverage. There have therefore been several
recent works that propose the use of LLMs as an alternative to single-cell
foundation models, achieving competitive results. However, there is little
understanding of what factors drive this performance, along with a strong focus
on using LLMs as an alternative, rather than complementary approach to
single-cell foundation models. In this study, we therefore investigate what
biological insights contribute toward the performance of LLMs when applied to
single-cell data, and introduce scMPT; a model which leverages synergies
between scGPT, and single-cell representations from LLMs that capture these
insights. scMPT demonstrates stronger, more consistent performance than either
of its component models, which frequently have large performance gaps between
each other across datasets. We also experiment with alternate fusion methods,
demonstrating the potential of combining specialized reasoning models with
scGPT to improve performance. This study ultimately showcases the potential for
LLMs to complement single-cell foundation models and drive improvements in
single-cell analysis.

</details>


### [186] [On the Efficiency of Training Robust Decision Trees](https://arxiv.org/abs/2507.10048)
*Benedict Gerlach,Marie Anastacio,Holger H. Hoos*

Key words: 机器学习,鲁棒训练,决策树,验证时间

TL;DR: 本文研究了训练对抗鲁棒决策树的高效流程，提出了自动选择扰动大小的算法，分析了训练时间与验证时间的关系。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着机器学习在工业中的快速应用，其可信赖性受到关注，但鲁棒训练流程的效率和可持续性仍需改进。

Method: 提出了一个三阶段流程：自动选择扰动大小并证明可用小模型估测，训练对抗鲁棒模型并评估其时间和准确性，验证模型鲁棒性。

Result: 发现验证时间与训练时间无关，对整体流程效率至关重要。

Conclusion: 通过自动化选择扰动大小和小模型估测，显著提高了训练对抗鲁棒决策树的效率。

Abstract: As machine learning gets adopted into the industry quickly, trustworthiness
is increasingly in focus. Yet, efficiency and sustainability of robust training
pipelines still have to be established. In this work, we consider a simple
pipeline for training adversarially robust decision trees and investigate the
efficiency of each step. Our pipeline consists of three stages. Firstly, we
choose the perturbation size automatically for each dataset. For that, we
introduce a simple algorithm, instead of relying on intuition or prior work.
Moreover, we show that the perturbation size can be estimated from smaller
models than the one intended for full training, and thus significant gains in
efficiency can be achieved. Secondly, we train state-of-the-art adversarial
training methods and evaluate them regarding both their training time and
adversarial accuracy. Thirdly, we certify the robustness of each of the models
thus obtained and investigate the time required for this. We find that
verification time, which is critical to the efficiency of the full pipeline, is
not correlated with training time.

</details>


### [187] [Compression Method for Deep Diagonal State Space Model Based on $H^2$ Optimal Reduction](https://arxiv.org/abs/2507.10078)
*Hiroki Sakamoto,Kazuhiro Sato*

Key words: 深度学习, 线性SSM, 模型降阶, H²控制理论, 参数缩减

TL;DR: 该研究提出了一种基于控制理论中H²模型降阶技术的高效参数缩减方法，用于深度学习中的线性SSM模型，显著减少了参数数量而不影响性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 线性SSM模型在处理序列数据时具有长程依赖捕捉能力，但参数量大限制了其在资源受限设备上的部署。

Method: 应用控制理论中的H²模型降阶技术对线性SSM组件进行参数缩减。

Result: 实验结果显示，该方法在LRA基准测试中优于现有的平衡截断法，成功将SSM参数缩减至1/32且性能无损。

Conclusion: 所提方法有效解决了线性SSM模型参数过多的问题，为资源受限设备上的部署提供了可行方案。

Abstract: Deep learning models incorporating linear SSMs have gained attention for
capturing long-range dependencies in sequential data. However, their large
parameter sizes pose challenges for deployment on resource-constrained devices.
In this study, we propose an efficient parameter reduction method for these
models by applying $H^{2}$ model order reduction techniques from control theory
to their linear SSM components. In experiments, the LRA benchmark results show
that the model compression based on our proposed method outperforms an existing
method using the Balanced Truncation, while successfully reducing the number of
parameters in the SSMs to $1/32$ without sacrificing the performance of the
original models.

</details>


### [188] [Towards High Supervised Learning Utility Training Data Generation: Data Pruning and Column Reordering](https://arxiv.org/abs/2507.10088)
*Tung Sum Thomas Kwok,Zeyong Zhang,Chi-Hua Wang,Guang Cheng*

Key words: 合成数据、监督学习、PRRO、数据剪枝、列重排序

TL;DR: 提出了一种名为PRRO的新方法，通过数据剪枝和列重排序来提升合成数据在监督学习中的性能，实验显示其显著优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 合成数据在监督学习中表现不佳，尤其是类别不平衡和数据关系未被充分建模的问题。

Method: PRRO方法结合了数据剪枝（优化信号噪声比）和列重排序（对齐数据建模结构）。

Result: 在22个公共数据集上，PRRO生成的合成数据平均提升了26.74%的性能，最高达871.46%。

Conclusion: PRRO有效提升了合成数据的实用性和类别平衡性，促进了数据合成与监督学习的无缝集成。

Abstract: Tabular data synthesis for supervised learning ('SL') model training is
gaining popularity in industries such as healthcare, finance, and retail.
Despite the progress made in tabular data generators, models trained with
synthetic data often underperform compared to those trained with original data.
This low SL utility of synthetic data stems from class imbalance exaggeration
and SL data relationship overlooked by tabular generator. To address these
challenges, we draw inspirations from techniques in emerging data-centric
artificial intelligence and elucidate Pruning and ReOrdering ('PRRO'), a novel
pipeline that integrates data-centric techniques into tabular data synthesis.
PRRO incorporates data pruning to guide the table generator towards
observations with high signal-to-noise ratio, ensuring that the class
distribution of synthetic data closely matches that of the original data.
Besides, PRRO employs a column reordering algorithm to align the data modeling
structure of generators with that of SL models. These two modules enable PRRO
to optimize SL utility of synthetic data. Empirical experiments on 22 public
datasets show that synthetic data generated using PRRO enhances predictive
performance compared to data generated without PRRO. Specifically, synthetic
replacement of original data yields an average improvement of 26.74% and up to
871.46% improvement using PRRO, while synthetic appendant to original data
results with PRRO-generated data results in an average improvement of 6.13% and
up to 200.32%. Furthermore, experiments on six highly imbalanced datasets show
that PRRO enables the generator to produce synthetic data with a class
distribution that resembles the original data more closely, achieving a
similarity improvement of 43%. Through PRRO, we foster a seamless integration
of data synthesis to subsequent SL prediction, promoting quality and accessible
data analysis.

</details>


### [189] [A Variance-Reduced Cubic-Regularized Newton for Policy Optimization](https://arxiv.org/abs/2507.10120)
*Cheng Sun,Zhen Zhang,Shaofu Yang*

Key words: 二阶策略优化, 方差减少, Hessian估计, 样本复杂度, 分布偏移

TL;DR: 提出了一种新的二阶策略优化算法VR-CR-PN，结合了Hessian辅助的方差减少技术，解决了分布偏移问题，且无需重要性采样，样本复杂度显著提升。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有二阶方法在样本复杂度上表现不佳或依赖重要性采样的不现实假设，亟需一种高效且通用的优化方法。

Method: 提出VR-CR-PN算法，结合方差减少和立方正则化策略牛顿方法，并引入新型Hessian估计器。

Result: 理论证明VR-CR-PN在一般非凸条件下的样本复杂度为$\tilde{\mathcal{O}}(\epsilon^{-3})$，优于先前结果$\tilde{\mathcal{O}}(\epsilon^{-3.5})$。

Conclusion: VR-CR-PN为二阶策略优化提供了高效且通用的解决方案，未来可进一步探索其应用潜力。

Abstract: In this paper, we study a second-order approach to policy optimization in
reinforcement learning. Existing second-order methods often suffer from
suboptimal sample complexity or rely on unrealistic assumptions about
importance sampling. To overcome these limitations, we propose VR-CR-PN, a
variance-reduced cubic-regularized policy Newton algorithm. To the best of our
knowledge, this is the first algorithm that integrates Hessian-aided variance
reduction with second-order policy optimization, effectively addressing the
distribution shift problem and achieving best-known sample complexity under
general nonconvex conditions but without the need for importance sampling. We
theoretically establish that VR-CR-PN achieves a sample complexity of
$\tilde{\mathcal{O}}(\epsilon^{-3})$ to reach an $\epsilon$-second-order
stationary point, significantly improving upon the previous best result of
$\tilde{\mathcal{O}}(\epsilon^{-3.5})$ under comparable assumptions. As an
additional contribution, we introduce a novel Hessian estimator for the
expected return function, which admits a uniform upper bound independent of the
horizon length $H$, allowing the algorithm to achieve horizon-independent
sample complexity.

</details>


### [190] [Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting](https://arxiv.org/abs/2507.10132)
*Usman Gani Joy*

Key words: 能源预测, Neural ODE, 图注意力, 小波变换, 自适应学习

TL;DR: 本文提出了一种结合神经ODE、图注意力、多分辨率小波变换和自适应频率学习的神经网络框架，用于能源供需预测，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 准确预测能源供需是优化可持续能源系统的关键，但由于可再生能源的可变性和动态消费模式，这一任务极具挑战性。

Method: 模型结合神经ODE、图注意力、小波变换和自适应学习，使用Runge-Kutta方法和残差连接捕获时空模式。

Result: 在多个数据集上测试表明，该模型在预测性能上优于现有方法，且通过SHAP分析增强了可解释性。

Conclusion: 该框架对复杂时间依赖性的建模能力及其可解释性，使其适合可持续能源应用。

Abstract: Accurate forecasting of energy demand and supply is critical for optimizing
sustainable energy systems, yet it is challenged by the variability of
renewable sources and dynamic consumption patterns. This paper introduces a
neural framework that integrates continuous-time Neural Ordinary Differential
Equations (Neural ODEs), graph attention, multi-resolution wavelet
transformations, and adaptive learning of frequencies to address the issues of
time series prediction. The model employs a robust ODE solver, using the
Runge-Kutta method, paired with graph-based attention and residual connections
to better understand both structural and temporal patterns. Through
wavelet-based feature extraction and adaptive frequency modulation, it adeptly
captures and models diverse, multi-scale temporal dynamics. When evaluated
across seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity
transformer temperature), and Waste, Solar, and Hydro (renewable energy), this
architecture consistently outperforms state-of-the-art baselines in various
forecasting metrics, proving its robustness in capturing complex temporal
dependencies. Furthermore, the model enhances interpretability through SHAP
analysis, making it suitable for sustainable energy applications.

</details>


### [191] [MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping](https://arxiv.org/abs/2507.10158)
*Obaidullah Zaland,Erik Elmroth,Monowar Bhuyan*

Key words: 联邦学习, 机器人抓取, 非独立同分布数据, MTF-Grasp

TL;DR: MTF-Grasp 是一种多层级联邦学习方法，用于解决机器人抓取任务中非独立同分布数据导致的性能下降问题，通过选择数据质量和数量较高的顶级机器人来提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 机器人抓取任务在联邦学习环境中缺乏探索，由于数据的非独立同分布和低数量问题，导致模型性能下降。

Method: 提出 MTF-Grasp 方法，选择数据质量高且数量多的顶级机器人训练种子模型，再将这些模型分发给低级机器人。

Result: 在 Cornell 和 Jacquard 抓取数据集上，MTF-Grasp 比传统联邦学习方法性能提升高达 8%。

Conclusion: MTF-Grasp 有效缓解了非独立同分布数据对机器人抓取任务的影响，提升了模型性能。

Abstract: Federated Learning (FL) is a promising machine learning paradigm that enables
participating devices to train privacy-preserved and collaborative models. FL
has proven its benefits for robotic manipulation tasks. However, grasping tasks
lack exploration in such settings where robots train a global model without
moving data and ensuring data privacy. The main challenge is that each robot
learns from data that is nonindependent and identically distributed (non-IID)
and of low quantity. This exhibits performance degradation, particularly in
robotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL
approach for robotic grasping, acknowledging the unique challenges posed by the
non-IID data distribution across robots, including quantitative skewness.
MTF-Grasp harnesses data quality and quantity across robots to select a set of
"top-level" robots with better data distribution and higher sample count. It
then utilizes top-level robots to train initial seed models and distribute them
to the remaining "low-level" robots, reducing the risk of model performance
degradation in low-level robots. Our approach outperforms the conventional FL
setup by up to 8% on the quantity-skewed Cornell and Jacquard grasping
datasets.

</details>


### [192] [Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation](https://arxiv.org/abs/2507.10160)
*Manuel Röder,Christoph Raab,Frank-Michael Schleif*

Key words: 联邦学习, 域适配, 资源受限, 流式数据, 非静态环境

TL;DR: FedAcross+是一个高效的联邦学习框架，针对工业环境中的实际客户端适配问题，通过冻结预训练模型的骨干和分类器，仅调整域自适应线性层，降低计算开销，并支持流式数据和非静态环境。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决联邦学习中昂贵的数据标注需求、客户端数据采集的协变量偏移以及资源受限环境下模型更新的不实用性。

Method: 利用预训练模型（包括深度骨干、适配模块和分类器），在客户端适配时冻结骨干和分类器，仅调整域自适应线性层，并扩展至流式数据处理。

Result: 实验证明FedAcross+在低端设备上能以少量目标样本实现竞争性适配，有效应对域偏移问题。

Conclusion: FedAcross+框架适用于资源受限环境，支持零星的模型更新，实现无缝部署。

Abstract: Federated Learning has emerged as a leading paradigm for decentralized,
privacy-preserving learning, particularly relevant in the era of interconnected
edge devices equipped with sensors. However, the practical implementation of
Federated Learning faces three primary challenges: the need for human
involvement in costly data labelling processes for target adaptation, covariate
shift in client device data collection due to environmental factors affecting
sensors, leading to discrepancies between source and target samples, and the
impracticality of continuous or regular model updates in resource-constrained
environments due to limited data transmission capabilities and technical
constraints on channel availability and energy efficiency. To tackle these
issues, we expand upon an efficient and scalable Federated Learning framework
tailored for real-world client adaptation in industrial settings. This
framework leverages a pre-trained source model comprising a deep backbone, an
adaptation module, and a classifier running on a powerful server. By freezing
the backbone and classifier during client adaptation on resource-constrained
devices, we allow the domain adaptive linear layer to handle target domain
adaptation, thus minimizing overall computational overhead. Furthermore, this
setup, designated as FedAcross+, is extended to encompass the processing of
streaming data, thereby rendering the solution suitable for non-stationary
environments. Extensive experimental results demonstrate the effectiveness of
FedAcross+ in achieving competitive adaptation on low-end client devices with
limited target samples, successfully addressing the challenge of domain shift.
Moreover, our framework accommodates sporadic model updates within
resource-constrained environments, ensuring practical and seamless deployment.

</details>


### [193] [Understanding the Rank of Tensor Networks via an Intuitive Example-Driven Approach](https://arxiv.org/abs/2507.10170)
*Wuyang Zhou,Giorgos Iacovides,Kriton Konstantinidis,Ilya Kisil,Danilo Mandic*

Key words: 张量网络, 张量秩, 数据分解, 领域知识, 图形化方法

TL;DR: 这篇论文旨在通过实际案例和直观图示，澄清张量网络分解中关键的张量秩概念，帮助读者理解其选择和解释。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 张量秩在张量网络分解中至关重要，但因其缺乏统一的意义和直观解释，常被视为经验调参的参数而非基于领域知识的设计参数。

Method: 通过领域知识指导张量秩的选择，并采用图形化方法解释复杂张量结构中的秩概念，揭示其与矩阵秩的关系。

Result: 提供了一种直观的工具，帮助读者更好地理解和选择张量秩，支持张量方法在实践和教育中的应用。

Conclusion: 论文希望读者能够清晰统一地理解张量秩，并在实际应用中更好地选择和解释张量方法。

Abstract: Tensor Network (TN) decompositions have emerged as an indispensable tool in
Big Data analytics owing to their ability to provide compact low-rank
representations, thus alleviating the ``Curse of Dimensionality'' inherent in
handling higher-order data. At the heart of their success lies the concept of
TN ranks, which governs the efficiency and expressivity of TN decompositions.
However, unlike matrix ranks, TN ranks often lack a universal meaning and an
intuitive interpretation, with their properties varying significantly across
different TN structures. Consequently, TN ranks are frequently treated as
empirically tuned hyperparameters, rather than as key design parameters
inferred from domain knowledge. The aim of this Lecture Note is therefore to
demystify the foundational yet frequently misunderstood concept of TN ranks
through real-life examples and intuitive visualizations. We begin by
illustrating how domain knowledge can guide the selection of TN ranks in
widely-used models such as the Canonical Polyadic (CP) and Tucker
decompositions. For more complex TN structures, we employ a self-explanatory
graphical approach that generalizes to tensors of arbitrary order. Such a
perspective naturally reveals the relationship between TN ranks and the
corresponding ranks of tensor unfoldings (matrices), thereby circumventing
cumbersome multi-index tensor algebra while facilitating domain-informed TN
design. It is our hope that this Lecture Note will equip readers with a clear
and unified understanding of the concept of TN rank, along with the necessary
physical insight and intuition to support the selection, explainability, and
deployment of tensor methods in both practical applications and educational
contexts.

</details>


### [194] [Play Style Identification Using Low-Level Representations of Play Traces in MicroRTS](https://arxiv.org/abs/2507.10172)
*Ruizhe Yu Xia,Jeremy Gow,Simon Lucas*

Key words: 游戏风格识别，CNN-LSTM自编码器，MicroRTS，无监督学习

TL;DR: 该研究探索了使用无监督CNN-LSTM自编码器模型从低级游戏轨迹数据中获取潜在表示，以减少对领域专业知识的依赖。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 通过识别游戏风格为游戏设计提供见解并实现自适应体验，同时提升游戏代理的性能。

Method: 采用无监督CNN-LSTM自编码器模型直接从MicroRTS的低级游戏轨迹数据中学习潜在表示。

Result: 模型在潜在空间中实现了对不同游戏代理的有意义分离，降低了领域专业知识的需求及其相关偏见。

Conclusion: 该方法可用于指导探索多样化的游戏风格，减少对领域抽象的依赖。

Abstract: Play style identification can provide valuable game design insights and
enable adaptive experiences, with the potential to improve game playing agents.
Previous work relies on domain knowledge to construct play trace
representations using handcrafted features. More recent approaches incorporate
the sequential structure of play traces but still require some level of domain
abstraction. In this study, we explore the use of unsupervised CNN-LSTM
autoencoder models to obtain latent representations directly from low-level
play trace data in MicroRTS. We demonstrate that this approach yields a
meaningful separation of different game playing agents in the latent space,
reducing reliance on domain expertise and its associated biases. This latent
space is then used to guide the exploration of diverse play styles within
studied AI players.

</details>


### [195] [T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs](https://arxiv.org/abs/2507.10183)
*Alireza Dizaji,Benedict Aaron Tjandra,Mehrab Hamidi,Shenyang Huang,Guillaume Rabusseau*

Key words: 动态图学习,时态图神经网络,基准测试,时态推理

TL;DR: 提出了一种名为T-GRAB的新基准测试，用于系统评估时态图神经网络（TGNNs）捕捉周期性、因果性和长程依赖等核心时态模式的能力。实验揭示了当前模型的局限性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有时态图神经网络（TGNNs）在捕捉核心时态模式（如周期性、因果性和长程依赖）方面的能力尚不明确，需要系统评估。

Method: 设计了T-GRAB基准测试，包含合成任务以隔离关键时态能力，并评估了11种时态图学习方法。

Result: 实验结果显示当前TGNNs在通用时态模式能力上存在明显不足，尤其是对周期性、因果性和长程依赖的捕捉。

Conclusion: T-GRAB揭示了传统基准测试未发现的模型局限，为开发更具时态推理能力的架构提供了方向。

Abstract: Dynamic graph learning methods have recently emerged as powerful tools for
modelling relational data evolving through time. However, despite extensive
benchmarking efforts, it remains unclear whether current Temporal Graph Neural
Networks (TGNNs) effectively capture core temporal patterns such as
periodicity, cause-and-effect, and long-range dependencies. In this work, we
introduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set
of synthetic tasks designed to systematically probe the capabilities of TGNNs
to reason across time. T-GRAB provides controlled, interpretable tasks that
isolate key temporal skills: counting/memorizing periodic repetitions,
inferring delayed causal effects, and capturing long-range dependencies over
both spatial and temporal dimensions. We evaluate 11 temporal graph learning
methods on these tasks, revealing fundamental shortcomings in their ability to
generalize temporal patterns. Our findings offer actionable insights into the
limitations of current models, highlight challenges hidden by traditional
real-world benchmarks, and motivate the development of architectures with
stronger temporal reasoning abilities. The code for T-GRAB can be found at:
https://github.com/alirezadizaji/T-GRAB.

</details>


### [196] [Learning Private Representations through Entropy-based Adversarial Training](https://arxiv.org/abs/2507.10194)
*Tassilo Klein,Moin Nabi*

Key words: 对抗性学习，隐私保护，焦点熵，表示学习

TL;DR: 一种对抗性表示学习方法，通过引入焦点熵来保护用户隐私，同时在多个基准测试中展示高预测效用。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在保护用户隐私的同时学习具有高预测能力的表示。

Method: 采用对抗性表示学习方法，提出焦点熵以减少信息泄漏。

Result: 在多个基准测试中表现良好，预测效用高且隐私泄漏适中。

Conclusion: 焦点熵方法有效平衡了隐私保护与预测效用。

Abstract: How can we learn a representation with high predictive power while preserving
user privacy? We present an adversarial representation learning method for
sanitizing sensitive content from the learned representation. Specifically, we
introduce a variant of entropy - focal entropy, which mitigates the potential
information leakage of the existing entropy-based approaches. We showcase
feasibility on multiple benchmarks. The results suggest high target utility at
moderate privacy leakage.

</details>


### [197] [A Graph Sufficiency Perspective for Neural Networks](https://arxiv.org/abs/2507.10215)
*Cencheng Shen,Yuexiao Dong*

Key words: 神经网络, 统计充分性, 图变量, 深度学习, 无限宽度

TL;DR: 本文通过图变量和统计充分性分析神经网络，将神经网络层解释为基于图的变换，并证明在无限宽度限制下，层输出是层输入的充分统计量，且在有限宽度网络中也可实现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为神经网络提供新的统计理解，通过统计充分性和图论表示来重新解释其工作原理。

Method: 将神经网络层建模为基于图的变换，神经单元作为输入和学习锚点间的成对函数。通过密集锚点假设，证明无限宽度下的充分性，并在有限宽度网络中设计区域分离输入分布和锚点。

Result: 理论证明在无限宽度限制下，层输出是输入的充分统计量，且训练过程中能保持充分性。有限宽度网络通过特定条件也能实现充分性。

Conclusion: 本文为神经网络提供了一种统计和图形理论的新视角，揭示了其在统计充分性方面的潜力。

Abstract: This paper analyzes neural networks through graph variables and statistical
sufficiency. We interpret neural network layers as graph-based transformations,
where neurons act as pairwise functions between inputs and learned anchor
points. Within this formulation, we establish conditions under which layer
outputs are sufficient for the layer inputs, that is, each layer preserves the
conditional distribution of the target variable given the input variable. Under
dense anchor point assumptions, we prove that asymptotic sufficiency holds in
the infinite-width limit and is preserved throughout training. To align more
closely with practical architectures, we further show that sufficiency can be
achieved with finite-width networks by assuming region-separated input
distributions and constructing appropriate anchor points. Our framework covers
fully connected layers, general pairwise functions, ReLU and sigmoid
activations, and convolutional neural networks. This work bridges statistical
sufficiency, graph-theoretic representations, and deep learning, providing a
new statistical understanding of neural networks.

</details>


### [198] [Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with Sharp Gradients](https://arxiv.org/abs/2507.10241)
*Vikas Dwivedi,Balaji Srinivasan,Monica Sigovan,Bruno Sixou*

Key words: KAPI-ELM, PI-ELM, PDE, 贝叶斯优化, 尖锐梯度

TL;DR: KAPI-ELM是一种基于RBF的PI-ELM扩展方法，通过轻量级贝叶斯优化框架解决PDE中的局部尖锐梯度问题，兼具速度与表现力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统PI-ELM虽速度快，但固定输入层限制了其在尖锐梯度问题中的表现，因此需要优化方法提升适应性。

Method: 结合贝叶斯优化（BO）优化输入层分布参数与最小二乘法优化输出层参数，提升模型的适应性与表现力。

Result: 在多个PDE基准测试中表现出色，尤其在刚性问题中精度超过或匹配其他先进方法，且参数更少。

Conclusion: KAPI-ELM是一种高效、可扩展且通用的物理知情学习框架，适用于刚性问题。

Abstract: This paper introduces the Kernel Adaptive Physics-Informed Extreme Learning
Machine (KAPI-ELM), an adaptive Radial Basis Function (RBF)-based extension of
PI-ELM designed to solve both forward and inverse Partial Differential Equation
(PDE) problems involving localized sharp gradients. While PI-ELMs outperform
the traditional Physics-Informed Neural Networks (PINNs) in speed due to their
single-shot, least square optimization, this advantage comes at a cost: their
fixed, randomly initialized input layer limits their ability to capture sharp
gradients. To overcome this limitation, we introduce a lightweight Bayesian
Optimization (BO) framework that, instead of adjusting each input layer
parameter individually as in traditional backpropagation, learns a small set of
hyperparameters defining the statistical distribution from which the input
weights are drawn. This novel distributional optimization strategy -- combining
BO for input layer distributional parameters with least-squares optimization
for output layer network parameters -- enables KAPI-ELM to preserve PI-ELM's
speed while matching or exceeding the expressiveness of PINNs. We validate the
proposed methodology on several challenging forward and inverse PDE benchmarks,
including a 1D singularly perturbed convection-diffusion equation, a 2D Poisson
equation with sharp localized sources, and a time-dependent advection equation.
Notably, KAPI-ELM achieves state-of-the-art accuracy in both forward and
inverse settings. In stiff PDE regimes, it matches or even outperforms advanced
methods such as the Extended Theory of Functional Connections (XTFC), while
requiring nearly an order of magnitude fewer tunable parameters. These results
establish the potential of KAPI-ELM as a scalable, interpretable, and
generalizable physics-informed learning framework, especially in stiff PDE
regimes.

</details>


### [199] [Conditional Chemical Language Models are Versatile Tools in Drug Discovery](https://arxiv.org/abs/2507.10273)
*Lu Zhu,Emmanuel Noutahi*

Key words: 生成化学语言模型,药物发现,条件生成,SAFE-T,虚拟筛选

TL;DR: SAFE-T是一种基于生物背景的化学建模框架，通过条件生成分子序列来优化分子设计和评估，显著提升了早期药物发现的效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 生成化学语言模型（CLMs）在分子设计中表现出色，但其在药物发现中的应用受限，缺乏可靠的奖励信号和输出可解释性。

Method: 提出了SAFE-T框架，通过条件建模片段分子序列的似然性，支持虚拟筛选、药物-靶标相互作用预测等任务，并进行目标导向的分子生成。

Result: 在多个预测和生成基准测试中，SAFE-T表现优于或与现有方法相当，同时计算速度更快，并能捕捉已知的结构-活性关系。

Conclusion: SAFE-T展示了条件生成CLMs在统一评分和生成任务中的潜力，可加速早期药物发现。

Abstract: Generative chemical language models (CLMs) have demonstrated strong
capabilities in molecular design, yet their impact in drug discovery remains
limited by the absence of reliable reward signals and the lack of
interpretability in their outputs. We present SAFE-T, a generalist chemical
modeling framework that conditions on biological context -- such as protein
targets or mechanisms of action -- to prioritize and design molecules without
relying on structural information or engineered scoring functions. SAFE-T
models the conditional likelihood of fragment-based molecular sequences given a
biological prompt, enabling principled scoring of molecules across tasks such
as virtual screening, drug-target interaction prediction, and activity cliff
detection. Moreover, it supports goal-directed generation by sampling from this
learned distribution, aligning molecular design with biological objectives. In
comprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA,
ACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves
performance comparable to or better than existing approaches while being
significantly faster. Fragment-level attribution further reveals that SAFE-T
captures known structure-activity relationships, supporting interpretable and
biologically grounded design. Together with its computational efficiency, these
results demonstrate that conditional generative CLMs can unify scoring and
generation to accelerate early-stage drug discovery.

</details>


### [200] [Average Sensitivity of Hierarchical $k$-Median Clustering](https://arxiv.org/abs/2507.10296)
*Shijie Li,Weiqiang He,Ruobing Bai,Pan Peng*

Key words: 层次聚类, k-Median, 平均敏感性, 鲁棒性

TL;DR: 本文研究了层次k-Median聚类问题的平均敏感性，并提出了一种高效算法，证明了其低平均敏感性和高聚类质量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于现代算法处理的数据集通常大且动态，如果层次聚类对数据的小扰动敏感，算法的可用性会大幅降低。

Method: 研究了层次k-Median聚类问题，分析了算法的平均敏感性，并提出了一种高效算法。

Result: 提出的算法具有低平均敏感性和高聚类质量，而单链接聚类和CLNSS算法的确定性变体则表现出高平均敏感性。

Conclusion: 实验验证了所提算法的鲁棒性和有效性。

Abstract: Hierarchical clustering is a widely used method for unsupervised learning
with numerous applications. However, in the application of modern algorithms,
the datasets studied are usually large and dynamic. If the hierarchical
clustering is sensitive to small perturbations of the dataset, the usability of
the algorithm will be greatly reduced. In this paper, we focus on the
hierarchical $k$ -median clustering problem, which bridges hierarchical and
centroid-based clustering while offering theoretical appeal, practical utility,
and improved interpretability. We analyze the average sensitivity of algorithms
for this problem by measuring the expected change in the output when a random
data point is deleted. We propose an efficient algorithm for hierarchical
$k$-median clustering and theoretically prove its low average sensitivity and
high clustering quality. Additionally, we show that single linkage clustering
and a deterministic variant of the CLNSS algorithm exhibit high average
sensitivity, making them less stable. Finally, we validate the robustness and
effectiveness of our algorithm through experiments.

</details>


### [201] [Recognizing Dementia from Neuropsychological Tests with State Space Models](https://arxiv.org/abs/2507.10311)
*Liming Wang,Saurabhchand Bhati,Cody Karjadi,Rhoda Au,James Glass*

Key words: 早期检测,痴呆分类,状态空间模型,语音分析

TL;DR: Demenba是一种基于状态空间模型的自动痴呆分类框架，能够通过语音记录推断认知衰退。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 痴呆的早期检测对及时医疗干预和改善患者预后至关重要，而传统神经心理学测试依赖手动评分。

Method: 提出Demenba框架，利用状态空间模型线性扩展记忆和计算量，结合大规模语言模型提升性能。

Result: 在Framingham Heart Study数据集上，Demenba比现有方法性能提升21%，且参数更少。

Conclusion: Demenba为痴呆评估工具提供了透明且可扩展的解决方案。

Abstract: Early detection of dementia is critical for timely medical intervention and
improved patient outcomes. Neuropsychological tests are widely used for
cognitive assessment but have traditionally relied on manual scoring. Automatic
dementia classification (ADC) systems aim to infer cognitive decline directly
from speech recordings of such tests. We propose Demenba, a novel ADC framework
based on state space models, which scale linearly in memory and computation
with sequence length. Trained on over 1,000 hours of cognitive assessments
administered to Framingham Heart Study participants, some of whom were
diagnosed with dementia through adjudicated review, our method outperforms
prior approaches in fine-grained dementia classification by 21\%, while using
fewer parameters. We further analyze its scaling behavior and demonstrate that
our model gains additional improvement when fused with large language models,
paving the way for more transparent and scalable dementia assessment tools.
Code: https://anonymous.4open.science/r/Demenba-0861

</details>


### [202] [Convergence of Agnostic Federated Averaging](https://arxiv.org/abs/2507.10325)
*Herlock,Rahimi,Dionysis Kalogerias*

Key words: 联邦学习, FedAvg, 随机客户端参与, 收敛性分析, 非均匀分布

TL;DR: 该论文分析了联邦学习（FL）中客户端参与不稳定和未知偏置的挑战，首次针对非均匀随机客户端参与下的FedAvg算法提供了收敛性证明，并展示了其优于加权聚合方法的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究联邦学习在客户端参与不稳定且未知偏置的情况下的优化问题，填补现有收敛性分析假设与实际部署不符的空白。

Method: 针对随机且变大小的客户端参与，理论分析了“无偏FedAvg”算法的收敛性，适用于凸且可能非光滑的损失函数。

Result: 证明了在非均匀随机客户端参与下，FedAvg的收敛速率为$\mathcal{O}(1/\sqrt{T})$，并展示了其优于加权聚合的性能。

Conclusion: 无偏FedAvg在未知客户端参与分布的情况下仍能有效收敛，且优于已知权重的加权聚合方法。

Abstract: Federated learning (FL) enables decentralized model training without
centralizing raw data. However, practical FL deployments often face a key
realistic challenge: Clients participate intermittently in server aggregation
and with unknown, possibly biased participation probabilities. Most existing
convergence results either assume full-device participation, or rely on
knowledge of (in fact uniform) client availability distributions -- assumptions
that rarely hold in practice. In this work, we characterize the optimization
problem that consistently adheres to the stochastic dynamics of the well-known
\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and
variably-sized) client availability, and rigorously establish its convergence
for convex, possibly nonsmooth losses, achieving a standard rate of order
$\mathcal{O}(1/\sqrt{T})$, where $T$ denotes the aggregation horizon. Our
analysis provides the first convergence guarantees for agnostic FedAvg under
general, non-uniform, stochastic client participation, without knowledge of the
participation distribution. We also empirically demonstrate that agnostic
FedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg
variants, even with server-side knowledge of participation weights.

</details>


### [203] [MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of Imputation Methods for IMU-based Motion Capture Data](https://arxiv.org/abs/2507.10334)
*Mahmoud Bekhit,Ahmad Salah,Ahmed Salim Alrawahi,Tarek Attia,Ahmed Ali,Esraa Eldesokey,Ahmed Fathalla*

Key words: 运动捕捉、缺失数据填补、多变量分析、GAN、IMU

TL;DR: 该论文通过比较统计、机器学习和深度学习方法，填补了IMU运动捕捉数据缺失值填补领域的空白，并提出首个公开数据集。多变量方法在复杂缺失情况下表现显著优于单变量方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 运动捕捉数据中的缺失值限制了其应用，目前缺乏对IMU数据填补方法的系统性评估，本研究旨在填补这一空白并提供实用建议。

Method: 研究比较了统计、机器学习和深度学习方法，包括生成对抗填补网络（GAIN）和迭代填补器，并考虑了单变量和多变量场景。

Result: 多变量方法在复杂缺失情况下优于单变量方法，平均绝对误差降低50%（从10.8降至5.8）。

Conclusion: 多变量框架（如GAIN）在复杂缺失情况下表现最佳，为未来研究提供了基准和实用建议。

Abstract: Motion capture (MoCap) data from wearable Inertial Measurement Units (IMUs)
is vital for applications in sports science, but its utility is often
compromised by missing data. Despite numerous imputation techniques, a
systematic performance evaluation for IMU-derived MoCap time-series data is
lacking. We address this gap by conducting a comprehensive comparative analysis
of statistical, machine learning, and deep learning imputation methods. Our
evaluation considers three distinct contexts: univariate time-series,
multivariate across subjects, and multivariate across kinematic angles. To
facilitate this benchmark, we introduce the first publicly available MoCap
dataset designed specifically for imputation, featuring data from 53 karate
practitioners. We simulate three controlled missingness mechanisms: missing
completely at random (MCAR), block missingness, and a novel value-dependent
pattern at signal transition points. Our experiments, conducted on 39 kinematic
variables across all subjects, reveal that multivariate imputation frameworks
consistently outperform univariate approaches, particularly for complex
missingness. For instance, multivariate methods achieve up to a 50% mean
absolute error reduction (MAE from 10.8 to 5.8) compared to univariate
techniques for transition point missingness. Advanced models like Generative
Adversarial Imputation Networks (GAIN) and Iterative Imputers demonstrate the
highest accuracy in these challenging scenarios. This work provides a critical
baseline for future research and offers practical recommendations for improving
the integrity and robustness of Mo-Cap data analysis.

</details>


### [204] [Some Super-approximation Rates of ReLU Neural Networks for Korobov Functions](https://arxiv.org/abs/2507.10345)
*Yuwen Li,Guozhi Zhang*

Key words: ReLU神经网络, Korobov函数, $L_p$范数, $W^1_p$范数, 超逼近误差

TL;DR: 论文研究了ReLU神经网络对Korobov函数的$L_p$和$W^1_p$范数近似误差，提出了一种基于稀疏网格有限元和位提取技术的分析方法，得到了接近最优的超逼近误差界。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索ReLU神经网络在高维空间中的逼近能力，克服传统方法中维度灾难的问题，提供更优的误差界限。

Method: 利用稀疏网格有限元和位提取技术，分析网络宽度和深度对逼近误差的影响。

Result: 在$L_p$范数中得到了$2m$阶的超逼近误差界，在$W^1_p$范数中得到了$2m-2$阶的超逼近误差界。

Conclusion: ReLU神经网络的表达能力在高维空间中表现优异，受维度灾难的影响较小。

Abstract: This paper examines the $L_p$ and $W^1_p$ norm approximation errors of ReLU
neural networks for Korobov functions. In terms of network width and depth, we
derive nearly optimal super-approximation error bounds of order $2m$ in the
$L_p$ norm and order $2m-2$ in the $W^1_p$ norm, for target functions with
$L_p$ mixed derivative of order $m$ in each direction. The analysis leverages
sparse grid finite elements and the bit extraction technique. Our results
improve upon classical lowest order $L_\infty$ and $H^1$ norm error bounds and
demonstrate that the expressivity of neural networks is largely unaffected by
the curse of dimensionality.

</details>


### [205] [Parallel Sampling of Diffusion Models on $SO(3)$](https://arxiv.org/abs/2507.10347)
*Yan-Ting Chen,Hao-Wei Chen,Tsu-Ching Hsiao,Chun-Yi Lee*

Key words: SO(3)流形,扩散模型,加速算法,Picard迭代,姿态模糊

TL;DR: 提出一种在SO(3)流形上加速扩散过程的算法，无需任务奖励损失即可实现4.9倍的加速。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 扩散模型固有的顺序性导致去噪时间过长，影响效率，需改进。

Method: 采用数值Picard迭代方法适配SO(3)空间，加速扩散过程。

Result: 实验显示算法最高提速4.9倍，单样本生成延迟显著降低。

Conclusion: 方法有效加速了SO(3)上的扩散过程，且不降低任务性能。

Abstract: In this paper, we design an algorithm to accelerate the diffusion process on
the $SO(3)$ manifold. The inherently sequential nature of diffusion models
necessitates substantial time for denoising perturbed data. To overcome this
limitation, we proposed to adapt the numerical Picard iteration for the $SO(3)$
space. We demonstrate our algorithm on an existing method that employs
diffusion models to address the pose ambiguity problem. Moreover, we show that
this acceleration advantage occurs without any measurable degradation in task
reward. The experiments reveal that our algorithm achieves a speed-up of up to
4.9$\times$, significantly reducing the latency for generating a single sample.

</details>


### [206] [Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning](https://arxiv.org/abs/2507.10348)
*Yichen Li*

Key words: 异构联邦学习, 特征蒸馏, 正交投影, 知识偏差, FedFD

TL;DR: 论文提出了一种名为FedFD的特征蒸馏方法，用于解决模型异构联邦学习中的知识偏差问题，通过正交投影对齐特征，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的异构联邦学习方法主要依赖logit蒸馏，但无法解决异构模型带来的知识偏差，导致训练不稳定和性能不佳。

Method: 提出FedFD方法，利用特征蒸馏和正交投影技术，为每个客户端模型架构维护投影层以对齐特征，从而更好地整合异构模型知识。

Result: 实验表明，FedFD在性能上优于现有方法。

Conclusion: FedFD通过特征对齐和正交投影，有效解决了异构模型的知识偏差问题，提升了联邦学习的稳定性和性能。

Abstract: Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing
attention for its ability to aggregate knowledge from heterogeneous models
while keeping private data locally. To better aggregate knowledge from clients,
ensemble distillation, as a widely used and effective technique, is often
employed after global aggregation to enhance the performance of the global
model. However, simply combining Hetero-FL and ensemble distillation does not
always yield promising results and can make the training process unstable. The
reason is that existing methods primarily focus on logit distillation, which,
while being model-agnostic with softmax predictions, fails to compensate for
the knowledge bias arising from heterogeneous models. To tackle this challenge,
we propose a stable and efficient Feature Distillation for model-heterogeneous
Federated learning, dubbed FedFD, that can incorporate aligned feature
information via orthogonal projection to integrate knowledge from heterogeneous
models better. Specifically, a new feature-based ensemble federated knowledge
distillation paradigm is proposed. The global model on the server needs to
maintain a projection layer for each client-side model architecture to align
the features separately. Orthogonal techniques are employed to re-parameterize
the projection layer to mitigate knowledge bias from heterogeneous models and
thus maximize the distilled knowledge. Extensive experiments show that FedFD
achieves superior performance compared to state-of-the-art methods.

</details>


### [207] [TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting](https://arxiv.org/abs/2507.10349)
*Zhiyuan Zhao,Sitan Yang,Kin G. Olivares,Boris N. Oreshkin,Stan Vitebsky,Michael W. Mahoney,B. Aditya Prakash,Dmitry Efimov*

Key words: 需求预测,多时间序列预测,Transformer,高峰事件,供应链管理

TL;DR: 提出了一种名为Temporal-Aligned Transformer (TAT)的多时间序列预测模型，用于提升高峰需求预测准确性，实验表明其在高峰需求预测上性能优于现有方法30%。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 需求预测对供应链管理至关重要，尤其是在高峰销售事件中，传统的预测方法难以准确预测高峰需求。

Method: 提出TAT模型，包含编码器和解码器，嵌入创新的Temporal Alignment Attention (TAA)机制，利用已知的上下文变量（如节假日和促销信息）提升预测性能。

Result: 在两个大型电商数据集上测试，TAT在高峰需求预测上带来30%的准确率提升，同时整体性能与现有方法相当。

Conclusion: TAT通过上下文对齐机制显著提升了高峰事件的需求预测准确性，为供应链管理提供了更可靠的解决方案。

Abstract: Multi-horizon time series forecasting has many practical applications such as
demand forecasting. Accurate demand prediction is critical to help make buying
and inventory decisions for supply chain management of e-commerce and physical
retailers, and such predictions are typically required for future horizons
extending tens of weeks. This is especially challenging during high-stake sales
events when demand peaks are particularly difficult to predict accurately.
However, these events are important not only for managing supply chain
operations but also for ensuring a seamless shopping experience for customers.
To address this challenge, we propose Temporal-Aligned Transformer (TAT), a
multi-horizon forecaster leveraging apriori-known context variables such as
holiday and promotion events information for improving predictive performance.
Our model consists of an encoder and decoder, both embedded with a novel
Temporal Alignment Attention (TAA), designed to learn context-dependent
alignment for peak demand forecasting. We conduct extensive empirical analysis
on two large-scale proprietary datasets from a large e-commerce retailer. We
demonstrate that TAT brings up to 30% accuracy improvement on peak demand
forecasting while maintaining competitive overall performance compared to other
state-of-the-art methods.

</details>


### [208] [Enhanced DeepONet for 1-D consolidation operator learning: an architectural investigation](https://arxiv.org/abs/2507.10368)
*Yongjin Choi,Chenying Liu,Jorge Macedo*

Key words: DeepONets, 岩土工程, 替代模型, 一维固结, 科学机器学习

TL;DR: 本研究系统地评估了几种DeepONet架构在一维固结问题中的表现，提出了一种改进的Trunknet Fourier feature-enhanced DeepONet（Model 4），其性能优于标准架构，计算效率提升了1.5到100倍，展示了DeepONets在岩土工程中的潜力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: DeepONets在PDE控制系统中的算子学习领域表现优异，但其在岩土工程中的应用较少。本研究旨在填补这一空白，探索DeepONets在岩土工程中的适用性。

Method: 研究了三种DeepONet架构：标准DeepONet（Model 1和2）、物理启发架构（Model 3），并提出了一种改进的Trunknet Fourier feature-enhanced DeepONet（Model 4）以解决快速变化函数的捕捉问题。

Result: Model 4在捕捉快速变化函数时表现最佳，计算效率比传统显式和隐式求解器高出1.5到100倍。

Conclusion: DeepONets在岩土工程中具有高效的替代建模潜力，为科学机器学习在岩土领域的进一步应用奠定了基础。

Abstract: Deep Operator Networks (DeepONets) have emerged as a powerful surrogate
modeling framework for learning solution operators in PDE-governed systems.
While their use is expanding across engineering disciplines, applications in
geotechnical engineering remain limited. This study systematically evaluates
several DeepONet architectures for the one-dimensional consolidation problem.
We initially consider three architectures: a standard DeepONet with the
coefficient of consolidation embedded in the branch net (Models 1 and 2), and a
physics-inspired architecture with the coefficient embedded in the trunk net
(Model 3). Results show that Model 3 outperforms the standard configurations
(Models 1 and 2) but still has limitations when the target solution (excess
pore pressures) exhibits significant variation. To overcome this limitation, we
propose a Trunknet Fourier feature-enhanced DeepONet (Model 4) that addresses
the identified limitations by capturing rapidly varying functions. All proposed
architectures achieve speedups ranging from 1.5 to 100 times over traditional
explicit and implicit solvers, with Model 4 being the most efficient. Larger
computational savings are expected for more complex systems than the explored
1D case, which is promising. Overall, the study highlights the potential of
DeepONets to enable efficient, generalizable surrogate modeling in geotechnical
applications, advancing the integration of scientific machine learning in
geotechnics, which is at an early stage.

</details>


### [209] [Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis](https://arxiv.org/abs/2507.10382)
*Yue Ding,Conor McCarthy,Kevin O'Shea,Mingming Liu*

Key words: 共享电动车；云技术；LLM；RAG框架；路线推荐

TL;DR: 论文提出了一种基于云和LLM的共享电动车平台，集成了移动应用进行个性化路线推荐，并通过优化模块和RAG框架进行了性能评估。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着智能出行和共享电动车服务的兴起，结合云技术与LLM提供全面的端到端解决方案变得至关重要。

Method: 开发了一个云基LLM驱动的共享电动车平台，包含移动应用和RAG框架，并针对不同场景优化了路线推荐。

Result: 优化模块在旅行时间和成本上表现良好，RAG框架在系统操作员和用户查询上的平均执行准确率分别为0.81和0.98。

Conclusion: 该平台展示了对智能出行需求的响应能力，特别是在个性化路线推荐上表现优异。

Abstract: With the rise of smart mobility and shared e-mobility services, numerous
advanced technologies have been applied to this field. Cloud-based traffic
simulation solutions have flourished, offering increasingly realistic
representations of the evolving mobility landscape. LLMs have emerged as
pioneering tools, providing robust support for various applications, including
intelligent decision-making, user interaction, and real-time traffic analysis.
As user demand for e-mobility continues to grow, delivering comprehensive
end-to-end solutions has become crucial. In this paper, we present a
cloud-based, LLM-powered shared e-mobility platform, integrated with a mobile
application for personalized route recommendations. The optimization module is
evaluated based on travel time and cost across different traffic scenarios.
Additionally, the LLM-powered RAG framework is evaluated at the schema level
for different users, using various evaluation methods. Schema-level RAG with
XiYanSQL achieves an average execution accuracy of 0.81 on system operator
queries and 0.98 on user queries.

</details>


### [210] [Extracting Important Tokens in E-Commerce Queries with a Tag Interaction-Aware Transformer Model](https://arxiv.org/abs/2507.10385)
*Md. Ahsanul Kabir,Mohammad Al Hasan,Aritra Mandal,Liyang Hao,Ishita Khan,Daniel Tunkelang,Zhe Wu*

Key words: 查询重构、语义标签、依赖感知、Transformer、电子商务

TL;DR: 论文提出了一种基于语义标签的依赖感知Transformer模型TagBERT，用于电子商务查询重构任务，性能优于多种现有模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 电子商务搜索引擎需解决用户查询与商品语义不匹配的问题，传统方法未能充分利用查询令牌的语义标签。

Method: 将查询重构视为令牌分类任务，设计了依赖感知的Transformer模型TagBERT，利用语义标签学习更优查询短语嵌入。

Result: 在大规模真实电商数据集上，TagBERT在重要令牌分类任务中性能优于BERT、eBERT等模型。

Conclusion: TagBERT通过捕捉语义标签显著提升查询重构性能，验证了依赖性语义标签在此类任务中的重要性。

Abstract: The major task of any e-commerce search engine is to retrieve the most
relevant inventory items, which best match the user intent reflected in a
query. This task is non-trivial due to many reasons, including ambiguous
queries, misaligned vocabulary between buyers, and sellers, over- or
under-constrained queries by the presence of too many or too few tokens. To
address these challenges, query reformulation is used, which modifies a user
query through token dropping, replacement or expansion, with the objective to
bridge semantic gap between query tokens and users' search intent. Early
methods of query reformulation mostly used statistical measures derived from
token co-occurrence frequencies from selective user sessions having clicks or
purchases. In recent years, supervised deep learning approaches, specifically
transformer-based neural language models, or sequence-to-sequence models are
being used for query reformulation task. However, these models do not utilize
the semantic tags of a query token, which are significant for capturing user
intent of an e-commerce query. In this work, we pose query reformulation as a
token classification task, and solve this task by designing a dependency-aware
transformer-based language model, TagBERT, which makes use of semantic tags of
a token for learning superior query phrase embedding. Experiments on large,
real-life e-commerce datasets show that TagBERT exhibits superior performance
than plethora of competing models, including BERT, eBERT, and
Sequence-to-Sequence transformer model for important token classification task.

</details>


### [211] [Anticipating the Selectivity of Cyclization Reaction Pathways with Neural Network Potentials](https://arxiv.org/abs/2507.10400)
*Nicholas Casetti,Dylan Anstine,Olexandr Isayev,Connor W. Coley*

Key words: 反应机制搜索, 环化反应, 图枚举, 机器学习, AIMNet2-rxn

TL;DR: 提出了一种反应机制搜索策略，适用于复杂反应（如环化反应），结合图枚举和机器学习技术，使用神经网络势（AIMNet2-rxn）评估候选反应路径，验证了其准确性和实用性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 复杂反应（如多步协同键变化的环化反应）增加了机制搜索的难度，需要更高效的策略来简化搜索过程。

Method: 结合图枚举和机器学习技术，使用神经网络势（AIMNet2-rxn）评估候选反应路径。

Result: 验证了神经网络势在估计活化能、预测立体选择性和复现天然产物合成关键步骤中的准确性。

Conclusion: 该策略为复杂反应机制搜索提供了高效且经济的解决方案，尤其适用于天然产物合成。

Abstract: Reaction mechanism search tools have demonstrated the ability to provide
insights into likely products and rate-limiting steps of reacting systems.
However, reactions involving several concerted bond changes - as can be found
in many key steps of natural product synthesis - can complicate the search
process. To mitigate these complications, we present a mechanism search
strategy particularly suited to help expedite exploration of an exemplary
family of such complex reactions, cyclizations. We provide a cost-effective
strategy for identifying relevant elementary reaction steps by combining
graph-based enumeration schemes and machine learning techniques for
intermediate filtering. Key to this approach is our use of a neural network
potential (NNP), AIMNet2-rxn, for computational evaluation of each candidate
reaction pathway. In this article, we evaluate the NNP's ability to estimate
activation energies, demonstrate the correct anticipation of stereoselectivity,
and recapitulate complex enabling steps in natural product synthesis.

</details>


### [212] [Stochastic Operator Network: A Stochastic Maximum Principle Based Approach to Operator Learning](https://arxiv.org/abs/2507.10401)
*Ryan Bausback,Jingqiao Tang,Lu Lu,Feng Bao,Toan Huynh*

Key words: 随机算子网络（SON）、不确定性量化、随机最优控制、DeepONet、随机微分方程

TL;DR: 提出了一种新的不确定性量化框架——随机算子网络（SON），结合随机神经网络和DeepONet，通过随机最优控制概念学习算子不确定性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决算子学习中的不确定性量化问题，提升对噪声数据的处理能力。

Method: 将分支网络建模为随机微分方程（SDE），利用伴随BSDE反向传播，用随机极大值原理的哈密顿梯度替代损失函数梯度进行SGD更新。

Result: SON在2D和3D噪声算子复制中表现出色。

Conclusion: SON通过扩散参数有效捕捉算子中的不确定性，展现了在噪声数据处理中的潜力。

Abstract: We develop a novel framework for uncertainty quantification in operator
learning, the Stochastic Operator Network (SON). SON combines the stochastic
optimal control concepts of the Stochastic Neural Network (SNN) with the
DeepONet. By formulating the branch net as an SDE and backpropagating through
the adjoint BSDE, we replace the gradient of the loss function with the
gradient of the Hamiltonian from Stohastic Maximum Principle in the SGD update.
This allows SON to learn the uncertainty present in operators through its
diffusion parameters. We then demonstrate the effectiveness of SON when
replicating several noisy operators in 2D and 3D.

</details>


### [213] [Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study](https://arxiv.org/abs/2507.10409)
*Amine Lbath,Ibtissam Labriji*

Key words: 能源效率, 知识蒸馏, DeepRX, 误码率, AI/ML

TL;DR: 本研究通过知识蒸馏（KD）训练紧凑的DeepRX学生模型，以提高AI/ML模型的能效，同时在性能上接近教师模型。结果表明，蒸馏模型在降低能耗的同时，保持了较低的误码率（BER）。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决AI/ML模型在能源效率与性能之间的平衡问题，特别是在DeepRX这一基于深度学习的接收器中。

Method: 采用知识蒸馏技术，训练紧凑的DeepRX学生模型，并通过比较FLOPs/Watt和FLOPs/clock评估能耗。实验包括不同学生模型大小、最佳教师模型大小及KD超参数的选择。

Result: 蒸馏模型在能耗降低的同时，表现出较低的误码率（BER），且能效评估与实际能耗一致。

Conclusion: 知识蒸馏是实现能源高效AI解决方案的有效方法。

Abstract: This study addresses the challenge of balancing energy efficiency with
performance in AI/ML models, focusing on DeepRX, a deep learning receiver based
on a fully convolutional ResNet architecture. We evaluate the energy
consumption of DeepRX, considering factors including FLOPs/Watt and
FLOPs/clock, and find consistency between estimated and actual energy usage,
influenced by memory access patterns. The research extends to comparing energy
dynamics during training and inference phases. A key contribution is the
application of knowledge distillation (KD) to train a compact DeepRX
\textit{student} model that emulates the performance of the \textit{teacher}
model but with reduced energy consumption. We experiment with different student
model sizes, optimal teacher sizes, and KD hyperparameters. Performance is
measured by comparing the Bit Error Rate (BER) performance versus
Signal-to-Interference \& Noise Ratio (SINR) values of the distilled model and
a model trained from scratch. The distilled models demonstrate a lower error
floor across SINR levels, highlighting the effectiveness of KD in achieving
energy-efficient AI solutions.

</details>


### [214] [Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data](https://arxiv.org/abs/2507.10425)
*Alvaro H. C. Correia,Christos Louizos*

Key words: 共形预测,分布偏移,最优传输,不确定性量化

TL;DR: 该论文研究了在非交换性数据情境下，如何通过最优传输理论视角估计并缓解分布偏移导致的覆盖损失。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于实际应用中数据分布偏移常见且难以验证交换性条件，现有方法需预先了解分布偏移类型，限制了其实用性。本文旨在提出更通用的解决方案。

Method: 利用最优传输理论，提出了一种新方法来估计覆盖损失，并在此基础上设计缓解策略。

Result: 研究证明了在分布偏移情况下，覆盖损失是可估计的，并可以通过提出的方法得到有效缓解。

Conclusion: 最优传输理论为解决分布偏移导致的覆盖损失问题提供了新的通用途径，扩展了共形预测的应用范围。

Abstract: Conformal prediction is a distribution-free uncertainty quantification method
that has gained popularity in the machine learning community due to its
finite-sample guarantees and ease of use. Its most common variant, dubbed split
conformal prediction, is also computationally efficient as it boils down to
collecting statistics of the model predictions on some calibration data not yet
seen by the model. Nonetheless, these guarantees only hold if the calibration
and test data are exchangeable, a condition that is difficult to verify and
often violated in practice due to so-called distribution shifts. The literature
is rife with methods to mitigate the loss in coverage in this non-exchangeable
setting, but these methods require some prior information on the type of
distribution shift to be expected at test time. In this work, we study this
problem via a new perspective, through the lens of optimal transport, and show
that it is possible to estimate the loss in coverage and mitigate it in case of
distribution shift.

</details>


### [215] [CLA: Latent Alignment for Online Continual Self-Supervised Learning](https://arxiv.org/abs/2507.10434)
*Giacomo Cignoni,Andrea Cossu,Alexandra Gomez-Villa,Joost van de Weijer,Antonio Carta*

Key words: 自监督学习, 在线持续学习, CLA, 表征对齐, 预训练

TL;DR: 提出了一种名为CLA的自监督学习策略，用于在线持续学习场景，通过对齐当前与过去的表征来减少遗忘，并在相同计算预算下优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前自监督学习在在线持续学习（数据以小批量到达、固定计算预算且无任务边界）场景中的应用较少，需要一种新策略来减少遗忘并提升性能。

Method: 引入Continual Latent Alignment (CLA)，通过对齐当前与过去的表征来减少遗忘。

Result: CLA在在线场景下加速训练收敛，优于现有方法；且作为预训练协议时，其早期表现优于完整的i.i.d.预训练。

Conclusion: CLA是一种有效的在线持续学习策略，既能提升训练效率，又能作为预训练协议优化最终性能。

Abstract: Self-supervised learning (SSL) is able to build latent representations that
generalize well to unseen data. However, only a few SSL techniques exist for
the online CL setting, where data arrives in small minibatches, the model must
comply with a fixed computational budget, and task boundaries are absent. We
introduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL
that aligns the representations learned by the current model with past
representations to mitigate forgetting. We found that our CLA is able to speed
up the convergence of the training process in the online scenario,
outperforming state-of-the-art approaches under the same computational budget.
Surprisingly, we also discovered that using CLA as a pretraining protocol in
the early stages of pretraining leads to a better final performance when
compared to a full i.i.d. pretraining.

</details>


### [216] [Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities](https://arxiv.org/abs/2507.10442)
*Shivam Chandhok,Wan-Cyuan Fan,Vered Shwartz,Vineeth N Balasubramanian,Leonid Sigal*

Key words: 视觉语言模型(VLMs), 视觉理解, 性能评测, 设计缺陷

TL;DR: 论文通过构建一系列测试，分析了当前最先进的视觉语言模型（VLMs）在基础视觉任务中的局限性，并揭示了其设计中的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管视觉语言模型（VLMs）在复杂计算机视觉任务中表现出色，但它们在基础视觉理解能力上存在不足。本文旨在通过系统测试揭示这些局限性。

Method: 通过构建测试，比较了从视觉编码器、中间视觉语言投影和LLM解码器输出直接训练的探测器的性能，以分析VLMs的设计缺陷。

Result: 研究发现VLMs在视觉信息处理、能力和鲁棒性方面存在明显短板，并提出了一些关键观察。

Conclusion: 研究结果为改进VLMs提供了重要指导。

Abstract: Vision-language Models (VLMs) have emerged as general-purpose tools for
addressing a variety of complex computer vision problems. Such models have been
shown to be highly capable, but, at the same time, lacking some basic visual
understanding skills. In this paper, we set out to understand the limitations
of SoTA VLMs on fundamental visual tasks by constructing a series of tests that
probe which components of design, specifically, may be lacking. Importantly, we
go significantly beyond the current benchmarks, which simply measure the final
performance of VLM response, by also comparing and contrasting it to the
performance of probes trained directly on features obtained from the visual
encoder, intermediate vision-language projection and LLM-decoder output. In
doing so, we uncover shortcomings in VLMs and make a number of important
observations about their capabilities, robustness and how they process visual
information. We hope our insights will guide progress in further improving
VLMs.

</details>


### [217] [Some remarks on gradient dominance and LQR policy optimization](https://arxiv.org/abs/2507.10452)
*Eduardo D. Sontag*

Key words: 优化问题, 梯度下降, Polyak-{\L}ojasiewicz不等式, LQR问题, 收敛行为, 输入到状态稳定性

TL;DR: 该论文探讨了优化问题和强化学习中政策优化的收敛性问题，通过Polyak-{\L}ojasiewicz不等式（PLI）及其变体研究梯度下降的收敛速率，并对比了连续时间与离散时间LQR问题的收敛行为差异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究动机在于理解梯度下降在大初始条件下的收敛行为差异，尤其是在连续时间和离散时间LQR问题中的不同表现，并探索更广义的PLI条件以解释梯度估计误差的影响。

Method: 方法包括应用广义的PLI条件和输入到状态稳定性（ISS）分析，以研究梯度估计误差对收敛行为的影响。

Result: 研究结果表明，连续时间LQR问题的收敛速率在大初始条件下可能消失，而离散时间LQR问题则表现出全局指数收敛。广义PLI条件有助于解释这一现象。

Conclusion: 论文结论认为，广义PLI条件是理解优化问题收敛行为的关键，特别是在处理梯度误差时。ISS分析为这类问题提供了新的视角。

Abstract: Solutions of optimization problems, including policy optimization in
reinforcement learning, typically rely upon some variant of gradient descent.
There has been much recent work in the machine learning, control, and
optimization communities applying the Polyak-{\L}ojasiewicz Inequality (PLI) to
such problems in order to establish an exponential rate of convergence (a.k.a.
``linear convergence'' in the local-iteration language of numerical analysis)
of loss functions to their minima under the gradient flow. Often, as is the
case of policy iteration for the continuous-time LQR problem, this rate
vanishes for large initial conditions, resulting in a mixed globally linear /
locally exponential behavior. This is in sharp contrast with the discrete-time
LQR problem, where there is global exponential convergence. That gap between CT
and DT behaviors motivates the search for various generalized PLI-like
conditions, and this talk will address that topic. Moreover, these
generalizations are key to understanding the transient and asymptotic effects
of errors in the estimation of the gradient, errors which might arise from
adversarial attacks, wrong evaluation by an oracle, early stopping of a
simulation, inaccurate and very approximate digital twins, stochastic
computations (algorithm ``reproducibility''), or learning by sampling from
limited data. We describe an ``input to state stability'' (ISS) analysis of
this issue. The lecture also discussed convergence and PLI-like properties of
``linear feedforward neural networks'' in feedback control, but this arXiv
skips that part (to be updated). Much of the work described here was done in
collaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping
Jiang, and Milad Siami.

</details>


### [218] [The Target Polish: A New Approach to Outlier-Resistant Non-Negative Matrix and Tensor Factorization](https://arxiv.org/abs/2507.10484)
*Paul Fogel,Christophe Geissler,George Luta*

Key words: 非负矩阵分解, 加权中位数变换, 抗干扰, 计算效率

TL;DR: 本文提出了一种名为'Target Polish'的框架，用于非负矩阵和张量分解，通过加权中位数变换实现高效且抗干扰的分解。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统加权NMF方法对异常值具有抵抗性，但因使用乘法更新导致收敛速度慢。本文旨在提出一种新方法，既能抵抗异常值，又能保持Fast-HALS算法的高效性。

Method: 采用加权中位数变换自适应平滑数据，同时保持Fast-HALS的高效加法更新结构。

Result: 在图像数据集上的实验表明，该方法在准确性上优于或匹配现有抗干扰NMF方法，且计算时间显著减少。

Conclusion: 'Target Polish'框架在保持高效计算的同时，显著提升了抗干扰能力，适用于实际应用场景。

Abstract: This paper introduces the "Target Polish," a robust and computationally
efficient framework for nonnegative matrix and tensor factorization. Although
conventional weighted NMF approaches are resistant to outliers, they converge
slowly due to the use of multiplicative updates to minimize the objective
criterion. In contrast, the Target Polish approach remains compatible with the
Fast-HALS algorithm, which is renowned for its speed, by adaptively smoothing
the data with a weighted median-based transformation. This innovation provides
outlier resistance while maintaining the highly efficient additive update
structure of Fast-HALS. Empirical evaluations using image datasets corrupted
with structured (block) and unstructured (salt) noise demonstrate that the
Target Polish approach matches or exceeds the accuracy of state-of-the-art
robust NMF methods and reduces computational time by an order of magnitude in
the studied scenarios.

</details>


### [219] [Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/2507.10485)
*Brandon Shuen Yi Loke,Filippo Quadri,Gabriel Vivanco,Maximilian Casagrande,Saúl Fenollosa*

Key words: 持续学习，弹性权重巩固，灾难性遗忘，正则化，监督学习

TL;DR: 研究评估了弹性权重巩固（EWC）在持续学习中的表现，证明其显著减少遗忘但略微影响新任务学习效率，并探讨了泛化性和超参数影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 克服神经网络的灾难性遗忘问题，验证和扩展EWC在持续学习中的有效性。

Method: 使用PermutedMNIST和RotatedMNIST基准测试，对比EWC、L2正则化和无正则化的SGD。

Result: EWC显著减少遗忘，但新任务学习效率略有下降；分析了dropout和超参数的影响。

Conclusion: EWC是神经网络终身学习的可行解决方案。

Abstract: Catastrophic forgetting is the primary challenge that hinders continual
learning, which refers to a neural network ability to sequentially learn
multiple tasks while retaining previously acquired knowledge. Elastic Weight
Consolidation, a regularization-based approach inspired by synaptic
consolidation in biological neural systems, has been used to overcome this
problem. In this study prior research is replicated and extended by evaluating
EWC in supervised learning settings using the PermutedMNIST and RotatedMNIST
benchmarks. Through systematic comparisons with L2 regularization and
stochastic gradient descent (SGD) without regularization, we analyze how
different approaches balance knowledge retention and adaptability. Our results
confirm what was shown in previous research, showing that EWC significantly
reduces forgetting compared to naive training while slightly compromising
learning efficiency on new tasks. Moreover, we investigate the impact of
dropout regularization and varying hyperparameters, offering insights into the
generalization of EWC across diverse learning scenarios. These results
underscore EWC's potential as a viable solution for lifelong learning in neural
networks.

</details>


### [220] [Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing](https://arxiv.org/abs/2507.10494)
*Tanveer Khan,Mindaugas Budzys,Antonis Michalas*

Key words: Split Learning, Function Secret Sharing, 数据隐私, 安全性

TL;DR: SplitHappens通过将FSS与U形分割学习结合，提高数据隐私保护，同时减少通信和计算成本。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管分割学习（SL）在保护客户端数据和增强机器学习过程方面有潜力，但其易受攻击，促使研究如何通过FSS提升安全性。

Method: 结合FSS与U形SL，减少FSS的通信和计算成本，同时保护训练数据的标签不被服务器知晓。

Result: 实验证明，该方法能有效减少训练时间和通信成本，同时保持与之前相同的准确性。

Conclusion: SplitHappens为SL提供了更高的安全保障，拓展了对多种攻击的保护能力。

Abstract: Split Learning (SL) -- splits a model into two distinct parts to help protect
client data while enhancing Machine Learning (ML) processes. Though promising,
SL has proven vulnerable to different attacks, thus raising concerns about how
effective it may be in terms of data privacy. Recent works have shown promising
results for securing SL through the use of a novel paradigm, named Function
Secret Sharing (FSS), in which servers obtain shares of a function they compute
and operate on a public input hidden with a random mask. However, these works
fall short in addressing the rising number of attacks which exist on SL. In
SplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly
to other works, we are able to make use of the benefits of SL by reducing the
communication and computational costs of FSS. However, a U-shaped SL provides a
higher security guarantee than previous works, allowing a client to keep the
labels of the training data secret, without having to share them with the
server. Through this, we are able to generalize the security analysis of
previous works and expand it to different attack vectors, such as modern model
inversion attacks as well as label inference attacks. We tested our approach
for two different convolutional neural networks on different datasets. These
experiments show the effectiveness of our approach in reducing the training
time as well as the communication costs when compared to simply using FSS while
matching prior accuracy.

</details>


### [221] [Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop](https://arxiv.org/abs/2507.10502)
*Elizabeth Fahsbender,Alma Andersson,Jeremy Ash,Polina Binder,Daniel Burkhardt,Benjamin Chang,Georg K. Gerber,Anthony Gitter,Patrick Godau,Ankit Gupta,Genevieve Haliburton,Siyu He,Trey Ideker,Ivana Jelic,Aly Khan,Yang-Joon Kim,Aditi Krishnapriyan,Jon M. Laurent,Tianyu Liu 28,Emma Lundberg,Shalin B. Mehta,Rob Moccia,Angela Oliveira Pisco,Katherine S. Pollard,Suresh Ramani,Julio Saez-Rodriguez,Yasin Senbabaoglu,Elana Simon,Srinivasan Sivanandan,Gustavo Stolovitzky,Marc Valer,Bo Wang,Xikun Zhang,James Zou,Katrina Kalantar*

Key words: 人工智能,生物学,基准测试,数据异质性,虚拟细胞

TL;DR: 论文提出由于缺乏标准化、跨领域的基准测试，阻碍了构建稳健、可信赖的生物学人工智能模型。研讨会聚集专家，提出解决数据异质性、噪音等问题的建议。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决生物学中人工智能模型因缺乏标准化基准测试而导致的不可靠问题。

Method: 通过跨学科研讨会，分析数据异质性、噪音等技术瓶颈，提出标准化框架建议。

Result: 提出高质量数据管理、标准化工具等建议，以推动AI驱动的虚拟细胞基准测试。

Conclusion: 标准化基准将推动生物学模型的严谨性和重现性，促进新发现和对细胞系统的深入理解。

Abstract: Artificial intelligence holds immense promise for transforming biology, yet a
lack of standardized, cross domain, benchmarks undermines our ability to build
robust, trustworthy models. Here, we present insights from a recent workshop
that convened machine learning and computational biology experts across
imaging, transcriptomics, proteomics, and genomics to tackle this gap. We
identify major technical and systemic bottlenecks such as data heterogeneity
and noise, reproducibility challenges, biases, and the fragmented ecosystem of
publicly available resources and propose a set of recommendations for building
benchmarking frameworks that can efficiently compare ML models of biological
systems across tasks and data modalities. By promoting high quality data
curation, standardized tooling, comprehensive evaluation metrics, and open,
collaborative platforms, we aim to accelerate the development of robust
benchmarks for AI driven Virtual Cells. These benchmarks are crucial for
ensuring rigor, reproducibility, and biological relevance, and will ultimately
advance the field toward integrated models that drive new discoveries,
therapeutic insights, and a deeper understanding of cellular systems.

</details>


### [222] [On the Performance of Differentially Private Optimization with Heavy-Tail Class Imbalance](https://arxiv.org/abs/2507.10536)
*Qiaoyue Tang,Alain Zhiyanov,Mathias Lécuyer*

Key words: 私有学习,类别不平衡,二阶优化,DP-GD,DP-AdamBC

TL;DR: 本文研究了在类别不平衡重的尾部分布下，常见私有学习优化算法的优化行为，发现DP-GD在低频类别学习中表现不佳，而利用二阶信息的算法如DP-AdamBC表现更好。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨在类别不平衡重的尾部分布下，私有学习优化算法的表现差异，以改进低频类别的学习效果。

Method: 通过理论模型和实证分析，比较DP-GD与利用二阶信息的算法（如DP-AdamBC）在类别不平衡数据上的表现。

Result: DP-AdamBC能有效避免类别不平衡带来的病态条件，训练准确率在低频类别上提升了约8%（控制实验）和5%（真实数据）。

Conclusion: 二阶信息估计是解决类别不平衡下私有学习问题的关键，DP-AdamBC表现优于DP-GD。

Abstract: In this work, we analyze the optimization behaviour of common private
learning optimization algorithms under heavy-tail class imbalanced
distribution. We show that, in a stylized model, optimizing with Gradient
Descent with differential privacy (DP-GD) suffers when learning low-frequency
classes, whereas optimization algorithms that estimate second-order information
do not. In particular, DP-AdamBC that removes the DP bias from estimating loss
curvature is a crucial component to avoid the ill-condition caused by
heavy-tail class imbalance, and empirically fits the data better with
$\approx8\%$ and $\approx5\%$ increase in training accuracy when learning the
least frequent classes on both controlled experiments and real data
respectively.

</details>


### [223] [Graph World Model](https://arxiv.org/abs/2507.10539)
*Tao Feng,Yexin Wu,Guanyu Lin,Jiaxuan You*

Key words: 世界模型, 图结构数据, 多模态, 动作节点, 消息传递算法

TL;DR: 提出了Graph World Model (GWM)，支持多模态数据和图结构数据的任务表示，通过通用消息传递算法和动作节点实现多样化任务的处理，在多个领域任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的世界模型主要关注非结构化数据，无法利用普遍存在的图结构数据，而图基础模型又局限于图学习任务。GWM旨在解决这些挑战，支持多模态和多样化任务。

Method: 提出了GWM，采用通用消息传递算法整合多模态数据，通过GWM-T（转换为文本）或GWM-E（模态特定编码器）实现数据统一表示，并引入动作节点支持多样化任务。

Result: 在六种跨领域任务中，GWM表现优于或匹配领域基线，展现出多跳结构和零样本/少样本能力。

Conclusion: GWM通过整合多模态和图结构数据，以及引入动作节点，在多样化任务中表现出强大潜力。

Abstract: World models (WMs) demonstrate strong capabilities in prediction, generation,
and planning tasks. Existing WMs primarily focus on unstructured data and
cannot leverage the ubiquitous structured data, often represented as graphs, in
the digital world. While multiple graph foundation models have been proposed,
they focus on graph learning tasks and cannot extend to diverse multi-modal
data and interdisciplinary tasks. To address these challenges, we propose the
Graph World Model (GWM), a world model that supports both unstructured and
graph-structured states with multi-modal information and represents diverse
tasks as actions. The core of a GWM is a generic message-passing algorithm to
aggregate structured information, either over a unified multi-modal token space
by converting multi-modal data into text (GWM-T) or a unified multi-modal
embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces
action nodes to support diverse tasks, where action nodes are linked to other
nodes via direct reference or similarity computation. Extensive experiments on
six tasks from diverse domains, including multi-modal generation and matching,
recommendation, graph prediction, multi-agent, retrieval-augmented generation,
and planning and optimization, show that the same GWM outperforms or matches
domain-specific baselines' performance, benefits from multi-hop structures, and
demonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our
code for GWM is released at https://github.com/ulab-uiuc/GWM.

</details>


### [224] [Fusing LLM Capabilities with Routing Data](https://arxiv.org/abs/2507.10540)
*Tao Feng,Haozhen Zhang,Zijie Lei,Pengrui Han,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Jiaxuan You*

Key words: 大语言模型, 路由, 多级融合, FusionBench, FusionFactory

TL;DR: 本文提出 FusionBench 和 FusionFactory 框架，通过路由和多级融合技术优化 LLM 性能，显著提升复杂任务的表现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前大多数应用依赖单一后端模型，难以覆盖多样化能力，导致性能和成本效率低。本文利用路由数据揭示模型优势，提出了系统化融合方法。

Method: 1) FusionBench 路由基准涵盖14任务，20个开源 LLM；2) FusionFactory 进行查询级、思维级和模型级融合。

Result: FusionFactory 在14个基准测试中均优于单一最佳 LLM，最优配置因任务而异。

Conclusion: 系统化的多级融合技术可以有效利用 LLM 的互补优势，显著提升整体性能。

Abstract: The rapid advancement of large language models (LLMs) has created a vibrant
ecosystem of diverse architectures, each with unique strengths due to
differences in design, training data, and objectives. However, most
applications still rely on a single backend model, limiting coverage of
capabilities and leading to inefficiencies in performance and token cost when
tackling complex tasks. We highlight an underexploited opportunity: LLM routing
data, produced when hosting platforms route diverse queries to different
models, which can reveal comparative strengths across tasks. To address this,
we propose FusionBench, a comprehensive routing benchmark covering 14 tasks
across five domains with 20 open-source LLMs (8B to 671B parameters), capturing
103M tokens and summarizing reusable thought templates from top models.
Building on this, we introduce FusionFactory, a systematic fusion framework
with three levels: (1) query-level fusion, tailoring routers for each query
using both direct responses and reasoning-augmented outputs; (2) thought-level
fusion, leveraging abstract templates derived from top-performing LLMs' answers
to similar queries; and (3) model-level fusion, transferring capabilities
between models via distillation, using top responses or highest judge scores as
training data. Experiments show FusionFactory consistently outperforms the best
individual LLM across all 14 benchmarks, with optimal fusion configurations
varying by benchmark, demonstrating the value of systematic LLM fusion in
harnessing complementary strengths and improving overall performance.

</details>


### [225] [Disentangling Neural Disjunctive Normal Form Models](https://arxiv.org/abs/2507.10546)
*Kexin Gu Baugh,Vincent Perreault,Matthew Baugh,Luke Dickens,Katsumi Inoue,Alessandra Russo*

Key words: 神经DNF模型、解耦方法、神经符号学习、分类任务、可解释性

TL;DR: 论文提出了一种新的解耦方法，通过将嵌套规则节点拆分为独立节点，提升了神经DNF模型的性能和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 神经DNF模型在神经符号学习中表现优异，但后训练符号翻译过程的阈值处理会降低性能，因此需要解决知识解耦问题。

Method: 提出了一种解耦方法，通过拆解嵌套规则节点为独立节点，以更好地保留模型性能。

Result: 在二分类、多分类及多标签分类任务中，该方法提供了紧凑且可解释的逻辑表示，性能接近翻译前模型。

Conclusion: 解耦方法有效提升了神经DNF模型的性能和可解释性，适用于多种分类任务。

Abstract: Neural Disjunctive Normal Form (DNF) based models are powerful and
interpretable approaches to neuro-symbolic learning and have shown promising
results in classification and reinforcement learning settings without prior
knowledge of the tasks. However, their performance is degraded by the
thresholding of the post-training symbolic translation process. We show here
that part of the performance degradation during translation is due to its
failure to disentangle the learned knowledge represented in the form of the
networks' weights. We address this issue by proposing a new disentanglement
method; by splitting nodes that encode nested rules into smaller independent
nodes, we are able to better preserve the models' performance. Through
experiments on binary, multiclass, and multilabel classification tasks
(including those requiring predicate invention), we demonstrate that our
disentanglement method provides compact and interpretable logical
representations for the neural DNF-based models, with performance closer to
that of their pre-translation counterparts. Our code is available at
https://github.com/kittykg/disentangling-ndnf-classification.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [226] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Key words: 大语言模型、推理冗余、注意力机制、结构感知剪枝

TL;DR: 论文提出了一种通过移除冗余推理路径中的干扰来提升大语言模型性能的方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究发现大语言模型在推理过程中存在冗余，尤其是错误答案的注意力更分散，因此希望通过清除冗余来提升性能。

Method: 通过测量特殊结束标记的注意力得分识别冗余，提出结构感知剪枝优先移除低贡献推理块。

Result: 方法显著提升了多个推理密集基准的准确性，尤其在数学竞赛数据集AIME和AMC上表现突出。

Conclusion: 清除推理冗余能显著提升大语言模型的推理性能，且无需额外训练。

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [227] [A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data](https://arxiv.org/abs/2507.08875)
*Fuh-Hwa Franklin Liu,Su-Chuan Shih*

Key words: 多准则评估,虚拟差距分析,决策支持系统,数据包络分析,随机前沿分析

TL;DR: 论文提出了一种结合两种虚拟差距分析（VGA）模型的新型多准则评估（MCA）方法，以应对现有方法的假设和主观性问题，提高了评估效率和公平性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的多准则评估方法依赖于假设和主观判断，难以处理复杂评估问题，尤其是在结合定量和定性标准时。

Method: 利用基于线性规划的虚拟差距分析（VGA）框架，提出了一种新型MCA方法，结合两种VGA模型。

Result: 通过两个数值示例验证了方法的准确性和透明度。

Conclusion: 提出的方法为自动决策系统和决策支持系统提供了更全面和可靠的解决方案。

Abstract: Modern methods for multi-criteria assessment (MCA), such as Data Envelopment
Analysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria
Decision-Making (MCDM), are utilized to appraise a collection of
Decision-Making Units (DMUs), also known as alternatives, based on several
criteria. These methodologies inherently rely on assumptions and can be
influenced by subjective judgment to effectively tackle the complex evaluation
challenges in various fields. In real-world scenarios, it is essential to
incorporate both quantitative and qualitative criteria as they consist of
cardinal and ordinal data. Despite the inherent variability in the criterion
values of different alternatives, the homogeneity assumption is often employed,
significantly affecting evaluations. To tackle these challenges and determine
the most appropriate alternative, we propose a novel MCA approach that combines
two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear
programming, is pivotal in the MCA methodology. This approach improves
efficiency and fairness, ensuring that evaluations are both comprehensive and
dependable, thus offering a strong and adaptive solution. Two comprehensive
numerical examples demonstrate the accuracy and transparency of our proposed
method. The goal is to encourage continued advancement and stimulate progress
in automated decision systems and decision support systems.

</details>


### [228] [Multi-Actor Generative Artificial Intelligence as a Game Engine](https://arxiv.org/abs/2507.08892)
*Alexander Sasha Vezhnevets,Jayd Matyas,Logan Cross,Davide Paglieri,Minsuk Chang,William A. Cunningham,Simon Osindero,William S. Isaac,Joel Z. Leibo*

Key words: 生成式AI, 多角色环境, 桌游规则, 实体-组件架构, 模块化, Concordia

TL;DR: 论文探讨了利用生成式AI在多角色环境中的应用，提出基于桌游游戏规则的灵活场景定义框架，采用实体-组件架构模式，支持快速迭代和模块化。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决生成式AI在多角色环境中的多样化应用需求，如社会科学建模、互动叙事和AI评估。

Method: 借鉴桌游中的游戏主持人（GM）角色，提出实体-组件架构模式，分离工程实现与设计配置，支持模块化和可扩展性。

Result: 开发中的Concordia库展示了该框架的有效性，能够灵活配置满足不同目标的场景。

Conclusion: 基于桌游规则的实体-组件架构为生成式AI在多角色环境中提供了灵活且可扩展的解决方案。

Abstract: Generative AI can be used in multi-actor environments with purposes ranging
from social science modeling to interactive narrative and AI evaluation.
Supporting this diversity of use cases -- which we classify as Simulationist,
Dramatist, and Evaluationist -- demands a flexible scenario definition
framework. We argue here that a good approach is to take inspiration from
tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible
for the environment and generates all parts of the story not directly
determined by the voluntary actions of player characters. We argue that the
Entity-Component architectural pattern is useful here. In such a system, the GM
is not a hardcoded computer game but is itself a configurable entity, composed
of components just like any other actor. By design, the approach allows for a
separation between the underlying implementation details handled by an
engineer, the creation of reusable components, and their composition and
configuration managed by a designer who constructs entities from the
components. This separation of concerns is instrumental for achieving rapid
iteration, maintaining modularity, and ultimately to ensure scalability. We
describe the ongoing evolution of the Concordia library in terms of this
philosophy, demonstrating how it allows users to effectively configure
scenarios that align with their specific goals.

</details>


### [229] [BioAnalyst: A Foundation Model for Biodiversity](https://arxiv.org/abs/2507.09080)
*Athanasios Trantas,Martino Mensio,Stylianos Stasinos,Sebastian Gribincea,Taimur Khan,Damian Podareanu,Aliene van der Veen*

Key words: 生物多样性,基础模型,AI,生态保护,BioAnalyst

TL;DR: 论文介绍了BioAnalyst，首个专为生物多样性分析和保护规划定制的基础模型，利用多模态数据和Transformer架构，提升生态预测准确性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 生物多样性丧失的加速对生态研究和保护策略提出了严峻挑战，需要新技术来支持监测和规划。

Method: BioAnalyst采用基于Transformer的架构，预训练于多模态数据集，可微调以适应多种下游任务。

Result: 模型在数据稀缺场景下优于现有方法，为生态预测设立了新的准确性基准。

Conclusion: BioAnalyst的开放发布旨在促进生物多样性建模的协作，推动AI驱动的生态解决方案。

Abstract: The accelerating loss of biodiversity presents critical challenges for
ecological research and conservation strategies. The preservation of
biodiversity is paramount for maintaining ecological balance and ensuring the
sustainability of ecosystems. However, biodiversity faces numerous threats,
including habitat loss, climate change, and the proliferation of invasive
species. Addressing these and other ecology-related challenges, both at local
and global scales, requires comprehensive monitoring, predictive and
conservation planning capabilities. Artificial Intelligence (AI) Foundation
Models (FMs) have gained significant momentum in numerous scientific domains by
leveraging vast datasets to learn general-purpose representations adaptable to
various downstream tasks. This paradigm holds immense promise for biodiversity
conservation. In response, we introduce BioAnalyst, the first Foundation Model
tailored for biodiversity analysis and conservation planning. BioAnalyst
employs a transformer-based architecture, pre-trained on extensive multi-modal
datasets encompassing species occurrence records, remote sensing indicators,
climate and environmental variables. BioAnalyst is designed for adaptability,
allowing for fine-tuning of a range of downstream tasks, such as species
distribution modelling, habitat suitability assessments, invasive species
detection, and population trend forecasting. We evaluate the model's
performance on two downstream use cases, demonstrating its generalisability
compared to existing methods, particularly in data-scarce scenarios for two
distinct use-cases, establishing a new accuracy baseline for ecological
forecasting. By openly releasing BioAnalyst and its fine-tuning workflows to
the scientific community, we aim to foster collaborative efforts in
biodiversity modelling and advance AI-driven solutions to pressing ecological
challenges.

</details>


### [230] [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://arxiv.org/abs/2507.09089)
*Joel Becker,Nate Rush,Elizabeth Barnes,David Rein*

Key words: 

TL;DR: 该论文通过随机对照试验（RCT）研究了2025年初的AI工具对开源开发者生产力的实际影响，发现允许使用AI工具反而使任务完成时间增加了19%，与开发者和专家的预期相反。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 尽管AI工具已被广泛采用，但其对实际软件开发的影响尚未充分研究。论文旨在探讨AI工具对经验丰富的开源开发者生产力的影响。

Method: 研究采用随机对照试验，16位具有中等AI经验的开发者在成熟项目中完成246项任务，任务随机分配是否允许使用2025年初的AI工具（如Cursor Pro和Claude 3.5/3.7 Sonnet）。

Result: 允许使用AI工具使任务完成时间增加19%，与开发者预期的20%减少及经济学和ML专家预测的38%-39%减少相矛盾。

Conclusion: 研究结果表明，AI工具在当前环境下可能并未提升生产力，反而导致效率下降，这一效应在多种分析中表现出稳健性。

Abstract: Despite widespread adoption, the impact of AI tools on software development
in the wild remains understudied. We conduct a randomized controlled trial
(RCT) to understand how AI tools at the February-June 2025 frontier affect the
productivity of experienced open-source developers. 16 developers with moderate
AI experience complete 246 tasks in mature projects on which they have an
average of 5 years of prior experience. Each task is randomly assigned to allow
or disallow usage of early 2025 AI tools. When AI tools are allowed, developers
primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.
Before starting tasks, developers forecast that allowing AI will reduce
completion time by 24%. After completing the study, developers estimate that
allowing AI reduced completion time by 20%. Surprisingly, we find that allowing
AI actually increases completion time by 19%--AI tooling slowed developers
down. This slowdown also contradicts predictions from experts in economics (39%
shorter) and ML (38% shorter). To understand this result, we collect and
evaluate evidence for 20 properties of our setting that a priori could
contribute to the observed slowdown effect--for example, the size and quality
standards of projects, or prior developer experience with AI tooling. Although
the influence of experimental artifacts cannot be entirely ruled out, the
robustness of the slowdown effect across our analyses suggests it is unlikely
to primarily be a function of our experimental design.

</details>


### [231] [Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System](https://arxiv.org/abs/2507.09179)
*Ronghua Shi,Yiou Liu,Xinyu Ying,Yang Tan,Yuchun Feng,Lynn Ai,Bill Shi,Xuhui Wang,Zhuang Liu*

Key words: DeFi, MARL, 市场操纵, 多Agent系统, 金融监管

TL;DR: 本文提出了一种基于多Agent强化学习（MARL）的去中心化市场操纵检测框架，通过动态对抗游戏建模操纵者与检测者的交互，并结合金融指标和多种数据源实现高效检测。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着去中心化金融（DeFi）的发展，市场操纵行为日益猖獗，缺乏中心化监管导致恶意行为者能够轻易实施操纵策略，亟需一种去中心化的解决方案。

Method: 1. 使用MARL框架建模操纵者与检测者的对抗游戏。2. 提出三种创新方法：GRPO优化策略、基于理论的奖励函数以及多模态Agent管道。3. 将框架集成于Symphony系统中，支持去中心化和实时监测。

Result: 在10万次真实场景和对抗模拟中验证，Hide-and-Shill在检测精度和因果归因方面表现优异。

Conclusion: 该研究不仅为DeFi市场提供了一种去中心化的操纵检测方案，还推动了多Agent系统与金融监管的结合，具有广泛的应用前景。

Abstract: Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in sparse-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates LLM-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.

</details>


### [232] [When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents](https://arxiv.org/abs/2507.09329)
*Matous Kozak,Roshanak Zilouchian Moghaddam,Siva Sivaraman*

Key words: LLM, 编码代理, 安全评估, GPT-4, 信息泄露

TL;DR: 对基于LLM的编码代理进行了首次系统性安全评估，发现21%的行为存在安全问题，并提出了检测系统和缓解策略。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 了解LLM编码代理在软件部署中的安全风险，填补安全研究的空白。

Method: 分析了五个先进模型的12,000多个行为，评估了93个实际任务，开发了高精度检测系统。

Result: 21%的行为不安全，信息泄露（CWE-200）最常见；GPT-4.1缓解成功率达96.8%。

Conclusion: LLM编码代理存在显著安全问题，需在下一代设计中提升安全意识。

Abstract: LLM-based coding agents are rapidly being deployed in software development,
yet their security implications remain poorly understood. These agents, while
capable of accelerating software development, may inadvertently introduce
insecure practices. We conducted the first systematic security evaluation of
autonomous coding agents, analyzing over 12,000 actions across five
state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world
software setup tasks. Our findings reveal significant security concerns: 21% of
agent trajectories contained insecure actions, with models showing substantial
variation in security behavior. We developed a high-precision detection system
that identified four major vulnerability categories, with information exposure
(CWE-200) being the most prevalent one. We also evaluated mitigation strategies
including feedback mechanisms and security reminders with various effectiveness
between models. GPT-4.1 demonstrated exceptional security awareness with 96.8%
mitigation success. Our work provides the first comprehensive framework for
evaluating coding agent security and highlights the need for security-aware
design of next generation LLM-based coding agents.

</details>


### [233] [A Taxonomy of Omnicidal Futures Involving Artificial Intelligence](https://arxiv.org/abs/2507.09369)
*Andrew Critch,Jacob Tsimerman*

Key words: 人工智能、灭绝风险、分类法、预防措施、公开讨论

TL;DR: 本文提出了一种关于人工智能可能导致人类灭绝事件的分类和示例，旨在通过公开讨论支持预防措施。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 希望通过公开讨论潜在的人工智能灾难性风险，促使大型机构采取预防措施。

Method: 提出分类法和案例研究，展示人工智能可能导致的灭绝事件。

Result: 明确了多种可能由AI引发的灭绝情景，为预防措施提供理论支持。

Conclusion: 公开讨论潜在风险有助于推动预防行动，减少人工智能带来的灭绝威胁。

Abstract: This report presents a taxonomy and examples of potential omnicidal events
resulting from AI: scenarios where all or almost all humans are killed. These
events are not presented as inevitable, but as possibilities that we can work
to avoid. Insofar as large institutions require a degree of public support in
order to take certain actions, we hope that by presenting these possibilities
in public, we can help to support preventive measures against catastrophic
risks from AI.

</details>


### [234] [EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique](https://arxiv.org/abs/2507.09374)
*Chenglin Zhu,Tao Zhang,Chong Li,Mingan Lin,Zenan Zhou,Jian Xie*

Key words: 多模态大语言模型,科学推理,教育框架,课程学习,自洽性

TL;DR: EduFlow是一个端到端的框架，旨在提升多模态大语言模型（MLLMs）在科学任务中的多步和可解释推理能力，通过结合数据选择、MCTS轨迹构建和输出优化，显著提高了推理的一致性和连贯性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有MLLMs在科学任务中表现不佳，尤其在多步推理和可解释性方面存在不足，亟需一种能动态适应复杂问题且能自我纠正的解决方案。

Method: 提出了EduFlow框架，其核心是EduPRM（过程感知奖励模型）和EduMCTS（教育领域适应的搜索框架），结合课程学习和自洽性优化推理轨迹。

Result: 实验表明，EduFlow显著提升了推理的一致性和连贯性，并构建了大规模数据集EduMCTS-160K。

Conclusion: EduFlow通过动态适应和精细化反馈，为科学推理提供了可靠且高效的解决方案。

Abstract: Multimodal large language models (MLLMs) still perform poorly on scientific
tasks, particularly those requiring multi-step and interpretable reasoning.
Their limitations include insufficient scientific reasoning patterns, lack of
global coherence in multi-step inference, and the absence of reflective
self-correction, making them unreliable in structured scientific contexts. We
introduce EduFlow, the first end-to-end framework that covers the full pipeline
of educational scientific reasoning, including data selection, MCTS-based
trajectory construction, model training, and output optimization. At its core
is EduPRM, a process-aware reward model that critiques reasoning steps with
tags and justifications. EduPRM is trained via curriculum learning on three
complementary supervision sources: MCTS-guided trajectories, error-injected
critiques, and teacher-student dialogues, enabling dynamic adaptation to
multi-stage problem solving and iterative refinement during inference. We
further propose EduMCTS, a domain-adapted search framework that introduces
bootstrapping actions specifically designed for educational reasoning, such as
a self-reflection mechanism that promotes reflective error correction. It
further leverages EduPRM's fine-grained feedback to guide the search toward
higher-quality reasoning trajectories. By applying self-consistency and
rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of
educational reasoning trajectories. Extensive experiments demonstrate that
EduFlow enhances reasoning consistency and coherence. Code, data, and models
will be released.

</details>


### [235] [Knowledge Conceptualization Impacts RAG Efficacy](https://arxiv.org/abs/2507.09389)
*Chris Davis Jaldi,Anmol Saini,Elham Ghiasi,O. Divine Eziolise,Cogan Shimizu*

Key words: 可解释性,神经符号AI,大型语言模型,生成AI,知识表示

TL;DR: 论文探讨了如何设计可转移且可解释的神经符号AI系统，重点关注了“主动性检索增强生成”系统，研究了知识的不同概念化和表示对AI代理查询知识库的影响。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 结合可解释性和适应性，设计能够跨领域应用的神经符号AI系统，尤其是在大型语言模型和生成AI领域。

Method: 通过系统性评估，研究知识的结构和复杂性如何影响AI代理（如LLM）在查询三元组存储时的表现。

Result: 研究结果表明，知识的不同概念化和表示方式对AI代理的查询效果有显著影响。

Conclusion: 知识的表示方式是提高AI系统可解释性和适应性的关键因素之一。

Abstract: Explainability and interpretability are cornerstones of frontier and
next-generation artificial intelligence (AI) systems. This is especially true
in recent systems, such as large language models (LLMs), and more broadly,
generative AI. On the other hand, adaptability to new domains, contexts, or
scenarios is also an important aspect for a successful system. As such, we are
particularly interested in how we can merge these two efforts, that is,
investigating the design of transferable and interpretable neurosymbolic AI
systems. Specifically, we focus on a class of systems referred to as ''Agentic
Retrieval-Augmented Generation'' systems, which actively select, interpret, and
query knowledge sources in response to natural language prompts. In this paper,
we systematically evaluate how different conceptualizations and representations
of knowledge, particularly the structure and complexity, impact an AI agent (in
this case, an LLM) in effectively querying a triplestore. We report our
results, which show that there are impacts from both approaches, and we discuss
their impact and implications.

</details>


### [236] [LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing](https://arxiv.org/abs/2507.09407)
*Quanyan Zhu*

Key words: LLM-Stackelberg博弈、大语言模型、推理均衡、行为均衡、鱼叉式钓鱼

TL;DR: LLM-Stackelberg框架结合大语言模型（LLM）于领导者与追随者的策略互动中，提出推理与行为均衡及推测推理均衡，展示其在网络安全领域的应用潜力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统Stackelberg博弈假设完全信息和理性代理人，而本研究通过LLM引入结构化推理提示和概率行为生成，以捕捉有限理性、信息不对称和认知适应。

Method: 提出LLM-Stackelberg博弈框架，定义推理与行为均衡及推测推理均衡，并通过鱼叉式钓鱼案例展示模型的认知丰富性和对抗潜力。

Result: LLM-Stackelberg博弈为网络安全、错误信息和推荐系统等领域提供了强有力的决策建模范式。

Conclusion: 该框架成功整合了LLM的认知能力与博弈论，适用于复杂策略互动的建模与分析。

Abstract: We introduce the framework of LLM-Stackelberg games, a class of sequential
decision-making models that integrate large language models (LLMs) into
strategic interactions between a leader and a follower. Departing from
classical Stackelberg assumptions of complete information and rational agents,
our formulation allows each agent to reason through structured prompts,
generate probabilistic behaviors via LLMs, and adapt their strategies through
internal cognition and belief updates. We define two equilibrium concepts:
reasoning and behavioral equilibrium, which aligns an agent's internal
prompt-based reasoning with observable behavior, and conjectural reasoning
equilibrium, which accounts for epistemic uncertainty through parameterized
models over an opponent's response. These layered constructs capture bounded
rationality, asymmetric information, and meta-cognitive adaptation. We
illustrate the framework through a spearphishing case study, where a sender and
a recipient engage in a deception game using structured reasoning prompts. This
example highlights the cognitive richness and adversarial potential of
LLM-mediated interactions. Our results show that LLM-Stackelberg games provide
a powerful paradigm for modeling decision-making in domains such as
cybersecurity, misinformation, and recommendation systems.

</details>


### [237] [GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective](https://arxiv.org/abs/2507.09495)
*Hang Wang,Junshan Zhang*

Key words: 多智能体强化学习, 生成式AI, 主动决策, 协作智能

TL;DR: 论文提出从反应式到主动式多智能体强化学习的范式转变，利用生成式AI技术提升智能体的预测和协调能力，解决传统方法的局限性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统多智能体强化学习方法难以应对指数增长的动作空间、非静态环境和部分可观测性问题，需要一种新的主动式智能框架。

Method: 采用生成式AI技术，将智能体建模为能合成多智能体动态、预测未来交互的生成模型，实现前瞻性决策和协调行动。

Result: 该方法通过生成式AI的识别与生成能力，提升了智能体的主动决策、协调和动态适应能力。

Conclusion: 生成式AI驱动的多智能体强化学习有望实现真正的协作智能，在自主系统、机器人等领域解决传统框架难以处理的协调问题。

Abstract: Multi-agent reinforcement learning faces fundamental challenges that
conventional approaches have failed to overcome: exponentially growing joint
action spaces, non-stationary environments where simultaneous learning creates
moving targets, and partial observability that constrains coordination. Current
methods remain reactive, employing stimulus-response mechanisms that fail when
facing novel scenarios. We argue for a transformative paradigm shift from
reactive to proactive multi-agent intelligence through generative AI-based
reinforcement learning. This position advocates reconceptualizing agents not as
isolated policy optimizers, but as sophisticated generative models capable of
synthesizing complex multi-agent dynamics and making anticipatory decisions
based on predictive understanding of future interactions. Rather than
responding to immediate observations, generative-RL agents can model
environment evolution, predict other agents' behaviors, generate coordinated
action sequences, and engage in strategic reasoning accounting for long-term
dynamics. This approach leverages pattern recognition and generation
capabilities of generative AI to enable proactive decision-making, seamless
coordination through enhanced communication, and dynamic adaptation to evolving
scenarios. We envision this paradigm shift will unlock unprecedented
possibilities for distributed intelligence, moving beyond individual
optimization toward emergent collective behaviors representing genuine
collaborative intelligence. The implications extend across autonomous systems,
robotics, and human-AI collaboration, promising solutions to coordination
challenges intractable under traditional reactive frameworks.

</details>


### [238] [Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.09534)
*Guanquan Wang,Takuya Hiraoka,Yoshimasa Tsuruoka*

Key words: 一致性轨迹规划, 离线强化学习, 扩散模型, D4RL基准

TL;DR: 本文提出了高效的单步轨迹生成方法CTP，显著降低了计算成本，同时在性能上优于现有的扩散规划方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有基于扩散模型的规划方法因迭代采样过程计算成本高，CTP旨在解决这一问题。

Method: 利用一致性轨迹模型（CTM）进行离线强化学习的轨迹优化，支持快速单步轨迹生成。

Result: 在D4RL基准测试中，CTP在长时域任务中表现优异，且推理速度提升120倍。

Conclusion: CTP是一种高效、低延迟的离线规划方法，适用于高性能需求场景。

Abstract: This paper introduces Consistency Trajectory Planning (CTP), a novel offline
model-based reinforcement learning method that leverages the recently proposed
Consistency Trajectory Model (CTM) for efficient trajectory optimization. While
prior work applying diffusion models to planning has demonstrated strong
performance, it often suffers from high computational costs due to iterative
sampling procedures. CTP supports fast, single-step trajectory generation
without significant degradation in policy quality. We evaluate CTP on the D4RL
benchmark and show that it consistently outperforms existing diffusion-based
planning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves
higher normalized returns while using significantly fewer denoising steps. In
particular, CTP achieves comparable performance with over $120\times$ speedup
in inference time, demonstrating its practicality and effectiveness for
high-performance, low-latency offline planning.

</details>


### [239] [Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling](https://arxiv.org/abs/2507.09540)
*Ali Safa,Farida Mohsen,Ali Al-Zawqari*

Key words: Spiking Neural Networks, Reinforcement Learning, Metropolis-Hastings, Neuromorphic Computing

TL;DR: 论文提出了一种基于Metropolis-Hastings采样的新型训练框架，用于强化学习中SNNs的无梯度训练，并在两个标准控制任务中取得了优于传统方法的性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 由于SNNs的脉冲通信不可微分，传统梯度方法在强化学习中训练SNNs面临挑战。论文旨在解决这一问题。

Method: 采用Metropolis-Hastings采样技术，通过迭代提议和概率接受参数更新，绕过反向传播的限制。

Result: 在AcroBot和CartPole任务中，该方法在累积奖励、网络资源和训练回合数上优于传统DQL和已有SNN方法。

Conclusion: 提出的MH采样框架为SNNs在RL任务中的训练提供了有效且资源高效的解决方案。

Abstract: Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient
alternatives to traditional Deep Neural Networks (DNNs) for real-time control
systems. However, their training presents several challenges, particularly for
reinforcement learning (RL) tasks, due to the non-differentiable nature of
spike-based communication. In this work, we introduce what is, to our
knowledge, the first framework that employs Metropolis-Hastings (MH) sampling,
a Bayesian inference technique, to train SNNs for dynamical agent control in RL
environments without relying on gradient-based methods. Our approach
iteratively proposes and probabilistically accepts network parameter updates
based on accumulated reward signals, effectively circumventing the limitations
of backpropagation while enabling direct optimization on neuromorphic
platforms. We evaluated this framework on two standard control benchmarks:
AcroBot and CartPole. The results demonstrate that our MH-based approach
outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL
approaches in terms of maximizing the accumulated reward while minimizing
network resources and training episodes.

</details>


### [240] [eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation](https://arxiv.org/abs/2507.09588)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Key words: eSapiens, AIaaS, LLM, 数据安全, 混合检索, THOR代理

TL;DR: eSapiens是一个面向企业的AIaaS平台，整合了企业数据、工作流程和主流LLM，提供数据安全和知识保留，并通过AI代理提升团队效率和业务成果。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 满足企业对AI资产控制、数据安全和高效工作流程的需求，特别是在法律和金融等高风险领域。

Method: 结合结构化文档输入、混合向量检索和LangChain无代码编排，支持多种LLMs，并通过THOR代理处理结构化查询。

Result: 实验表明，512令牌分块的检索精度最高（Top-3准确率91.3%），生成的上下文一致性提高23%。

Conclusion: eSapiens能有效支持高信任度AI工作流程，适用于法律和金融等关键领域。

Abstract: We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a
business-oriented trifecta: proprietary data, operational workflows, and any
major agnostic Large Language Model (LLM). eSapiens gives businesses full
control over their AI assets, keeping everything in-house for AI knowledge
retention and data security. eSapiens AI Agents (Sapiens) empower your team by
providing valuable insights and automating repetitive tasks, enabling them to
focus on high-impact work and drive better business outcomes.
  The system integrates structured document ingestion, hybrid vector retrieval,
and no-code orchestration via LangChain, and supports top LLMs including
OpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which
handles structured SQL-style queries and generates actionable insights over
enterprise databases.
  To evaluate the system, we conduct two experiments. First, a retrieval
benchmark on legal corpora reveals that a chunk size of 512 tokens yields the
highest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation
quality test using TRACe metrics across five LLMs shows that eSapiens delivers
more context-consistent outputs with up to a 23% improvement in factual
alignment.
  These results demonstrate the effectiveness of eSapiens in enabling
trustworthy, auditable AI workflows for high-stakes domains like legal and
finance.

</details>


### [241] [The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development](https://arxiv.org/abs/2507.09611)
*Jenis Winsta*

Key words: 人工智能, 环境挑战, 伦理问题, 可持续发展, 电子废物, 计算资源不平等, 网络安全

TL;DR: 这篇综述探讨了人工智能带来的环境与伦理挑战，包括能源消耗、电子废物、计算资源不平等和网络安全系统的隐藏能源负担，呼吁负责任的发展实践。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 人工智能的快速发展带来了被忽视的环境和伦理问题，需要通过系统性的研究来解决这些问题。

Method: 通过分析近期研究和机构报告，论文聚焦了AI在能源消耗、电子废物、计算资源不平等和网络安全方面的环境影响。

Result: 揭示了高排放、硬件快速更新、全球基础设施差异和网络安全能源需求等系统性挑战。

Conclusion: AI的发展需要与伦理责任和环境保护相结合，以实现可持续和包容的技术未来。

Abstract: Artificial intelligence (AI) has made remarkable progress in recent years,
yet its rapid expansion brings overlooked environmental and ethical challenges.
This review explores four critical areas where AI's impact extends beyond
performance: energy consumption, electronic waste (e-waste), inequality in
compute access, and the hidden energy burden of cybersecurity systems. Drawing
from recent studies and institutional reports, the paper highlights systemic
issues such as high emissions from model training, rising hardware turnover,
global infrastructure disparities, and the energy demands of securing AI. By
connecting these concerns, the review contributes to Responsible AI discourse
by identifying key research gaps and advocating for sustainable, transparent,
and equitable development practices. Ultimately, it argues that AI's progress
must align with ethical responsibility and environmental stewardship to ensure
a more inclusive and sustainable technological future.

</details>


### [242] [Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs](https://arxiv.org/abs/2507.09617)
*Margherita Martorana,Francesca Urgese,Mark Adamik,Ilaria Tiddi*

Key words: 服务机器人, 神经符号框架, 知识图谱, 本体论, 多模态语言模型

TL;DR: 本文提出了一种神经符号框架，结合多模态语言模型的感知能力与知识图谱和本体论的结构化表示，以支持机器人应用的互操作性。通过实验，发现GPT-o1和LLaMA 4 Maverick表现最优。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 家用服务机器人需要处理复杂动态环境，但目前系统依赖特定硬件和软件的专有方案，难以跨平台扩展。知识图谱和本体论虽能解决互操作性问题，但对原始感官数据处理不足；多模态语言模型擅长感知但缺乏透明度和知识基础。

Method: 提出神经符号框架，结合多模态语言模型与知识图谱，生成符合本体论的知识图谱，以平台无关方式指导机器人行为。实验整合了机器人感知数据、本体论和五种多模态模型（三种LLaMA和两种GPT模型），并评估不同神经符号交互模式。

Result: GPT-o1和LLaMA 4 Maverick表现最佳，但新模型性能不一定更好，表明集成策略对生成符合本体论的知识图谱至关重要。

Conclusion: 神经符号框架能有效结合感知与结构化表示，支持机器人互操作性，但模型选择与集成策略需谨慎。

Abstract: Personal service robots are deployed to support daily living in domestic
environments, particularly for elderly and individuals requiring assistance.
These robots must perceive complex and dynamic surroundings, understand tasks,
and execute context-appropriate actions. However, current systems rely on
proprietary, hard-coded solutions tied to specific hardware and software,
resulting in siloed implementations that are difficult to adapt and scale
across platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to
enable interoperability across systems, through structured and standardized
representations of knowledge and reasoning. However, symbolic systems such as
KGs and ontologies struggle with raw and noisy sensory input. In contrast,
multimodal language models are well suited for interpreting input such as
images and natural language, but often lack transparency, consistency, and
knowledge grounding. In this work, we propose a neurosymbolic framework that
combines the perceptual strengths of multimodal language models with the
structured representations provided by KGs and ontologies, with the aim of
supporting interoperability in robotic applications. Our approach generates
ontology-compliant KGs that can inform robot behavior in a platform-independent
manner. We evaluated this framework by integrating robot perception data,
ontologies, and five multimodal models (three LLaMA and two GPT models), using
different modes of neural-symbolic interaction. We assess the consistency and
effectiveness of the generated KGs across multiple runs and configurations, and
perform statistical analyzes to evaluate performance. Results show that GPT-o1
and LLaMA 4 Maverick consistently outperform other models. However, our
findings also indicate that newer models do not guarantee better results,
highlighting the critical role of the integration strategy in generating
ontology-compliant KGs.

</details>


### [243] [humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems](https://arxiv.org/abs/2507.09626)
*Rodion Nazarov,Anthony Quinn,Robert Shorten,Jakub Marecek*

Key words: AI系统, 多智能体, 公平性, 鲁棒性, PyTorch, 闭环模型

TL;DR: 本文介绍了一个基于PyTorch的工具包，用于在多智能体系统中建模AI系统的互联及其重复使用特性，提供公平性和鲁棒性的闭环保障。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: AI系统经常与多个智能体交互，需要提供公平性和鲁棒性的先验保障。

Method: 使用随机控制技术建模AI系统与智能体的互联，并通过基于PyTorch的工具包实现闭环公平性和鲁棒性保障。

Result: 工具包简化了多智能体闭环模型公平性保障的复杂性。

Conclusion: 该工具包为AI系统与多智能体交互提供了实用的公平性和鲁棒性保障解决方案。

Abstract: Artificial intelligence (AI) systems often interact with multiple agents. The
regulation of such AI systems often requires that {\em a priori\/} guarantees
of fairness and robustness be satisfied. With stochastic models of agents'
responses to the outputs of AI systems, such {\em a priori\/} guarantees
require non-trivial reasoning about the corresponding stochastic systems. Here,
we present an open-source PyTorch-based toolkit for the use of stochastic
control techniques in modelling interconnections of AI systems and properties
of their repeated uses. It models robustness and fairness desiderata in a
closed-loop fashion, and provides {\em a priori\/} guarantees for these
interconnections. The PyTorch-based toolkit removes much of the complexity
associated with the provision of fairness guarantees for closed-loop models of
multi-agent systems.

</details>


### [244] [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
*Jason Zhu,Hongyu Li*

Key words: 大型推理模型, 自适应推理, 推理效率, 简洁推理

TL;DR: 大型推理模型（LRMs）在复杂任务上表现出色，但生成长而冗余的推理链浪费资源。本文总结了简洁与自适应推理的研究进展。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: LRMs在面对简单问题时仍生成冗余推理链，导致资源浪费和响应时间增加，阻碍其实际应用。

Method: 综述了近期关于简洁和自适应推理的方法论，包括适应输入难度的推理技术。

Result: 概述了相关方法、基准和未来挑战，帮助研究者快速了解该领域。

Conclusion: 倡导研究自适应推理技术以优化LRMs的使用效率。

Abstract: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

</details>


### [245] [Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations](https://arxiv.org/abs/2507.09742)
*Xiaofeng Xiao,Bo Shen,Xubo Yue*

Key words: 因果分析、深度Q网络、传感器布局、异常检测、强化学习

TL;DR: 该论文提出了一种基于因果关系的深度Q网络（Causal DQ）方法，用于在部分可观察的传感器布局中进行异常检测，通过结合因果信息，提高了训练效率和异常检测速度。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 由于资源有限，无法在所有位置部署传感器进行实时监控，因此需要开发一种最优的传感器布局策略，以在部分可观察的系统中快速检测异常。现有方法大多忽略因果关系或依赖人为干预，存在实际应用中的局限性。

Method: 提出Causal DQ方法，通过在Q网络训练的每个阶段整合因果信息，优化传感器布局策略。

Result: 该方法实现了更快的收敛速度和更严格的理论误差界限，显著缩短了异常检测时间，适用于大规模实时数据流。

Conclusion: Causal DQ不仅适用于传感器布局问题，其核心理念还可应用于其他强化学习任务，为工程应用中的因果机器学习方法开辟了新途径。

Abstract: Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume
of data streams requiring real-time monitoring continues to grow. However, due
to limited resources, it is impractical to place sensors at every location to
detect unexpected shifts. Therefore, it is necessary to develop an optimal
sensor placement strategy that enables partial observability of the system
while detecting anomalies as quickly as possible. Numerous approaches have been
proposed to address this challenge; however, most existing methods consider
only variable correlations and neglect a crucial factor: Causality. Moreover,
although a few techniques incorporate causal analysis, they rely on
interventions-artificially creating anomalies-to identify causal effects, which
is impractical and might lead to catastrophic losses. In this paper, we
introduce a causality-informed deep Q-network (Causal DQ) approach for
partially observable sensor placement in anomaly detection. By integrating
causal information at each stage of Q-network training, our method achieves
faster convergence and tighter theoretical error bounds. Furthermore, the
trained causal-informed Q-network significantly reduces the detection time for
anomalies under various settings, demonstrating its effectiveness for sensor
placement in large-scale, real-world data streams. Beyond the current
implementation, our technique's fundamental insights can be applied to various
reinforcement learning problems, opening up new possibilities for real-world
causality-informed machine learning methods in engineering applications.

</details>


### [246] [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
*Bradley P. Allen,Prateek Chhikara,Thomas Macaulay Ferguson,Filip Ilievski,Paul Groth*

Key words: 大语言模型、形式语义、非一致逻辑、神经符号推理、知识利用

TL;DR: 提出了一种将大语言模型（LLM）与非一致逻辑的形式语义结合的方法，以解决其输出逻辑不一致的问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 尽管LLM在自然语言理解和生成方面表现出色，但其输出存在逻辑不一致问题，需要一种方法利用其广泛参数化知识进行形式推理。

Method: 将LLM直接集成到非一致逻辑的形式语义解释函数中，并通过实验验证其可行性。

Result: 实验表明该方法可行，并提供了一个理论框架，既能利用LLM的知识，又能保持逻辑的健全性和完整性。

Conclusion: 该方法为神经符号推理提供了一种新的理论框架，同时解决了LLM的逻辑不一致问题。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

</details>


### [247] [Technical Requirements for Halting Dangerous AI Activities](https://arxiv.org/abs/2507.09801)
*Peter Barnett,Aaron Scher,David Abecassis*

Key words: AI风险、AI治理、技术干预、协调暂停

TL;DR: 本文探讨了AI快速发展带来的风险，并提出了通过技术干预实现协调暂停危险AI活动的治理方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: AI系统的快速发展带来了前所未有的风险，如失控、滥用、地缘政治不稳定和权力集中。为了避免最坏情况，政府需要主动建立协调暂停危险AI发展的能力。

Method: 提出了关键的技术干预措施，以协调暂停危险的AI活动，并探讨了这些措施如何限制各种危险AI活动。

Result: 这些技术干预措施可以为潜在的AI治理计划提供技术基础。

Conclusion: 通过技术干预实现协调暂停危险AI活动是可行的，并为AI治理提供了技术支持。

Abstract: The rapid development of AI systems poses unprecedented risks, including loss
of control, misuse, geopolitical instability, and concentration of power. To
navigate these risks and avoid worst-case outcomes, governments may proactively
establish the capability for a coordinated halt on dangerous AI development and
deployment. In this paper, we outline key technical interventions that could
allow for a coordinated halt on dangerous AI activities. We discuss how these
interventions may contribute to restricting various dangerous AI activities,
and show how these interventions can form the technical foundation for
potential AI governance plans.

</details>


### [248] [Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation](https://arxiv.org/abs/2507.09850)
*Wei Du,Branislav Kisacanin,George Armstrong,Shubham Toshniwal,Ivan Moshkov,Alexan Ayrapetyan,Sadegh Mahdavi,Dan Zhao,Shizhe Diao,Dragan Masulovic,Marius Stanean,Advaith Avadhanam,Max Wang,Ashmit Dutta,Shitij Govil,Sri Yanamandara,Mihir Tandon,Sriram Ananthakrishnan,Vedant Rathi,David Zhang,Joonseok Kang,Leon Luo,Titu Andreescu,Boris Ginsburg,Igor Gitman*

Key words: Chain-of-Thought, reasoning distillation, fine-tuning, prompt engineering

TL;DR: 通过少量高质量的长链思维（CoT）示例微调基础模型，可以显著提升其推理能力，远超更大模型的性能。虽然尝试使用非推理模型数据或人工注释数据，但仍不及专家CoT的效果。研究突出了数据质量对推理蒸馏的关键作用。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索如何仅通过少量高质量的CoT示例或提示，在不进行大量微调的情况下，激活基础模型的推理能力。

Method: 使用20个来自	exttt{QwQ-32B-Preview}的长CoT示例微调基础模型	exttt{Qwen2.5-32B}，并尝试使用非推理模型数据和人工注释数据结合提示工程等方法。

Result: 微调后的模型性能超过更大的	exttt{Qwen2.5-Math-72B-Instruct}，但非专家CoT数据效果不佳。

Conclusion: 少量高质量的专家CoT数据可以显著提升基础模型推理能力，但其独特属性难以复制。

Abstract: Reasoning-capable language models achieve state-of-the-art performance in
diverse complex tasks by generating long, explicit Chain-of-Thought (CoT)
traces. While recent works show that base models can acquire such reasoning
traces via reinforcement learning or distillation from stronger models like
DeepSeek-R1, previous works demonstrate that even short CoT prompting without
fine-tuning is able to improve reasoning. We ask whether long CoT can be
induced in a base model using only prompting or minimal tuning. Using just 20
long CoT examples from the reasoning model \texttt{QwQ-32B-Preview}, we lightly
fine-tune the base model \texttt{Qwen2.5-32B}. The resulting model outperforms
the much larger \texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of
high-quality examples can unlock strong reasoning capabilities. We further
explore using CoT data from non-reasoning models and human annotators, enhanced
with prompt engineering, multi-pass editing, and structural guidance. However,
neither matches the performance of reasoning model traces, suggesting that
certain latent qualities of expert CoT are difficult to replicate. We analyze
key properties of reasoning data, such as problem difficulty, diversity, and
answer length, that influence reasoning distillation. While challenges remain,
we are optimistic that carefully curated human-written CoT, even in small
quantities, can activate reasoning behaviors in base models. We release our
human-authored dataset across refinement stages and invite further
investigation into what makes small-scale reasoning supervision so effective.

</details>


### [249] [Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems](https://arxiv.org/abs/2507.09854)
*Aniruddha Chattopadhyay,Raj Dandekar,Kaushik Roy*

Key words: 神经符号AI, 大语言模型, 符号接地, 学习与推理

TL;DR: 本文提出将指令调整的大语言模型重新解释为基于模型的符号AI系统，通过自然语言作为符号层，利用模型内部表示空间实现接地。研究了新的学习与推理方法，并在不同复杂度的演绎推理中验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 结合神经网络与传统符号AI的优势，探索大语言模型在符号AI中的新应用，以提升学习效率和推理可靠性。

Method: 将大语言模型重新定义为基于模型的符号AI系统，利用自然语言作为符号层，开发新的学习与推理方法。

Result: 初步评估表明，该方法在不同复杂度的演绎推理任务中提高了学习效率和推理可靠性。

Conclusion: 研究展示了将大语言模型重新解释为符号AI系统的潜力，为神经符号AI的发展提供了新思路。

Abstract: Neurosymbolic artificial intelligence (AI) systems combine neural network and
classical symbolic AI mechanisms to exploit the complementary strengths of
large scale, generalizable learning and robust, verifiable reasoning. Numerous
classifications of neurosymbolic AI illustrate how these two components can be
integrated in distinctly different ways. In this work, we propose
reinterpreting instruction tuned large language models as model grounded
symbolic AI systems where natural language serves as the symbolic layer and
grounding is achieved through the models internal representation space. Within
this framework, we investigate and develop novel learning and reasoning
approaches that preserve structural similarities to traditional learning and
reasoning paradigms. Preliminary evaluations across axiomatic deductive
reasoning procedures of varying complexity provide insights into the
effectiveness of our approach in improving learning efficiency and reasoning
reliability.

</details>


### [250] [VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains](https://arxiv.org/abs/2507.09884)
*Xuzhao Li,Xuchen Li,Shiyu Hu,Yongzhen Guo,Wentao Zhang*

Key words: 大型语言模型, 强化学习, 验证器, 跨领域评估, 基准测试

TL;DR: 论文提出VerifyBench，一个跨领域基准测试，评估大型语言模型（LLM）反馈验证器的性能，揭示专业验证器和通用模型的优缺点及其对输入结构的敏感性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有验证器对复杂、多样化的模型生成响应缺乏一致性和灵活性，限制了强化学习与可验证奖励（RLVR）的发展。

Method: 构建包含4000个专家级问题的跨领域基准测试（数学、物理、化学、生物），通过严格标注和多维度实验框架，对比专业验证器和通用LLM的性能。

Result: 专业验证器准确率高但召回率低；通用模型包容性强但精度不稳定；验证器对输入结构敏感且跨领域泛化能力有限。

Conclusion: 研究揭示了当前验证器技术的瓶颈，为未来改进提供了关键见解。

Abstract: Large language models (LLMs) increasingly rely on reinforcement learning (RL)
to enhance their reasoning capabilities through feedback. A critical challenge
is verifying the consistency of model-generated responses and reference
answers, since these responses are often lengthy, diverse, and nuanced.
Rule-based verifiers struggle with complexity, prompting the use of model-based
verifiers. However, specialized verifiers lack flexibility, while general LLM
judges can be inconsistent. Existing research primarily focuses on building
better verifiers, yet a systematic evaluation of different types of verifiers'
performance across domains remains lacking, severely constraining the reliable
development of Reinforcement Learning with Verifiable Reward (RLVR). To address
this, we propose VerifyBench--a cross-domain comprehensive benchmark for
systematically evaluating verifiers. We construct 4,000 expert-level questions
covering mathematics, physics, chemistry, and biology. Each question is
equipped with reference answers and diverse responses. The reliability of the
evaluation is ensured through a rigorous annotation process conducted by a
multidisciplinary expert team. We design a four-dimensional experimental
framework to comprehensively compare the performance boundaries of specialized
verifiers and general LLMs under combined conditions of extracted answers vs.
complete responses, and short vs. long outputs. Our evaluation uncovers
fundamental trade-offs in verifiers: while specialized verifiers achieve
leading accuracy, they exhibit deficiencies in recall; general models show
stronger inclusivity but unstable precision. More importantly, we discover
verifiers' high sensitivity to input structure and inherent limitations in
cross-domain generalization, providing critical insights into the bottlenecks
of current verifier technology.

</details>


### [251] [DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models](https://arxiv.org/abs/2507.09955)
*Luolin Xiong,Haofen Wang,Xi Chen,Lu Sheng,Yun Xiong,Jingping Liu,Yanghua Xiao,Huajun Chen,Qing-Long Han,Yang Tang*

Key words: DeepSeek, AI模型, 开源, MLA, MoE

TL;DR: DeepSeek发布V3和R1系列模型，分析其创新算法和工程突破，探讨其对AI竞争的影���，并展望未来趋势。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: DeepSeek的V3和R1系列模型因其低成本、高性能和开源优势引起全球关注，本文旨在分析其技术创新和行业影响。

Method: 回顾大型AI模型的演变，重点介绍DeepSeek的创新算法（MLA、MoE等）和工程突破（训练、推理优化等）。

Result: DeepSeek模型在多领域表现优异，对AI竞争格局产生显著影响。

Conclusion: DeepSeek的创新为大型AI模型的发展提供了新方向，未来需关注数据、训练和推理技术的进步。

Abstract: DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their
V3 and R1 series models, which attracted global attention due to their low
cost, high performance, and open-source advantages. This paper begins by
reviewing the evolution of large AI models focusing on paradigm shifts, the
mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.
Subsequently, the paper highlights novel algorithms introduced by DeepSeek,
including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),
Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).
The paper then explores DeepSeek engineering breakthroughs in LLM scaling,
training, inference, and system-level optimization architecture. Moreover, the
impact of DeepSeek models on the competitive AI landscape is analyzed,
comparing them to mainstream LLMs across various fields. Finally, the paper
reflects on the insights gained from DeepSeek innovations and discusses future
trends in the technical and engineering development of large AI models,
particularly in data, training, and reasoning.

</details>


### [252] [Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient](https://arxiv.org/abs/2507.09989)
*Xiaoyang Yu,Youfang Lin,Shuo Wang,Sheng Han*

Key words: 多智能体强化学习，参数共享，单调改进，OMDPG，最优边际Q函数

TL;DR: OMDPG算法解决了异构多智能体强化学习中单调改进与部分参数共享的冲突，通过引入最优边际Q函数和广义Q批评家，实现了高性能合作。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 异构多智能体强化学习中，单调改进与部分参数共享存在冲突，直接结合会导致策略更新基线漂移问题。

Method: 提出OMDPG算法，用最优边际Q函数替代序列计算，引入广义Q批评家，采用集中式批评家分组执行架构。

Result: 在SMAC和MAMuJoCo环境中，OMDPG优于现有MARL基线。

Conclusion: OMDPG成功平衡了单调改进与部分参数共享的需求，提升了异构MARL的性能。

Abstract: In heterogeneous multi-agent reinforcement learning (MARL), achieving
monotonic improvement plays a pivotal role in enhancing performance. The HAPPO
algorithm proposes a feasible solution by introducing a sequential update
scheme, which requires independent learning with No Parameter-sharing (NoPS).
However, heterogeneous MARL generally requires Partial Parameter-sharing
(ParPS) based on agent grouping to achieve high cooperative performance. Our
experiments prove that directly combining ParPS with the sequential update
scheme leads to the policy updating baseline drift problem, thereby failing to
achieve improvement. To solve the conflict between monotonic improvement and
ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)
algorithm. First, we replace the sequentially computed $Q_{\psi}^s(s,a_{1:i})$
with the Optimal Marginal Q (OMQ) function $\phi_{\psi}^*(s,a_{1:i})$ derived
from Q-functions. This maintains MAAD's monotonic improvement while eliminating
the conflict through optimal joint action sequences instead of sequential
policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)
as the critic function, employing pessimistic uncertainty-constrained loss to
optimize different Q-value estimations. This provides the required Q-values for
OMQ computation and stable baselines for actor updates. Finally, we implement a
Centralized Critic Grouped Actor (CCGA) architecture that simultaneously
achieves ParPS in local policy networks and accurate global Q-function
computation. Experimental results in SMAC and MAMuJoCo environments demonstrate
that OMDPG outperforms various state-of-the-art MARL baselines.

</details>


### [253] [On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model](https://arxiv.org/abs/2507.10000)
*Mark Burgess*

Key words: Promise Theory, 语义时空模型, 意图性, 多尺度异常, 时空一致性

TL;DR: 论文提出了一种基于Promise Theory的语义时空模型，用于低成本、低计算需求下评估数据的潜在意图性，通过多尺度异常和时空一致性区分意图内容与环境背景。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 自Searle在哲学领域解构意图和意向性后，科学与技术中意图的实际意义鲜少被研究。本文旨在填补这一空白，提供一种实用方法评估数据的潜在意图性。

Method: 利用Promise Theory的语义时空模型，通过多尺度异常检测和时空一致性分析，区分意图内容与环境背景。该方法无需语言知识或大规模计算。

Result: 研究实现了对潜在意图性的初级但实用解释，适用于计算能力有限的系统，甚至基本生物体也可完成。概念形成水平取决于代理的记忆能力。

Conclusion: 该方法为低成本、低计算需求下的意图性分析提供了可行途径，但概念形成受限于代理的记忆能力。

Abstract: Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

</details>


### [254] [Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.10007)
*Zijun Chen,Wenbo Hu,Richang Hong*

Key words: 思维链推理、大型语言模型、多模态、注意力机制、置信度预测、动态路径选择

TL;DR: 文章提出了一种通过利用模型内在的真实性编码来校准思维链（CoT）推理准确性的新方法，显著提升了推理的可靠性和准确性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 思维链推理在大型语言模型和多模态语言模型中表现出强大的推理能力，但其可靠性常受中间步骤错误积累的影响，因此需要一种方法来提升推理的准确性。

Method: 通过发现特定注意力头激活能可靠反映推理步骤的真实性，训练一个置信度预测器动态选择最可信的推理路径。

Result: 实验表明，该方法在数学、符号和常识推理任务中显著优于现有基线方法，并在单模态和多模态场景中表现优越。

Conclusion: 该方法为提升思维链推理的可靠性提供了新途径，具有广泛的应用潜力。

Abstract: Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning
capabilities in both large language models (LLMs) and multimodal large language
models (MLLMs). However, its reliability is often undermined by the
accumulation of errors in intermediate steps. This paper introduces an novel
approach to calibrate the CoT reasoning accuracy by leveraging the model's
intrinsic veracity encoding. We discover that specific attention head
activations reliably reflect the truthfulness of reasoning steps in CoT. Based
on this insight, we train a confidence predictor to evaluate the correctness of
each reasoning step using these truthfulness-sensitive activations, dynamically
selecting the most plausible reasoning path via beam search. Experimental
results demonstrate that our method significantly outperforms the
state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and
Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and
commonsense reasoning tasks, exhibiting superior accuracy and reliability in
both unimodal and multimodal settings. We further validate the approach on
large reasoning models, confirming its applicability to specialized reasoning
models. Additionally, we explore the role of the model's self-correction
ability in CoT reasoning. This work provides a novel reliability improvement
path for CoT reasoning with broad application potential.

</details>


### [255] [Automating SPARQL Query Translations between DBpedia and Wikidata](https://arxiv.org/abs/2507.10045)
*Malte Christian Bartels,Debayan Banerjee,Ricardo Usbeck*

Key words: LLMs, SPARQL, Knowledge Graphs, interoperability, translation

TL;DR: 研究评估了LLMs在不同知识图谱间自动翻译SPARQL查询的能力，发现性能因模型和提示策略而异。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 填补知识图谱互操作性研究中SPARQL自动翻译的空白。

Method: 使用三种LLM模型，通过零-shot、少-shot和思维链变体测试在两个基准上的翻译效果。

Result: 不同模型和提示策略性能差异显著，维基数据到DBpedia的翻译效果优于反向。

Conclusion: LLMs在SPARQL翻译中表现出潜力，但需进一步优化。

Abstract: This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

</details>


### [256] [On Gradual Semantics for Assumption-Based Argumentation](https://arxiv.org/abs/2507.10076)
*Anna Rapberger,Fabrizio Russo,Antonio Rago,Francesca Toni*

Key words: 渐进语义，假设基础论辩，量化双极论辩框架，平衡性，单调性

TL;DR: 该论文提出了针对假设基础论辩（ABA）框架的渐进语义家族，填补了该领域的研究空白，并通过实验验证了其与现有方法的对比和收敛性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 渐进语义在计算论辩中对论点的可接受性提供了细粒度的度量，但在假设基础论辩（ABA）中尚未得到充分研究。本文旨在填补这一空白。

Method: 作者通过双极集论辩框架作为ABA框架的抽象，并扩展了量化双极论辩框架（QBAF）的模块化渐进语义，提出了一系列新的渐进语义。

Result: 提出的渐进ABA语义满足平衡性和单调性等理想性质，并通过实验验证了与现有方法的比较和收敛性。

Conclusion: 该研究为ABA框架提供了有效的渐进语义方法，扩展了渐进语义的应用范围，并验证了其合理性和实用性。

Abstract: In computational argumentation, gradual semantics are fine-grained
alternatives to extension-based and labelling-based semantics . They ascribe a
dialectical strength to (components of) arguments sanctioning their degree of
acceptability. Several gradual semantics have been studied for abstract,
bipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,
to a lesser extent, for some forms of structured argumentation. However, this
has not been the case for assumption-based argumentation (ABA), despite it
being a popular form of structured argumentation with several applications
where gradual semantics could be useful. In this paper, we fill this gap and
propose a family of novel gradual semantics for equipping assumptions, which
are the core components in ABA frameworks, with dialectical strengths. To do
so, we use bipolar set-based argumentation frameworks as an abstraction of
(potentially non-flat) ABA frameworks and generalise state-of-the-art modular
gradual semantics for QBAFs. We show that our gradual ABA semantics satisfy
suitable adaptations of desirable properties of gradual QBAF semantics, such as
balance and monotonicity. We also explore an argument-based approach that
leverages established QBAF modular semantics directly, and use it as baseline.
Finally, we conduct experiments with synthetic ABA frameworks to compare our
gradual ABA semantics with its argument-based counterpart and assess
convergence.

</details>


### [257] [BlueGlass: A Framework for Composite AI Safety](https://arxiv.org/abs/2507.10106)
*Harshal Nandigramwar,Syed Qutub,Kay-Ulrich Scholl*

Key words: AI安全, 视觉语言模型, 框架, 分布评估, 稀疏自编码器

TL;DR: BlueGlass框架旨在通过整合多样化安全工具，提升AI系统的安全性，并以视觉语言模型为例展示了其效用。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着AI系统能力增强和普及，确保其安全性变得至关重要。现有安全工具难以独立提供全面保障，需要综合方法。

Method: 引入BlueGlass框架，统一基础设施以整合和组合多元化安全工具，覆盖模型内部与输出。

Result: 通过三种安全分析展示了框架的实用性，包括分布评估、分层动态分析和稀疏自编码器识别。

Conclusion: BlueGlass为构建更稳健可靠的AI系统提供了基础架构和重要发现。

Abstract: As AI systems become increasingly capable and ubiquitous, ensuring the safety
of these systems is critical. However, existing safety tools often target
different aspects of model safety and cannot provide full assurance in
isolation, highlighting a need for integrated and composite methodologies. This
paper introduces BlueGlass, a framework designed to facilitate composite AI
safety workflows by providing a unified infrastructure enabling the integration
and composition of diverse safety tools that operate across model internals and
outputs. Furthermore, to demonstrate the utility of this framework, we present
three safety-oriented analyses on vision-language models for the task of object
detection: (1) distributional evaluation, revealing performance trade-offs and
potential failure modes across distributions; (2) probe-based analysis of layer
dynamics highlighting shared hierarchical learning via phase transition; and
(3) sparse autoencoders identifying interpretable concepts. More broadly, this
work contributes foundational infrastructure and findings for building more
robust and reliable AI systems.

</details>


### [258] [Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration](https://arxiv.org/abs/2507.10119)
*Sadig Gojayev,Ahmad Anaqreh,Carolina Fortuna*

Key words: 边缘-云系统, 应用迁移, 汉诺塔问题, MDP, AI规划, 强化学习

TL;DR: 该论文研究了边缘-云系统中应用程序迁移的自动编排问题，比较了基于MDP的AI规划和强化学习方法，重点关注类似汉诺塔问题的模型分类。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 边缘-云系统中的应用迁移能提高服务质量并降低成本，但现有方法多为启发式，需更高效的自动编排技术。

Method: 通过马尔可夫决策过程（MDP）框架，分析和比较AI规划与强化学习方法，提出基于状态空间定义的新分类。

Result: 论文提出了一种新的状态空间分类方法，并对适用于汉诺塔类迁移问题的技术进行了比较分析。

Conclusion: 研究为新兴计算连续体环境中的应用迁移编排提供了技术参考和方法比较。

Abstract: Application migration in edge-cloud system enables high QoS and cost
effective service delivery. However, automatically orchestrating such migration
is typically solved with heuristic approaches. Starting from the Markov
Decision Process (MDP), in this paper, we identify, analyze and compare
selected state-of-the-art Artificial Intelligence (AI) planning and
Reinforcement Learning (RL) approaches for solving the class of edge-cloud
application migration problems that can be modeled as Towers of Hanoi (ToH)
problems. We introduce a new classification based on state space definition and
analyze the compared models also through this lense. The aim is to understand
available techniques capable of orchestrating such application migration in
emerging computing continuum environments.

</details>


### [259] [Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making](https://arxiv.org/abs/2507.10124)
*Thomas T. Hills*

Key words: LLM, 偏见, 元认知提示, 人类心理学, 去偏

TL;DR: 摘要讨论了利用人类心理学的策略来减少LLM（大型语言模型）中的偏见，特别是通过一种名为“你能出错吗？”的元认知提示，以引导模型暴露潜在偏见和矛盾证据。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 由于LLM仍在发展中，当前的偏见问题可能随着模型更新而变化，因此需要一种通用的去偏方法，而人类决策中的去偏策略提供了可行的思路。

Method: 借鉴人类决策中的元认知提示（如“你能出错吗？”），在LLM生成初始回答后，通过提示引导其反思潜在偏见、错误和矛盾证据。

Result: 实验表明，这种提示能有效让LLM暴露初始回答中未显示的偏见、替代观点和矛盾信息，揭示模型与用户对提示理解的偏差。

Conclusion: 人类心理学为提示工程提供了新方向，利用长期积累的决策优化策略，能显著提升LLM的自我反思能力和去偏效果。

Abstract: Identifying bias in LLMs is ongoing. Because they are still in development,
what is true today may be false tomorrow. We therefore need general strategies
for debiasing that will outlive current models. Strategies developed for
debiasing human decision making offer one promising approach as they
incorporate an LLM-style prompt intervention designed to bring latent knowledge
into awareness during decision making. LLMs trained on vast amounts of
information contain information about potential biases, counter-arguments, and
contradictory evidence, but that information may only be brought to bear if
prompted. Metacognitive prompts developed in the human decision making
literature are designed to achieve this, and as I demonstrate here, they show
promise with LLMs. The prompt I focus on here is "could you be wrong?"
Following an LLM response, this prompt leads LLMs to produce additional
information, including why they answered as they did, errors, biases,
contradictory evidence, and alternatives, none of which were apparent in their
initial response. Indeed, this metaknowledge often reveals that how LLMs and
users interpret prompts are not aligned. Here I demonstrate this prompt using a
set of questions taken from recent articles about LLM biases, including
implicit discriminatory biases and failures of metacognition. "Could you be
wrong" prompts the LLM to identify its own biases and produce cogent
metacognitive reflection. I also present another example involving convincing
but incomplete information, which is readily corrected by the metacognitive
prompt. In sum, this work argues that human psychology offers a new avenue for
prompt engineering, leveraging a long history of effective prompt-based
improvements to human decision making.

</details>


### [260] [FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring](https://arxiv.org/abs/2507.10134)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Key words: 无人机, 野火监测, 信息年龄, LLM, 上下文学习

TL;DR: 论文提出了一种基于LLM的在线飞行资源分配方案（FRSICL），用于无人机野火监测系统，通过自然语言任务描述和环境反馈动态优化飞行控制与数据收集，最小化信息年龄（AoI）。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 无人机在野火监测中的早期检测至关重要，但现有深度强化学习（DRL）方法存在采样效率低、仿真与现实差距大等问题，不适用于时间敏感任务。FRSICL旨在实时动态优化，避免DRL的局限性。

Method: FRSICL利用LLM支持的上下文学习，通过自然语言任务描述和环境反馈，实时生成数据收集计划和速度控制策略，无需大量重复训练。

Result: 仿真结果表明，FRSICL在平均AoI优化上优于PPO和最近邻基线方法。

Conclusion: FRSICL为无人机野火监测提供了一种高效、动态的资源分配方案，优于传统DRL方法。

Abstract: Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in
wildfire monitoring, where early detection minimizes environmental impact. In
UAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor
transmission scheduling and velocity is critical for minimizing Age of
Information (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has
been used for such optimization; however, its limitations such as low sampling
efficiency, simulation-to-reality gaps, and complex training render it
unsuitable for time-critical applications like wildfire monitoring. This paper
introduces a new online Flight Resource Allocation scheme based on LLM-Enabled
In-Context Learning (FRSICL) to jointly optimize the UAV's flight control and
data collection schedule along the trajectory in real time, thereby
asymptotically minimizing the average AoI across ground sensors. In contrast to
DRL, FRSICL generates data collection schedules and controls velocity using
natural language task descriptions and feedback from the environment, enabling
dynamic decision-making without extensive retraining. Simulation results
confirm the effectiveness of the proposed FRSICL compared to Proximal Policy
Optimization (PPO) and Nearest-Neighbor baselines.

</details>


### [261] [Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review](https://arxiv.org/abs/2507.10142)
*Siyi Hu,Mohamad A Hady,Jianglin Qiao,Jimmy Cao,Mahardhika Pratama,Ryszard Kowalczyk*

Key words: 多智能体强化学习, 适应性, 动态环境, 性能评估

TL;DR: 该论文提出“适应性”作为评估多智能体强化学习（MARL）在动态环境中可靠性的统一概念，并分为三个维度：学习适应性、策略适应性和场景驱动适应性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: MARL在现实多智能体系统（MAS）中应用受限，因其复杂动态环境的挑战，如智能体数量波动、任务目标变化和执行条件不一致。

Method: 提出适应性框架，包含学习适应性、策略适应性和场景驱动适应性三个维度，以评估MARL在动态环境中的性能。

Result: 通过适应性视角支持更系统的MARL性能评估，有助于开发更适合动态现实MAS的算法。

Conclusion: 适应性框架为MARL在动态现实环境中的部署提供了理论基础和评估工具。

Abstract: Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in
coordinating multiple agents across simulated benchmarks and constrained
scenarios. However, its deployment in real-world multi-agent systems (MAS)
remains limited, primarily due to the complex and dynamic nature of such
environments. These challenges arise from multiple interacting sources of
variability, including fluctuating agent populations, evolving task goals, and
inconsistent execution conditions. Together, these factors demand that MARL
algorithms remain effective under continuously changing system configurations
and operational demands. To better capture and assess this capacity for
adjustment, we introduce the concept of \textit{adaptability} as a unified and
practically grounded lens through which to evaluate the reliability of MARL
algorithms under shifting conditions, broadly referring to any changes in the
environment dynamics that may occur during learning or execution. Centred on
the notion of adaptability, we propose a structured framework comprising three
key dimensions: learning adaptability, policy adaptability, and scenario-driven
adaptability. By adopting this adaptability perspective, we aim to support more
principled assessments of MARL performance beyond narrowly defined benchmarks.
Ultimately, this survey contributes to the development of algorithms that are
better suited for deployment in dynamic, real-world multi-agent systems.

</details>


### [262] [Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation](https://arxiv.org/abs/2507.10156)
*Lubnaa Abdur Rahman,Ioannis Papathanail,Stavroula Mougiakakou*

Key words: 瑞士食品知识图谱, LLM, 饮食评估, 营养信息, Graph-RAG

TL;DR: 提出了瑞士食品知识图谱（SwissFKG），整合食谱、食材、营养数据及饮食限制等信息，利用LLM丰富图谱，并展示其在用户特定营养查询中的应用。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决现有自动饮食评估系统忽视非视觉因素（如食材替代和个人饮食需求）及瑞士食品信息分散的问题。

Method: 建立LLM驱动的知识图谱填充流程，评估四款现成LLM的食品知识增强能力，并开发Graph-RAG应用展示图谱的自然语言查询能力。

Result: LLM能有效丰富知识图谱的营养信息，Graph-RAG应用能帮助回答用户特定营养问题。

Conclusion: SwissFKG为结合视觉、上下文和文化维度的新一代饮食评估工具奠定基础。

Abstract: AI has driven significant progress in the nutrition field, especially through
multimedia-based automatic dietary assessment. However, existing automatic
dietary assessment systems often overlook critical non-visual factors, such as
recipe-specific ingredient substitutions that can significantly alter
nutritional content, and rarely account for individual dietary needs, including
allergies, restrictions, cultural practices, and personal preferences. In
Switzerland, while food-related information is available, it remains
fragmented, and no centralized repository currently integrates all relevant
nutrition-related aspects within a Swiss context. To bridge this divide, we
introduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our
best knowledge, to unite recipes, ingredients, and their substitutions with
nutrient data, dietary restrictions, allergen information, and national
nutrition guidelines under one graph. We establish a LLM-powered enrichment
pipeline for populating the graph, whereby we further present the first
benchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge
augmentation. Our results demonstrate that LLMs can effectively enrich the
graph with relevant nutritional information. Our SwissFKG goes beyond recipe
recommendations by offering ingredient-level information such as allergen and
dietary restriction information, and guidance aligned with nutritional
guidelines. Moreover, we implement a Graph-RAG application to showcase how the
SwissFKG's rich natural-language data structure can help LLM answer
user-specific nutrition queries, and we evaluate LLM-embedding pairings by
comparing user-query responses against predefined expected answers. As such,
our work lays the foundation for the next generation of dietary assessment
tools that blend visual, contextual, and cultural dimensions of eating.

</details>


### [263] [Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?](https://arxiv.org/abs/2507.10174)
*Yumi Omori,Zixuan Dong,Keith Ross*

Key words: 决策变换器,离线强化学习,稀疏奖励,行为克隆

TL;DR: 本文通过实验证明，在稀疏奖励环境中，基于MLP的过滤行为克隆（FBC）比决策变换器（DT）表现更优。FBC简单高效且数据需求更少，质疑DT的适用性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探讨决策变换器（DT）在离线强化学习中的实际优势，尤其是在稀疏奖励环境中的应用效果。

Method: 在机器人操纵任务（Robomimic）和运动基准（D4RL）上，比较FBC与DT的性能。FBC通过过滤低质量轨迹后进行行为克隆。

Result: FBC在稀疏奖励环境中表现优于或与DT相当，且更高效。DT在稀疏和密集奖励环境中均不占优。

Conclusion: DT在多种环境中可能并非最优选择，提出对其适用性的质疑。

Abstract: In recent years, extensive work has explored the application of the
Transformer architecture to reinforcement learning problems. Among these,
Decision Transformer (DT) has gained particular attention in the context of
offline reinforcement learning due to its ability to frame return-conditioned
policy learning as a sequence modeling task. Most recently, Bhargava et al.
(2024) provided a systematic comparison of DT with more conventional MLP-based
offline RL algorithms, including Behavior Cloning (BC) and Conservative
Q-Learning (CQL), and claimed that DT exhibits superior performance in
sparse-reward and low-quality data settings.
  In this paper, through experimentation on robotic manipulation tasks
(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered
Behavior Cloning (FBC) achieves competitive or superior performance compared to
DT in sparse-reward environments. FBC simply filters out low-performing
trajectories from the dataset and then performs ordinary behavior cloning on
the filtered dataset. FBC is not only very straightforward, but it also
requires less training data and is computationally more efficient. The results
therefore suggest that DT is not preferable for sparse-reward environments.
From prior work, arguably, DT is also not preferable for dense-reward
environments. Thus, we pose the question: Is DT ever preferable?

</details>


### [264] [Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks](https://arxiv.org/abs/2507.10208)
*Hamzah Ziadeh,Hendrik Knoche*

Key words: 可解释人工智能（XAI）、数据分�、任务分类、研究设计

TL;DR: 本文提出了一种将XAI研究分类和比较的方法，解决了任务描述不足、脱离背景研究和缺乏目标用户测试等问题，为XAI领域提供了设计和报告研究的指导。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: XAI研究因任务理解不足导致设计建议缺乏和结果矛盾，需从多领域整合方法以分类和比较研究。

Method: 结合可视化分析、认知科学和仪表盘设计，提出基于‘什么、为什么、谁’三个维度的XAI研究分类方法。

Result: 发现主要问题为任务描述不足、脱离背景研究和用户测试不足，建议研究需明确用户的领域、AI及数据分析专长。

Conclusion: 提供了XAI研究设计和报告的指南，帮助研究者识别相关研究、填补研究空白并处理设计矛盾。

Abstract: Research into explainable artificial intelligence (XAI) for data analysis
tasks suffer from a large number of contradictions and lack of concrete design
recommendations stemming from gaps in understanding the tasks that require AI
assistance. In this paper, we drew on multiple fields such as visual analytics,
cognition, and dashboard design to propose a method for categorising and
comparing XAI studies under three dimensions: what, why, and who. We identified
the main problems as: inadequate descriptions of tasks, context-free studies,
and insufficient testing with target users. We propose that studies should
specifically report on their users' domain, AI, and data analysis expertise to
illustrate the generalisability of their findings. We also propose study
guidelines for designing and reporting XAI tasks to improve the XAI community's
ability to parse the rapidly growing field. We hope that our contribution can
help researchers and designers better identify which studies are most relevant
to their work, what gaps exist in the research, and how to handle contradictory
results regarding XAI design.

</details>


### [265] [Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence](https://arxiv.org/abs/2507.10281)
*Jiaming Tian,Liyao Li,Wentao Ye,Haobo Wang,Lingxin Wang,Lihua Yu,Zujie Ren,Gang Chen,Junbo Zhao*

Key words: 表格代理, LLM, 噪声, 语义复杂性, Text-to-SQL

TL;DR: 该综述探讨了基于大语言模型（LLM）的表格代理如何处理现实世界表格任务中的噪声和语义复杂性，提出了五个核心能力，并指出了学术基准与实际场景之间的性能差距。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现实世界的表格任务常因噪声、结构异质性和语义复杂性而未被充分研究，而现有研究多针对干净的学术数据集。

Method: 定义了五个核心能力（C1-C5），用于分析和比较当前方法，并深入研究了Text-to-SQL代理的性能。

Result: 发现开源模型在学术基准和实际场景之间存在性能差距。

Conclusion: 提出了提高表格代理在实际应用中鲁棒性、泛化性和效率的可操作建议。

Abstract: Tables are fundamental in domains such as finance, healthcare, and public
administration, yet real-world table tasks often involve noise, structural
heterogeneity, and semantic complexity--issues underexplored in existing
research that primarily targets clean academic datasets. This survey focuses on
LLM-based Table Agents, which aim to automate table-centric workflows by
integrating preprocessing, reasoning, and domain adaptation. We define five
core competencies--C1: Table Structure Understanding, C2: Table and Query
Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable
Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze
and compare current approaches. In addition, a detailed examination of the
Text-to-SQL Agent reveals a performance gap between academic benchmarks and
real-world scenarios, especially for open-source models. Finally, we provide
actionable insights to improve the robustness, generalization, and efficiency
of LLM-based Table Agents in practical settings.

</details>


### [266] [Instance space analysis of the capacitated vehicle routing problem](https://arxiv.org/abs/2507.10397)
*Alessandra M. M. M. Gouvêa,Nuno Paulos,Eduardo Uchoa e Mariá C. V. Nascimento*

Key words: CVRP, Instance Space Analysis, Metaheuristics, Dimensionality Reduction

TL;DR: 本文通过实例空间分析（ISA）方法，研究了CVRP问题中实例特征与元启发式算法性能的关系，并提出了一个便于扩展的投影矩阵。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决CVRP研究中实例特征与元启发式算法性能关系的复杂性。

Method: 结合ISA方法和DIMACS竞赛数据集，通过PRELIM、SIFTED和PILOT阶段进行降维和机器学习分析。

Result: 识别了23个相关实例特征，并创建了二维实例空间投影。

Conclusion: 提供了易于扩展的投影矩阵，为CVRP实例分析提供了新方法。

Abstract: This paper seeks to advance CVRP research by addressing the challenge of
understanding the nuanced relationships between instance characteristics and
metaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a
valuable tool that allows for a new perspective on the field. By combining the
ISA methodology with a dataset from the DIMACS 12th Implementation Challenge on
Vehicle Routing, our research enabled the identification of 23 relevant
instance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,
which employ dimensionality reduction and machine learning methods, allowed us
to create a two-dimensional projection of the instance space to understand how
the structure of instances affect the behavior of MHs. A key contribution of
our work is that we provide a projection matrix, which makes it straightforward
to incorporate new instances into this analysis and allows for a new method for
instance analysis in the CVRP field.

</details>


### [267] [SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning](https://arxiv.org/abs/2507.10421)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Key words: 学生辍学预测，情感分析，BERT，XGBoost，学习分析

TL;DR: 该论文提出了一种结合BERT情感分析和XGBoost数据建模的新方法，用于预测远程学习中的学生辍学风险，准确率达到84%，优于基线模型。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 远程学习中学生辍学问题严重，早期预测对干预和促进学生坚持学习至关重要。

Method: 使用BERT对学生的评论进行情感分析，并结合XGBoost分析社会人口和行为数据。通过特征重要性技术选择关键特征并进行融合。

Result: 模型在未见数据上表现优异，准确率达84%，且在精确率和F1分数等指标上优于基线模型（82%）。

Conclusion: 该方法为开发个性化策略以减少辍学率和鼓励学生坚持学习提供了重要工具。

Abstract: School dropout is a serious problem in distance learning, where early
detection is crucial for effective intervention and student perseverance.
Predicting student dropout using available educational data is a widely
researched topic in learning analytics. Our partner's distance learning
platform highlights the importance of integrating diverse data sources,
including socio-demographic data, behavioral data, and sentiment analysis, to
accurately predict dropout risks. In this paper, we introduce a novel model
that combines sentiment analysis of student comments using the Bidirectional
Encoder Representations from Transformers (BERT) model with socio-demographic
and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We
fine-tuned BERT on student comments to capture nuanced sentiments, which were
then merged with key features selected using feature importance techniques in
XGBoost. Our model was tested on unseen data from the next academic year,
achieving an accuracy of 84\%, compared to 82\% for the baseline model.
Additionally, the model demonstrated superior performance in other metrics,
such as precision and F1-score. The proposed method could be a vital tool in
developing personalized strategies to reduce dropout rates and encourage
student perseverance

</details>


### [268] [Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures](https://arxiv.org/abs/2507.10446)
*Sudarshan Babu*

Key words: 

TL;DR: 论文提出了通过神经记忆和超网络设计解决小样本场景下的知识迁移问题，应用于3D场景生成和分子属性预测。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决在数据稀缺领域（如计算化学、计算免疫学）中，传统大模型训练不可行的问题。

Method: 使用神经记忆和超网络设计，结合MAML方法，实现小样本下的高效知识迁移。

Result: 在3D场景生成和分子属性预测中，实现了高效的小样本学习和迁移。

Conclusion: 超网络设计在小样本场景下能有效获取泛化性强的先验知识。

Abstract: The ability to transfer knowledge from prior experiences to novel tasks
stands as a pivotal capability of intelligent agents, including both humans and
computational models. This principle forms the basis of transfer learning,
where large pre-trained neural networks are fine-tuned to adapt to downstream
tasks. Transfer learning has demonstrated tremendous success, both in terms of
task adaptation speed and performance. However there are several domains where,
due to lack of data, training such large pre-trained models or foundational
models is not a possibility - computational chemistry, computational
immunology, and medical imaging are examples. To address these challenges, our
work focuses on designing architectures to enable efficient acquisition of
priors when large amounts of data are unavailable. In particular, we
demonstrate that we can use neural memory to enable adaptation on
non-stationary distributions with only a few samples. Then we demonstrate that
our hypernetwork designs (a network that generates another network) can acquire
more generalizable priors than standard networks when trained with Model
Agnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene
generation, demonstrating that they can acquire priors efficiently on just a
handful of training scenes, thereby leading to faster text-to-3D generation. We
then extend our hypernetwork framework to perform 3D segmentation on novel
scenes with limited data by efficiently transferring priors from earlier viewed
scenes. Finally, we repurpose an existing molecular generative method as a
pre-training framework that facilitates improved molecular property prediction,
addressing critical challenges in computational immunology

</details>


### [269] [DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology](https://arxiv.org/abs/2507.10522)
*Jennifer D'Souza,Endres Keno Sander,Andrei Aioanei*

Key words: DeepResearch, LLM, 科学合成, 递归探索, 参数配置

TL;DR: DeepResearch$^{\text{Eco}}$是一个基于LLM的新型系统，支持通过递归、深度和广度控制的探索来自动合成科学文献，提升检索多样性和细节。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 旨在解决传统检索增强生成流水线在文献合成中的局限性，提供用户可控的透明推理和参数配置，提高领域证据的高通量整合。

Method: 采用递归、深度和广度控制的探索方法，结合参数驱动的配置和透明推理，优化科学文献检索与合成。

Result: 在49个生态研究问题中，实现了最高21倍的来源整合提升和14.9倍的每千字来源整合增长，高参数设置下达到专家级分析深度和多样性。

Conclusion: DeepResearch$^{\text{Eco}}$为科学文献合成提供了高效、可控且透明的新方法，显著提升了检索和整合能力。

Abstract: We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [270] [AirScape: An Aerial Generative World Model with Motion Controllability](https://arxiv.org/abs/2507.08885)
*Baining Zhao,Rongze Tang,Mingyuan Jia,Ziyou Wang,Fanghang Man,Xin Zhang,Yu Shang,Weichen Zhang,Chen Gao,Wei Wu,Xin Wang,Xinlei Chen,Yong Li*

Key words: 机器人, 空间想象力, 六自由度, 世界模型, 运动意图

TL;DR: AirScape是一种为六自由度空中代理设计的世界模型，旨在预测未来观察序列，通过视觉输入和运动意图实现。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决机器人在三维空间中预测自身运动意图结果的基本问题，探索更通用的空间想象力能力。

Method: 构建了11k视频-意图对的数据集，并通过两阶段训练计划训练基础模型，使其可控且符合物理时空约束。

Result: 开发了AirScape模型，能够基于当前视觉输入和运动意图预测未来观察序列。

Conclusion: AirScape为六自由度空中代理提供了首个世界模型，展示了其空间想象力和可控性。

Abstract: How to enable robots to predict the outcomes of their own motion intentions
in three-dimensional space has been a fundamental problem in embodied
intelligence. To explore more general spatial imagination capabilities, here we
present AirScape, the first world model designed for six-degree-of-freedom
aerial agents. AirScape predicts future observation sequences based on current
visual inputs and motion intentions. Specifically, we construct an dataset for
aerial world model training and testing, which consists of 11k video-intention
pairs. This dataset includes first-person-view videos capturing diverse drone
actions across a wide range of scenarios, with over 1,000 hours spent
annotating the corresponding motion intentions. Then we develop a two-phase
training schedule to train a foundation model -- initially devoid of embodied
spatial knowledge -- into a world model that is controllable by motion
intentions and adheres to physical spatio-temporal constraints.

</details>


### [271] [DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA](https://arxiv.org/abs/2507.09176)
*Han Ye,Yuqiang Jin,Jinyuan Liu,Tao Li,Wen-An Zhang,Minglei Fu*

Key words: LiDAR标定,无目标标定,束调整,多LiDAR系统,外参标定

TL;DR: 本文提出一种无目标多LiDAR外参标定框架，通过LiDAR束调整和迭代优化，无需重叠视野或初始参数估计，显著提高了标定精度与鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 多LiDAR系统的外参标定对3D地图重建系统性能至关重要，传统方法依赖重叠视野或手动标注，效率低且不灵活。

Method: 采用LiDAR束调整（LBA）与迭代优化结合的框架，通过连续扫描构建参考点云地图，并引入自适应权重机制抑制误差与异常值。

Result: 在非重叠传感器配置下，平均平移误差5毫米，旋转误差0.2度，初始误差容忍度达0.4米/30度，性能优于现有方法。

Conclusion: 该方法无需专用设施或手动调参，开放源代码，为多LiDAR系统提供了高效、鲁棒的标定方案。

Abstract: Accurate extrinsic calibration of multiple LiDARs is crucial for improving
the foundational performance of three-dimensional (3D) map reconstruction
systems. This paper presents a novel targetless extrinsic calibration framework
for multi-LiDAR systems that does not rely on overlapping fields of view or
precise initial parameter estimates. Unlike conventional calibration methods
that require manual annotations or specific reference patterns, our approach
introduces a unified optimization framework by integrating LiDAR bundle
adjustment (LBA) optimization with robust iterative refinement. The proposed
method constructs an accurate reference point cloud map via continuous scanning
from the target LiDAR and sliding-window LiDAR bundle adjustment, while
formulating extrinsic calibration as a joint LBA optimization problem. This
method effectively mitigates cumulative mapping errors and achieves
outlier-resistant parameter estimation through an adaptive weighting mechanism.
Extensive evaluations in both the CARLA simulation environment and real-world
scenarios demonstrate that our method outperforms state-of-the-art calibration
techniques in both accuracy and robustness. Experimental results show that for
non-overlapping sensor configurations, our framework achieves an average
translational error of 5 mm and a rotational error of 0.2{\deg}, with an
initial error tolerance of up to 0.4 m/30{\deg}. Moreover, the calibration
process operates without specialized infrastructure or manual parameter tuning.
The code is open source and available on GitHub
(\underline{https://github.com/Silentbarber/DLBAcalib})

</details>


### [272] [Towards Human-level Dexterity via Robot Learning](https://arxiv.org/abs/2507.09117)
*Gagan Khandate*

Key words: 灵巧智能, 多指操作, 强化学习, 模仿学习, 触觉感知

TL;DR: 论文探讨了通过强化学习和模仿学习提升多指机器人手的灵巧性，克服了现有计算感觉运动学习的局限性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 多指灵巧性是人体物理智能的高峰，但机器人实现类似灵巧性仍面临计算感觉运动学习的基本限制。

Method: 采用结构化探索强化学习和基于视觉触觉的人类示范模仿学习技术。

Result: 构建了一个高效的强化学习框架，结合采样规划直接探索，提升了多指灵巧性。

Conclusion: 通过直接解决根本限制，论文为机器人多指灵巧性学习提供了有效方法。

Abstract: Dexterous intelligence -- the ability to perform complex interactions with
multi-fingered hands -- is a pinnacle of human physical intelligence and
emergent higher-order cognitive skills. However, contrary to Moravec's paradox,
dexterous intelligence in humans appears simple only superficially. Many
million years were spent co-evolving the human brain and hands including rich
tactile sensing. Achieving human-level dexterity with robotic hands has long
been a fundamental goal in robotics and represents a critical milestone toward
general embodied intelligence. In this pursuit, computational sensorimotor
learning has made significant progress, enabling feats such as arbitrary
in-hand object reorientation. However, we observe that achieving higher levels
of dexterity requires overcoming very fundamental limitations of computational
sensorimotor learning.
  I develop robot learning methods for highly dexterous multi-fingered
manipulation by directly addressing these limitations at their root cause.
Chiefly, through key studies, this disseration progressively builds an
effective framework for reinforcement learning of dexterous multi-fingered
manipulation skills. These methods adopt structured exploration, effectively
overcoming the limitations of random exploration in reinforcement learning. The
insights gained culminate in a highly effective reinforcement learning that
incorporates sampling-based planning for direct exploration. Additionally, this
thesis explores a new paradigm of using visuo-tactile human demonstrations for
dexterity, introducing corresponding imitation learning techniques.

</details>


### [273] [On the Importance of Neural Membrane Potential Leakage for LIDAR-based Robot Obstacle Avoidance using Spiking Neural Networks](https://arxiv.org/abs/2507.09538)
*Zainab Ali,Lujayn Al-Amir,Ali Safa*

Key words: 脉冲神经网络, LiDAR, 机器人导航, 避障, 膜泄漏

TL;DR: 论文探讨了基于脉冲神经网络（SNNs）的机器人导航与避障方法，研究了神经元膜泄漏对SNN在LiDAR数据处理中的影响，并通过调整泄漏常数达到与CNN相当的精度。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 由于SNN在神经形态计算中具有高精度且低资源消耗的特点，适用于电池资源有限的自主机器人（如无人机和漫游车）。

Method: 搭建了带LiDAR的机器人平台，收集标注数据集，并研究SNN中LIF神经元膜泄漏对LiDAR数据处理精度的影响。

Result: 通过调整膜泄漏常数，SNN的避障精度可达与CNN相当的水平。

Conclusion: 膜泄漏常数调优对SNN性能至关重要，相关LiDAR数据集已开源以促进未来研究。

Abstract: Using neuromorphic computing for robotics applications has gained much
attention in recent year due to the remarkable ability of Spiking Neural
Networks (SNNs) for high-precision yet low memory and compute complexity
inference when implemented in neuromorphic hardware. This ability makes SNNs
well-suited for autonomous robot applications (such as in drones and rovers)
where battery resources and payload are typically limited. Within this context,
this paper studies the use of SNNs for performing direct robot navigation and
obstacle avoidance from LIDAR data. A custom robot platform equipped with a
LIDAR is set up for collecting a labeled dataset of LIDAR sensing data together
with the human-operated robot control commands used for obstacle avoidance.
Crucially, this paper provides what is, to the best of our knowledge, a first
focused study about the importance of neuron membrane leakage on the SNN
precision when processing LIDAR data for obstacle avoidance. It is shown that
by carefully tuning the membrane potential leakage constant of the spiking
Leaky Integrate-and-Fire (LIF) neurons used within our SNN, it is possible to
achieve on-par robot control precision compared to the use of a non-spiking
Convolutional Neural Network (CNN). Finally, the LIDAR dataset collected during
this work is released as open-source with the hope of benefiting future
research.

</details>


### [274] [Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks](https://arxiv.org/abs/2507.09725)
*Gabriel G. Gattaux,Julien R. Serres,Franck Ruffier,Antoine Wystrach*

Key words: 视觉归巢,蘑菇体,自主导航,生物启发,路径积分

TL;DR: 提出了一种基于侧化蘑菇体（MB）架构的视觉归巢系统，首次在真实世界中实现了自主车辆的视觉归巢，通过实验验证了其鲁棒性和资源效率。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 受蚂蚁在极少感官输入和少量学习行走下实现稳健视觉归巢的启发，研究探索了蘑菇体模型在视觉归巢中的应用，为自主导航提供生物启发解决方案。

Method: 采用侧化MB架构，利用角路径积分（PI）信号的正负值对全景视图进行分类，存储在MB中形成“目标在左”和“目标在右”两个记忆库。通过四个递增实验验证系统性能。

Result: 实验表明系统能在自然户外环境中实现稳健归巢，包括模拟巢动态、解耦学习行走归巢、随机行走归巢以及精确停止行为。系统运行频率8 Hz，内存占用低于9 kB。

Conclusion: 该系统为自主视觉归巢提供了一种基于生物学原理、资源高效的解决方案，功能上类似于机器人中的基于路点的位置控制。

Abstract: Ants achieve robust visual homing with minimal sensory input and only a few
learning walks, inspiring biomimetic solutions for autonomous navigation. While
Mushroom Body (MB) models have been used in robotic route following, they have
not yet been applied to visual homing. We present the first real-world
implementation of a lateralized MB architecture for visual homing onboard a
compact autonomous car-like robot. We test whether the sign of the angular path
integration (PI) signal can categorize panoramic views, acquired during
learning walks and encoded in the MB, into "goal on the left" and "goal on the
right" memory banks, enabling robust homing in natural outdoor settings. We
validate this approach through four incremental experiments: (1) simulation
showing attractor-like nest dynamics; (2) real-world homing after decoupled
learning walks, producing nest search behavior; (3) homing after random walks
using noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal
behavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to
control velocity. This mimics the accurate homing behavior of ants and
functionally resembles waypoint-based position control in robotics, despite
relying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with
32x32 pixel views and a memory footprint under 9 kB, our system offers a
biologically grounded, resource-efficient solution for autonomous visual
homing.

</details>


### [275] [Multi-residual Mixture of Experts Learning for Cooperative Control in Multi-vehicle Systems](https://arxiv.org/abs/2507.09836)
*Vindula Jayawardana,Sirui Li,Yashar Farid,Cathy Wu*

Key words: 自主车辆（AV），拉格朗日交通控制，残差强化学习，混合专家模型，环保驾驶

TL;DR: 论文提出了MRMEL框架，用于自主车辆（AV）的拉格朗日交通控制，通过动态选择最优策略和混合专家学习，显著降低了车辆排放。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 传统固定位置执行器（如交通信号）无法适应多样化交通场景，自主车辆作为移动执行器面临多智能体复杂性和混合动机的挑战，亟需改进控制策略。

Method: 提出MRMEL框架，结合残差强化学习和混合专家模型，动态校正并选择最优控制策略，适应不同交通场景。

Result: 在多个城市的真实交通场景中，MRMEL实现了4%-9%的额外排放降低，优于所有基线方法。

Conclusion: MRMEL为复杂交通场景下的自主车辆控制提供了高效、适应性强的解决方案，验证了其在环保驾驶中的实际价值。

Abstract: Autonomous vehicles (AVs) are becoming increasingly popular, with their
applications now extending beyond just a mode of transportation to serving as
mobile actuators of a traffic flow to control flow dynamics. This contrasts
with traditional fixed-location actuators, such as traffic signals, and is
referred to as Lagrangian traffic control. However, designing effective
Lagrangian traffic control policies for AVs that generalize across traffic
scenarios introduces a major challenge. Real-world traffic environments are
highly diverse, and developing policies that perform robustly across such
diverse traffic scenarios is challenging. It is further compounded by the joint
complexity of the multi-agent nature of traffic systems, mixed motives among
participants, and conflicting optimization objectives subject to strict
physical and external constraints. To address these challenges, we introduce
Multi-Residual Mixture of Expert Learning (MRMEL), a novel framework for
Lagrangian traffic control that augments a given suboptimal nominal policy with
a learned residual while explicitly accounting for the structure of the traffic
scenario space. In particular, taking inspiration from residual reinforcement
learning, MRMEL augments a suboptimal nominal AV control policy by learning a
residual correction, but at the same time dynamically selects the most suitable
nominal policy from a pool of nominal policies conditioned on the traffic
scenarios and modeled as a mixture of experts. We validate MRMEL using a case
study in cooperative eco-driving at signalized intersections in Atlanta, Dallas
Fort Worth, and Salt Lake City, with real-world data-driven traffic scenarios.
The results show that MRMEL consistently yields superior performance-achieving
an additional 4%-9% reduction in aggregate vehicle emissions relative to the
strongest baseline in each setting.

</details>


### [276] [Demonstrating the Octopi-1.5 Visual-Tactile-Language Model](https://arxiv.org/abs/2507.09985)
*Samson Yu,Kelvin Lin,Harold Soh*

Key words: 触觉模型, 视觉-触觉-语言模型, 多模态学习, 触觉传感器, RAG模块

TL;DR: Octopi-1.5是一个新型的视觉-触觉-语言模型，通过多部分触觉信号处理和RAG模块改进性能，并配备手持触觉界面TMI进行实时交互。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 触觉对机器人在灵活操作、材料识别和视觉遮挡场景中至关重要，Octopi-1.5旨在提升触觉理解能力并支持动态学习新对象。

Method: Octopi-1.5引入多部分触觉信号处理和RAG模块，配备GelSight和TAC-02触觉传感器的TMI手持界面。

Result: Octopi-1.5能通过触觉输入和常识知识解决推理任务，如识别抓取的物体并建议操作方法。

Conclusion: Octopi-1.5展示了视觉-触觉-语言模型的进展，激发了对这一领域的进一步兴趣，同时揭示了其局限性。

Abstract: Touch is recognized as a vital sense for humans and an equally important
modality for robots, especially for dexterous manipulation, material
identification, and scenarios involving visual occlusion. Building upon very
recent work in touch foundation models, this demonstration will feature
Octopi-1.5, our latest visual-tactile-language model. Compared to its
predecessor, Octopi-1.5 introduces the ability to process tactile signals from
multiple object parts and employs a simple retrieval-augmented generation (RAG)
module to improve performance on tasks and potentially learn new objects
on-the-fly. The system can be experienced live through a new handheld
tactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile
sensors. This convenient and accessible setup allows users to interact with
Octopi-1.5 without requiring a robot. During the demonstration, we will
showcase Octopi-1.5 solving tactile inference tasks by leveraging tactile
inputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5
will identify objects being grasped and respond to follow-up queries about how
to handle it (e.g., recommending careful handling for soft fruits). We also
plan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.
With live interactions, this demonstration aims to highlight both the progress
and limitations of VTLMs such as Octopi-1.5 and to foster further interest in
this exciting field. Code for Octopi-1.5 and design files for the TMI gripper
are available at https://github.com/clear-nus/octopi-1.5.

</details>


### [277] [Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization](https://arxiv.org/abs/2507.09160)
*Jialei Huang,Shuo Wang,Fanqi Lin,Yihang Hu,Chuan Wen,Yang Gao*

Key words: Vision-Language-Action, 触觉感知, 混合力控制, 零样本泛化, 自适应推理

TL;DR: 本文介绍了Tactile-VLA框架，将视觉、语言、动作与触觉融合，通过混合力控制与推理模块实现精准物理交互，展示在接触丰富任务中的零样本泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: Vision-Language-Action (VLA)模型在视觉语言任务中表现优异，但在物理交互中缺乏精细控制。本文旨在通过触觉反馈提升VLA在真实世界中的交互能力。

Method: 提出Tactile-VLA框架，结合混合位置-力控制器和触觉反馈推理模块，将模型意图转化为精准动作并动态调整策略。

Result: 实验表明，Tactile-VLA在触觉感知指令跟随、触觉常识利用及自适应触觉推理三方面表现优异，且能通过少量演示激活先验知识实现零样本泛化。

Conclusion: VLA的隐含知识包含物理交互的语义理解，通过触觉传感器连接可显著提升其在接触丰富任务中的表现。

Abstract: Vision-Language-Action (VLA) models have shown remarkable achievements,
driven by the rich implicit knowledge of their vision-language components.
However, achieving generalist robotic agents demands precise grounding into
physical interactions, especially in contact-rich scenarios where fine-grained
force control is essential. We advance VLAs' implicit knowledge beyond
identifying what to do, towards guiding how to physically interact with real
world. This paper introduces Tactile-VLA, a novel framework that deeply fuses
vision, language, action, and tactile sensing. This framework incorporates a
hybrid position-force controller to translate the model's intentions into
precise physical actions and a reasoning module that allows the robot to adapt
its strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's
effectiveness and generalizability in three key aspects: (1) enabling
tactile-aware instruction following, (2) utilizing tactile-relevant
commonsense, and (3) facilitating adaptive tactile-involved reasoning. A key
finding is that the VLM's prior knowledge already contains semantic
understanding of physical interaction; by connecting it to the robot's tactile
sensors with only a few demonstrations, we can activate this prior knowledge to
achieve zero-shot generalization in contact-rich tasks.

</details>


### [278] [TGLD: A Trust-Aware Game-Theoretic Lane-Changing Decision Framework for Automated Vehicles in Heterogeneous Traffic](https://arxiv.org/abs/2507.10075)
*Jie Pan,Tianyi Wang,Yangyang Wang,Junfeng Jiao,Christian Claudel*

Key words: 自动驾驶车辆，信任感知，游戏论，车道变换，协作行为

TL;DR: 研究提出了一种信任感知的游戏论车道变换决策框架（TGLD），通过动态评估人类驾驶车辆的信任水平，实现了自动驾驶车辆与人类驾驶车辆的高效协作。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 自动驾驶车辆需要与人类驾驶车辆在社会兼容的行为中有效协作，但现有车道变换框架忽视了人类驾驶车辆的动态信任水平。

Method: 1. 构建多车辆联盟游戏模型，结合实时信任评估；2. 开发在线信任评估方法；3. 考虑社会兼容性目标，最小化对周围车辆的干扰。

Result: 实验验证了TGLD框架的有效性，自动驾驶车辆能够根据信任水平和驾驶风格调整策略，显著提升了车道变换效率和安全性。

Conclusion: 信任机制的引入提升了自动驾驶车辆与人类驾驶车辆的交互透明性和适应性。

Abstract: Automated vehicles (AVs) face a critical need to adopt socially compatible
behaviors and cooperate effectively with human-driven vehicles (HVs) in
heterogeneous traffic environment. However, most existing lane-changing
frameworks overlook HVs' dynamic trust levels, limiting their ability to
accurately predict human driver behaviors. To address this gap, this study
proposes a trust-aware game-theoretic lane-changing decision (TGLD) framework.
First, we formulate a multi-vehicle coalition game, incorporating fully
cooperative interactions among AVs and partially cooperative behaviors from HVs
informed by real-time trust evaluations. Second, we develop an online trust
evaluation method to dynamically estimate HVs' trust levels during
lane-changing interactions, guiding AVs to select context-appropriate
cooperative maneuvers. Lastly, social compatibility objectives are considered
by minimizing disruption to surrounding vehicles and enhancing the
predictability of AV behaviors, thereby ensuring human-friendly and
context-adaptive lane-changing strategies. A human-in-the-loop experiment
conducted in a highway on-ramp merging scenario validates our TGLD approach.
Results show that AVs can effectively adjust strategies according to different
HVs' trust levels and driving styles. Moreover, incorporating a trust mechanism
significantly improves lane-changing efficiency, maintains safety, and
contributes to transparent and adaptive AV-HV interactions.

</details>


### [279] [Unified Linear Parametric Map Modeling and Perception-aware Trajectory Planning for Mobile Robotics](https://arxiv.org/abs/2507.09340)
*Hongyu Nie,Xingyu Li,Xu Liu,Zhaotong Tan,Sen Mei,Wenbo Su*

Key words: 自主导航，RMRP，RPATR，无人机，地面车辆，感知感知策略

TL;DR: 提出了一种名为RMRP（随机映射与随机投影）的新方法，通过高维映射和稀疏随机投影构建轻量级线性参数地图，解决了移动机器人在复杂环境中的感知与规划问题，同时提出了RPATR框架用于无人机和地面车辆的导航优化。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 移动机器人在大规模复杂环境中的自主导航面临计算负担重、传感器遮挡和地形不规则等挑战，缺乏感知感知策略。

Method: RMRP方法通过高维映射和稀疏随机投影构建线性参数地图；RPATR框架结合网格和ESDF地图，使用分析梯度优化路径和轨迹。

Result: 实验验证显示，该方法在时间、内存和准确性上表现优异，适用于高速无人机和地面车辆的高效安全导航。

Conclusion: RMRP和RPATR框架为移动机器人在复杂环境中的导航提供了高效、安全的解决方案。

Abstract: Autonomous navigation in mobile robots, reliant on perception and planning,
faces major hurdles in large-scale, complex environments. These include heavy
computational burdens for mapping, sensor occlusion failures for UAVs, and
traversal challenges on irregular terrain for UGVs, all compounded by a lack of
perception-aware strategies. To address these challenges, we introduce Random
Mapping and Random Projection (RMRP). This method constructs a lightweight
linear parametric map by first mapping data to a high-dimensional space,
followed by a sparse random projection for dimensionality reduction. Our novel
Residual Energy Preservation Theorem provides theoretical guarantees for this
process, ensuring critical geometric properties are preserved. Based on this
map, we propose the RPATR (Robust Perception-Aware Trajectory Planner)
framework. For UAVs, our method unifies grid and Euclidean Signed Distance
Field (ESDF) maps. The front-end uses an analytical occupancy gradient to
refine initial paths for safety and smoothness, while the back-end uses a
closed-form ESDF for trajectory optimization. Leveraging the trained RMRP
model's generalization, the planner predicts unobserved areas for proactive
navigation. For UGVs, the model characterizes terrain and provides closed-form
gradients, enabling online planning to circumvent large holes. Validated in
diverse scenarios, our framework demonstrates superior mapping performance in
time, memory, and accuracy, and enables computationally efficient, safe
navigation for high-speed UAVs and UGVs. The code will be released to foster
community collaboration.

</details>


### [280] [Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based Diffusion and Potential Fields](https://arxiv.org/abs/2507.09383)
*Wondmgezahu Teshome,Kian Behzad,Octavia Camps,Michael Everett,Milad Siami,Mario Sznaier*

Key words: 运动规划, 扩散模型, 人工势场, 追逃问题, 点云

TL;DR: 结合能量扩散模型与人工势场，提出实时复杂环境中稳健的轨迹生成框架。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决追逃问题中的运动规划挑战。

Method: 利用点云直接处理障碍物信息，结合无分类器引导训练和局部势场采样。

Result: 在动态场景中有效生成并优化轨迹，适应部分观察条件。

Conclusion: 展示了在复杂环境下稳健的实时轨迹生成能力，适用于追逃场景。

Abstract: Motivated by the problem of pursuit-evasion, we present a motion planning
framework that combines energy-based diffusion models with artificial potential
fields for robust real time trajectory generation in complex environments. Our
approach processes obstacle information directly from point clouds, enabling
efficient planning without requiring complete geometric representations. The
framework employs classifier-free guidance training and integrates local
potential fields during sampling to enhance obstacle avoidance. In dynamic
scenarios, the system generates initial trajectories using the diffusion model
and continuously refines them through potential field-based adaptation,
demonstrating effective performance in pursuit-evasion scenarios with partial
pursuer observability.

</details>


### [281] [Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance](https://arxiv.org/abs/2507.10500)
*Kyungtae Han,Yitao Chen,Rohit Gupta,Onur Altintas*

Key words: 自动驾驶, 生成式AI, 自然语言交互, 场景感知, ADAS

TL;DR: SC-ADAS是一个结合生成式AI的模块化框架，通过自然语言交互提升ADAS的灵活性和适应性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 当前ADAS缺乏场景理解和自然语言交互能力，限制了其在动态环境中的适应性。

Method: 集成大型语言模型、视觉到文本解释和结构化功能调用，支持基于视觉和传感器的多轮对话。

Result: 在CARLA模拟器中实现无需微调的ADAS命令执行，展示了场景感知与对话推理的结合。

Conclusion: SC-ADAS展示了智能驾驶辅助的潜力，但也存在延迟和令牌增长等挑战。

Abstract: While autonomous driving technologies continue to advance, current Advanced
Driver Assistance Systems (ADAS) remain limited in their ability to interpret
scene context or engage with drivers through natural language. These systems
typically rely on predefined logic and lack support for dialogue-based
interaction, making them inflexible in dynamic environments or when adapting to
driver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a
modular framework that integrates Generative AI components including large
language models, vision-to-text interpretation, and structured function calling
to enable real-time, interpretable, and adaptive driver assistance. SC-ADAS
supports multi-turn dialogue grounded in visual and sensor context, allowing
natural language recommendations and driver-confirmed ADAS control. Implemented
in the CARLA simulator with cloud-based Generative AI, the system executes
confirmed user intents as structured ADAS commands without requiring model
fine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and
revisited multi-turn interactions, highlighting trade-offs such as increased
latency from vision-based context retrieval and token growth from accumulated
dialogue history. These results demonstrate the feasibility of combining
conversational reasoning, scene perception, and modular ADAS control to support
the next generation of intelligent driver assistance.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [282] [Credit Card Fraud Detection Using RoFormer Model With Relative Distance Rotating Encoding](https://arxiv.org/abs/2507.09385)
*Kevin Reyes,Vasco Cortez*

Key words: 欺诈检测, ReDRE编码, RoFormer模型, 时间序列, Transformer

TL;DR: 论文提出了一种名为ReDRE的新型方法，通过将相对距离旋转编码（ReDRE）融入RoFormer模型，改进了交易欺诈检测，增强了时间序列数据的表征能力。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 欺诈检测是金融系统的重要挑战，支付网关公司需提升交易授权率并减少欺诈行为，以优化用户体验和业务可持续性。

Method: 将ReDRE编码融入RoFormer模型，通过角度旋转增强Transformer中时间序列数据的表征，以更好地捕捉时间依赖性和事件关系。

Result: 该方法提高了欺诈检测的准确性，尤其在捕捉时间依赖性和事件关系方面表现优异。

Conclusion: ReDRE编码的引入为交易欺诈检测提供了新思路，显著提升了模型性能。

Abstract: Fraud detection is one of the most important challenges that financial
systems must address. Detecting fraudulent transactions is critical for payment
gateway companies like Flow Payment, which process millions of transactions
monthly and require robust security measures to mitigate financial risks.
Increasing transaction authorization rates while reducing fraud is essential
for providing a good user experience and building a sustainable business. For
this reason, discovering novel and improved methods to detect fraud requires
continuous research and investment for any company that wants to succeed in
this industry. In this work, we introduced a novel method for detecting
transactional fraud by incorporating the Relative Distance Rotating Encoding
(ReDRE) in the RoFormer model. The incorporation of angle rotation using ReDRE
enhances the characterization of time series data within a Transformer, leading
to improved fraud detection by better capturing temporal dependencies and event
relationships.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [283] [Overview of the TREC 2023 deep learning track](https://arxiv.org/abs/2507.08890)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Hossein A. Rahmani,Daniel Campos,Jimmy Lin,Ellen M. Voorhees,Ian Soboroff*

Key words: TREC Deep Learning, MS MARCO, LLM, 合成查询, 信息检索

TL;DR: TREC Deep Learning track第五年，继续使用MS MARCO数据集，优化测试设计，并引入LLM提示方法，其表现优于之前的nnlm方法。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 为了进一步优化信息检索任务，通过更难的测试集和引入LLM提示方法，提升模型性能。

Method: 重复去年的测试设计，但基于更优的数据集v2，并引入LLM提示方法和合成查询。

Result: LLM提示方法优于之前的nnlm方法，合成查询与人工查询结果一致（τ=0.8487）。未发现明显偏见。

Conclusion: LLM提示方法有效，未来可在其他任务中推广。合成查询需人工筛选，但结果可靠。

Abstract: This is the fifth year of the TREC Deep Learning track. As in previous years,
we leverage the MS MARCO datasets that made hundreds of thousands of
human-annotated training labels available for both passage and document ranking
tasks. We mostly repeated last year's design, to get another matching test set,
based on the larger, cleaner, less-biased v2 passage and document set, with
passage ranking as primary and document ranking as a secondary task (using
labels inferred from passage). As we did last year, we sample from MS MARCO
queries that were completely held out, unused in corpus construction, unlike
the test queries in the first three years. This approach yields a more
difficult test with more headroom for improvement. Alongside the usual MS MARCO
(human) queries from MS MARCO, this year we generated synthetic queries using a
fine-tuned T5 model and using a GPT-4 prompt.
  The new headline result this year is that runs using Large Language Model
(LLM) prompting in some way outperformed runs that use the "nnlm" approach,
which was the best approach in the previous four years. Since this is the last
year of the track, future iterations of prompt-based ranking can happen in
other tracks. Human relevance assessments were applied to all query types, not
just human MS MARCO queries. Evaluation using synthetic queries gave similar
results to human queries, with system ordering agreement of $\tau=0.8487$.
However, human effort was needed to select a subset of the synthetic queries
that were usable. We did not see clear evidence of bias, where runs using GPT-4
were favored when evaluated using synthetic GPT-4 queries, or where runs using
T5 were favored when evaluated on synthetic T5 queries.

</details>


### [284] [DS@GT at Touché: Large Language Models for Retrieval-Augmented Debate](https://arxiv.org/abs/2507.09090)
*Anthony Miyaguchi,Conor Johnston,Aaryan Potdar*

Key words: 大型语言模型, 辩论能力, 评估能力, 检索增强

TL;DR: 研究了大型语言模型在辩论中的表现，包括其辩论能力和评估能力，发现其在提供相关论据时表现良好，但回答冗长且评估一致。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 探索大型语言模型在结构化辩论中的能力及其对辩论话语的评估能力。

Method: 部署六个公开可用的大型语言模型，进行检索增强辩论和评估，衡量质量、数量、方式和关联性四个指标。

Result: 模型在提供相关论据时表现良好，但回答冗长且评估一致。

Conclusion: 大型语言模型在辩论中表现优秀，但需要解决回答冗长的问题。

Abstract: Large Language Models (LLMs) demonstrate strong conversational abilities. In
this Working Paper, we study them in the context of debating in two ways: their
ability to perform in a structured debate along with a dataset of arguments to
use and their ability to evaluate utterances throughout the debate. We deploy
six leading publicly available models from three providers for the
Retrieval-Augmented Debate and Evaluation. The evaluation is performed by
measuring four key metrics: Quality, Quantity, Manner, and Relation. Throughout
this task, we found that although LLMs perform well in debates when given
related arguments, they tend to be verbose in responses yet consistent in
evaluation. The accompanying source code for this paper is located at
https://github.com/dsgt-arc/touche-2025-rad.

</details>


### [285] [GraphRunner: A Multi-Stage Framework for Efficient and Accurate Graph-Based Retrieval](https://arxiv.org/abs/2507.08945)
*Savini Kashmira,Jayanaka L. Dantanarayana,Krisztián Flautner,Lingjia Tang,Jason Mars*

Key words: 知识图谱, RAG, 多跳检索, LLM, 推理错误, 幻觉

TL;DR: 传统RAG方法在处理结构化知识图谱时表现不佳，本文提出GraphRunner框架，通过三阶段（规划、验证、执行）和多跳探索提升检索性能，减少错误和幻觉，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 解决基于知识图谱的结构化数据检索中传统RAG方法的局限性，尤其是LLM推理错误和幻觉问题。

Method: 提出GraphRunner框架，分三个阶段（规划、验证、执行），支持多跳探索并生成全局遍历计划，通过验证减少错误。

Result: 在GRBench数据集上，GraphRunner性能提升10-50%，推理成本降低3.0-12.9倍，响应时间缩短2.5-7.1倍。

Conclusion: GraphRunner在图形检索任务中更高效、鲁棒，显著优于现有方法。

Abstract: Conventional Retrieval Augmented Generation (RAG) approaches are common in
text-based applications. However, they struggle with structured, interconnected
datasets like knowledge graphs, where understanding underlying relationships is
crucial for accurate retrieval. A common direction in graph-based retrieval
employs iterative, rule-based traversal guided by Large Language Models (LLMs).
Such existing iterative methods typically combine reasoning with single hop
traversal at each step, making them vulnerable to LLM reasoning errors and
hallucinations that ultimately hinder the retrieval of relevant information.
  To address these limitations, we propose GraphRunner, a novel graph-based
retrieval framework that operates in three distinct stages: planning,
verification, and execution. This introduces high-level traversal actions that
enable multi-hop exploration in a single step. It also generates a holistic
traversal plan, which is verified against the graph structure and pre-defined
traversal actions, reducing reasoning errors and detecting hallucinations
before execution. GraphRunner significantly reduces LLM reasoning errors and
detects hallucinations through validation. Our evaluation using the GRBench
dataset shows that GraphRunner consistently outperforms existing approaches,
achieving 10-50% performance improvements over the strongest baseline while
reducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x,
making it significantly more robust and efficient for graph-based retrieval
tasks.

</details>


### [286] [MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora](https://arxiv.org/abs/2507.09924)
*Tuan-Luc Huynh,Thuy-Trang Vu,Weiqing Wang,Trung Le,Dragan Gašević,Yuan-Fang Li,Thanh-Toan Do*

Key words: 生成检索,低秩适应,外分布驱动,模型索引更新

TL;DR: MixLoRA-DSI是一种结合可扩展低秩适应专家混合和分层外分布驱动的扩展策略的新框架，用于生成检索中的模型索引持续更新。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 持续更新生成检索中的模型索引面临计算成本高、资源受限的问题，需要一种高效且低参数开销的解决方案。

Method: 提出MixLoRA-DSI，结合可扩展低秩适应专家混合和分层OOD驱动的扩展策略，仅在检测到大量OOD文档时选择性引入新专家。

Result: 在NQ320k和MS MARCO Passage实验中，MixLoRA-DSI表现优于全模型更新基线，参数开销最小且训练成本显著降低。

Conclusion: MixLoRA-DSI为解决生成检索中模型索引更新的计算和资源挑战提供了一种高效且低成本的方案。

Abstract: Continually updating model-based indexes in generative retrieval with new
documents remains challenging, as full retraining is computationally expensive
and impractical under resource constraints. We propose MixLoRA-DSI, a novel
framework that combines an expandable mixture of Low-Rank Adaptation experts
with a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead
of allocating new experts for each new corpus, our proposed expansion strategy
enables sublinear parameter growth by selectively introducing new experts only
when significant number of OOD documents are detected. Experiments on NQ320k
and MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update
baselines, with minimal parameter overhead and substantially lower training
costs.

</details>


### [287] [PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware Query Optimization](https://arxiv.org/abs/2507.10057)
*Sangwoo Park,Jinheon Baek,Soyeong Jeong,Sung Ju Hwang*

Key words: 科学论文检索, 文档到文档检索, 多粒度表示, PRISM, SciFullBench

TL;DR: 论文提出了一种名为PRISM的新型文档到文档检索方法，通过多粒度表示提升科学论文检索效果。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 传统方法仅使用稀疏和高层次的摘要作为替代，而PRISM则通过多视图分解和细粒度嵌入来更全面地捕捉论文内容。

Method: PRISM将查询论文分解为多个特定视角的表示，并与候选论文的多维度分段匹配。同时，提出了SciFullBench这一包含完整论文上下文的新基准。

Result: 实验结果显示，PRISM比现有基线方法的性能平均提高了4.3%。

Conclusion: PRISM通过多粒度表示和全文档上下文的有效利用，显著提升了科学论文检索的准确性。

Abstract: Scientific paper retrieval, particularly framed as document-to-document
retrieval, aims to identify relevant papers in response to a long-form query
paper, rather than a short query string. Previous approaches to this task have
focused on abstracts, embedding them into dense vectors as surrogates for full
documents and calculating similarity across them, although abstracts provide
only sparse and high-level summaries. To address this, we propose PRISM, a
novel document-to-document retrieval method that introduces multiple,
fine-grained representations for both the query and candidate papers. In
particular, each query paper is decomposed into multiple aspect-specific views
and individually embedded, which are then matched against candidate papers
similarity segmented to consider their multifaceted dimensions. Moreover, we
present SciFullBench, a novel benchmark in which the complete and segmented
context of full papers for both queries and candidates is available. Then,
experimental results show that PRISM improves performance by an average of 4.3%
over existing retrieval baselines.

</details>


### [288] [Identifying Offline Metrics that Predict Online Impact: A Pragmatic Strategy for Real-World Recommender Systems](https://arxiv.org/abs/2507.09566)
*Timo Wilm,Philipp Normann*

Key words: 推荐系统, 离线指标, 在线实验, 帕累托前沿, 数据驱动决策

TL;DR: 论文提出了一种通过帕累托前沿逼近方法，识别与在线影响对齐的离线指标的策略，并通过大规模在线实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 解决推荐系统中离线指标与在线指标之间关系不明确的挑战。

Method: 采用帕累托前沿逼近方法，设计模型无关的策略，同时服务于多个测试组。

Result: 在线实验验证了离线指标与点击率、点击后转化率和销量之间的显著相关性。

Conclusion: 该策略为行业实践者提供了理解离线-在线指标关系的工具，支持数据驱动的决策。

Abstract: A critical challenge in recommender systems is to establish reliable
relationships between offline and online metrics that predict real-world
performance. Motivated by recent advances in Pareto front approximation, we
introduce a pragmatic strategy for identifying offline metrics that align with
online impact. A key advantage of this approach is its ability to
simultaneously serve multiple test groups, each with distinct offline
performance metrics, in an online experiment controlled by a single model. The
method is model-agnostic for systems with a neural network backbone, enabling
broad applicability across architectures and domains. We validate the strategy
through a large-scale online experiment in the field of session-based
recommender systems on the OTTO e-commerce platform. The online experiment
identifies significant alignments between offline metrics and real-word
click-through rate, post-click conversion rate and units sold. Our strategy
provides industry practitioners with a valuable tool for understanding
offline-to-online metric relationships and making informed, data-driven
decisions.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [289] [Mind the Gap: Navigating Inference with Optimal Transport Maps](https://arxiv.org/abs/2507.08867)
*Malte Algren,Tobias Golling,Francesco Armando Di Bello,Christopher Pollard*

Key words: 机器学习, 最优传输, 高能物理, 喷注标记, 校准

TL;DR: 本文提出了一种基于最优传输的校准方法，解决了机器学习在高能物理中因仿真与实验数据不匹配而导致的性能下降问题。

<details>
  <summary>Details</summary>

Main category: physics.data-an

Motivation: 机器学习在高能物理中的应用依赖于高质量的仿真数据，但仿真与实验数据的不匹配限制了其效果。本文旨在解决这一问题。

Method: 采用基于最优传输的校准方法，应用于高维仿真数据，并通过喷注标记任务验证其性能。

Result: 校准后的高维表示显著提升了喷注风味信息在LHC分析中的应用效果。

Conclusion: 该方法为高能物理中的校准问题提供了有效解决方案，并具有广泛的科学应用潜力。

Abstract: Machine learning (ML) techniques have recently enabled enormous gains in
sensitivity across the sciences. In particle physics, much of this progress has
relied on excellent simulations of a wide range of physical processes. However,
due to the sophistication of modern machine learning (ML) algorithms and their
reliance on high-quality training samples, discrepancies between simulation and
experimental data can significantly limit the effectiveness of ML techniques.
In this work, we present a solution to this ``mis-specification'' problem: a
calibration approach based on optimal transport, which we apply to
high-dimensional simulations for the first time. We demonstrate the performance
of our approach through jet tagging, using a CMS-inspired dataset. A
128-dimensional internal jet representation from a powerful general-purpose
classifier is studied; after calibrating this internal ``latent''
representation, we find that a wide variety of quantities derived from it for
downstream tasks are also properly calibrated: using this calibrated
high-dimensional representation, powerful new applications of jet flavor
information can be utilized in LHC analyses. This is a key step toward allowing
properly-calibrated ``foundation models'' in particle physics. More broadly,
this calibration framework has broad applications for correcting
high-dimensional simulations across the sciences.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [290] [Advancing network resilience theories with symbolized reinforcement learning](https://arxiv.org/abs/2507.08827)
*Yu Zheng,Jingtao Ding,Depeng Jin,Jianxi Gao,Yong Li*

Key words: 网络韧性, 拓扑, 动力学, AI, 自动理论发现

TL;DR: 该论文提出了一种自动发现网络韧性理论的方法，结合拓扑和动力学，显著提升了现有理论的准确性。

<details>
  <summary>Details</summary>

Main category: physics.soc-ph

Motivation: 研究网络韧性的理论以预防现实系统中的崩溃（如物种灭绝或金融危机），现有理论仅从拓扑角度分析，忽略了动力学的作用。

Method: 通过学习AI解决网络拆卸问题的方法，将其攻击策略符号化为理论公式，自动发现同时考虑拓扑和动力学的韧性理论。

Result: 该方法发现了首个结合拓扑和动力学的韧性理论，并改进了现有理论的准确性（提升37.5%）。

Conclusion: 研究为设计系统崩溃的早期预警信号提供了新见解，显著提升了复杂网络的人类理解。

Abstract: Many complex networks display remarkable resilience under external
perturbations, internal failures and environmental changes, yet they can
swiftly deteriorate into dysfunction upon the removal of a few keystone nodes.
Discovering theories that measure network resilience offers the potential to
prevent catastrophic collapses--from species extinctions to financial
crise--with profound implications for real-world systems. Current resilience
theories address the problem from a single perspective of topology, neglecting
the crucial role of system dynamics, due to the intrinsic complexity of the
coupling between topology and dynamics which exceeds the capabilities of human
analytical methods. Here, we report an automatic method for resilience theory
discovery, which learns from how AI solves a complicated network dismantling
problem and symbolizes its network attack strategies into theoretical formulas.
This proposed self-inductive approach discovers the first resilience theory
that accounts for both topology and dynamics, highlighting how the correlation
between node degree and state shapes overall network resilience, and offering
insights for designing early warning signals of systematic collapses.
Additionally, our approach discovers formulas that refine existing
well-established resilience theories with over 37.5% improvement in accuracy,
significantly advancing human understanding of complex networks with AI.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [291] [A Framework for Predictive Directional Trading Based on Volatility and Causal Inference](https://arxiv.org/abs/2507.09347)
*Ivan Letteri*

Key words: 金融模型,机器学习,因果推理,交易策略,波动率

TL;DR: 本研究提出了一个新颖的框架，结合统计方法与机器学习模型，以识别并利用金融市场的预测性领先-滞后关系，其交易策略表现优异。

<details>
  <summary>Details</summary>

Main category: q-fin.ST

Motivation: 为了增强对股票间预测性关系的识别和利用，提供一种系统化且稳健的方法。

Method: 采用高斯混合模型(GMM)聚类股票，构建多阶段因果推断流程，结合格兰杰因果检验、PCMCI测试和有效转移熵等方法，并使用时序对齐和KNN分类器确定最佳交易时机。

Result: 在2023年6月至8月的测试中，策略总回报达15.38%，显著优于买入持有策略的10.39%，夏普比率达2.17，部分配对胜率为100%。

Conclusion: 研究为基于波动率的因果关系的交易策略开发提供了系统方法，对金融建模和算法交易具有重要价值。

Abstract: Purpose: This study introduces a novel framework for identifying and
exploiting predictive lead-lag relationships in financial markets. We propose
an integrated approach that combines advanced statistical methodologies with
machine learning models to enhance the identification and exploitation of
predictive relationships between equities. Methods: We employed a Gaussian
Mixture Model (GMM) to cluster nine prominent stocks based on their mid-range
historical volatility profiles over a three-year period. From the resulting
clusters, we constructed a multi-stage causal inference pipeline, incorporating
the Granger Causality Test (GCT), a customised Peter-Clark Momentary
Conditional Independence (PCMCI) test, and Effective Transfer Entropy (ETE) to
identify robust, predictive linkages. Subsequently, Dynamic Time Warping (DTW)
and a K-Nearest Neighbours (KNN) classifier were utilised to determine the
optimal time lag for trade execution. The resulting strategy was rigorously
backtested. Results: The proposed volatility-based trading strategy, tested
from 8 June 2023 to 12 August 2023, demonstrated substantial efficacy. The
portfolio yielded a total return of 15.38%, significantly outperforming the
10.39% return of a comparative Buy-and-Hold strategy. Key performance metrics,
including a Sharpe Ratio up to 2.17 and a win rate up to 100% for certain
pairs, confirmed the strategy's viability. Conclusion: This research
contributes a systematic and robust methodology for identifying profitable
trading opportunities derived from volatility-based causal relationships. The
findings have significant implications for both academic research in financial
modelling and the practical application of algorithmic trading, offering a
structured approach to developing resilient, data-driven strategies.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [292] [Machine-Precision Prediction of Low-Dimensional Chaotic Systems](https://arxiv.org/abs/2507.09652)
*Christof Schötz,Niklas Boers*

Key words: 混沌系统, 回归学习, 高精度计算, Lyapunov时间

TL;DR: 通过高精度回归方法，论文在无噪声数据中学习低维混沌系统，显著超越现有工作。

<details>
  <summary>Details</summary>

Main category: nlin.CD

Motivation: 低维混沌系统常用于测试学习方法，本文旨在展示如何通过高精度回归达到机器精度。

Method: 使用高次多项式特征和512位算术的普通最小二乘回归，学习无噪声观测数据。

Result: 在Lorenz-63等系统中，预测时间显著提高，达到32至105 Lyapunov时间。

Conclusion: 无噪声数据下学习低维混沌系统已解决，精度超越传统数值求解器。

Abstract: Low-dimensional chaotic systems such as the Lorenz-63 model are commonly used
to benchmark system-agnostic methods for learning dynamics from data. Here we
show that learning from noise-free observations in such systems can be achieved
up to machine precision: using ordinary least squares regression on high-degree
polynomial features with 512-bit arithmetic, our method exceeds the accuracy of
standard 64-bit numerical ODE solvers of the true underlying dynamical systems.
Depending on the configuration, we obtain valid prediction times of 32 to 105
Lyapunov times for the Lorenz-63 system, dramatically outperforming prior work
that reaches 13 Lyapunov times at most. We further validate our results on
Thomas' Cyclically Symmetric Attractor, a non-polynomial chaotic system that is
considerably more complex than the Lorenz-63 model, and show that similar
results extend also to higher dimensions using the spatiotemporally chaotic
Lorenz-96 model. Our findings suggest that learning low-dimensional chaotic
systems from noise-free data is a solved problem.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [293] [Semantic Source Code Segmentation using Small and Large Language Models](https://arxiv.org/abs/2507.08992)
*Abdelhalim Dahou,Ansgar Scherp,Sebastian Kurten,Brigitte Mathiak,Madhu Chauhan*

Key words: 代码分割、R语言、语言模型、自动化、机器学习

TL;DR: 论文提出了一种自动化、领域特定的方法，用于研究R代码的分割，并比较了两种方法及不同语言模型的效果。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 随着代码库的增长，尤其是对于低资源语言如R及其研究领域（如社会科学、心理学），手动和语法分析方法变得不切实际，因此需要自动化解决方案来提升代码分割的效率。

Method: 提出了两种新颖的方法：基于上下文的逐行分析和基于范围的片段确定，并使用大型和小型语言模型（LLMs/SLMs）进行实验。

Result: 基于上下文的逐行分析方法优于基于范围的分割，且小型语言模型（如CodeBERT和CodeT5+的编码器版本）表现优于大型语言模型。

Conclusion: 小型语言模型在仅经过少量手动注释代码微调的情况下，表现出色，表明它们在特定领域的潜力。

Abstract: Source code segmentation, dividing code into functionally coherent segments,
is crucial for knowledge retrieval and maintenance in software development.
While enabling efficient navigation and comprehension of large codebases,
manual and syntactic analysis approaches have become impractical as
repositories grow, especially for low-resource languages like R and their
research domains (e.g., social sciences, psychology).This paper introduces an
automated, domain-specific approach for research R code segmentation using
Large and Small Language Models (LLMs/SLMs). It presents two novel approaches
and a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:
line-by-line analysis with context and range-based segment determination. We
experiment with LLMs and fine-tuned SLMs. To support the generalizability of
our approaches, we also include experiments on Python code from the computer
science domain.Our results show that context-based line-by-line analysis is
superior over range-based segmentation.Using smaller language models like
CodeBERT and an encoder-only version of CodeT5+ are better than their LLM
counterparts. Most notably, these two best-performing models did not see R code
during pre-training versus the LLMs but were only fine-tuned on 4,130 lines of
manually annotated code.

</details>


### [294] [Evaluating LLMs on Sequential API Call Through Automated Test Generation](https://arxiv.org/abs/2507.09481)
*Yuheng Huang,Da Song,Zhenlan Ji,Shuai Wang,Lei Ma*

Key words: 大型语言模型、API集成、基准测试、StateGen、StateEval

TL;DR: 论文提出了StateGen框架，自动生成涉及顺序API交互的多样化编码任务，以填补现有基准测试的不足。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现有基准测试主要依赖手动收集的测试用例，难以自动检查语义正确性，且忽略了顺序API调用的复杂交互。

Method: StateGen结合状态机约束求解、能量采样和控制流注入生成可执行程序，再通过LLM代理将其转换为自然语言任务描述。

Result: 构建了StateEval基准测试，包含120个已验证的测试用例，覆盖三个代表性场景。实验证实StateGen能有效生成挑战性任务。

Conclusion: StateGen为评估和改进LLM工具使用提供了新方法，凸显了当前LLM在API集成中的不足。

Abstract: By integrating tools from external APIs, Large Language Models (LLMs) have
expanded their promising capabilities in a diverse spectrum of complex
real-world tasks. However, testing, evaluation, and analysis of LLM tool use
remain in their early stages. Most existing benchmarks rely on manually
collected test cases, many of which cannot be automatically checked for
semantic correctness and instead depend on static methods such as string
matching. Additionally, these benchmarks often overlook the complex
interactions that occur between sequential API calls, which are common in
real-world applications. To fill the gap, in this paper, we introduce StateGen,
an automated framework designed to generate diverse coding tasks involving
sequential API interactions. StateGen combines state-machine-based API
constraint solving and validation, energy-based sampling, and control-flow
injection to generate executable programs. These programs are then translated
into human-like natural language task descriptions through a collaboration of
two LLM agents. Utilizing StateGen, we construct StateEval, a benchmark
encompassing 120 verified test cases spanning across three representative
scenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental
results confirm that StateGen can effectively generate challenging and
realistic API-oriented tasks, highlighting areas for improvement in current
LLMs incorporating APIs.

</details>


### [295] [Accelerating Drug Discovery Through Agentic AI: A Multi-Agent Approach to Laboratory Automation in the DMTA Cycle](https://arxiv.org/abs/2507.09023)
*Yao Fehlis,Charles Crain,Aidan Jensen,Michael Watson,James Juhasz,Paul Mandel,Betty Liu,Shawn Mahon,Daren Wilson,Nick Lynch-Jonely,Ben Leedom,David Fuller*

Key words: AI框架，药物发现，实验室自动化，DMTA循环，多代理系统

TL;DR: 论文介绍了一种名为Tippy的新型AI框架，通过专门设计的AI代理在药物发现的DMTA循环中实现实验室自动化，显著提升了工作效率和跨学科协作。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 传统药物发现方法难以满足现代治疗开发需求，需要创新技术提升效率和科学性。

Method: 提出多代理系统Tippy，包含五个专门代理（Supervisor, Molecule, Lab, Analysis, Report）和Safety Guardrail监督，专注于DMTA循环的各阶段。

Result: Tippy显著提升了工作流程效率、决策速度和跨学科协调，为AI辅助药物发现提供了新范式。

Conclusion: Tippy是首个针对DMTA循环的AI代理生产级实现，展示了AI在实验室自动化中的巨大潜力。

Abstract: The pharmaceutical industry faces unprecedented challenges in drug discovery,
with traditional approaches struggling to meet modern therapeutic development
demands. This paper introduces a novel AI framework, Tippy, that transforms
laboratory automation through specialized AI agents operating within the
Design-Make-Test-Analyze (DMTA) cycle. Our multi-agent system employs five
specialized agents - Supervisor, Molecule, Lab, Analysis, and Report, with
Safety Guardrail oversight - each designed to excel in specific phases of the
drug discovery pipeline. Tippy represents the first production-ready
implementation of specialized AI agents for automating the DMTA cycle,
providing a concrete example of how AI can transform laboratory workflows. By
leveraging autonomous AI agents that reason, plan, and collaborate, we
demonstrate how Tippy accelerates DMTA cycles while maintaining scientific
rigor essential for pharmaceutical research. The system shows significant
improvements in workflow efficiency, decision-making speed, and
cross-disciplinary coordination, offering a new paradigm for AI-assisted drug
discovery.

</details>


### [296] [SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments](https://arxiv.org/abs/2507.09063)
*Avi Arora,Jinu Jang,Roshanak Zilouchian Moghaddam*

Key words: LLM代理,环境配置,基准测试,软件任务

TL;DR: SetupBench是一个新的基准测试，专注于评估现代LLM代理在裸机Linux环境中安装依赖和配置任务的能力。通过评估OpenHands代理，发现其成功率低，并揭示了系统性问题。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现有基准测试未充分评估LLM代理在实际环境中的任务完成能力，SetupBench填补了这一空白。

Method: 开发了一个包含93个实例的基准测试，涵盖多种语言生态和数据库配置任务。

Result: OpenHands代理在任务中的成功率较低，尤其在仓库设置和数据库配置中表现不佳。

Conclusion: SetupBench为下一代软件代理提供了严格的评估标准，揭示了当前系统在实际环境中的局限性。

Abstract: Modern Large Language Model (LLM) agents promise end to end assistance with
real-world software tasks, yet existing benchmarks evaluate LLM agents almost
exclusively in pre-baked environments where every dependency is pre-installed.
To fill this gap, we introduce SetupBench, a 93 instance benchmark that
isolates the environment-bootstrap skill: starting from a bare Linux sandbox,
an agent must install packages, resolve dependency conflicts, initialize
databases, and configure background services. Our tasks span seven language
ecosystems, five database engines, and multi-service orchestration scenarios,
each accompanies by a natural language problem statement and a deterministic
success command. Through evaluation of OpenHands, a state-of-the-art coding
agent, we find low success rates across task categories, with particular
challenges in repository setup (38.9-57.4%) and local database configuration
(20.0-53.3%). Our analysis reveals systematic failure modes including
incomplete development tooling installation, hallucinated task constraints, and
non-persistent environment modifications that break agent-human collaboration
workflows. We identify substantial inefficiencies in agent exploration
strategies, with 38-89% of actions being unnecessary compared to optimal human
behavior. These findings highlight gaps in current agents' practical
environment-bootstrap capabilities. By targeting this critical yet
under-evaluated capability, SetupBench provides a rigorous yard-stick for the
next generation of software developer agents aiming to solve end to end
real-wold tasks.

</details>


### [297] [SPICE: An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation](https://arxiv.org/abs/2507.09108)
*Aaditya Bhatia,Gustavo A. Oliva,Gopi Krishnan Rajbahadur,Haoxiang Zhang,Yihao Chen,Zhilong Chen,Arthur Leung,Dayi Lin,Boyuan Chen,Ahmed E. Hassan*

Key words: SPICE, 软件工程, 数据集标注, 自动化流程, 成本效益

TL;DR: SPICE是一个可扩展的自动化标注流程，用于高效标注软件工程数据集，显著降低成本并提高规模。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 高质量标注数据集对软件工程基础模型至关重要，但人工标注成本高且耗时长。

Method: SPICE结合上下文感知代码导航、理性驱动的提示和多轮共识，生成接近专家标注的标签。

Result: SPICE标注1000个实例的成本从10万美元降至5.10美元，与人工标注达成高度一致。

Conclusion: SPICE为大规模软件工程数据集创建提供了经济高效的解决方案，并发布了工具和新数据集。

Abstract: High-quality labeled datasets are crucial for training and evaluating
foundation models in software engineering, but creating them is often
prohibitively expensive and labor-intensive. We introduce SPICE, a scalable,
automated pipeline for labeling SWE-bench-style datasets with annotations for
issue clarity, test coverage, and effort estimation. SPICE combines
context-aware code navigation, rationale-driven prompting, and multi-pass
consensus to produce labels that closely approximate expert annotations.
SPICE's design was informed by our own experience and frustration in labeling
more than 800 instances from SWE-Gym. SPICE achieves strong agreement with
human-labeled SWE-bench Verified data while reducing the cost of labeling 1,000
instances from around $100,000 (manual annotation) to just $5.10. These results
demonstrate SPICE's potential to enable cost-effective, large-scale dataset
creation for SE-focused FMs. To support the community, we release both SPICE
tool and SPICE Bench, a new dataset of 6,802 SPICE-labeled instances curated
from 291 open-source projects in SWE-Gym (over 13x larger than SWE-bench
Verified).

</details>


### [298] [Enhancing Interpretability in Software Change Management with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.09315)
*Yongqian Sun,Weihua Kuang,Chao Shen,Xidao Wen,Tinghua Zheng,Heng Liu,Shenglin Zhang,Bo Wu,Dan Pei*

Key words: SCELM, 软件变更管理, 自动化框架, 服务失败, 经济损失

TL;DR: 论文摘要介绍了SCELM框架，用于自动化管理软件变更以减少服务失败和经济损失。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现代在线服务中频繁的软件变更带来了高风险，需要有效管理以减少失败和损失。

Method: 提出SCELM，一个端到端的自动化软件变更评估与生命周期管理框架。

Result: SCELM提高了软件变更管理的效率和精准度。

Conclusion: SCELM能显著减少服务失败和经济损失。

Abstract: In modern online services, frequent software changes introduce significant
risks. To tackle this challenge, we propose SCELM (Software Change Evaluation
and Lifecycle Management), an end-to-end automated framework for software
change management. SCELM aims to manage software changes efficiently and
precisely, significantly reducing service failures and economic losses.

</details>


### [299] [A Serverless Architecture for Real-Time Stock Analysis using Large Language Models: An Iterative Development and Debugging Case Study](https://arxiv.org/abs/2507.09583)
*Taniv Ashraf*

Key words: 大型语言模型, 金融分析, 实时系统, 自动化流程, 调试方法

TL;DR: 本文介绍了一种基于Google Gemini API的服务器系统，用于实时股票分析，通过自动化数据处理和静态前端展示成果。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 利用大型语言模型（LLMs）普及金融数据分析，为个人提供低成本、高效的AI金融工具。

Method: 结合Gemini API进行定性评估，通过GitHub Actions自动化数据处理，采用事件驱动架构。

Result: 最终系统成本接近零，展示了个体构建复杂AI金融工具的可行性。

Conclusion: 讨论了LLMs在金融分析中的作用、调试方法的重要性以及人机协作在软件开发中的新范式。

Abstract: The advent of powerful, accessible Large Language Models (LLMs) like Google's
Gemini presents new opportunities for democratizing financial data analysis.
This paper documents the design, implementation, and iterative debugging of a
novel, serverless system for real-time stock analysis. The system leverages the
Gemini API for qualitative assessment, automates data ingestion and processing
via GitHub Actions, and presents the findings through a decoupled, static
frontend. We detail the architectural evolution of the system, from initial
concepts to a robust, event-driven pipeline, highlighting the practical
challenges encountered during deployment. A significant portion of this paper
is dedicated to a case study on the debugging process, covering common software
errors, platform-specific permission issues, and rare, environment-level
platform bugs. The final architecture operates at a near-zero cost,
demonstrating a viable model for individuals to build sophisticated AI-powered
financial tools. The operational application is publicly accessible, and the
complete source code is available for review. We conclude by discussing the
role of LLMs in financial analysis, the importance of robust debugging
methodologies, and the emerging paradigm of human-AI collaboration in software
development.

</details>


### [300] [OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit Optimization](https://arxiv.org/abs/2507.09682)
*Laura Baird,Armin Moin*

Key words: 量子电路优化、深度强化学习、NISQ、协调引擎、硬件感知

TL;DR: 提出了OrQstrator框架，利用深度强化学习进行量子电路优化，通过三个互补的优化模块和一个中央协调引擎来改进电路性能，适应NISQ时代的硬件限制。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 在NISQ时代，量子电路的优化需要适应硬件限制和噪声问题，传统方法效率不足，需要一种更智能的框架。

Method: OrQstrator结合了深度强化学习、特定领域优化器和参数化电路实例化器，中央协调引擎根据电路结构和硬件约束智能选择优化模块。

Result: 系统生成了针对硬件优化的电路，提升了门数、深度和预期保真度等性能指标。

Conclusion: OrQstrator为NISQ时代的量子电路优化提供了一种高效且智能的解决方案。

Abstract: We propose a novel approach, OrQstrator, which is a modular framework for
conducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum
(NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our
orchestration engine intelligently selects among three complementary circuit
optimizers: A DRL-based circuit rewriter trained to reduce depth and gate count
via learned rewrite sequences; a domain-specific optimizer that performs
efficient local gate resynthesis and numeric optimization; a parameterized
circuit instantiator that improves compilation by optimizing template circuits
during gate set translation. These modules are coordinated by a central
orchestration engine that learns coordination policies based on circuit
structure, hardware constraints, and backend-aware performance features such as
gate count, depth, and expected fidelity. The system outputs an optimized
circuit for hardware-aware transpilation and execution, leveraging techniques
from an existing state-of-the-art approach, called the NISQ Analyzer, to adapt
to backend constraints.

</details>


### [301] [Prompting for Performance: Exploring LLMs for Configuring Software](https://arxiv.org/abs/2507.09790)
*Helge Spieker,Théo Matricon,Nassim Belmecheri,Jørn Eirik Betten,Gauthier Le Bartz Lyan,Heraldo Borges,Quentin Mazouni,Dennis Gross,Arnaud Gotlieb,Mathieu Acher*

Key words: 大型语言模型, 软件配置, 性能优化, 机器学习, 配置选项

TL;DR: 论文探讨了大型语言模型（LLMs）是否能通过提示辅助软件性能配置，初步结果显示LLMs在部分任务中表现良好，但也存在幻觉或表面推理的问题。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 软件配置选择对性能指标有显著影响，但传统机器学习方法计算成本高，本研究探索LLMs是否能在配置任务中提供帮助。

Method: 通过评估LLMs在不同可配置系统（如编译器、视频编码器）中识别选项、排序配置和推荐高性能配置的能力。

Result: 初步结果发现，LLMs在某些任务中与专家知识对齐良好，但也可能出现幻觉或表面推理。

Conclusion: 这些发现为LLMs在软件配置中的系统性评估和解决方案设计提供了初步基础。

Abstract: Software systems usually provide numerous configuration options that can
affect performance metrics such as execution time, memory usage, binary size,
or bitrate. On the one hand, making informed decisions is challenging and
requires domain expertise in options and their combinations. On the other hand,
machine learning techniques can search vast configuration spaces, but with a
high computational cost, since concrete executions of numerous configurations
are required. In this exploratory study, we investigate whether large language
models (LLMs) can assist in performance-oriented software configuration through
prompts. We evaluate several LLMs on tasks including identifying relevant
options, ranking configurations, and recommending performant configurations
across various configurable systems, such as compilers, video encoders, and SAT
solvers. Our preliminary results reveal both positive abilities and notable
limitations: depending on the task and systems, LLMs can well align with expert
knowledge, whereas hallucinations or superficial reasoning can emerge in other
cases. These findings represent a first step toward systematic evaluations and
the design of LLM-based solutions to assist with software configuration.

</details>


### [302] [Turning the Tide: Repository-based Code Reflection](https://arxiv.org/abs/2507.09866)
*Wei Zhang,Jian Yang,Jiaxi Yang,Ya Wang,Zhoujun Li,Zeyu Cui,Binyuan Hui,Junyang Lin*

Key words: 代码LLM, 基准测试, 代码库修改, 数据污染, 多文件代码

TL;DR: 论文介绍了LiveRepoReflection，一个针对多文件代码库环境下的代码理解和生成能力评估的挑战性基准测试，包含1,888个严格筛选的测试用例，并提出RepoReflection-Instruct数据集用于训练模型。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现有的代码LLM评估基准忽略了代码库修改场景，且动态基准测试中仍存在数据污染和改进反思能力的挑战。

Method: 提出了LiveRepoReflection基准测试和RepoReflection-Instruct数据集，通过两轮对话过程训练模型RepoReflectionCoder。

Result: 基准测试评估了40多个LLM，并在多语言代码库环境中展示了模型的表现。

Conclusion: LiveRepoReflection填补了代码库修改场景的评估空白，为代码LLM的研究提供了更全面的基准。

Abstract: Code large language models (LLMs) enhance programming by understanding and
generating code across languages, offering intelligent feedback, bug detection,
and code updates through reflection, improving development efficiency and
accessibility. While benchmarks (e.g. HumanEval/LiveCodeBench) evaluate code
generation and real-world relevance, previous works ignore the scenario of
modifying code in repositories. Considering challenges remaining in improving
reflection capabilities and avoiding data contamination in dynamic benchmarks,
we introduce LiveRepoReflection, a challenging benchmark for evaluating code
understanding and generation in multi-file repository contexts, featuring 1,888
rigorously filtered test cases across $6$ programming languages to ensure
diversity, correctness, and high difficulty. Further, we create
RepoReflection-Instruct, a large-scale, quality-filtered instruction-tuning
dataset derived from diverse sources, used to train RepoReflectionCoder through
a two-turn dialogue process involving code generation and error-driven repair.
The leaderboard evaluates over 40 LLMs to reflect the model performance of
repository-based code reflection.

</details>


### [303] [Breaking the Myth: Can Small Models Infer Postconditions Too?](https://arxiv.org/abs/2507.10182)
*Gehao Zhang,Zhenting Wang,Juan Zhai*

Key words: 软件规范,语言模型,微调,高效,自动化生成

TL;DR: 论文展示了小型、经过微调的语言模型能以更低的计算成本高效生成高质量的软件规范，匹配或超越大型模型。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 解决手动编写软件规范繁琐且易错的问题，同时探索是否必须依赖大型语言模型。

Method: 构建专用数据集，微调7B参数代码模型，处理实际代码库依赖并保留前置状态信息。

Result: 在真实Java缺陷基准测试中，小型模型在语法、语义正确性及区分错误能力上优于大型模型。

Conclusion: 针对小数据集微调小型模型可替代资源密集型大模型，提供高效的自动化规范生成方案。

Abstract: Formal specifications are essential for ensuring software correctness, yet
manually writing them is tedious and error-prone. Large Language Models (LLMs)
have shown promise in generating such specifications from natural language
intents, but the giant model size and high computational demands raise a
fundamental question: Do we really need large models for this task? In this
paper, we show that a small, fine-tuned language model can achieve high-quality
postcondition generation with much lower computational costs. We construct a
specialized dataset of prompts, reasoning logs, and postconditions, then
supervise the fine-tuning of a $7$B-parameter code model. Our approach tackles
real-world repository dependencies and preserves pre-state information,
allowing for expressive and accurate specifications. We evaluate the model on a
benchmark of real-world Java bugs (Defects4J) and compare against both
proprietary giants (e.g., GPT-4o) and open-source large models. Empirical
results demonstrate that our compact model matches or outperforms significantly
larger counterparts in syntax correctness, semantic correctness, and
bug-distinguishing capability. These findings highlight that targeted
fine-tuning on a modest dataset can enable small models to achieve results
formerly seen only in massive, resource-heavy LLMs, offering a practical and
efficient path for the real-world adoption of automated specification
generation.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [304] [Automatic Contouring of Spinal Vertebrae on X-Ray using a Novel Sandwich U-Net Architecture](https://arxiv.org/abs/2507.09158)
*Sunil Munthumoduku Krishna Murthy,Kumar Rajamani,Srividya Tirunellai Rajamani,Yupei Li,Qiyang Sun,Bjoern W. Schuller*

Key words: 脊柱活动性疾病、U-Net、椎体分割、X光图像、自动化

TL;DR: 论文提出了一种改进的U-Net结构用于X光图像中胸椎的准确分割，显著提升了分割精度。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 脊柱活动性疾病的评估需要精确的椎体分割和轮廓提取，传统手动方法耗时且易出错，自动化方法更为高效。

Method: 研究采用了一种带有双激活函数的“三明治”U-Net结构，用于X光图像中胸椎的分割。

Result: 该方法在Dice分数上比基线U-Net模型提高了4.1%，分割精度更优。

Conclusion: 改进的U-Net结构在椎体分割任务中表现出更高的准确性和可靠性。

Abstract: In spinal vertebral mobility disease, accurately extracting and contouring
vertebrae is essential for assessing mobility impairments and monitoring
variations during flexion-extension movements. Precise vertebral contouring
plays a crucial role in surgical planning; however, this process is
traditionally performed manually by radiologists or surgeons, making it
labour-intensive, time-consuming, and prone to human error. In particular,
mobility disease analysis requires the individual contouring of each vertebra,
which is both tedious and susceptible to inconsistencies. Automated methods
provide a more efficient alternative, enabling vertebra identification,
segmentation, and contouring with greater accuracy and reduced time
consumption. In this study, we propose a novel U-Net variation designed to
accurately segment thoracic vertebrae from anteroposterior view on X-Ray
images. Our proposed approach, incorporating a ``sandwich" U-Net structure with
dual activation functions, achieves a 4.1\% improvement in Dice score compared
to the baseline U-Net model, enhancing segmentation accuracy while ensuring
reliable vertebral contour extraction.

</details>


### [305] [PanoDiff-SR: Synthesizing Dental Panoramic Radiographs using Diffusion and Super-resolution](https://arxiv.org/abs/2507.09227)
*Sanyam Jain,Bruna Neves de Freitas,Andreas Basse-OConnor,Alexandros Iosifidis,Ruben Pauwels*

Key words: 合成医学图像、扩散生成、超分辨率、牙科放射图像

TL;DR: 本文提出了一种结合扩散生成（PanoDiff）和超分辨率（SR）的方法，用于生成高质量、逼真的合成牙科全景放射图像，以解决公共数据集稀缺的问题。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 解决公共医疗数据集稀缺的问题，并为人工智能研究和教育提供合成数据。

Method: 采用扩散生成模型生成低分辨率种子图像，再通过超分辨率模型提升分辨率，其中SR模型采用先进的transformer学习局部-全局关系。

Result: 合成高分辨率图像与实际图像的FID分数为40.69，临床专家区分真实与合成图像的准确率为68.5%。

Conclusion: 该方法能够生成逼真的合成牙科图像，可用于研究和教育。

Abstract: There has been increasing interest in the generation of high-quality,
realistic synthetic medical images in recent years. Such synthetic datasets can
mitigate the scarcity of public datasets for artificial intelligence research,
and can also be used for educational purposes. In this paper, we propose a
combination of diffusion-based generation (PanoDiff) and Super-Resolution (SR)
for generating synthetic dental panoramic radiographs (PRs). The former
generates a low-resolution (LR) seed of a PR (256 X 128) which is then
processed by the SR model to yield a high-resolution (HR) PR of size 1024 X
512. For SR, we propose a state-of-the-art transformer that learns local-global
relationships, resulting in sharper edges and textures. Experimental results
demonstrate a Frechet inception distance score of 40.69 between 7243 real and
synthetic images (in HR). Inception scores were 2.55, 2.30, 2.90 and 2.98 for
real HR, synthetic HR, real LR and synthetic LR images, respectively. Among a
diverse group of six clinical experts, all evaluating a mixture of 100
synthetic and 100 real PRs in a time-limited observation, the average accuracy
in distinguishing real from synthetic images was 68.5% (with 50% corresponding
to random guessing).

</details>


### [306] [AI-Enhanced Pediatric Pneumonia Detection: A CNN-Based Approach Using Data Augmentation and Generative Adversarial Networks (GANs)](https://arxiv.org/abs/2507.09759)
*Abdul Manaf,Nimra Mughal*

Key words: 肺炎, 儿科, 胸部X光, 机器学习, GAN, 数据增强

TL;DR: 该论文提出了一种基于机器学习的儿科胸部肺炎分类系统，旨在通过胸部X光图像辅助医疗专业人员诊断肺炎。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 肺炎是五岁以下儿童死亡的主要原因，需要准确的胸部X光诊断。

Method: 使用CNN模型，训练于5,863张标记的儿童胸部X光图像，并通过数据增强和GAN生成合成图像解决数据不足和类别不平衡问题。

Result: 结合原始、增强和GAN生成数据的模型在准确率和F1分数上表现最佳，并通过Flask网页应用实现实时分类。

Conclusion: 深度学习和GAN在提高儿科肺炎诊断准确性和效率方面具有潜力，特别适用于资源有限的临床环境。

Abstract: Pneumonia is a leading cause of mortality in children under five, requiring
accurate chest X-ray diagnosis. This study presents a machine learning-based
Pediatric Chest Pneumonia Classification System to assist healthcare
professionals in diagnosing pneumonia from chest X-ray images. The CNN-based
model was trained on 5,863 labeled chest X-ray images from children aged 0-5
years from the Guangzhou Women and Children's Medical Center. To address
limited data, we applied augmentation techniques (rotation, zooming, shear,
horizontal flipping) and employed GANs to generate synthetic images, addressing
class imbalance. The system achieved optimal performance using combined
original, augmented, and GAN-generated data, evaluated through accuracy and F1
score metrics. The final model was deployed via a Flask web application,
enabling real-time classification with probability estimates. Results
demonstrate the potential of deep learning and GANs in improving diagnostic
accuracy and efficiency for pediatric pneumonia classification, particularly
valuable in resource-limited clinical settings
https://github.com/AbdulManaf12/Pediatric-Chest-Pneumonia-Classification

</details>


### [307] [Multi-omic Prognosis of Alzheimer's Disease with Asymmetric Cross-Modal Cross-Attention Network](https://arxiv.org/abs/2507.08855)
*Yang Ming,Jiang Shi Zhong,Zhou Su Juan*

Key words: 阿尔茨海默病, 多模态融合, 深度学习, 不对称跨模态交叉注意力机制, 医学诊断

TL;DR: 本文提出了一种新型深度学习算法框架，通过融合多模态医学数据，采用不对称跨模态交叉注意力机制，显著提高了阿尔茨海默病诊断的准确性。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 传统卷积神经网络和简单的特征拼接方法在多模态数据融合中表现不佳，无法有效利用互补信息，因此需要开发更高效的多模态特征融合方法。

Method: 采用不对称跨模态交叉注意力机制，融合脑部PET、MRI、遗传数据和临床数据等多模态信息，提升诊断效果。

Result: 算法在测试集上的准确率达到94.88%，显著优于传统单模态和多模态深度学习模型。

Conclusion: 不对称跨模态交叉注意力机制能有效捕获多模态数据的交互特征，为阿尔茨海默病的诊断提供了新思路。

Abstract: Alzheimer's Disease (AD) is an irreversible neurodegenerative disease
characterized by progressive cognitive decline as its main symptom. In the
research field of deep learning-assisted diagnosis of AD, traditional
convolutional neural networks and simple feature concatenation methods fail to
effectively utilize the complementary information between multimodal data, and
the simple feature concatenation approach is prone to cause the loss of key
information during the process of modal fusion. In recent years, the
development of deep learning technology has brought new possibilities for
solving the problem of how to effectively fuse multimodal features. This paper
proposes a novel deep learning algorithm framework to assist medical
professionals in AD diagnosis. By fusing medical multi-view information such as
brain fluorodeoxyglucose positron emission tomography (PET), magnetic resonance
imaging (MRI), genetic data, and clinical data, it can accurately detect the
presence of AD, Mild Cognitive Impairment (MCI), and Cognitively Normal (CN).
The innovation of the algorithm lies in the use of an asymmetric cross-modal
cross-attention mechanism, which can effectively capture the key information
features of the interactions between different data modal features. This paper
compares the asymmetric cross-modal cross-attention mechanism with the
traditional algorithm frameworks of unimodal and multimodal deep learning
models for AD diagnosis, and evaluates the importance of the asymmetric
cross-modal cross-attention mechanism. The algorithm model achieves an accuracy
of 94.88% on the test set.

</details>


### [308] [VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models](https://arxiv.org/abs/2507.08982)
*Hanene F. Z. Brachemi Meftah,Wassim Hamidouche,Sid Ahmed Fezza,Olivier Déforges*

Key words: 视觉语言模型, 隐私保护, 对抗攻击, 感兴趣区域, 语义完整性

TL;DR: 论文提出一种新颖的对抗攻击策略，用于保护视觉语言模型（VLM）中的隐私信息，通过选择性隐藏图像中的感兴趣区域（ROI）来防止敏感内容被模型处理。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 随着视觉语言模型的广泛应用，用户隐私问题日益突出，特别是在模型可能无意中处理或暴露私人视觉信息的情况下。

Method: 将隐私保护问题建模为对抗攻击问题，提出一种选择性隐藏ROI的攻击策略，保持未掩码区域的高语义一致性。

Result: 实验表明，该策略在三种先进VLMs（LLaVA、Instruct-BLIP、BLIP2-T5）中实现了高达98%的目标ROI检测减少，同时保持图像全局语义完整。

Conclusion: 该研究为多模态模型的隐私保护提供了实用工具，推动了隐私意识的提升。

Abstract: Recent years have witnessed remarkable progress in developing Vision-Language
Models (VLMs) capable of processing both textual and visual inputs. These
models have demonstrated impressive performance, leading to their widespread
adoption in various applications. However, this widespread raises serious
concerns regarding user privacy, particularly when models inadvertently process
or expose private visual information. In this work, we frame the preservation
of privacy in VLMs as an adversarial attack problem. We propose a novel attack
strategy that selectively conceals information within designated Region Of
Interests (ROIs) in an image, effectively preventing VLMs from accessing
sensitive content while preserving the semantic integrity of the remaining
image. Unlike conventional adversarial attacks that often disrupt the entire
image, our method maintains high coherence in unmasked areas. Experimental
results across three state-of-the-art VLMs namely LLaVA, Instruct-BLIP, and
BLIP2-T5 demonstrate up to 98% reduction in detecting targeted ROIs, while
maintaining global image semantics intact, as confirmed by high similarity
scores between clean and adversarial outputs. We believe that this work
contributes to a more privacy conscious use of multimodal models and offers a
practical tool for further research, with the source code publicly available
at: https://github.com/hbrachemi/Vlm_defense-attack.

</details>


### [309] [Advanced U-Net Architectures with CNN Backbones for Automated Lung Cancer Detection and Segmentation in Chest CT Images](https://arxiv.org/abs/2507.09898)
*Alireza Golkarieha,Kiana Kiashemshakib,Sajjad Rezvani Boroujenic,Nasibeh Asadi Isakand*

Key words: U-Net, CNN, 肺癌检测, 分割, 分类

TL;DR: 研究用U-Net与不同CNN主干网络结合检测和分割肺部CT图像中的肺癌，ResNet50表现最佳，Xception分类效果最优。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 解决临床中需要准确诊断工具的需求。

Method: 用ResNet50、VGG16和Xception作为U-Net主干，结合CNN分类器和传统机器学习方法。

Result: ResNet50分割效果最佳（Dice: 0.9495），Xception分类准确率达99.1%，混合模型表现优异。

Conclusion: U-Net结合先进CNN主干可有效支持肺癌早期诊断和临床决策。

Abstract: This study investigates the effectiveness of U-Net architectures integrated
with various convolutional neural network (CNN) backbones for automated lung
cancer detection and segmentation in chest CT images, addressing the critical
need for accurate diagnostic tools in clinical settings. A balanced dataset of
832 chest CT images (416 cancerous and 416 non-cancerous) was preprocessed
using Contrast Limited Adaptive Histogram Equalization (CLAHE) and resized to
128x128 pixels. U-Net models were developed with three CNN backbones: ResNet50,
VGG16, and Xception, to segment lung regions. After segmentation, CNN-based
classifiers and hybrid models combining CNN feature extraction with traditional
machine learning classifiers (Support Vector Machine, Random Forest, and
Gradient Boosting) were evaluated using 5-fold cross-validation. Metrics
included accuracy, precision, recall, F1-score, Dice coefficient, and ROC-AUC.
U-Net with ResNet50 achieved the best performance for cancerous lungs (Dice:
0.9495, Accuracy: 0.9735), while U-Net with VGG16 performed best for
non-cancerous segmentation (Dice: 0.9532, Accuracy: 0.9513). For
classification, the CNN model using U-Net with Xception achieved 99.1 percent
accuracy, 99.74 percent recall, and 99.42 percent F1-score. The hybrid
CNN-SVM-Xception model achieved 96.7 percent accuracy and 97.88 percent
F1-score. Compared to prior methods, our framework consistently outperformed
existing models. In conclusion, combining U-Net with advanced CNN backbones
provides a powerful method for both segmentation and classification of lung
cancer in CT scans, supporting early diagnosis and clinical decision-making.

</details>


### [310] [A Brain Tumor Segmentation Method Based on CLIP and 3D U-Net with Cross-Modal Semantic Guidance and Multi-Level Feature Fusion](https://arxiv.org/abs/2507.09966)
*Mingda Zhang*

Key words: 脑肿瘤分割, MRI, 多级融合, CLIP, 3D U-Net

TL;DR: 该研究提出了一种多级融合架构，整合像素级、特征级和语义级信息，结合CLIP模型的语义理解能力和3D U-Net的空间特征提取优势，显著提升了脑肿瘤分割的精确度。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 脑肿瘤形态多样且三维空间关系复杂，现有方法主要依赖MR图像的视觉特征，忽略了医学报告中的语义知识。

Method: 采用多级融合架构，通过3D-2D语义桥接、跨模态语义引导和基于语义的注意力机制，融合CLIP模型和3D U-Net的优势。

Result: 在BraTS 2020数据集上，整体Dice系数达到0.8567，比传统3D U-Net提升4.8%，增强肿瘤区域提升7.3%。

Conclusion: 语义级融合可显著提升脑肿瘤分割性能，尤其对临床关键区域效果明显。

Abstract: Precise segmentation of brain tumors from magnetic resonance imaging (MRI) is
essential for neuro-oncology diagnosis and treatment planning. Despite advances
in deep learning methods, automatic segmentation remains challenging due to
tumor morphological heterogeneity and complex three-dimensional spatial
relationships. Current techniques primarily rely on visual features extracted
from MRI sequences while underutilizing semantic knowledge embedded in medical
reports. This research presents a multi-level fusion architecture that
integrates pixel-level, feature-level, and semantic-level information,
facilitating comprehensive processing from low-level data to high-level
concepts. The semantic-level fusion pathway combines the semantic understanding
capabilities of Contrastive Language-Image Pre-training (CLIP) models with the
spatial feature extraction advantages of 3D U-Net through three mechanisms:
3D-2D semantic bridging, cross-modal semantic guidance, and semantic-based
attention mechanisms. Experimental validation on the BraTS 2020 dataset
demonstrates that the proposed model achieves an overall Dice coefficient of
0.8567, representing a 4.8% improvement compared to traditional 3D U-Net, with
a 7.3% Dice coefficient increase in the clinically important enhancing tumor
(ET) region.

</details>


### [311] [DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in Histopathology](https://arxiv.org/abs/2507.10250)
*Ashkan Shakarami,Lorenzo Nicole,Rocco Cappellesso,Angelo Paolo Dei Tos,Stefano Ghidoni*

Key words: 癌症诊断, 视觉Transformer, 病理学, AI辅助诊断

TL;DR: 提出新型AI系统DepViT-CAD，用于病理切片多类癌症诊断，核心为多注意力视觉Transformer（MAViT），在大规模真实数据集验证中表现优异。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 提高癌症诊断的准确性和时效性，支持临床决策。

Method: 设计MAViT捕获肿瘤形态特征，基于1008张全切片图像训练，验证于TCGA和临床病例。

Result: 在TCGA和临床病例中分别达到94.11%和92%的诊断灵敏度。

Conclusion: DepViT-CAD结合先进架构和大规模验证，为AI辅助癌症诊断提供可靠方案。

Abstract: Accurate and timely cancer diagnosis from histopathological slides is vital
for effective clinical decision-making. This paper introduces DepViT-CAD, a
deployable AI system for multi-class cancer diagnosis in histopathology. At its
core is MAViT, a novel Multi-Attention Vision Transformer designed to capture
fine-grained morphological patterns across diverse tumor types. MAViT was
trained on expert-annotated patches from 1008 whole-slide images, covering 11
diagnostic categories, including 10 major cancers and non-tumor tissue.
DepViT-CAD was validated on two independent cohorts: 275 WSIs from The Cancer
Genome Atlas and 50 routine clinical cases from pathology labs, achieving
diagnostic sensitivities of 94.11% and 92%, respectively. By combining
state-of-the-art transformer architecture with large-scale real-world
validation, DepViT-CAD offers a robust and scalable approach for AI-assisted
cancer diagnostics. To support transparency and reproducibility, software and
code will be made publicly available at GitHub.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [312] [Evolution of Fear and Social Rewards in Prey-Predator Relationship](https://arxiv.org/abs/2507.09992)
*Yuji Kanagawa,Kenji Doya*

Key words: 恐惧进化, 社交奖励, 捕食者-猎物模拟, 强化学习, 环境条件

TL;DR: 该论文通过分布式进化模拟研究了恐惧与社交奖励的进化关系，发现社交奖励对猎物生存更为重要，而恐惧仅在获得社交奖励后进化。模拟还揭示了捕食者能力对恐惧进化的影响及其稳定性。

<details>
  <summary>Details</summary>

Main category: q-bio.PE

Motivation: 研究恐惧与环境条件、其他奖励（如食物奖励和社交奖励）进化之间的关系，以理解恐惧行为的进化机制。

Method: 开发分布式进化模拟，其中猎物和捕食者共同进化其内在奖励功能（包括恐惧和社交奖励），并通过强化学习习得行为。

Result: 社交奖励对猎物生存至关重要，恐惧仅在社交奖励之后进化；捕食者能力增强会促进恐惧进化，但非进化捕食者更稳定；静止威胁下正向奖励与恐惧对立进化。

Conclusion: 恐惧与社交奖励在进化过程中存在复杂相互作用，受捕食者和威胁特性的影响。

Abstract: Fear is a critical brain function for detecting danger and learning to avoid
specific stimuli that can lead to danger. While fear is believed to have
evolved under pressure from predators, experimentally reproducing the evolution
is challenging. To investigate the relationship between environmental
conditions, the evolution of fear, and the evolution of other rewards, such as
food reward and social reward, we developed a distributed evolutionary
simulation. In our simulation, prey and predator agents co-evolve their innate
reward functions, including a possibly fear-like term for observing predators,
and learn behaviors via reinforcement learning. Surprisingly, our simulation
revealed that social reward for observing the same species is more important
for prey to survive, and fear-like negative reward for observing predators
evolves only after acquiring social reward. We also found that the predator
with increased hunting ability (larger mouth) amplified fear emergence, but
also that fear evolution is more stable with non-evolving predators that are
bad at chasing prey. Additionally, unlike for predators, we found that positive
rewards evolve in opposition to fear for stationary threats, as areas with
abundant leftover food develop around them. These findings suggest that fear
and social reward have had a complex interplay with each other through
evolution, along with the nature of predators and threats.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [313] [History Matching under Uncertainty of Geological Scenarios with Implicit Geological Realism Control with Generative Deep Learning and Graph Convolutions](https://arxiv.org/abs/2507.10201)
*Gleb Shishaev,Vasily Demyanov,Daniel Arnold*

Key words: 变分自编码器, 图方法, 储层建模, 地质不确定性, 潜在空间分析

TL;DR: 论文提出了一种基于图的变分自编码器架构，通过低维潜在空间处理不同地质场景的不确定性，并在储层建模中使用图方法替代传统格点深度学习方法。实验证明该方法在合成数据集上具有可行性。

<details>
  <summary>Details</summary>

Main category: stat.AP

Motivation: 解决地质场景不确定性，尤其是在储层建模中，通过图方法提升地质真实感的隐式控制。

Method: 利用基于图的变分自编码器和Geodesic度量，通过潜在变量生成模型控制地质真实感。

Result: 在包含单通道和双通道的3D地质合成数据集实验中，证明了方法的有效性。

Conclusion: 图方法在储层建模中展示了潜力，通过潜在空间分析工具（如PCA、t-SNE和TDA）验证了其结构。

Abstract: The graph-based variational autoencoder represents an architecture that can
handle the uncertainty of different geological scenarios, such as depositional
or structural, through the concept of a lowerdimensional latent space. The main
difference from recent studies is utilisation of a graph-based approach in
reservoir modelling instead of the more traditional lattice-based deep learning
methods. We provide a solution to implicitly control the geological realism
through the latent variables of a generative model and Geodesic metrics. Our
experiments of AHM with synthetic dataset that consists of 3D realisations of
channelised geological representations with two distinct scenarios with one and
two channels shows the viability of the approach. We offer in-depth analysis of
the latent space using tools such as PCA, t-SNE, and TDA to illustrate its
structure.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [314] [FinTeam: A Multi-Agent Collaborative Intelligence System for Comprehensive Financial Scenarios](https://arxiv.org/abs/2507.10448)
*Yingqian Wu,Qiushi Wang,Zefei Long,Rong Ye,Zhongtian Lu,Xianyin Zhang,Bingxuan Li,Wei Chen,Liwen Zhang,Zhongyu Wei*

Key words: FinTeam, 多代理系统, 财务报告, LLM代理, 金融分析

TL;DR: FinTeam是一个多代理协作金融系统，通过四个LLM代理（文档分析、分析师、会计师和顾问）协同工作，生成高质量的财务报告。

<details>
  <summary>Details</summary>

Main category: cs.CE

Motivation: 现有的LLM模型在财务场景中表现不足，无法全面分析复杂任务。受企业任务分工启发，开发了FinTeam系统。

Method: FinTeam由四个特定金融领域的LLM代理组成，通过协作完成任务。代理通过专业数据集训练，并在真实在线投资论坛任务上评估。

Result: FinTeam生成报告的人为接受率为62.00%，超越GPT-4o和Xuanyuan基准模型，并在FinCUGE和FinEval上分别提升7.43%和2.06%。

Conclusion: FinTeam通过多代理协作显著提升了财务报告生成任务的性能，展示了在真实金融场景中的潜力。

Abstract: Financial report generation tasks range from macro- to micro-economics
analysis, also requiring extensive data analysis. Existing LLM models are
usually fine-tuned on simple QA tasks and cannot comprehensively analyze real
financial scenarios. Given the complexity, financial companies often distribute
tasks among departments. Inspired by this, we propose FinTeam, a financial
multi-agent collaborative system, with a workflow with four LLM agents:
document analyzer, analyst, accountant, and consultant. We train these agents
with specific financial expertise using constructed datasets. We evaluate
FinTeam on comprehensive financial tasks constructed from real online
investment forums, including macroeconomic, industry, and company analysis. The
human evaluation shows that by combining agents, the financial reports generate
from FinTeam achieved a 62.00% acceptance rate, outperforming baseline models
like GPT-4o and Xuanyuan. Additionally, FinTeam's agents demonstrate a 7.43%
average improvement on FinCUGE and a 2.06% accuracy boost on FinEval. Project
is available at https://github.com/FudanDISC/DISC-FinLLM/.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [315] [Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language Model Inference](https://arxiv.org/abs/2507.09010)
*Chun-Ting Chen,HanGyeol Mun,Jian Meng,Mohamed S. Abdelfattah,Jae-sun Seo*

Key words: 边缘计算,LLM推理,脉动阵列,量化,能效

TL;DR: 这篇论文提出了一种基于混合脉动阵列（HSA）架构的边缘LLM推理加速器，通过优化推理效率和减少外部内存访问，实现了高面积和能源效率。

<details>
  <summary>Details</summary>

Main category: cs.AR

Motivation: 边缘LLM推理需要高能效和低延迟，但现有方法在内存访问和计算效率上存在不足，因此需要一种新的加速器设计。

Method: 采用混合脉动阵列（HSA）架构，结合MXINT4权重量化和优化的数据流，减少内存访问次数，同时设计专用硬件单元处理非线性和位置编码操作。

Result: 在1.3B参数的LLM上，该加速器实现了247/117 token/s/mm2的性能，比现有方法提升了2.45x/13.5x，同时保持高能源效率。

Conclusion: 提出的HSA架构显著提升了边缘LLM推理的效率，为低延迟和高能效的推理提供了可行的解决方案。

Abstract: Edge inference for large language models (LLM) offers secure, low-latency,
and cost-effective inference solutions. We emphasize that an edge accelerator
should achieve high area efficiency and minimize external memory access (EMA)
during the memory-bound decode stage, while maintaining high energy efficiency
during the compute intensive prefill stage. This paper proposes an edge LLM
inference accelerator featuring a hybrid systolic array (HSA) architecture that
optimizes inference efficiency in both stages. To further reduce EMA, we adopt
MXINT4 weight quantization and propose an optimized dataflow tailored for HSA,
ensuring negligible dequantization overhead and achieving 100% hardware
utilization with minimal accuracy loss under edge DRAM bandwidth constraints.
For non-linear operations, we incorporate optimized root mean square
normalization (RMSNorm) and rotary position embedding (RoPE) units, reducing
their latency, area, and memory access overhead while enabling end-to-end
inference on our accelerator. Our solution achieves 247/117 (token/s/mm2) while
running a 1.3B LLM on long-input/long-output scenarios, providing >2.45x/13.5x
improvement over existing approaches, while maintaining superior energy
efficiency in token generation.

</details>


### [316] [BitParticle: Partializing Sparse Dual-Factors to Build Quasi-Synchronizing MAC Arrays for Energy-efficient DNNs](https://arxiv.org/abs/2507.09780)
*Feilong Qiaoyuan,Jihe Wang,Zhiyu Sun,Linying Wu,Yuanhua Xiao,Danghui Wang*

Key words: DNN加速, 位级稀疏性, MAC单元

TL;DR: 论文提出了一种利用位级稀疏性的乘累加（MAC）单元，解决了传统方法无法同时利用双因子稀疏性和周期波动的问题，显著提高了面积和能效。

<details>
  <summary>Details</summary>

Main category: cs.AR

Motivation: 量化深度神经网络中的位级稀疏性具有优化乘累加操作的潜力，但传统方法无法同时利用双因子稀疏性，且周期波动导致MAC单元利用率低。

Method: 通过基于粒子化的方法设计了一种支持双因子稀疏性的MAC单元，并提出准同步调度方案以提升灵活性。

Result: 精确版本的面积效率提升了29.2%，近似版本的能效进一步提高了7.5%。

Conclusion: 所提设计在硬件效率和性能上均优于现有方案，适用于DNN加速。

Abstract: Bit-level sparsity in quantized deep neural networks (DNNs) offers
significant potential for optimizing Multiply-Accumulate (MAC) operations.
However, two key challenges still limit its practical exploitation. First,
conventional bit-serial approaches cannot simultaneously leverage the sparsity
of both factors, leading to a complete waste of one factor' s sparsity. Methods
designed to exploit dual-factor sparsity are still in the early stages of
exploration, facing the challenge of partial product explosion. Second, the
fluctuation of bit-level sparsity leads to variable cycle counts for MAC
operations. Existing synchronous scheduling schemes that are suitable for
dual-factor sparsity exhibit poor flexibility and still result in significant
underutilization of MAC units. To address the first challenge, this study
proposes a MAC unit that leverages dual-factor sparsity through the emerging
particlization-based approach. The proposed design addresses the issue of
partial product explosion through simple control logic, resulting in a more
area- and energy-efficient MAC unit. In addition, by discarding less
significant intermediate results, the design allows for further hardware
simplification at the cost of minor accuracy loss. To address the second
challenge, a quasi-synchronous scheme is introduced that adds cycle-level
elasticity to the MAC array, reducing pipeline stalls and thereby improving MAC
unit utilization. Evaluation results show that the exact version of the
proposed MAC array architecture achieves a 29.2% improvement in area efficiency
compared to the state-of-the-art bit-sparsity-driven architecture, while
maintaining comparable energy efficiency. The approximate variant further
improves energy efficiency by 7.5%, compared to the exact version. Index-Terms:
DNN acceleration, Bit-level sparsity, MAC unit

</details>


### [317] [Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving](https://arxiv.org/abs/2507.10178)
*Wonung Kim,Yubin Lee,Yoonsung Kim,Jinwoo Hwang,Seongryong Oh,Jiyong Jung,Aziz Huseynov,Woong Gyu Park,Chang Hyun Park,Divya Mahajan,Jongse Park*

Key words: Transformer, LLM, post-transformer, 内存带宽, 吞吐量, Pimba, MX量化

TL;DR: 论文探讨了Transformer和post-transformer LLMs的性能挑战，提出了基于内存带宽限制的统一服务系统Pimba，结合状态更新单元和低精度算术优化，显著提升了吞吐量。

<details>
  <summary>Details</summary>

Main category: cs.AR

Motivation: 当前Transformer在长序列处理中的计算和内存成本高，限制了可扩展性。社区探索了SSMs、线性注意力等post-transformer架构，需统一高效服务框架。

Method: 分析性能特点，提出基于SPUs的Pimba系统，结合状态更新引擎和MX量化算术，优化内存访问和计算效率。

Result: Pimba在吞吐量上比GPU和GPU+PIM系统分别提升3.2倍和2.1倍。

Conclusion: Pimba通过统一架构和硬件优化，有效解决了Transformer和post-transformer LLMs的扩展性问题。

Abstract: Transformers are the driving force behind today's Large Language Models
(LLMs), serving as the foundation for their performance and versatility. Yet,
their compute and memory costs grow with sequence length, posing scalability
challenges for long-context inferencing. In response, the algorithm community
is exploring alternative architectures, such as state space models (SSMs),
linear attention, and recurrent neural networks (RNNs), which we refer to as
post-transformers. This shift presents a key challenge: building a serving
system that efficiently supports both transformer and post-transformer LLMs
within a unified framework. To address this challenge, we analyze the
performance characteristics of transformer and post-transformer LLMs. Despite
their algorithmic differences, both are fundamentally limited by memory
bandwidth under batched inference due to attention in transformers and state
updates in post-transformers. Further analyses suggest two additional insights:
(1) state update operations, unlike attention, incur high hardware cost, making
per-bank PIM acceleration inefficient, and (2) different low-precision
arithmetic methods offer varying accuracy-area tradeoffs, while we identify
Microsoft's MX as the Pareto-optimal choice. Building on these insights, we
design Pimba as an array of State-update Processing Units (SPUs), each shared
between two banks to enable interleaved access to PIM. Each SPU includes a
State-update Processing Engine (SPE) that comprises element-wise multipliers
and adders using MX-based quantized arithmetic, enabling efficient execution of
state update and attention operations. Our evaluation shows that, compared to
LLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x
higher token generation throughput, respectively.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [318] [Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents](https://arxiv.org/abs/2507.08944)
*Enhao Zhang,Erkang Zhu,Gagan Bansal,Adam Fourney,Hussein Mozannar,Jack Gerrits*

Key words: 大型语言模型, 多代理系统, 并行执行, 事件驱动通信, 任务优化

TL;DR: M1-Parallel 框架通过并行运行多代理团队和异步通信，显著降低了多代理系统的高延迟问题，提升了任务完成率。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 基于大型语言模型的多代理系统在处理复杂任务时存在高延迟问题，需要一种方法优化其性能。

Method: 提出 M1-Parallel 框架，通过并行运行多代理团队和事件驱动的异步通信模型，探索不同的解决方案路径。

Result: 实验显示 M1-Parallel 可实现 2.2 倍的加速并保持准确性，同时提升任务完成率。

Conclusion: 并行执行计划是优化多代理系统在高复杂度任务中性能的有效方法。

Abstract: Large language model (LLM)-based multi-agent systems have demonstrated
remarkable promise for tackling complex tasks by breaking them down into
subtasks that are iteratively planned, executed, observed, and refined. Despite
their effectiveness, these systems often incur high latency because real-world
problems frequently demand multiple iterative cycles of reasoning steps. To
address this challenge, we propose M1-Parallel, a framework that concurrently
runs multiple multi-agent teams in parallel to uncover distinct solution paths.
By leveraging an event-driven communication model with asynchronous messaging,
M1-Parallel efficiently capitalizes on the inherent diversity of valid plans to
either reduce end-to-end latency or boost task completion rates. Our
experiments on complex tasks show that M1-Parallel with early termination
achieves up to $2.2\times$ speedup while preserving accuracy, and that
M1-Parallel with aggregation yields higher task completion rates. We further
investigate strategies aimed at encouraging diverse execution plans but observe
no additional performance gains over repeated sampling. Overall, these findings
underscore the potential of parallel plan execution for optimizing multi-agent
systems for real-world, high-complexity reasoning tasks.

</details>


### [319] [TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit](https://arxiv.org/abs/2507.09788)
*Paulo Salem,Robert Sim,Christopher Olsen,Prerit Saxena,Rafael Barcelos,Yi Ding*

Key words: Large Language Models, Multiagent Systems, simulation toolkit, human behavior, TinyTroupe

TL;DR: 论文介绍了TinyTroupe，一个用于模拟人类行为的工具包，解决了现有多智能体系统在人格定义和行为模拟方面的不足。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 现有多智能体系统在精细人格定义和行为模拟方面功能有限，无法满足行为研究和社会模拟的需求。

Method: 通过引入TinyTroupe工具包，提供详细的人格定义和LLM驱动机制，支持个体或群体行为问题的建模与解决。

Result: 通过案例展示了工具包的实用性，并进行了定量和定性评估，揭示了其可能性和限制。

Conclusion: TinyTroupe不仅是一个具体的Python实现，更是一种可应用于其他场景的新概念贡献。

Abstract: Recent advances in Large Language Models (LLM) have led to a new class of
autonomous agents, renewing and expanding interest in the area. LLM-powered
Multiagent Systems (MAS) have thus emerged, both for assistive and simulation
purposes, yet tools for realistic human behavior simulation -- with its
distinctive challenges and opportunities -- remain underdeveloped. Existing MAS
libraries and tools lack fine-grained persona specifications, population
sampling facilities, experimentation support, and integrated validation, among
other key capabilities, limiting their utility for behavioral studies, social
simulation, and related applications. To address these deficiencies, in this
work we introduce TinyTroupe, a simulation toolkit enabling detailed persona
definitions (e.g., nationality, age, occupation, personality, beliefs,
behaviors) and programmatic control via numerous LLM-driven mechanisms. This
allows for the concise formulation of behavioral problems of practical
interest, either at the individual or group level, and provides effective means
for their solution. TinyTroupe's components are presented using representative
working examples, such as brainstorming and market research sessions, thereby
simultaneously clarifying their purpose and demonstrating their usefulness.
Quantitative and qualitative evaluations of selected aspects are also provided,
highlighting possibilities, limitations, and trade-offs. The approach, though
realized as a specific Python implementation, is meant as a novel conceptual
contribution, which can be partially or fully incorporated in other contexts.
The library is available as open source at
https://github.com/microsoft/tinytroupe.

</details>


### [320] [How to Train a Leader: Hierarchical Reasoning in Multi-Agent LLMs](https://arxiv.org/abs/2507.08960)
*Andrew Estornell,Jean-Francois Ton,Muhammad Faaiz Taufiq,Hang Li*

Key words: 多智能体框架, LLM, 协作推理, MLPO

TL;DR: 提出了一种分层多智能体框架MLPO，通过训练单个领导者LLM来协调未训练的同伴智能体，显著提升了推理任务的性能。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 现有的多智能体框架在训练和推理时计算成本高，希望通过训练单个领导者LLM来高效协调多智能体，提高性能。

Method: 提出MLPO方法，训练领导者评估和合成智能体响应，无需额外价值网络或显式反馈。

Result: 在BBH、MATH和MMLU任务上，MLPO显著优于单智能体和多智能体基线。

Conclusion: MLPO展示了通过训练单个灵活领导者，在多智能体LLM系统中实现高效协作推理的有效性。

Abstract: Large Language Models (LLMs) have achieved strong performance on a wide range
of complex reasoning tasks, yet further gains are often possible by leveraging
the complementary strengths of multiple models. While multi-agent frameworks
can improve solution quality by leveraging multiple LLMs, existing methods are
often computationally expensive, both at training and inference time. In this
work, we introduce a hierarchical multi-agent framework that addresses these
challenges by training only a single leader LLM to coordinate a team of
untrained peer agents. To this end, we propose Multi-agent guided Leader Policy
\textbf{O}ptimization (MLPO), a novel approach which trains the leader to
evaluate and synthesize agent responses without auxiliary value networks or
explicit agent feedback. Leaders trained with MLPO exhibit improved performance
not only when interacting with the agent team at inference time, but also enjoy
improved performance when deployed in single-agent settings without the team.
Empirical results on Big-Bench Hard (BBH), MATH, and MMLU demonstrate that our
framework achieves substantial performance improvements over both single-agent
and multi-agent baselines. Our results highlight the effectiveness and
efficiency of training a single, flexible leader for collaborative reasoning in
multi-agent LLM systems.

</details>


### [321] [Large Population Models](https://arxiv.org/abs/2507.09901)
*Ayush Chopra*

Key words: 大群体模型（LPMs）、数字社会、集体行为、复杂系统、模拟

TL;DR: 论文提出大群体模型（LPMs）用于模拟复杂系统中的集体行为，结合高效计算、数据学习和隐私协议，以观察系统级结果并测试干预措施。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 解决社会紧迫问题（如疫情响应、供应链中断、气候适应）的复杂性，通过模拟大规模自主代理的行为来理解集体行为。

Method: LPMs通过三种创新方法扩展传统建模：高效计算模拟数百万代理、数学框架从多样化数据中学习、隐私保护通信协议。

Result: LPMs能够观察代理行为如何聚合成系统级结果，并测试干预措施，为现实世界部署提供测试环境。

Conclusion: LPMs通过模拟"数字社会"，填补个体行为与群体动态之间的空白，为集体智能和政策测试提供新路径。

Abstract: Many of society's most pressing challenges, from pandemic response to supply
chain disruptions to climate adaptation, emerge from the collective behavior of
millions of autonomous agents making decisions over time. Large Population
Models (LPMs) offer an approach to understand these complex systems by
simulating entire populations with realistic behaviors and interactions at
unprecedented scale. LPMs extend traditional modeling approaches through three
key innovations: computational methods that efficiently simulate millions of
agents simultaneously, mathematical frameworks that learn from diverse
real-world data streams, and privacy-preserving communication protocols that
bridge virtual and physical environments. This allows researchers to observe
how agent behavior aggregates into system-level outcomes and test interventions
before real-world implementation. While current AI advances primarily focus on
creating "digital humans" with sophisticated individual capabilities, LPMs
develop "digital societies" where the richness of interactions reveals emergent
phenomena. By bridging individual agent behavior and population-scale dynamics,
LPMs offer a complementary path in AI research illuminating collective
intelligence and providing testing grounds for policies and social innovations
before real-world deployment. We discuss the technical foundations and some
open problems here. LPMs are implemented by the AgentTorch framework
(github.com/AgentTorch/AgentTorch)

</details>


### [322] [Toolsuite for Implementing Multiagent Systems Based on Communication Protocols](https://arxiv.org/abs/2507.10324)
*Amit K. Chopra,Samuel H. Christie V,Munindar P. Singh*

Key words: 交互导向编程, 多智能体系统, 交互协议, 协议验证, 中间件

TL;DR: 本文介绍了交互导向编程（IOP）及其相关软件工具，为多智能体系统的开发和验证提供支持。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 多智能体系统的开发需要灵活的交互协议和角色实现，IOP提供了一种新方法来解决这一问题。

Method: 通过建模角色间的交互协议并实现相应的代理，开发了一套软件工具，包括协议验证和代理实现中间件。

Result: 提供了高效验证协议（如活跃性和安全性）的工具，并简化了代理的实现过程。

Conclusion: IOP及配套软件工具为多智能体系统的开发提供了有效的支持。

Abstract: Interaction-Oriented Programming (IOP) is an approach to building a
multiagent system by modeling the interactions between its roles via a flexible
interaction protocol and implementing agents to realize the interactions of the
roles they play in the protocol.
  In recent years, we have developed an extensive suite of software that
enables multiagent system developers to apply IOP. These include tools for
efficiently verifying protocols for properties such as liveness and safety and
middleware that simplifies the implementation of agents. This paper presents
some of that software suite.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [323] [Stochastic Approximation with Block Coordinate Optimal Stepsizes](https://arxiv.org/abs/2507.08963)
*Tao Jiang,Lin Xiao*

Key words: 随机逼近,块坐标步长,自适应步长,Adam,收敛性

TL;DR: 提出了一种基于块坐标步长的随机逼近方法，通过在线估计搜索方向二阶矩自适应调整步长，比Adam更节省内存和超参数。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 旨在最小化迭代点到最优点的预期距离，改进现有方法如Adam的高资源需求。

Method: 采用块坐标步长和在线二阶矩估计，提出简单条件估计器以减少内存和超参数。

Result: 新方法性能与Adam相当，但更高效，收敛性证明接近最优点的邻域。

Conclusion: 方法在非凸非光滑条件下适用，收敛性依赖于估计器的偏差和方差。

Abstract: We consider stochastic approximation with block-coordinate stepsizes and
propose adaptive stepsize rules that aim to minimize the expected distance from
the next iterate to an optimal point. These stepsize rules employ online
estimates of the second moment of the search direction along each block
coordinate. The popular Adam algorithm can be interpreted as a particular
heuristic for such estimation. By leveraging a simple conditional estimator, we
derive a new method that obtains comparable performance as Adam but requires
less memory and fewer hyper-parameters. We prove that this family of methods
converges almost surely to a small neighborhood of the optimal point, and the
radius of the neighborhood depends on the bias and variance of the
second-moment estimator. Our analysis relies on a simple aiming condition that
assumes neither convexity nor smoothness, thus has broad applicability.

</details>


### [324] [On the Gradient Domination of the LQG Problem](https://arxiv.org/abs/2507.09026)
*Kasra Fallah,Leonardo F. Toso,James Anderson*

Key words: 策略梯度, LQG问题, 历史表示法, 全局收敛, 稳定性保证

TL;DR: 论文研究了通过策略梯度（PG）方法解决线性二次高斯（LQG）调节问题，提出了一种新的参数化方法以实现全局收敛。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 尽管PG方法在解决线性二次调节（LQR）问题时表现出强大的理论保证，但在LQG设置中的理论理解仍然有限。LQG问题在经典参数化下缺乏梯度主导性，阻碍了全局收敛保证。

Method: 采用控制器的替代参数化方法，即历史表示法，通过过去p个时间步的输入和输出数据对控制输入进行参数化，从而建立梯度主导性和近似平滑性。

Result: 证明了在模型基和无模型设置下，策略梯度LQG具有全局收敛性和每次迭代的稳定性保证。数值实验验证了全局收敛性。

Conclusion: 通过历史表示法，PG方法可以在LQG问题中实现全局收敛，为相关研究提供了新的理论支持。

Abstract: We consider solutions to the linear quadratic Gaussian (LQG) regulator
problem via policy gradient (PG) methods. Although PG methods have demonstrated
strong theoretical guarantees in solving the linear quadratic regulator (LQR)
problem, despite its nonconvex landscape, their theoretical understanding in
the LQG setting remains limited. Notably, the LQG problem lacks gradient
dominance in the classical parameterization, i.e., with a dynamic controller,
which hinders global convergence guarantees. In this work, we study PG for the
LQG problem by adopting an alternative parameterization of the set of
stabilizing controllers and employing a lifting argument. We refer to this
parameterization as a history representation of the control input as it is
parameterized by past input and output data from the previous p time-steps.
This representation enables us to establish gradient dominance and approximate
smoothness for the LQG cost. We prove global convergence and per-iteration
stability guarantees for policy gradient LQG in model-based and model-free
settings. Numerical experiments on an open-loop unstable system are provided to
support the global convergence guarantees and to illustrate convergence under
different history lengths of the history representation.

</details>


### [325] [A Method for Learning to Solve Parametric Bilevel Optimization with Coupling Constraints](https://arxiv.org/abs/2507.09050)
*James Kotary,Himanshu Sharma,Ethan King,Draguna Vrabie,Ferdinando Fioretto,Jan Drgona*

Key words: 学习优化(L2O), 双层优化, 神经网络, 控制系统协同设计

TL;DR: 论文提出一种学习解决双层优化问题的新框架，利用现代优化问题的微分技术，扩展了传统单层优化的L2O方法。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 传统L2O方法主要针对单层优化问题，而双层优化因其复杂性在时间严格场景中难以解决，亟需高效解决方案。

Method: 通过训练神经网络作为参数化双层优化的高效近似器，并利用优化问题的微分技术。

Result: 该框架在合成双层问题和控制系统协同设计问题上表现良好，展示了神经网络在双层优化中的潜力。

Conclusion: 该框架扩展了L2O的应用范围，为复杂双层优化问题提供了实用解决方案。

Abstract: Learning to Optimize (L2O) is a subfield of machine learning (ML) in which ML
models are trained to solve parametric optimization problems. The general goal
is to learn a fast approximator of solutions to constrained optimization
problems, as a function of their defining parameters. Prior L2O methods focus
almost entirely on single-level programs, in contrast to the bilevel programs,
whose constraints are themselves expressed in terms of optimization
subproblems. Bilevel programs have numerous important use cases but are
notoriously difficult to solve, particularly under stringent time demands. This
paper proposes a framework for learning to solve a broad class of challenging
bilevel optimization problems, by leveraging modern techniques for
differentiation through optimization problems. The framework is illustrated on
an array of synthetic bilevel programs, as well as challenging control system
co-design problems, showing how neural networks can be trained as efficient
approximators of parametric bilevel optimization.

</details>


### [326] [Nesterov Finds GRAAL: Optimal and Adaptive Gradient Method for Convex Optimization](https://arxiv.org/abs/2507.09823)
*Ekaterina Borodich,Dmitry Kovalev*

Key words: 自适应梯度方法, GRAAL算法, Nesterov加速, 最优收敛速率, Lipschitz平滑函数

TL;DR: 本文提出了一种结合Nesterov加速的GRAAL算法，以解决现有自适应梯度方法无法达到最优收敛速度的问题，证明了其在Lipschitz平滑函数上的最优收敛性。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 尽管现有的自适应梯度方法（如GRAAL）能够避免超参数调整和线搜索，并达到固定步长梯度下降的标准收敛速度，但它们无法达到Nesterov加速的最优收敛速率。本文旨在填补这一空白。

Method: 开发了一种结合Nesterov加速的GRAAL算法，通过适应目标函数的局部曲率计算步长，且无需初始步长的精确选择。

Result: 证明了新算法在Lipschitz平滑函数上达到了最优的收敛速率$​​\mathcal{O}(1/k^2)$，且对初始步长的选择具有鲁棒性。

Conclusion: 新算法在无需额外计算成本的情况下，实现了自适应梯度方法的最优收敛性能。

Abstract: In this paper, we focus on the problem of minimizing a continuously
differentiable convex objective function $\min_x f(x)$. Recently, several
adaptive gradient methods, including GRAAL (Malitsky, 2020), have been
developed. These methods estimate the local curvature of the objective function
to compute stepsizes, attain the standard convergence rate $\mathcal{O}(1/k)$
of fixed-stepsize gradient descent for Lipschitz-smooth functions, and do not
require any line search procedures or hyperparameter tuning. However, a natural
question arises: is it possible to accelerate the convergence of these
algorithms to match the optimal rate $\mathcal{O}(1/k^2)$ of the accelerated
gradient descent of Nesterov (1983)? Although some attempts have been made (Li
and Lan, 2023), the capabilities of the existing accelerated algorithms to
adapt to the curvature of the objective function are highly limited.
Consequently, we provide a positive answer to this question and develop GRAAL
with Nesterov acceleration. We prove that our algorithm achieves the desired
optimal convergence rate for Lipschitz smooth functions. Moreover, in contrast
to existing methods, it does so with an arbitrary, even excessively small,
initial stepsize at the cost of a logarithmic additive term in the iteration
complexity.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [327] [A Multi-Level Strategy for Deepfake Content Moderation under EU Regulation](https://arxiv.org/abs/2507.08879)
*Max-Paul Förster,Luca Deck,Raimund Weidlich,Niklas Kühl*

Key words: 深度伪造，欧盟监管，多层级策略，透明度规定，政治传播

TL;DR: 论文探讨了深度伪造技术对民主社会的风险，分析了欧盟的透明度规定及相关技术方法的有效性，提出了一种多层级策略以满足监管需求。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 深度伪造技术的普及增加了民主社会的风险，尤其是政治传播领域。欧盟的透明度规定虽有其作用，但缺乏行业和执行标准，需要更有效的解决方案。

Method: 通过多声部文献综述，总结了标记、检测和标注深度伪造技术的方法，并评估其在欧盟规定下的有效性。

Result: 研究发现单一方法无法满足监管和实际需求，因此提出了一种结合现有方法优势的多层级策略。

Conclusion: 多层级策略通过简单评分机制实现了可扩展性和实用性，同时适应不同类型的技术和特定上下文的风险加权。

Abstract: The growing availability and use of deepfake technologies increases risks for
democratic societies, e.g., for political communication on online platforms.
The EU has responded with transparency obligations for providers and deployers
of Artificial Intelligence (AI) systems and online platforms. This includes
marking deepfakes during generation and labeling deepfakes when they are
shared. However, the lack of industry and enforcement standards poses an
ongoing challenge. Through a multivocal literature review, we summarize methods
for marking, detecting, and labeling deepfakes and assess their effectiveness
under EU regulation. Our results indicate that individual methods fail to meet
regulatory and practical requirements. Therefore, we propose a multi-level
strategy combining the strengths of existing methods. To account for the masses
of content on online platforms, our multi-level strategy provides scalability
and practicality via a simple scoring mechanism. At the same time, it is
agnostic to types of deepfake technology and allows for context-specific risk
weighting.

</details>


### [328] [The Consistency-Acceptability Divergence of LLMs in Judicial Decision-Making: Task and Stakeholder Dimensions](https://arxiv.org/abs/2507.08881)
*Zhang MingDa,Xu Qing*

Key words: 大语言模型,司法系统,一致性-可接受性分歧,DTDMR-LJGF

TL;DR: 本文首次提出“一致性-可接受性分歧”概念，指技术一致性与社会接受度之间的差距，并通过分析2023-2025年数据，提出了DTDMR-LJGF框架以平衡技术效率与社会合法性。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 大语言模型（LLM）技术在司法系统中的整合揭示了技术一致性与社会接受度之间的悖论，亟需解决。

Method: 通过分析2023-2025年LLM司法应用数据，提出DTDMR-LJGF框架，结合智能任务分类和多角色互动。

Result: 研究发现LLM的技术一致性具有正负效应，DTDMR-LJGF框架可平衡技术与社会需求。

Conclusion: DTDMR-LJGF框架为构建技术与社会合法性平衡的LLM司法生态系统提供了理论与实用指导。

Abstract: The integration of large language model (LLM) technology into judicial
systems is fundamentally transforming legal practice worldwide. However, this
global transformation has revealed an urgent paradox requiring immediate
attention. This study introduces the concept of ``consistency-acceptability
divergence'' for the first time, referring to the gap between technical
consistency and social acceptance. While LLMs achieve high consistency at the
technical level, this consistency demonstrates both positive and negative
effects. Through comprehensive analysis of recent data on LLM judicial
applications from 2023--2025, this study finds that addressing this challenge
requires understanding both task and stakeholder dimensions. This study
proposes the Dual-Track Deliberative Multi-Role LLM Judicial Governance
Framework (DTDMR-LJGF), which enables intelligent task classification and
meaningful interaction among diverse stakeholders. This framework offers both
theoretical insights and practical guidance for building an LLM judicial
ecosystem that balances technical efficiency with social legitimacy.

</details>


### [329] [The Engineer's Dilemma: A Review of Establishing a Legal Framework for Integrating Machine Learning in Construction by Navigating Precedents and Industry Expectations](https://arxiv.org/abs/2507.08908)
*M. Z. Naser*

Key words: 机器学习, 法律框架, 工程实践, 法律责任, 监管

TL;DR: 论文探讨如何将机器学习（ML）技术融入工程行业，并通过法律原则和立法依据解决工程师在决策过程中面临的合法性和监管问题。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 工程行业尚未完全采用ML技术，导致工程师和利益相关者对法律框架的不确定性，因此需要填补这一空白。

Method: 通过类比推理，结合现有法律原则（如疏忽和产品责任）和行业经验，为ML技术在工程中的应用提供合法性支持。

Result: 提出了一种法律框架，帮助利益相关者评估ML驱动的工程解决方案中的责任、义务和益处。

Conclusion: 理解技术与法律先例的互动对于确立ML在工程实践中的合法性至关重要。

Abstract: Despite the widespread interest in machine learning (ML), the engineering
industry has not yet fully adopted ML-based methods, which has left engineers
and stakeholders uncertain about the legal and regulatory frameworks that
govern their decisions. This gap remains unaddressed as an engineer's
decision-making process, typically governed by professional ethics and
practical guidelines, now intersects with complex algorithmic outputs. To
bridge this gap, this paper explores how engineers can navigate legal
principles and legislative justifications that support and/or contest the
deployment of ML technologies. Drawing on recent precedents and experiences
gained from other fields, this paper argues that analogical reasoning can
provide a basis for embedding ML within existing engineering codes while
maintaining professional accountability and meeting safety requirements. In
exploring these issues, the discussion focuses on established liability
doctrines, such as negligence and product liability, and highlights how courts
have evaluated the use of predictive models. We further analyze how legislative
bodies and standard-setting organizations can furnish explicit guidance
equivalent to prior endorsements of emergent technologies. This exploration
stresses the vitality of understanding the interplay between technical
justifications and legal precedents for shaping an informed stance on ML's
legitimacy in engineering practice. Finally, our analysis catalyzes a legal
framework for integrating ML through which stakeholders can critically assess
the responsibilities, liabilities, and benefits inherent in ML-driven
engineering solutions.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [330] [Bridging Literature and the Universe Via A Multi-Agent Large Language Model System](https://arxiv.org/abs/2507.08958)
*Xiaowen Zhang,Zhenyu Bi,Xuan Wang,Tiziana Di Matteo,Rupert A. C. Croft*

Key words: 宇宙学模拟,多智能体系统,参数提取,LLM,自动化

TL;DR: SimAgents是一个多智能体系统，旨在自动从文献中提取宇宙学模拟参数并生成可执行脚本，提高物理研究的效率。

<details>
  <summary>Details</summary>

Main category: astro-ph.IM

Motivation: 宇宙学模拟和相关软件日益复杂，物理学家需从大量文献和手册中提取参数并生成脚本，这一过程耗时且易出错。

Method: SimAgents利用专业LLM智能体进行物理推理、模拟软件验证和工具执行，通过结构化通信确保参数物理合理且符合软件要求。

Result: 实验表明，SimAgents在收集的40多个模拟数据集上表现优异，能有效加速科学研究。

Conclusion: SimAgents展示了自动化宇宙学模拟参数提取和分析的潜力，为物理学家提供了高效工具。

Abstract: As cosmological simulations and their associated software become increasingly
complex, physicists face the challenge of searching through vast amounts of
literature and user manuals to extract simulation parameters from dense
academic papers, each using different models and formats. Translating these
parameters into executable scripts remains a time-consuming and error-prone
process. To improve efficiency in physics research and accelerate the
cosmological simulation process, we introduce SimAgents, a multi-agent system
designed to automate both parameter configuration from the literature and
preliminary analysis for cosmology research. SimAgents is powered by
specialized LLM agents capable of physics reasoning, simulation software
validation, and tool execution. These agents collaborate through structured
communication, ensuring that extracted parameters are physically meaningful,
internally consistent, and software-compliant. We also construct a cosmological
parameter extraction evaluation dataset by collecting over 40 simulations in
published papers from Arxiv and leading journals that cover diverse simulation
types. Experiments on the dataset demonstrate a strong performance of
SimAgents, highlighting its effectiveness and potential to accelerate
scientific research for physicists. Our demonstration video is available at:
https://youtu.be/w1zLpm_CaWA. The complete system and dataset are publicly
available at https://github.com/xwzhang98/SimAgents.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [331] [Analysing Health Misinformation with Advanced Centrality Metrics in Online Social Networks](https://arxiv.org/abs/2507.09055)
*Mkululi Sikosana,Sean Maudsley-Barton,Oluwaseun Ajao*

Key words: 健康错误信息, 中心性指标, 动态网络, 传播路径, 干预效果

TL;DR: 研究发现，结合传统和新颖的中心性指标能更有效地识别和减少健康错误信息在网络中的传播。

<details>
  <summary>Details</summary>

Main category: cs.SI

Motivation: 全球危机中健康错误信息的快速传播对公共健康和社会稳定构成挑战，传统中心性指标难以应对复杂动态的网络环境。

Method: 引入并比较三种新颖中心性指标：动态影响中心性、健康错误信息脆弱性中心性和传播中心性，结合FibVID和Monant数据集验证。

Result: 新指标比传统方法多识别44.83%的重要节点，干预效果提升25%，验证了新指标的广泛适用性。

Conclusion: 传统与新指标结合能更稳健地理解和减少健康错误信息在不同网络环境中的传播。

Abstract: The rapid spread of health misinformation on online social networks (OSNs)
during global crises such as the COVID-19 pandemic poses challenges to public
health, social stability, and institutional trust. Centrality metrics have long
been pivotal in understanding the dynamics of information flow, particularly in
the context of health misinformation. However, the increasing complexity and
dynamism of online networks, especially during crises, highlight the
limitations of these traditional approaches. This study introduces and compares
three novel centrality metrics: dynamic influence centrality (DIC), health
misinformation vulnerability centrality (MVC), and propagation centrality (PC).
These metrics incorporate temporal dynamics, susceptibility, and multilayered
network interactions. Using the FibVID dataset, we compared traditional and
novel metrics to identify influential nodes, propagation pathways, and
misinformation influencers. Traditional metrics identified 29 influential
nodes, while the new metrics uncovered 24 unique nodes, resulting in 42
combined nodes, an increase of 44.83%. Baseline interventions reduced health
misinformation by 50%, while incorporating the new metrics increased this to
62.5%, an improvement of 25%. To evaluate the broader applicability of the
proposed metrics, we validated our framework on a second dataset, Monant
Medical Misinformation, which covers a diverse range of health misinformation
discussions beyond COVID-19. The results confirmed that the advanced metrics
generalised successfully, identifying distinct influential actors not captured
by traditional methods. In general, the findings suggest that a combination of
traditional and novel centrality measures offers a more robust and
generalisable framework for understanding and mitigating the spread of health
misinformation in different online network contexts.

</details>


### [332] [Advanced Health Misinformation Detection Through Hybrid CNN-LSTM Models Informed by the Elaboration Likelihood Model (ELM)](https://arxiv.org/abs/2507.09149)
*Mkululi Sikosana,Sean Maudsley-Barton,Oluwaseun Ajao*

Key words: 健康虚假信息, ELM模型, CNN-LSTM, 社交媒体, 特征工程

TL;DR: 该研究结合心理学的精细加工可能性模型（ELM）和混合CNN-LSTM模型，提升社交媒体上健康虚假信息的检测效果，结果显示模型性能显著提升。

<details>
  <summary>Details</summary>

Main category: cs.SI

Motivation: COVID-19疫情期间的健康虚假信息对公共卫生构成挑战，研究旨在通过整合ELM特征和机器学习模型提高虚假信息检测的准确性和可靠性。

Method: 采用混合CNN-LSTM模型，并引入ELM的特征（如文本可读性、情感极性和启发式线索）进行虚假信息分类。结合特征工程进一步优化模型性能。

Result: 增强模型在准确率（97.37%）、精确率（96.88%）、召回率（98.50%）、F1分数（97.41%）和ROC-AUC（99.50%）上表现优异；结合特征工程后，性能进一步提升至精确率98.88%、召回率99.80%、F1分数99.41%、ROC-AUC99.80%。

Conclusion: ELM特征对提升虚假信息检测性能具有显著价值，展示了心理学理论在开发先进机器学习算法中的实际应用潜力。

Abstract: Health misinformation during the COVID-19 pandemic has significantly
challenged public health efforts globally. This study applies the Elaboration
Likelihood Model (ELM) to enhance misinformation detection on social media
using a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory
(LSTM) model. The model aims to enhance the detection accuracy and reliability
of misinformation classification by integrating ELM-based features such as text
readability, sentiment polarity, and heuristic cues (e.g., punctuation
frequency). The enhanced model achieved an accuracy of 97.37%, precision of
96.88%, recall of 98.50%, F1-score of 97.41%, and ROC-AUC of 99.50%. A combined
model incorporating feature engineering further improved performance, achieving
a precision of 98.88%, recall of 99.80%, F1-score of 99.41%, and ROC-AUC of
99.80%. These findings highlight the value of ELM features in improving
detection performance, offering valuable contextual information. This study
demonstrates the practical application of psychological theories in developing
advanced machine learning algorithms to address health misinformation
effectively.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [333] [Generation of structure-guided pMHC-I libraries using Diffusion Models](https://arxiv.org/abs/2507.08902)
*Sergio Mares,Ariel Espinoza Weinberger,Nilah M. Ioannidis*

Key words: pMHC-I, 扩散模型, 结构引导, 肽段设计, 免疫疗法

TL;DR: 论文提出了一种基于结构引导的pMHC-I肽段评测基准，利用扩散模型设计肽段，旨在减少现有数据集的偏见，提高个性化疫苗和T细胞免疫疗法的效果。

<details>
  <summary>Details</summary>

Main category: q-bio.QM

Motivation: 现有pMHC-I肽段发现方法受限于质谱和结合实验数据集的偏见，难以发现新肽段。论文旨在通过结构引导设计解决这一问题。

Method: 采用扩散模型设计pMHC-I肽段，基于晶体结构相互作用距离，构建独立于现有数据集的评测基准。

Result: 新设计的肽段具有高结构稳定性和残基多样性，现有序列预测器对其识别性能较差。

Conclusion: 结构引导的设计方法为无偏模型训练和评测提供了重要资源，揭示了传统评估中未发现的等位基因特异性限制。

Abstract: Personalized vaccines and T-cell immunotherapies depend critically on
identifying peptide-MHC class I (pMHC-I) interactions capable of eliciting
potent immune responses. However, current benchmarks and models inherit biases
present in mass-spectrometry and binding-assay datasets, limiting discovery of
novel peptide ligands. To address this issue, we introduce a structure-guided
benchmark of pMHC-I peptides designed using diffusion models conditioned on
crystal structure interaction distances. Spanning twenty high-priority HLA
alleles, this benchmark is independent of previously characterized peptides yet
reproduces canonical anchor residue preferences, indicating structural
generalization without experimental dataset bias. Using this resource, we
demonstrate that state-of-the-art sequence-based predictors perform poorly at
recognizing the binding potential of these structurally stable designs,
indicating allele-specific limitations invisible in conventional evaluations.
Our geometry-aware design pipeline yields peptides with high predicted
structural integrity and higher residue diversity than existing datasets,
representing a key resource for unbiased model training and evaluation. Our
code, and data are available at: https://github.com/sermare/struct-mhc-dev.

</details>


### [334] [From Classical Machine Learning to Emerging Foundation Models: Review on Multimodal Data Integration for Cancer Research](https://arxiv.org/abs/2507.09028)
*Amgad Muneer,Muhammad Waqas,Maliazurina B Saad,Eman Showkatian,Rukhmini Bandyopadhyay,Hui Xu,Wentao Li,Joe Y Chang,Zhongxing Liao,Cara Haymaker,Luisa Solis Soto,Carol C Wu,Natalie I Vokes,Xiuning Le,Lauren A Byers,Don L Gibbons,John V Heymach,Jianjun Zhang,Jia Wu*

Key words: 癌症研究, 多模态数据集成, 基础模型, 机器学习, 深度学习

TL;DR: 综述了多模态数据集成在癌症研究中的应用，重点探讨了从传统机器学习到基础模型的转变及其在肿瘤学中的潜力。

<details>
  <summary>Details</summary>

Main category: q-bio.QM

Motivation: 整合多模态数据以提取可操作的癌症研究洞察面临挑战，基础模型的出现为此提供了新途径。

Method: 回顾了多模态数据集成策略，分析了机器学习和深度学习的趋势，包括方法论、验证协议和开源资源。

Result: 总结了基础模型的进展与挑战，提出了当前集成方法为下一代大规模预训练模型奠定了基础。

Conclusion: 从传统ML到基础模型的转型为癌症研究中的大规模AI模型时代奠定了基础。

Abstract: Cancer research is increasingly driven by the integration of diverse data
modalities, spanning from genomics and proteomics to imaging and clinical
factors. However, extracting actionable insights from these vast and
heterogeneous datasets remains a key challenge. The rise of foundation models
(FMs) -- large deep-learning models pretrained on extensive amounts of data
serving as a backbone for a wide range of downstream tasks -- offers new
avenues for discovering biomarkers, improving diagnosis, and personalizing
treatment. This paper presents a comprehensive review of widely adopted
integration strategies of multimodal data to assist advance the computational
approaches for data-driven discoveries in oncology. We examine emerging trends
in machine learning (ML) and deep learning (DL), including methodological
frameworks, validation protocols, and open-source resources targeting cancer
subtype classification, biomarker discovery, treatment guidance, and outcome
prediction. This study also comprehensively covers the shift from traditional
ML to FMs for multimodal integration. We present a holistic view of recent FMs
advancements and challenges faced during the integration of multi-omics with
advanced imaging data. We identify the state-of-the-art FMs, publicly available
multi-modal repositories, and advanced tools and methods for data integration.
We argue that current state-of-the-art integrative methods provide the
essential groundwork for developing the next generation of large-scale,
pre-trained models poised to further revolutionize oncology. To the best of our
knowledge, this is the first review to systematically map the transition from
conventional ML to advanced FM for multimodal data integration in oncology,
while also framing these developments as foundational for the forthcoming era
of large-scale AI models in cancer research.

</details>


### [335] [A PBN-RL-XAI Framework for Discovering a "Hit-and-Run'' Therapeutic Strategy in Melanoma](https://arxiv.org/abs/2507.10136)
*Zhonglin Liu*

Key words: 抗PD-1免疫治疗, LOXL2, 强化学习, 概率布尔网络

TL;DR: 研究构建了一个动态概率布尔网络模型，通过强化学习发现四步抑制LOXL2蛋白是最有效策略，克服抗PD-1免疫治疗的先天耐药性。

<details>
  <summary>Details</summary>

Main category: q-bio.QM

Motivation: 解决转移性黑色素瘤中抗PD-1免疫治疗的先天耐药性问题。

Method: 使用转录组数据构建动态概率布尔网络模型，结合强化学习和可解释AI。

Result: 发现四步临时抑制LOXL2蛋白是最佳策略，可消除耐药分子标记。

Conclusion: 提出了时间依赖性治疗假设计算机框架。

Abstract: Innate resistance to anti-PD-1 immunotherapy remains a major clinical
challenge in metastatic melanoma, with the underlying molecular networks being
poorly understood. To address this, we constructed a dynamic Probabilistic
Boolean Network model using transcriptomic data from patient tumor biopsies to
elucidate the regulatory logic governing therapy response. We then employed a
reinforcement learning agent to systematically discover optimal, multi-step
therapeutic interventions and used explainable artificial intelligence to
mechanistically interpret the agent's control policy. The analysis revealed
that a precisely timed, 4-step temporary inhibition of the lysyl oxidase like 2
protein (LOXL2) was the most effective strategy. Our explainable analysis
showed that this ``hit-and-run" intervention is sufficient to erase the
molecular signature driving resistance, allowing the network to self-correct
without requiring sustained intervention. This study presents a novel,
time-dependent therapeutic hypothesis for overcoming immunotherapy resistance
and provides a powerful computational framework for identifying non-obvious
intervention protocols in complex biological systems.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [336] [LNN-powered Fluid Antenna Multiple Access](https://arxiv.org/abs/2507.08821)
*Pedro D. Alvim,Hugerles S. Silva,Ugo S. Dias,Osamah S. Badarneh,Felipe A. P. Figueiredo,Rausley A. A. de Souza*

Key words: 流体天线系统, 多标签分类, 液态神经网络, 端口选择, 中断概率

TL;DR: 该论文首次将流体天线系统中的端口选择问题建模为多标签分类任务，利用液态神经网络（LNNs）优化端口选择，降低中断概率。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 现有流体天线多址接入场景下，端口选择问题缺乏高效的解决方案，尤其是在端口观测受限的情况下。

Method: 将端口选择问题建模为多标签分类任务，应用液态神经网络（LNNs）预测最优端口，并结合α-μ衰落模型，通过超参数优化调整LNN结构以适应不同观测场景。

Result: 所提方法的中断概率低于现有方法。

Conclusion: 液态神经网络在流体天线系统中能够有效优化端口选择，显著提升性能。

Abstract: Fluid antenna systems represent an innovative approach in wireless
communication, recently applied in multiple access to optimize the
signal-to-interference-plus-noise ratio through port selection. This letter
frames the port selection problem as a multi-label classification task for the
first time, improving best-port selection with limited port observations. We
address this challenge by leveraging liquid neural networks (LNNs) to predict
the optimal port under emerging fluid antenna multiple access scenarios
alongside a more general $\alpha$-$\mu$ fading model. We also apply
hyperparameter optimization to refine LNN architectures for different
observation scenarios. Our approach yields lower outage probability values than
existing methods.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [337] [Lightweight Deep Learning-Based Channel Estimation for RIS-Aided Extremely Large-Scale MIMO Systems on Resource-Limited Edge Devices](https://arxiv.org/abs/2507.09627)
*Muhammad Kamran Saeed,Ashfaq Khokhar,Shakil Ahmed*

Key words: 6G, XL-MIMO, RIS, 信道估计, 深度学习

TL;DR: 论文提出了一种轻量级深度学习框架，用于高效估计XL-MIMO系统中的级联信道，降低计算复杂度并适用于资源受限的边缘设备。

<details>
  <summary>Details</summary>

Main category: cs.IT

Motivation: 随着XL-MIMO和RIS等6G技术的普及，高效且可扩展的信道估计变得至关重要，但现有方法在计算复杂度和大规模系统部署方面存在局限性。

Method: 利用信道的空间相关性，提出了一种基于分块的训练机制，降低输入维度并保留关键信息，实现大规模系统的可扩展训练。

Result: 仿真结果表明，该框架在不同条件下显著提升了估计精度并降低了计算复杂度，适用于不断增加的XL-MIMO天线和RIS元件。

Conclusion: 该框架为XL-MIMO系统中高效信道估计提供了可行的轻量级解决方案，适合资源受限的边缘设备。

Abstract: Next-generation wireless technologies such as 6G aim to meet demanding
requirements such as ultra-high data rates, low latency, and enhanced
connectivity. Extremely Large-Scale MIMO (XL-MIMO) and Reconfigurable
Intelligent Surface (RIS) are key enablers, with XL-MIMO boosting spectral and
energy efficiency through numerous antennas, and RIS offering dynamic control
over the wireless environment via passive reflective elements. However,
realizing their full potential depends on accurate Channel State Information
(CSI). Recent advances in deep learning have facilitated efficient cascaded
channel estimation. However, the scalability and practical deployment of
existing estimation models in XL-MIMO systems remain limited. The growing
number of antennas and RIS elements introduces a significant barrier to
real-time and efficient channel estimation, drastically increasing data volume,
escalating computational complexity, requiring advanced hardware, and resulting
in substantial energy consumption. To address these challenges, we propose a
lightweight deep learning framework for efficient cascaded channel estimation
in XL-MIMO systems, designed to minimize computational complexity and make it
suitable for deployment on resource-constrained edge devices. Using spatial
correlations in the channel, we introduce a patch-based training mechanism that
reduces the dimensionality of input to patch-level representations while
preserving essential information, allowing scalable training for large-scale
systems. Simulation results under diverse conditions demonstrate that our
framework significantly improves estimation accuracy and reduces computational
complexity, regardless of the increasing number of antennas and RIS elements in
XL-MIMO systems.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [338] [Counterfactual optimization for fault prevention in complex wind energy systems](https://arxiv.org/abs/2507.08849)
*Emilio Carrizosa,Martina Fischetti,Roshell Haaker,Juan Miguel Morales*

Key words: 机器学习,反事实优化,复杂系统,能源系统,故障恢复

TL;DR: 该论文提出了一种基于机器学习的优化控制策略，用于检测复杂系统中的故障并最小化恢复成本，特别针对海上风力涡轮机油变压器。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 现有机器学习模型仅能检测异常，而本文旨在进一步找到最小化干扰的恢复策略，填补了复杂系统中优化恢复的空白。

Method: 将问题建模为反事实优化问题，利用数学模型找到满足系统约束的最小调整方案。

Result: 在真实数据测试中，该方法适应性强，每年可为典型风电场节省约300万欧元。

Conclusion: 该方法不仅适用于能源系统，还为反事实优化在更广泛领域的应用开辟了新方向。

Abstract: Machine Learning models are increasingly used in businesses to detect faults
and anomalies in complex systems. In this work, we take this approach a step
further: beyond merely detecting anomalies, we aim to identify the optimal
control strategy that restores the system to a safe state with minimal
disruption. We frame this challenge as a counterfactual problem: given a
Machine Learning model that classifies system states as either good or
anomalous, our goal is to determine the minimal adjustment to the system's
control variables (i.e., its current status) that is necessary to return it to
the good state. To achieve this, we leverage a mathematical model that finds
the optimal counterfactual solution while respecting system specific
constraints. Notably, most counterfactual analysis in the literature focuses on
individual cases where a person seeks to alter their status relative to a
decision made by a classifier, such as for loan approval or medical diagnosis.
Our work addresses a fundamentally different challenge: optimizing
counterfactuals for a complex energy system, specifically an offshore wind
turbine oil type transformer. This application not only advances counterfactual
optimization in a new domain but also opens avenues for broader research in
this area. Our tests on real world data provided by our industrial partner show
that our methodology easily adapts to user preferences and brings savings in
the order of 3 million euros per year in a typical farm.

</details>


### [339] [Intersection of Reinforcement Learning and Bayesian Optimization for Intelligent Control of Industrial Processes: A Safe MPC-based DPG using Multi-Objective BO](https://arxiv.org/abs/2507.09864)
*Hossein Nejatbakhsh Esfahani,Javad Mohammadpour Velni*

Key words: MPC-RL, MOBO, CDPG, EHVI, 控制优化

TL;DR: 结合MPC-RL与MOBO的新框架，通过CDPG和EHVI实现高效、安全的参数调优，提升控制系统的性能和稳定性。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 传统MPC-RL方法存在收敛慢、策略学习次优及在线适应安全性问题，亟需改进。

Method: 提出MPC-RL-MOBO框架，结合CDPG和MOBO的EHVI采集函数，优化MPC参数。

Result: 框架在模型不完美时仍能实现高效、稳定和高性能的控制系统学习。

Conclusion: MPC-RL-MOBO为控制系统的强化学习提供了高效且安全的解决方案。

Abstract: Model Predictive Control (MPC)-based Reinforcement Learning (RL) offers a
structured and interpretable alternative to Deep Neural Network (DNN)-based RL
methods, with lower computational complexity and greater transparency. However,
standard MPC-RL approaches often suffer from slow convergence, suboptimal
policy learning due to limited parameterization, and safety issues during
online adaptation. To address these challenges, we propose a novel framework
that integrates MPC-RL with Multi-Objective Bayesian Optimization (MOBO). The
proposed MPC-RL-MOBO utilizes noisy evaluations of the RL stage cost and its
gradient, estimated via a Compatible Deterministic Policy Gradient (CDPG)
approach, and incorporates them into a MOBO algorithm using the Expected
Hypervolume Improvement (EHVI) acquisition function. This fusion enables
efficient and safe tuning of the MPC parameters to achieve improved closed-loop
performance, even under model imperfections. A numerical example demonstrates
the effectiveness of the proposed approach in achieving sample-efficient,
stable, and high-performance learning for control systems.

</details>


### [340] [Neural Two-Stage Stochastic Optimization for Solving Unit Commitment Problem](https://arxiv.org/abs/2507.09503)
*Zhentong Shao,Jingtao Qin,Nanpeng Yu*

Key words: 神经随机优化,两阶段随机单元调度,深度神经网络,混合整数线性规划,场景降维

TL;DR: 提出一种神经随机优化方法，用于高效处理高维不确定性场景下的两阶段随机单元调度问题，结合深度神经网络和MILP实现快速求解。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 高维不确定性场景下的两阶段随机单元调度问题难以高效求解，传统方法计算复杂且难以扩展。

Method: 利用深度神经网络近似第二阶段问题，并将其嵌入到第一阶段问题中，形成MILP，同时采用场景嵌入网络降维。

Result: 在IEEE测试系统中，最优性差距小于1%，且计算速度显著快于传统方法，模型规模不随场景数增加而变化。

Conclusion: 所提方法在求解随机单元调度问题时具有高效性和可扩展性。

Abstract: This paper proposes a neural stochastic optimization method for efficiently
solving the two-stage stochastic unit commitment (2S-SUC) problem under
high-dimensional uncertainty scenarios. The proposed method approximates the
second-stage recourse problem using a deep neural network trained to map
commitment decisions and uncertainty features to recourse costs. The trained
network is subsequently embedded into the first-stage UC problem as a
mixed-integer linear program (MILP), allowing for explicit enforcement of
operational constraints while preserving the key uncertainty characteristics. A
scenario-embedding network is employed to enable dimensionality reduction and
feature aggregation across arbitrary scenario sets, serving as a data-driven
scenario reduction mechanism. Numerical experiments on IEEE 5-bus, 30-bus, and
118-bus systems demonstrate that the proposed neural two-stage stochastic
optimization method achieves solutions with an optimality gap of less than 1%,
while enabling orders-of-magnitude speedup compared to conventional MILP
solvers and decomposition-based methods. Moreover, the model's size remains
constant regardless of the number of scenarios, offering significant
scalability for large-scale stochastic unit commitment problems.

</details>


### [341] [Symptom-Driven Personalized Proton Pump Inhibitors Therapy Using Bayesian Neural Networks and Model Predictive Control](https://arxiv.org/abs/2507.09685)
*Yutong Li,Ilya Kolmanovsky*

Key words: 质子泵抑制剂、贝叶斯神经网络、模型预测控制、个性化治疗、胃酸控制

TL;DR: 提出了一种基于症状的非侵入性方法，通过贝叶斯神经网络和模型预测控制动态调整质子泵抑制剂剂量，减少药物使用量65%。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 质子泵抑制剂的长期高剂量使用存在风险，但现有胃酸监测方法不切实际且效果有限。

Method: 基于贝叶斯神经网络预测症状，结合模型预测控制动态调整药物剂量。

Result: 仿真研究表明该方法可减少65%的药物使用，同时保持95%的抑酸效果。

Conclusion: 该方法为个性化质子泵抑制剂治疗提供了可行方案，减少治疗负担和用药风险。

Abstract: Proton Pump Inhibitors (PPIs) are the standard of care for gastric acid
disorders but carry significant risks when administered chronically at high
doses. Precise long-term control of gastric acidity is challenged by the
impracticality of invasive gastric acid monitoring beyond 72 hours and wide
inter-patient variability. We propose a noninvasive, symptom-based framework
that tailors PPI dosing solely on patient-reported reflux and digestive symptom
patterns. A Bayesian Neural Network prediction model learns to predict patient
symptoms and quantifies its uncertainty from historical symptom scores, meal,
and PPIs intake data. These probabilistic forecasts feed a chance-constrained
Model Predictive Control (MPC) algorithm that dynamically computes future PPI
doses to minimize drug usage while enforcing acid suppression with high
confidence - without any direct acid measurement. In silico studies over
diverse dietary schedules and virtual patient profiles demonstrate that our
learning-augmented MPC reduces total PPI consumption by 65 percent compared to
standard fixed regimens, while maintaining acid suppression with at least 95
percent probability. The proposed approach offers a practical path to
personalized PPI therapy, minimizing treatment burden and overdose risk without
invasive sensors.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [342] [Surprisingly High Redundancy in Electronic Structure Data](https://arxiv.org/abs/2507.09001)
*Sazzad Hossain,Ponkrshnan Thiagarajan,Shashank Pathrudkar,Stephanie Taylor,Abhijeet S. Gangan,Amartya S. Banerjee,Susanta Ghosh*

Key words: 机器学习, 电子结构, 数据集冗余, 修剪策略

TL;DR: 该研究发现电子结构数据中存在高度冗余性，挑战了大型数据集对准确机器学习预测的必需性，通过随机修剪和覆盖率修剪策略可大幅减少数据量和训练时间。

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

Motivation: 揭示电子结构数据中的冗余性，以证明大型数据集并非必需，从而减少计算成本和训练时间。

Method: 采用随机修剪和覆盖率修剪策略对数据集进行优化，并与重要性修剪方法对比。

Result: 随机修剪和覆盖率修剪能显著减少数据集规模（100倍）和训练时间（3倍以上），且保持化学精度和模型泛化能力。

Conclusion: 电子结构数据中的高冗余性可用于识别每个材料类别的最小代表性数据集。

Abstract: Machine Learning (ML) models for electronic structure rely on large datasets
generated through expensive Kohn-Sham Density Functional Theory simulations.
This study reveals a surprisingly high level of redundancy in such datasets
across various material systems, including molecules, simple metals, and
complex alloys. Our findings challenge the prevailing assumption that large,
exhaustive datasets are necessary for accurate ML predictions of electronic
structure. We demonstrate that even random pruning can substantially reduce
dataset size with minimal loss in predictive accuracy, while a state-of-the-art
coverage-based pruning strategy retains chemical accuracy and model
generalizability using up to 100-fold less data and reducing training time by
threefold or more. By contrast, widely used importance-based pruning methods,
which eliminate seemingly redundant data, can catastrophically fail at higher
pruning factors, possibly due to the significant reduction in data coverage.
This heretofore unexplored high degree of redundancy in electronic structure
data holds the potential to identify a minimal, essential dataset
representative of each material class.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [343] [Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI](https://arxiv.org/abs/2507.10510)
*Jiangkai Wu,Zhiyuan Ren,Liming Liu,Xinggong Zhang*

Key words: AI Video Chat, MLLM, Real-time Communication, Artic, Context-Aware Video Streaming, DeViBench

TL;DR: AI Video Chat是一种新型实时通信模式，其中一方是AI模型，挑战在于高延迟和网络不稳定。提出的Artic框架通过上下文感知视频流和抗丢包自适应帧率来优化。

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: 解决AI视频聊天中因MLLM推理和网络延迟导致的高延迟问题，提升AI与人类互动的实时性。

Method: 提出Artic框架，包含上下文感知视频流（优化比特率分配）和抗丢包自适应帧率（减少重传），并构建DeViBench基准测试。

Result: 显著降低比特率同时保持MLLM准确性，提升实时性。

Conclusion: Artic框架有效优化了AI视频聊天的网络需求，并提出了未来研究方向。

Abstract: AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),
where one peer is not a human, but a Multimodal Large Language Model (MLLM).
This makes interaction between humans and AI more intuitive, as if chatting
face-to-face with a real person. However, this poses significant challenges to
latency, because the MLLM inference takes up most of the response time, leaving
very little time for video streaming. Due to network uncertainty and
instability, transmission latency becomes a critical bottleneck preventing AI
from being like a real person. To address this, we propose Artic, an
AI-oriented Real-time Communication framework, exploring the network
requirement shift from "humans watching video" to "AI understanding video". To
reduce bitrate dramatically while maintaining MLLM accuracy, we propose
Context-Aware Video Streaming that recognizes the importance of each video
region for chat and allocates bitrate almost exclusively to chat-important
regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive
Frame Rate that leverages previous frames to substitute for lost/delayed frames
while avoiding bitrate waste. To evaluate the impact of video streaming quality
on MLLM accuracy, we build the first benchmark, named Degraded Video
Understanding Benchmark (DeViBench). Finally, we discuss some open questions
and ongoing solutions for AI Video Chat.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [344] [Lightweight Federated Learning over Wireless Edge Networks](https://arxiv.org/abs/2507.09546)
*Xiangwang Hou,Jingjing Wang,Jun Du,Chunxiao Jiang,Yong Ren,Dusit Niyato*

Key words: 联邦学习、无线网络、轻量级框架、模型剪枝、梯度量化、传输功率控制

TL;DR: 本文提出了一种轻量级联邦学习框架LTFL，整合了无线传输功率控制、模型剪枝和梯度量化，通过优化这些参数来最小化收敛差距，同时满足延迟和能源限制。实验表明LTFL优于现有方案。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 随着智能设备连接无线网络的快速增长，数据的快速产生需要机器学习技术来实现其价值。然而，集中式机器学习存在通信开销和隐私问题，而联邦学习在无线网络中的实际部署仍具挑战性。

Method: 提出LTFL框架，结合传输功率控制、模型剪枝和梯度量化，推导了包含传输误差、模型剪枝误差和梯度量化误差的FL收敛差距闭式表达式，并通过贝叶斯优化解决非凸优化问题。

Result: 在真实数据集上的实验结果表明，LTFL优于现有方案。

Conclusion: LTFL通过优化传输功率控制、模型剪枝和梯度量化，有效解决了联邦学习在无线网络中的部署挑战，并在实验中证明了其优越性。

Abstract: With the exponential growth of smart devices connected to wireless networks,
data production is increasing rapidly, requiring machine learning (ML)
techniques to unlock its value. However, the centralized ML paradigm raises
concerns over communication overhead and privacy. Federated learning (FL)
offers an alternative at the network edge, but practical deployment in wireless
networks remains challenging. This paper proposes a lightweight FL (LTFL)
framework integrating wireless transmission power control, model pruning, and
gradient quantization. We derive a closed-form expression of the FL convergence
gap, considering transmission error, model pruning error, and gradient
quantization error. Based on these insights, we formulate an optimization
problem to minimize the convergence gap while meeting delay and energy
constraints. To solve the non-convex problem efficiently, we derive closed-form
solutions for the optimal model pruning ratio and gradient quantization level,
and employ Bayesian optimization for transmission power control. Extensive
experiments on real-world datasets show that LTFL outperforms state-of-the-art
schemes.

</details>


### [345] [Efficient Federated Learning with Heterogeneous Data and Adaptive Dropout](https://arxiv.org/abs/2507.10430)
*Ji Liu,Beichen Ma,Yang Zhou,Jingbo Zhou,Ruoming Jin,Dejing Dou,Huaiyu Dai,Haixun Wang,Patrick Valduriez*

Key words: 联邦学习, 数据异构, 设备异构, 动态聚合, 自适应丢弃

TL;DR: 本文提出了 FedDHAD 联邦学习框架，通过动态异构模型聚合（FedDH）和自适应丢弃（FedAD）方法，有效解决了数据异构性和设备异构性问题，显著提升了准确性、效率和计算成本。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 联邦学习中数据分布异构和设备异构会导致模型准确率下降和收敛缓慢，亟需一种高效的解决方案。

Method: FedDHAD 框架结合了 FedDH（动态调整模型聚合权重以应对数据异构）和 FedAD（自适应丢弃神经元以提升设备异构下的准确性）。

Result: FedDHAD 在准确性（提高 6.7%）、效率（快 2.02 倍）和计算成本（降低 15.0%）上优于现有方法。

Conclusion: FedDHAD 是解决联邦学习中数据与设备异构性的有效框架，具有显著优势。

Abstract: Federated Learning (FL) is a promising distributed machine learning approach
that enables collaborative training of a global model using multiple edge
devices. The data distributed among the edge devices is highly heterogeneous.
Thus, FL faces the challenge of data distribution and heterogeneity, where
non-Independent and Identically Distributed (non-IID) data across edge devices
may yield in significant accuracy drop. Furthermore, the limited computation
and communication capabilities of edge devices increase the likelihood of
stragglers, thus leading to slow model convergence. In this paper, we propose
the FedDHAD FL framework, which comes with two novel methods: Dynamic
Heterogeneous model aggregation (FedDH) and Adaptive Dropout (FedAD). FedDH
dynamically adjusts the weights of each local model within the model
aggregation process based on the non-IID degree of heterogeneous data to deal
with the statistical data heterogeneity. FedAD performs neuron-adaptive
operations in response to heterogeneous devices to improve accuracy while
achieving superb efficiency. The combination of these two methods makes FedDHAD
significantly outperform state-of-the-art solutions in terms of accuracy (up to
6.7% higher), efficiency (up to 2.02 times faster), and computation cost (up to
15.0% smaller).

</details>


### [346] [ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism](https://arxiv.org/abs/2507.10069)
*Zedong Liu,Shenggan Cheng,Guangming Tan,Yang You,Dingwen Tao*

Key words: 多模态大语言模型, 服务系统, 弹性并行, 负载均衡, 前缀缓存

TL;DR: 本文提出了Elastic Multimodal Parallelism (EMP)和ElasticMM系统，用于高效服务多模态大语言模型(MLLMs)，显著降低延迟并提升吞吐量。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 现有的MLLM服务架构由于紧密耦合和复杂的推理流程，导致推理开销大、资源利用率低、响应延迟高。

Method: 提出EMP范式，通过模态感知负载均衡、弹性分区调度和统一前缀缓存等技术优化服务。

Result: ElasticMM相比现有系统，降低TTFT延迟4.2倍，吞吐量提升3.2-4.5倍。

Conclusion: EMP范式有效解决了MLLM服务中的资源异构性问题，显著提升了效率。

Abstract: Multimodal large language models (MLLMs) extend LLMs to handle images,
videos, and audio by incorporating feature extractors and projection modules.
However, these additional components -- combined with complex inference
pipelines and heterogeneous workloads -- introduce significant inference
overhead. Therefore, efficiently serving MLLMs remains a major challenge.
Current tightly coupled serving architectures struggle to distinguish between
mixed request types or adapt parallelism strategies to different inference
stages, leading to increased time-to-first-token (TTFT) latency and poor
resource utilization. To address this, we propose Elastic Multimodal
Parallelism (EMP), a new serving paradigm that elastically adapts to resource
heterogeneity across request types and inference stages. Building upon EMP, we
develop ElasticMM, an MLLM serving system that (1) separates requests into
independent modality groups with dynamic resource allocation via a
modality-aware load balancer; (2) decouples inference stages and enables
parallelism adjustment and adaptive scaling via elastic partition scheduling;
and (3) improves inference efficiency through unified multimodal prefix caching
and non-blocking encoding. Experiments on diverse real-world datasets show that
ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by
up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level
objectives (SLOs).

</details>


### [347] [Large-Scale Graph Building in Dynamic Environments: Low Latency and High Quality](https://arxiv.org/abs/2507.10139)
*Filipe Miguel Gonçalves de Almeida,CJ Carey,Hendrik Fichtenberger,Jonathan Halcrow,Silvio Lattanzi,André Linhares,Tao Meng,Ashkan Norouzi-Fard,Nikos Parotsidis,Bryan Perozzi,David Simcha*

Key words: Grale, Dynamic Grale Using ScaNN, ANN, graph construction, low latency

TL;DR: 论文提出了一种名为Dynamic Grale Using ScaNN (Dynamic GUS)的系统，继承了Grale的优势，并在动态环境中以低延迟维护图结构，适用于多种应用场景。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 现有工具Grale在离线环境中表现优异，但不适用于需要快速动态更新的场景。近似最近邻（ANN）系统虽能处理动态更新，但仅支持单一嵌入相似度。因此，需要一种既能继承Grale质量又能应对动态更新的系统。

Method: 提出了Dynamic Grale Using ScaNN (Dynamic GUS)，在Grale的基础上优化，实现了低延迟的动态图构造与维护。

Result: 该系统在Google内部多个场景（如Android安全与隐私）中部署，显著提升了效率。例如，在Android安全应用中，有害应用的检测速度提升至4倍。

Conclusion: Dynamic GUS系统成功将Grale的优势扩展到动态环境中，为大规模图构造与维护提供了高效解决方案。

Abstract: Learning and constructing large-scale graphs has attracted attention in
recent decades, resulting in a rich literature that introduced various systems,
tools, and algorithms. Grale is one of such tools that is designed for offline
environments and is deployed in more than 50 different industrial settings at
Google. Grale is widely applicable because of its ability to efficiently learn
and construct a graph on datasets with multiple types of features. However, it
is often the case that applications require the underlying data to evolve
continuously and rapidly and the updated graph needs to be available with low
latency. Such setting make the use of Grale prohibitive. While there are
Approximate Nearest Neighbor (ANN) systems that handle dynamic updates with low
latency, they are mostly limited to similarities over a single embedding.
  In this work, we introduce a system that inherits the advantages and the
quality of Grale, and maintains a graph construction in a dynamic setting with
tens of milliseconds of latency per request. We call the system Dynamic Grale
Using ScaNN (Dynamic GUS). Our system has a wide range of applications with
over 10 deployments at Google. One of the applications is in Android Security
and Privacy, where Dynamic Grale Using ScaNN enables capturing harmful
applications 4 times faster, before they can reach users.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [348] [Extending Defeasibility for Propositional Standpoint Logics](https://arxiv.org/abs/2507.10133)
*Nicholas Leisegang,Thomas Meyer,Ivan Varzinczak*

Key words: 可废止逻辑，立场逻辑，优先语义，表演算，计算复杂性

TL;DR: 本文提出了一种新的可废止命题立场逻辑框架，结合了多种理论，提供了优先语义和表演算，并证明了其计算复杂性为PSpace。

<details>
  <summary>Details</summary>

Main category: cs.LO

Motivation: 结合Kraus等人的可废止条件、Britz和Varzinczak的可废止必要性及不同可能性概念，以及Leisegang等人的可废止性方法，扩展Gómez Álvarez和Rudolph的立场逻辑，以表达可废止性。

Method: 整合多种可废止性理论，提出优先语义和表演算，证明其完备性和可靠性，并分析计算复杂性。

Result: 提出的逻辑框架可在隐含、立场模态算子和立场锐化语句层面表达可废止性，表演算计算复杂性为PSpace。

Conclusion: 新逻辑框架成功整合了多种可废止性理论，并在语义和计算复杂性方面取得了良好结果。

Abstract: In this paper, we introduce a new defeasible version of propositional
standpoint logic by integrating Kraus et al.'s defeasible conditionals, Britz
and Varzinczak's notions of defeasible necessity and distinct possibility,
along with Leisegang et al.'s approach to defeasibility into the standpoint
logics of G\'omez \'Alvarez and Rudolph. The resulting logical framework allows
for the expression of defeasibility on the level of implications, standpoint
modal operators, and standpoint-sharpening statements. We provide a
preferential semantics for this extended language and propose a tableaux
calculus, which is shown to be sound and complete with respect to preferential
entailment. We also establish the computational complexity of the tableaux
procedure to be in PSpace.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [349] [Clio-X: AWeb3 Solution for Privacy-Preserving AI Access to Digital Archives](https://arxiv.org/abs/2507.08853)
*Victoria L. Lemieux,Rosa Gil,Faith Molosiwa,Qihong Zhou,Binming Li,Roberto Garcia,Luis De La Torre Cubillo,Zehua Wang*

Key words: 隐私增强技术,Web3,档案管理,AI伦理,去中心化治理

TL;DR: 论文探讨了隐私增强技术（PETs）和Web3架构如何帮助档案管理在AI时代保护数据主权和伦理责任，并提出了Clio-X解决方案。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 当前AI数据实践中的隐私风险对档案管理的伦理和数据主权提出了挑战，需要一种既能保护隐私又能支持访问的方案。

Method: 提出了Clio-X，一种基于Web3的去中心化隐私优先解决方案，并通过用户评估分析了其原型。

Result: 研究发现Clio-X具有潜力，但也面临信任、系统透明度、经济和治理等障碍。

Conclusion: 结合技术保护和社区治理，Clio-X为文化遗产领域提供了伦理AI部署的新模型。

Abstract: As archives turn to artificial intelligence to manage growing volumes of
digital records, privacy risks inherent in current AI data practices raise
critical concerns about data sovereignty and ethical accountability. This paper
explores how privacy-enhancing technologies (PETs) and Web3 architectures can
support archives to preserve control over sensitive content while still being
able to make it available for access by researchers. We present Clio-X, a
decentralized, privacy-first Web3 digital solution designed to embed PETs into
archival workflows and support AI-enabled reference and access. Drawing on a
user evaluation of a medium-fidelity prototype, the study reveals both interest
in the potential of the solution and significant barriers to adoption related
to trust, system opacity, economic concerns, and governance. Using Rogers'
Diffusion of Innovation theory, we analyze the sociotechnical dimensions of
these barriers and propose a path forward centered on participatory design and
decentralized governance through a Clio-X Decentralized Autonomous
Organization. By integrating technical safeguards with community-based
oversight, Clio-X offers a novel model to ethically deploy AI in cultural
heritage contexts.

</details>


### [350] [Privacy-Utility-Fairness: A Balanced Approach to Vehicular-Traffic Management System](https://arxiv.org/abs/2507.08864)
*Poushali Sengupta,Sabita Maharjan,frank Eliassen,Yan Zhang*

Key words: 差分隐私, 车辆交通管理, 数据公平性, 地理数据保护, 噪声注入

TL;DR: 本文提出了一种新的算法，旨在解决基于位置的车辆交通管理系统中隐私性、实用性和公平性之间的平衡问题，通过差分隐私技术增强数据安全性，并在挪威的车辆位置数据上验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 现有的车辆交通管理方案在保护敏感地理数据时往往无法同时满足隐私保护、实用性和公平性的需求，导致隐私泄露和数据分析中的不公平问题。

Method: 采用差分隐私技术，结合查询式数据访问、迭代混洗和校准噪声注入，确保数据安全，并通过拉普拉斯机制满足epsilon-差分隐私标准。

Result: 在挪威的车辆位置数据上验证了算法的有效性，能够在不泄露敏感数据的同时保持交通管理和城市规划的实用性，并确保所有地理区域的公平表示。

Conclusion: 该算法在保护隐私的同时，显著提升了数据的实用性和公平性，适用于车辆交通管理系统。

Abstract: Location-based vehicular traffic management faces significant challenges in
protecting sensitive geographical data while maintaining utility for traffic
management and fairness across regions. Existing state-of-the-art solutions
often fail to meet the required level of protection against linkage attacks and
demographic biases, leading to privacy leakage and inequity in data analysis.
In this paper, we propose a novel algorithm designed to address the challenges
regarding the balance of privacy, utility, and fairness in location-based
vehicular traffic management systems. In this context, utility means providing
reliable and meaningful traffic information, while fairness ensures that all
regions and individuals are treated equitably in data use and decision-making.
Employing differential privacy techniques, we enhance data security by
integrating query-based data access with iterative shuffling and calibrated
noise injection, ensuring that sensitive geographical data remains protected.
We ensure adherence to epsilon-differential privacy standards by implementing
the Laplace mechanism. We implemented our algorithm on vehicular location-based
data from Norway, demonstrating its ability to maintain data utility for
traffic management and urban planning while ensuring fair representation of all
geographical areas without being overrepresented or underrepresented.
Additionally, we have created a heatmap of Norway based on our model,
illustrating the privatized and fair representation of the traffic conditions
across various cities. Our algorithm provides privacy in vehicular traffic

</details>


### [351] [RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2507.08862)
*Tianzhe Zhao,Jiaoyan Chen,Yanchi Ru,Haiping Zhu,Nan Hu,Jun Liu,Qika Lin*

Key words: 检索增强生成, 知识图谱, 数据中毒攻击, 安全性

TL;DR: 研究了基于知识图谱的检索增强生成（KG-RAG）方法在数据中毒攻击下的安全性问题，首次提出了一种隐蔽的攻击策略，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: KG-RAG方法在应用中存在安全风险，尤其是结构化数据易受数据中毒攻击，但目前相关研究较少。

Method: 提出了一种攻击策略，通过识别目标答案并插入扰动三元组来误导KG-RAG系统的推理链。

Result: 实验表明，即使最小扰动，攻击策略也能显著降低KG-RAG的性能。

Conclusion: KG-RAG系统在安全性方面存在明显漏洞，需要进一步研究防御措施。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
retrieving external data to mitigate hallucinations and outdated knowledge
issues. Benefiting from the strong ability in facilitating diverse data sources
and supporting faithful reasoning, knowledge graphs (KGs) have been
increasingly adopted in RAG systems, giving rise to KG-based RAG (KG-RAG)
methods. Though RAG systems are widely applied in various applications, recent
studies have also revealed its vulnerabilities to data poisoning attacks, where
malicious information injected into external knowledge sources can mislead the
system into producing incorrect or harmful responses. However, these studies
focus exclusively on RAG systems using unstructured textual data sources,
leaving the security risks of KG-RAG largely unexplored, despite the fact that
KGs present unique vulnerabilities due to their structured and editable nature.
In this work, we conduct the first systematic investigation of the security
issue of KG-RAG methods through data poisoning attacks. To this end, we
introduce a practical, stealthy attack setting that aligns with real-world
implementation. We propose an attack strategy that first identifies adversarial
target answers and then inserts perturbation triples to complete misleading
inference chains in the KG, increasing the likelihood that KG-RAG methods
retrieve and rely on these perturbations during generation. Through extensive
experiments on two benchmarks and four recent KG-RAG methods, our attack
strategy demonstrates strong effectiveness in degrading KG-RAG performance,
even with minimal KG perturbations. In-depth analyses are also conducted to
understand the safety threats within the internal stages of KG-RAG systems and
to explore the robustness of LLMs against adversarial knowledge.

</details>


### [352] [Towards Privacy-Preserving and Personalized Smart Homes via Tailored Small Language Models](https://arxiv.org/abs/2507.08878)
*Xinyu Huang,Leming Shen,Zijing Ma,Yuanqing Zheng*

Key words: LLM, smart home, privacy-preserving, on-device, HomeLLaMA

TL;DR: HomeLLaMA是一种基于小型语言模型(SLM)的本地智能家居助手，旨在解决现有基于LLM的智能家居助手在远程服务器处理用户命令时的隐私泄露问题，通过本地部署和隐私保护机制提供个性化服务。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 现有LLM智能家居助手需将用户命令、配置和隐私信息传输至远程服务器，引发隐私泄露担忧，因此开发本地化解决方案。

Method: 开发HomeLLaMA，通过学习云端LLM在本地提供个性化服务；提出PrivShield机制支持对隐私敏感度低的查询选择远程服务；构建DevFinder基准评估服务质量。

Result: 实验和用户研究(M=100)证明HomeLLaMA能在保障隐私的同时提供满意的个性化服务。

Conclusion: HomeLLaMA是隐私保护与个性化服务双赢的智能家居助手方案。

Abstract: Large Language Models (LLMs) have showcased remarkable generalizability in
language comprehension and hold significant potential to revolutionize
human-computer interaction in smart homes. Existing LLM-based smart home
assistants typically transmit user commands, along with user profiles and home
configurations, to remote servers to obtain personalized services. However,
users are increasingly concerned about the potential privacy leaks to the
remote servers. To address this issue, we develop HomeLLaMA, an on-device
assistant for privacy-preserving and personalized smart home serving with a
tailored small language model (SLM). HomeLLaMA learns from cloud LLMs to
deliver satisfactory responses and enable user-friendly interactions. Once
deployed, HomeLLaMA facilitates proactive interactions by continuously updating
local SLMs and user profiles. To further enhance user experience while
protecting their privacy, we develop PrivShield to offer an optional
privacy-preserving LLM-based smart home serving for those users, who are
unsatisfied with local responses and willing to send less-sensitive queries to
remote servers. For evaluation, we build a comprehensive benchmark DevFinder to
assess the service quality. Extensive experiments and user studies (M=100)
demonstrate that HomeLLaMA can provide personalized services while
significantly enhancing user privacy.

</details>


### [353] [EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions](https://arxiv.org/abs/2507.09762)
*Yasir Ech-Chammakhy,Anas Motii,Anass Rabii,Jaafar Chbili*

Key words: 黑客论坛,网络安全,无监督学习,Transformer,威胁检测

TL;DR: 本文提出了一种无监督框架，通过Transformer嵌入和对比学习自动检测、聚类和优先处理黑客论坛中的安全事件，为网络安全分析师提供可操作的威胁情报。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 黑客论坛是新兴网络安全威胁的重要预警来源，但其内容杂乱且噪音多，提取可操作情报仍具挑战性。

Method: 使用基于Transformer的嵌入模型，通过对比学习聚类相关讨论，并用每日排名机制优先处理事件。

Result: 实验表明，该方法有效降低噪音并识别高优先级威胁，帮助分析师采取主动响应。

Conclusion: 该框架将杂乱的黑客论坛讨论转化为结构化的可操作情报，解决了威胁检测和分析中的核心问题。

Abstract: Hacker forums provide critical early warning signals for emerging
cybersecurity threats, but extracting actionable intelligence from their
unstructured and noisy content remains a significant challenge. This paper
presents an unsupervised framework that automatically detects, clusters, and
prioritizes security events discussed across hacker forum posts. Our approach
leverages Transformer-based embeddings fine-tuned with contrastive learning to
group related discussions into distinct security event clusters, identifying
incidents like zero-day disclosures or malware releases without relying on
predefined keywords. The framework incorporates a daily ranking mechanism that
prioritizes identified events using quantifiable metrics reflecting timeliness,
source credibility, information completeness, and relevance. Experimental
evaluation on real-world hacker forum data demonstrates that our method
effectively reduces noise and surfaces high-priority threats, enabling security
analysts to mount proactive responses. By transforming disparate hacker forum
discussions into structured, actionable intelligence, our work addresses
fundamental challenges in automated threat detection and analysis.

</details>


### [354] [A Mixture of Linear Corrections Generates Secure Code](https://arxiv.org/abs/2507.09508)
*Weichen Yu,Ravi Mangal,Terry Zhuo,Matt Fredrikson,Corina S. Pasareanu*

Key words: 大型语言模型, 代码漏洞, 表示工程, MoC, 代码生成

TL;DR: 研究发现，大型语言模型（LLMs）内部已具备区分代码漏洞的能力，通过推断时调整生成概率（MoC方法）可显著提升代码安全性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 探究LLMs是否能在内部识别代码漏洞，以解决其生成代码时存在的漏洞问题。

Method: 采用表示工程技术分析LLMs对代码漏洞的内部表征，并提出基于混合修正（MoC）的推断时导向技术。

Result: MoC方法使Qwen2.5-Coder-7B的代码安全率提升8.9%，同时功能测试通过率提高2.1%。

Conclusion: LLMs具有识别漏洞的内部能力，通过MoC技术可显著优化生成代码的安全性与功能性。

Abstract: Large language models (LLMs) have become proficient at sophisticated
code-generation tasks, yet remain ineffective at reliably detecting or avoiding
code vulnerabilities. Does this deficiency stem from insufficient learning
about code vulnerabilities, or is it merely a result of ineffective prompting?
Using representation engineering techniques, we investigate whether LLMs
internally encode the concepts necessary to identify code vulnerabilities. We
find that current LLMs encode precise internal representations that distinguish
vulnerable from secure code--achieving greater accuracy than standard prompting
approaches. Leveraging these vulnerability-sensitive representations, we
develop an inference-time steering technique that subtly modulates the model's
token-generation probabilities through a mixture of corrections (MoC). Our
method effectively guides LLMs to produce less vulnerable code without
compromising functionality, demonstrating a practical approach to controlled
vulnerability management in generated code. Notably, MoC enhances the security
ratio of Qwen2.5-Coder-7B by 8.9\%, while simultaneously improving
functionality on HumanEval pass@1 by 2.1\%.

</details>


### [355] [Secure and Efficient UAV-Based Face Detection via Homomorphic Encryption and Edge Computing](https://arxiv.org/abs/2507.09860)
*Nguyen Van Duc,Bui Duc Manh,Quang-Trung Luu,Dinh Thai Hoang,Van-Linh Nguyen,Diep N. Nguyen*

Key words: 同态加密,无人机,人脸检测,隐私保护,CKKS

TL;DR: 提出了一种结合同态加密（HE）和机器学习的新方法，以解决无人机（UAV）人脸检测中的隐私问题，通过CKKS方案对加密数据直接计算，同时保证检测精度。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 无人机在动态环境中进行人脸检测时，由于其高分辨率图像和强大监控能力引发隐私问题，需要一种方法在保护隐私的同时维持检测性能。

Method: 提出了一种结合HE（特别是CKKS方案）和神经网络的框架，包括数据预处理的SIMD编码方法和直接在密文上进行的安全推理算法。

Result: 实验表明，该方法在保护数据隐私的同时，检测精度损失小于1%，显著提升了安全性与效率。

Conclusion: 该方法为无人机系统中安全的人脸检测提供了可行的解决方案，平衡了隐私保护和性能。

Abstract: This paper aims to propose a novel machine learning (ML) approach
incorporating Homomorphic Encryption (HE) to address privacy limitations in
Unmanned Aerial Vehicles (UAV)-based face detection. Due to challenges related
to distance, altitude, and face orientation, high-resolution imagery and
sophisticated neural networks enable accurate face recognition in dynamic
environments. However, privacy concerns arise from the extensive surveillance
capabilities of UAVs. To resolve this issue, we propose a novel framework that
integrates HE with advanced neural networks to secure facial data throughout
the inference phase. This method ensures that facial data remains secure with
minimal impact on detection accuracy. Specifically, the proposed system
leverages the Cheon-Kim-Kim-Song (CKKS) scheme to perform computations directly
on encrypted data, optimizing computational efficiency and security.
Furthermore, we develop an effective data encoding method specifically designed
to preprocess the raw facial data into CKKS form in a
Single-Instruction-Multiple-Data (SIMD) manner. Building on this, we design a
secure inference algorithm to compute on ciphertext without needing decryption.
This approach not only protects data privacy during the processing of facial
data but also enhances the efficiency of UAV-based face detection systems.
Experimental results demonstrate that our method effectively balances privacy
protection and detection performance, making it a viable solution for UAV-based
secure face detection. Significantly, our approach (while maintaining data
confidentially with HE encryption) can still achieve an accuracy of less than
1% compared to the benchmark without using encryption.

</details>


### [356] [Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix](https://arxiv.org/abs/2507.09990)
*Ming Wen,Jiaqi Zhu,Yuedong Xu,Yipeng Zhou,Dingding Han*

Key words: 大语言模型, LoRA, 联邦学习, 差分隐私, 草图技术

TL;DR: FedASK 是一个新颖的联邦 LoRA 框架，通过双阶段草图技术实现低秩适配器的高效更新，同时满足差分隐私。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 针对联邦 LoRA 框架中隐私泄露的问题，研究如何在保护隐私的同时有效更新低秩适配器。

Method: 提出两阶段草图管道，首先聚合隐私保护的本地更新，然后在服务器上重构全局矩阵。

Result: 实验表明，FedASK 在不同隐私设置和数据分布下均优于基线方法。

Conclusion: FedASK 成功解决了联邦 LoRA 中的隐私与学习能力的平衡问题。

Abstract: Large language models (LLMs) typically require fine-tuning for
domain-specific tasks, and LoRA offers a computationally efficient approach by
training low-rank adapters. LoRA is also communication-efficient for federated
LLMs when multiple users collaboratively fine-tune a global LLM model without
sharing their proprietary raw data. However, even the transmission of local
adapters between a server and clients risks serious privacy leakage. Applying
differential privacy (DP) to federated LoRA encounters a dilemma: adding noise
to both adapters amplifies synthetic noise on the model, while fixing one
adapter impairs the learnability of fine-tuning. In this paper, we propose
FedASK (Differentially Private Federated Low Rank Adaptation with Double
Sketching) , a novel federated LoRA framework to enable effective updating of
both low-rank adapters with robust differential privacy. Inspired by randomized
SVD, our key idea is a two-stage sketching pipeline. This pipeline first
aggregates carefully sketched, privacy-preserving local updates, and then
reconstructs the global matrices on the server to facilitate effective updating
of both adapters. We theoretically prove FedASK's differential privacy
guarantee and its exact aggregation property. Comprehensive experiments
demonstrate that FedASK consistently outperforms baseline methods across a
variety of privacy settings and data distributions.

</details>


### [357] [CAN-Trace Attack: Exploit CAN Messages to Uncover Driving Trajectories](https://arxiv.org/abs/2507.09624)
*Xiaojie Lin,Baihe Ma,Xu Wang,Guangsheng Yu,Ying He,Wei Ni,Ren Ping Liu*

Key words: 隐私攻击、CAN消息、轨迹重构、图匹配、驾驶轨迹

TL;DR: 该论文提出了一种名为CAN-Trace的新型隐私攻击机制，利用CAN消息（如车速和油门踏板位置）通过加权图匹配算法重构驾驶轨迹，成功率在城郊区域高达99.41%。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 现有驾驶轨迹隐私保护措施仍存在漏洞，传统GPS数据依赖方法易受数据中断影响，因此需开发更可靠的攻击方法以揭示潜在隐私风险。

Method: 提出一种轨迹重构算法，将CAN消息转化为加权图，并利用图匹配算法与道路网络比对，设计新指标评估匹配候选以容忍数据间隙和不准确性。

Result: 实证表明，CAN-Trace在城郊和城区攻击成功率分别达99.41%和90.59%。

Conclusion: CAN-Trace通过CAN消息高效重构驾驶轨迹，突显了现有车辆数据隐私保护的严重不足。

Abstract: Driving trajectory data remains vulnerable to privacy breaches despite
existing mitigation measures. Traditional methods for detecting driving
trajectories typically rely on map-matching the path using Global Positioning
System (GPS) data, which is susceptible to GPS data outage. This paper
introduces CAN-Trace, a novel privacy attack mechanism that leverages
Controller Area Network (CAN) messages to uncover driving trajectories, posing
a significant risk to drivers' long-term privacy. A new trajectory
reconstruction algorithm is proposed to transform the CAN messages,
specifically vehicle speed and accelerator pedal position, into weighted graphs
accommodating various driving statuses. CAN-Trace identifies driving
trajectories using graph-matching algorithms applied to the created graphs in
comparison to road networks. We also design a new metric to evaluate matched
candidates, which allows for potential data gaps and matching inaccuracies.
Empirical validation under various real-world conditions, encompassing
different vehicles and driving regions, demonstrates the efficacy of CAN-Trace:
it achieves an attack success rate of up to 90.59% in the urban region, and
99.41% in the suburban region.

</details>


### [358] [Logic layer Prompt Control Injection (LPCI): A Novel Security Vulnerability Class in Agentic Systems](https://arxiv.org/abs/2507.10457)
*Hammad Atta,Ken Huang,Manish Bhatt,Kamal Ahmed,Muhammad Aziz Ul Haq,Yasir Mehmood*

Key words: 大型语言模型（LLMs）、逻辑层提示控制注入（LPCI）、安全漏洞

TL;DR: 论文提出了一种新型攻击类别——逻辑层提示控制注入（LPCI），展示了LLMs在企业系统中的安全漏洞。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 大型语言模型（LLMs）在企业系统中的集成带来了新的隐蔽安全漏洞，尤其是在逻辑执行层和持久存储上下文中。

Method: 研究提出了LPCI攻击类别，通过在内存、向量存储或工具输出中嵌入编码、延迟和条件触发的负载来绕过输入过滤器。

Result: LPCI攻击能绕过常规输入筛选器，跨会话触发未经授权的行为。

Conclusion: LLMs的集成需要额外的安全防护措施来应对LPCI攻击的威胁。

Abstract: The integration of large language models (LLMs) into enterprise systems has
created a new class of covert security vulnerabilities, particularly within
logic-execution layers and persistent-memory contexts. In this paper, we
introduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category
in which encoded, delayed, and conditionally triggered payloads are embedded in
memory, vector stores, or tool outputs. These payloads can bypass conventional
input filters and trigger unauthorised behaviour across sessions.

</details>


### [359] [DNS Tunneling: Threat Landscape and Improved Detection Solutions](https://arxiv.org/abs/2507.10267)
*Novruz Amirov,Baran Isik,Bilal Ihsan Tuncer,Serif Bahtiyar*

Key words: DNS隧道, 机器学习, 安全检测

TL;DR: 提出了一种利用机器学习算法检测DNS隧道的新方法，通过分析DNS流量特征，结果显示该方法能有效识别隐蔽通信。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: DNS隧道可将恶意行为隐藏于看似正常的DNS流量中，传统规则或签名匹配方法对此检测效果有限。

Method: 采用机器学习算法，结合从DNS流量中提取的特征进行分析。

Result: 分析结果表明，该方法能准确检测DNS隧道。

Conclusion: 提出的机器学习方法是检测DNS隧道的有效候选方案。

Abstract: Detecting Domain Name System (DNS) tunneling is a significant challenge in
security due to its capacity to hide harmful actions within DNS traffic that
appears to be normal and legitimate. Traditional detection methods are based on
rule-based approaches or signature matching methods that are often insufficient
to accurately identify such covert communication channels. This research is
about effectively detecting DNS tunneling. We propose a novel approach to
detect DNS tunneling with machine learning algorithms. We combine machine
learning algorithms to analyze the traffic by using features extracted from DNS
traffic. Analyses results show that the proposed approach is a good candidate
to detect DNS tunneling accurately.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [360] [Context-Aware Regularization with Markovian Integration for Attention-Based Nucleotide Analysis](https://arxiv.org/abs/2507.09378)
*Mohammadsaleh Refahi,Mahdi Abavisani,Bahrad A. Sokhansanj,James R. Brown,Gail Rosen*

Key words: Transformer, 核苷酸序列分析, 自监督学习, 过渡矩阵损失, 长程依赖

TL;DR: CARMANIA是一个新的自监督预训练框架，通过结合NT预测和TM损失来捕获核苷酸序列中的长程依赖关系，显著提高了多基因组任务的性能。

<details>
  <summary>Details</summary>

Main category: q-bio.GN

Motivation: 现有的自回归Transformer在长序列分析中依赖固定长度的上下文窗口，导致无法有效捕获长程依赖关系。CARMANIA旨在解决这一局限性。

Method: CARMANIA通过在自监督预训练中引入过渡矩阵（TM）损失，将预测的标记过渡与输入序列的n-gram统计对齐，从而捕获更高阶的依赖关系。

Result: CARMANIA在多个基因组任务中表现优异，优于先前最佳模型至少7%，并在增强子和管家基因分类任务中取得显著提升（如增强子预测MCC提高34%）。

Conclusion: CARMANIA通过结合TM损失显著提高了模型对长程依赖关系的捕获能力，为基因组分析提供了更高效和准确的解决方案。

Abstract: Transformers have revolutionized nucleotide sequence analysis, yet capturing
long-range dependencies remains challenging. Recent studies show that
autoregressive transformers often exhibit Markovian behavior by relying on
fixed-length context windows for next-token prediction. However, standard
self-attention mechanisms are computationally inefficient for long sequences
due to their quadratic complexity and do not explicitly enforce global
transition consistency.
  We introduce CARMANIA (Context-Aware Regularization with Markovian
Integration for Attention-Based Nucleotide Analysis), a self-supervised
pretraining framework that augments next-token (NT) prediction with a
transition-matrix (TM) loss. The TM loss aligns predicted token transitions
with empirically derived n-gram statistics from each input sequence,
encouraging the model to capture higher-order dependencies beyond local
context. This integration enables CARMANIA to learn organism-specific sequence
structures that reflect both evolutionary constraints and functional
organization.
  We evaluate CARMANIA across diverse genomic tasks, including regulatory
element prediction, functional gene classification, taxonomic inference,
antimicrobial resistance detection, and biosynthetic gene cluster
classification. CARMANIA outperforms the previous best long-context model by at
least 7 percent, matches state-of-the-art on shorter sequences (exceeding prior
results on 20 out of 40 tasks while running approximately 2.5 times faster),
and shows particularly strong improvements on enhancer and housekeeping gene
classification tasks, including up to a 34 percent absolute gain in Matthews
correlation coefficient (MCC) for enhancer prediction. The TM loss boosts
accuracy in 33 of 40 tasks, especially where local motifs or regulatory
patterns drive prediction.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [361] [DiffNMR: Diffusion Models for Nuclear Magnetic Resonance Spectra Elucidation](https://arxiv.org/abs/2507.08854)
*Qingsong Yang,Binglan Wu,Xuwei Liu,Bo Chen,Wei Li,Gen Long,Xin Chen,Mingjun Xiao*

Key words: NMR光谱,分子结构解析,扩散模型,预训练策略,自动化分析

TL;DR: DiffNMR是一个新颖的端到端框架，利用条件离散扩散模型从NMR光谱中解析分子结构，通过扩散生成过程迭代优化分子图，提供高效、稳健的自动化分子分析解决方案。

<details>
  <summary>Details</summary>

Main category: physics.chem-ph

Motivation: 解析NMR光谱以推断分子结构具有挑战性，现有方法存在误差累积和全局一致性问题，需要一种新的解决方案。

Method: DiffNMR采用基于扩散的生成模型，包括两阶段预训练策略、检索初始化和相似性过滤，以及带RBF编码的专用NMR编码器。

Result: 实验表明，DiffNMR在NMR光谱解析中表现优异，提供了高效且稳健的分子结构解析能力。

Conclusion: DiffNMR为分子结构解析提供了一种高效且稳健的新方法，优于传统自回归方法。

Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy is a central characterization
method for molecular structure elucidation, yet interpreting NMR spectra to
deduce molecular structures remains challenging due to the complexity of
spectral data and the vastness of the chemical space. In this work, we
introduce DiffNMR, a novel end-to-end framework that leverages a conditional
discrete diffusion model for de novo molecular structure elucidation from NMR
spectra. DiffNMR refines molecular graphs iteratively through a diffusion-based
generative process, ensuring global consistency and mitigating error
accumulation inherent in autoregressive methods. The framework integrates a
two-stage pretraining strategy that aligns spectral and molecular
representations via diffusion autoencoder (Diff-AE) and contrastive learning,
the incorporation of retrieval initialization and similarity filtering during
inference, and a specialized NMR encoder with radial basis function (RBF)
encoding for chemical shifts, preserving continuity and chemical correlation.
Experimental results demonstrate that DiffNMR achieves competitive performance
for NMR-based structure elucidation, offering an efficient and robust solution
for automated molecular analysis.

</details>


### [362] [Accurate generation of chemical reaction transition states by conditional flow matching](https://arxiv.org/abs/2507.10530)
*Ping Tuo,Jiale Chen,Ju Li*

Key words: 过渡态, 生成模型, 条件流匹配, 化学精度, 高通量计算

TL;DR: 论文介绍了TS-GEN，一种条件流匹配生成模型，可直接从高斯先验生成过渡态几何结构，显著提升了生成速度和精度。

<details>
  <summary>Details</summary>

Main category: physics.chem-ph

Motivation: 过渡态结构的实验观测困难，依赖昂贵的DFT计算，因此需要高效且精确的生成方法。

Method: TS-GEN使用条件流匹配生成模型，通过嵌入反应物和产物构象作为条件信息，直接从噪声生成过渡态结构。

Result: TS-GEN在精度（0.004 Å RMSD）和速度（0.06秒/推断）上远超现有方法，87%的生成结构满足化学精度。

Conclusion: TS-GEN为高通量探索复杂反应网络提供了高效工具，有望推动新反应机理的研究。

Abstract: Transition state (TS) structures define the critical geometries and energy
barriers underlying chemical reactivity, yet their fleeting nature renders them
experimentally elusive and drives the reliance on costly, high-throughput
density functional theory (DFT) calculations. Here, we introduce TS-GEN, a
conditional flow-matching generative model that maps samples from a simple
Gaussian prior directly to transition-state saddle-point geometries in a
single, deterministic pass. By embedding both reactant and product
conformations as conditioning information, TS-GEN learns to transport latent
noise to true TS structures via an optimal-transport path, effectively
replacing the iterative optimization common in nudged-elastic band or
string-method algorithms. TS-GEN delivers unprecedented accuracy, achieving a
root-mean-square deviation of $0.004\ \rm{\mathring{A}}$ (vs. $0.103\
\rm{\mathring{A}}$ for prior state-of-the-art) and a mean barrier-height error
of $1.019\ {\rm kcal/mol}$ (vs. $2.864\ {\rm kcal/mol}$), while requiring only
$0.06\ {\rm s}$ GPU time per inference. Over 87% of generated TSs meet
chemical-accuracy criteria ($<1.58\ {\rm kcal/mol}$ error), substantially
outpacing existing methods. TS-GEN also exhibits strong transferability to
out-of-distribution reactions from a larger database. By uniting sub-angstrom
precision, sub-second speed, and broad applicability, TS-GEN will be highly
useful for high-throughput exploration of complex reaction networks, paving the
way to the exploration of novel chemical reaction mechanisms.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [363] [Covering a Few Submodular Constraints and Applications](https://arxiv.org/abs/2507.09879)
*Tanvi Bajpai,Chandra Chekuri,Pooja Kulkarni*

Key words: 子模约束, 覆盖问题, 近似算法, 加权覆盖函数

TL;DR: 论文研究了覆盖多个子模约束的问题，提出了针对固定常数r的随机双标准近似算法和加权覆盖函数的近似算法。

<details>
  <summary>Details</summary>

Main category: cs.DS

Motivation: 研究动机是解决当r为固定常数时的多子模约束覆盖问题，填补现有文献在r较大时的不足之处。

Method: 方法包括开发随机双标准近似算法和针对加权覆盖函数的具体近似算法。

Result: 结果显示，对于固定r，可以接近单子模约束时的近似效果。

Conclusion: 结论表明，固定r时可以获得与r=1时相似的近似结果，具有广泛的应用潜力。

Abstract: We consider the problem of covering multiple submodular constraints. Given a
finite ground set $N$, a cost function $c: N \rightarrow \mathbb{R}_+$, $r$
monotone submodular functions $f_1,f_2,\ldots,f_r$ over $N$ and requirements
$b_1,b_2,\ldots,b_r$ the goal is to find a minimum cost subset $S \subseteq N$
such that $f_i(S) \ge b_i$ for $1 \le i \le r$. When $r=1$ this is the
well-known Submodular Set Cover problem. Previous work
\cite{chekuri2022covering} considered the setting when $r$ is large and
developed bi-criteria approximation algorithms, and approximation algorithms
for the important special case when each $f_i$ is a weighted coverage function.
These are fairly general models and capture several concrete and interesting
problems as special cases. The approximation ratios for these problem are at
least $\Omega(\log r)$ which is unavoidable when $r$ is part of the input. In
this paper, motivated by some recent applications, we consider the problem when
$r$ is a \emph{fixed constant} and obtain two main results. For covering
multiple submodular constraints we obtain a randomized bi-criteria
approximation algorithm that for any given integer $\alpha \ge 1$ outputs a set
$S$ such that $f_i(S) \ge$ $(1-1/e^\alpha -\epsilon)b_i$ for each $i \in [r]$
and $\mathbb{E}[c(S)] \le (1+\epsilon)\alpha \cdot \sf{OPT}$. Second, when the
$f_i$ are weighted coverage functions from a deletion-closed set system we
obtain a $(1+\epsilon)$ $(\frac{e}{e-1})$ $(1+\beta)$-approximation where
$\beta$ is the approximation ratio for the underlying set cover instances via
the natural LP. These results show that one can obtain nearly as good an
approximation for any fixed $r$ as what one would achieve for $r=1$. We mention
some applications that follow easily from these general results and anticipate
more in the future.

</details>


### [364] [Phase transition of the Sinkhorn-Knopp algorithm](https://arxiv.org/abs/2507.09711)
*Kun He*

Key words: 矩阵缩放, Sinkhorn-Knopp算法, 密度阈值, 相变

TL;DR: 论文研究了矩阵缩放问题中的Sinkhorn-Knopp算法，揭示了其在密度阈值γ=1/2处的急剧相变。

<details>
  <summary>Details</summary>

Main category: cs.DS

Motivation: 尽管该算法在实践中表现出色，但其理论性能的上下界仍不明确，特别是其迭代次数的紧界限尚未建立。

Method: 通过分析归一化矩阵的密度γ，提出对于γ>1/2的矩阵，算法在O(log n - log ε)次迭代内收敛，而对于γ<1/2的矩阵，则需要Ω(n^{1/2}/ε)次迭代。

Result: 证明了Sinkhorn-Knopp算法在密度阈值γ=1/2处存在相变，并且对于γ>1/2的矩阵，运行时间是最优的。

Conclusion: 研究结果不仅解释了算法的强表现，还为未来优化提供了理论依据。

Abstract: The matrix scaling problem, particularly the Sinkhorn-Knopp algorithm, has
been studied for over 60 years. In practice, the algorithm often yields
high-quality approximations within just a few iterations. Theoretically,
however, the best-known upper bound places it in the class of
pseudopolynomial-time approximation algorithms. Meanwhile, the lower-bound
landscape remains largely unexplored. Two fundamental questions persist: what
accounts for the algorithm's strong empirical performance, and can a tight
bound on its iteration count be established?
  For an $n\times n$ matrix, its normalized version is obtained by dividing
each entry by its largest entry. We say that a normalized matrix has a density
$\gamma$ if there exists a constant $\rho > 0$ such that one row or column has
exactly $\lceil \gamma n \rceil$ entries with values at least $\rho$, and every
other row and column has at least $\lceil \gamma n \rceil$ such entries.
  For the upper bound, we show that the Sinkhorn-Knopp algorithm produces a
nearly doubly stochastic matrix in $O(\log n - \log \varepsilon)$ iterations
and $\widetilde{O}(n^2)$ time for all nonnegative square matrices whose
normalized version has a density $\gamma > 1/2$. Such matrices cover both the
algorithm's principal practical inputs and its typical theoretical regime, and
the $\widetilde{O}(n^2)$ runtime is optimal.
  For the lower bound, we establish a tight bound of
$\widetilde{\Omega}\left(n^{1/2}/\varepsilon\right)$ iterations for positive
matrices under the $\ell_2$-norm error measure. Moreover, for every $\gamma <
1/2$, there exists a matrix with density $\gamma$ for which the algorithm
requires $\Omega\left(n^{1/2}/\varepsilon\right)$ iterations.
  In summary, our results reveal a sharp phase transition in the Sinkhorn-Knopp
algorithm at the density threshold $\gamma = 1/2$.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [365] [AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data](https://arxiv.org/abs/2507.09100)
*Mohammad Abolnejadian,Shakiba Amirshahi,Matthew Brehmer,Anamaria Crisan*

Key words: 决策对话, LLM, 实时见解, 医患互动, 数据检索

TL;DR: 专家在决策对话中需快速做出复杂选择，但实时性限制了历史数据的利用。本文提出通过LLM代理的会话界面，实时整合相关历史数据生成见解，以医患互动为例验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 探索如何在实时决策对话中利用历史数据，帮助专家快速获取相关见解以提高决策质量。

Method: 开发基于LLM代理的会话界面，实时监听对话、识别问题与解决方案，并检索相关数据集生成见解。

Result: 原型系统在模拟医患对话中表现出有效性，但也揭示了挑战，为后续工作提供了方向。

Conclusion: 通过LLM代理实时整合历史数据可行，但需进一步优化以应对实际挑战。

Abstract: In decision-making conversations, experts must navigate complex choices and
make on-the-spot decisions while engaged in conversation. Although extensive
historical data often exists, the real-time nature of these scenarios makes it
infeasible for decision-makers to review and leverage relevant information.
This raises an interesting question: What if experts could utilize relevant
past data in real-time decision-making through insights derived from past data?
To explore this, we implemented a conversational user interface, taking
doctor-patient interactions as an example use case. Our system continuously
listens to the conversation, identifies patient problems and doctor-suggested
solutions, and retrieves related data from an embedded dataset, generating
concise insights using a pipeline built around a retrieval-based Large Language
Model (LLM) agent. We evaluated the prototype by embedding Health Canada
datasets into a vector database and conducting simulated studies using sample
doctor-patient dialogues, showing effectiveness but also challenges, setting
directions for the next steps of our work.

</details>


### [366] [SimStep: Chain-of-Abstractions for Incremental Specification and Debugging of AI-Generated Interactive Simulations](https://arxiv.org/abs/2507.09664)
*Zoe Kaputa,Anika Rajaram,Vryan Almanon Feliciano,Zhuoyue Lyu,Maneesh Agrawala,Hari Subramonyam*

Key words: 编程提示, 生成式AI, CoA框架, SimStep, 教育技术

TL;DR: 文章提出Chain-of-Abstractions (CoA)框架，通过在编程提示过程中分解任务为多层次的抽象表示，恢复编程的核心特性，并设计了SimStep工具支持教师创建交互式学习内容。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 生成式AI的编程提示模式为非程序员（如教师）提供了便利，但同时也丢失了编程的核心特性，如可追溯性和逐步细化。CoA框架旨在恢复这些特性。

Method: CoA框架将编程提示过程分解为多个任务对齐的抽象表示（如概念图、场景图等），并通过SimStep工具实现。此外，SimStep包含逆向修正过程以处理歧义。

Result: 评估显示，CoA能提升教师在编程提示流程中的控制力和可解释性。

Conclusion: CoA框架通过多层次抽象表示有效平衡了自然语言的灵活性与编程的核心特性。

Abstract: Programming-by-prompting with generative AI offers a new paradigm for
end-user programming, shifting the focus from syntactic fluency to semantic
intent. This shift holds particular promise for non-programmers such as
educators, who can describe instructional goals in natural language to generate
interactive learning content. Yet in bypassing direct code authoring, many of
programming's core affordances - such as traceability, stepwise refinement, and
behavioral testing - are lost. We propose the Chain-of-Abstractions (CoA)
framework as a way to recover these affordances while preserving the expressive
flexibility of natural language. CoA decomposes the synthesis process into a
sequence of cognitively meaningful, task-aligned representations that function
as checkpoints for specification, inspection, and refinement. We instantiate
this approach in SimStep, an authoring environment for teachers that scaffolds
simulation creation through four intermediate abstractions: Concept Graph,
Scenario Graph, Learning Goal Graph, and UI Interaction Graph. To address
ambiguities and misalignments, SimStep includes an inverse correction process
that surfaces in-filled model assumptions and enables targeted revision without
requiring users to manipulate code. Evaluations with educators show that CoA
enables greater authoring control and interpretability in
programming-by-prompting workflows.

</details>


### [367] [Visual Analytics for Explainable and Trustworthy Artificial Intelligence](https://arxiv.org/abs/2507.10240)
*Angelos Chatzimparmpas*

Key words: AI透明度,可视化分析,医疗诊断,交互式可视化,模型调优

TL;DR: 论文探讨了如何通过可视化分析解决AI系统透明度不足的问题，促进医疗等领域AI的应用。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: AI系统缺乏透明度导致专家难以信任其预测结果，阻碍了AI在医疗等领域的大规模应用。

Method: 引入可视化分析（VA），结合交互式图表与AI模型，设计了一套支持AI流程各阶段的可视化工具。

Result: 提出了一种创新可视化设计空间，并展示了支持数据处理、模型调优等任务的VA仪表板。

Conclusion: 可视化分析能有效提升AI系统的透明度与专家信任，推动AI在关键领域的应用。

Abstract: Our society increasingly depends on intelligent systems to solve complex
problems, ranging from recommender systems suggesting the next movie to watch
to AI models assisting in medical diagnoses for hospitalized patients. With the
iterative improvement of diagnostic accuracy and efficiency, AI holds
significant potential to mitigate medical misdiagnoses by preventing numerous
deaths and reducing an economic burden of approximately 450 EUR billion
annually. However, a key obstacle to AI adoption lies in the lack of
transparency: many automated systems function as "black boxes," providing
predictions without revealing the underlying processes. This opacity can hinder
experts' ability to trust and rely on AI systems. Visual analytics (VA)
provides a compelling solution by combining AI models with interactive
visualizations. These specialized charts and graphs empower users to
incorporate their domain expertise to refine and improve the models, bridging
the gap between AI and human understanding. In this work, we define,
categorize, and explore how VA solutions can foster trust across the stages of
a typical AI pipeline. We propose a design space for innovative visualizations
and present an overview of our previously developed VA dashboards, which
support critical tasks within the various pipeline stages, including data
processing, feature engineering, hyperparameter tuning, understanding,
debugging, refining, and comparing models.

</details>


### [368] [An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived Realism and Performance in Virtual Reality Environments](https://arxiv.org/abs/2507.10469)
*Mikko Korkiakoski,Saeid Sheikhi,Jesper Nyman,Jussi Saariniemi,Kalle Tapio,Panos Kostakos*

Key words: AI, VR, NPC, realism, interactivity, GPT-4 Turbo

TL;DR: 论文探讨了AI驱动的NPC在VR审讯模拟器中的表现，评估了其真实感、可用性和系统性能，展示了大型语言模型提升NPC互动的潜力。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 探索AI如何提升VR中NPC的真实感和交互性，改善用户体验。

Method: 使用GPT-4 Turbo驱动的NPC进行用户研究，评估了18名参与者的反馈，并通过多种问卷和延迟测量进行分析。

Result: NPC真实感评分为6.67/10，系统可用性评分为79.44，平均延迟7秒。

Conclusion: 大型语言模型能提升NPC的真实感和互动性，但需优化性能和情感深度以实现更高沉浸感。

Abstract: Advancements in artificial intelligence (AI) have significantly enhanced the
realism and interactivity of non-player characters (NPCs) in virtual reality
(VR), creating more engaging and believable user experiences. This paper
evaluates AI-driven NPCs within a VR interrogation simulator, focusing on their
perceived realism, usability, and system performance. The simulator features
two AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage
participants in a scenario to determine the suspect's guilt or innocence. A
user study with 18 participants assessed the system using the System Usability
Scale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent
Believability Questionnaire, alongside latency measurements for speech-to-text
(STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency.
Results showed an average cycle latency of 7 seconds, influenced by the
increasing conversational context. Believability scored 6.67 out of 10, with
high ratings in behavior, social relationships, and intelligence but moderate
scores in emotion and personality. The system achieved a SUS score of 79.44,
indicating good usability. These findings demonstrate the potential of large
language models to improve NPC realism and interaction in VR while highlighting
challenges in reducing system latency and enhancing emotional depth. This
research contributes to the development of more sophisticated AI-driven NPCs,
revealing the need for performance optimization to achieve increasingly
immersive virtual experiences.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [369] [AMix-1: A Pathway to Test-Time Scalable Protein Foundation Model](https://arxiv.org/abs/2507.08920)
*Changze Lv,Jiang Zhou,Siyu Long,Lihao Wang,Jiangtao Feng,Dongyu Xue,Yu Pei,Hao Wang,Zherui Zhang,Yuchen Cai,Zhiqiang Gao,Ziyuan Ma,Jiakai Hu,Chaochen Gao,Jingjing Gong,Yuxuan Song,Shuyi Zhang,Xiaoqing Zheng,Deyi Xiong,Lei Bai,Ya-Qin Zhang,Wei-Ying Ma,Bowen Zhou,Hao Zhou*

Key words: 蛋白质基础模型,贝叶斯流网络,MSA上下文学习,蛋白质设计,硅定向进化

TL;DR: AMix-1是一种基于贝叶斯流网络和系统训练方法的蛋白质基础模型，通过预测性扩展法则和MSA上下文学习策略，成功设计了性能显著提升的蛋白质变体。

<details>
  <summary>Details</summary>

Main category: q-bio.BM

Motivation: 开发一个可扩展且强大的蛋白质设计框架，以理解蛋白质结构并生成功能一致的蛋白质。

Method: 使用贝叶斯流网络构建模型，结合预测性扩展法则和MSA上下文学习策略，并通过测试时间扩展算法优化性能。

Result: 成功设计出活性提高50倍的AmeR变体，并展示了在硅定向进化中的可扩展性能提升。

Conclusion: AMix-1为下一代蛋白质设计奠定了基础，展示了强大的上下文学习和进化优化能力。

Abstract: We introduce AMix-1, a powerful protein foundation model built on Bayesian
Flow Networks and empowered by a systematic training methodology, encompassing
pretraining scaling laws, emergent capability analysis, in-context learning
mechanism, and test-time scaling algorithm. To guarantee robust scalability, we
establish a predictive scaling law and reveal the progressive emergence of
structural understanding via loss perspective, culminating in a strong
1.7-billion model. Building on this foundation, we devise a multiple sequence
alignment (MSA)-based in-context learning strategy to unify protein design into
a general framework, where AMix-1 recognizes deep evolutionary signals among
MSAs and consistently generates structurally and functionally coherent
proteins. This framework enables the successful design of a dramatically
improved AmeR variant with an up to $50\times$ activity increase over its wild
type. Pushing the boundaries of protein engineering, we further empower AMix-1
with an evolutionary test-time scaling algorithm for in silico directed
evolution that delivers substantial, scalable performance gains as verification
budgets are intensified, laying the groundwork for next-generation
lab-in-the-loop protein design.

</details>


### [370] [Conformation-Aware Structure Prediction of Antigen-Recognizing Immune Proteins](https://arxiv.org/abs/2507.09054)
*Frédéric A. Dreyer,Jan Ludwiczak,Karolis Martinkus,Brennan Abanades,Robert G. Alberstein,Pan Kessel,Pranav Rao,Jae Hyeon Lee,Richard Bonneau,Andrew M. Watkins,Franziska Seeger*

Key words: Ibex,免疫球蛋白结构预测,抗体,纳米抗体,T细胞受体,结合状态预测

TL;DR: Ibex是一个泛免疫球蛋白结构预测模型，能够高精度预测抗体、纳米抗体和T细胞受体的可变域结构，同时区分结合与非结合状态，且计算需求较低。

<details>
  <summary>Details</summary>

Main category: q-bio.BM

Motivation: 提高免疫球蛋白结构预测的准确性，尤其是在结合与非结合状态的区分上，以支持大分子设计和新疗法开发。

Method: Ibex利用标记的apo（未结合）和holo（结合）结构对进行训练，结合私有高分辨率抗体结构数据集，优化预测性能。

Result: Ibex在分布外性能上优于现有专业和通用蛋白质结构预测工具，同时计算需求显著降低。

Conclusion: Ibex为免疫球蛋白结构预测提供了高精度和高效的方法，有助于加速大分子设计和治疗开发。

Abstract: We introduce Ibex, a pan-immunoglobulin structure prediction model that
achieves state-of-the-art accuracy in modeling the variable domains of
antibodies, nanobodies, and T-cell receptors. Unlike previous approaches, Ibex
explicitly distinguishes between bound and unbound protein conformations by
training on labeled apo and holo structural pairs, enabling accurate prediction
of both states at inference time. Using a comprehensive private dataset of
high-resolution antibody structures, we demonstrate superior
out-of-distribution performance compared to existing specialized and general
protein structure prediction tools. Ibex combines the accuracy of cutting-edge
models with significantly reduced computational requirements, providing a
robust foundation for accelerating large molecule design and therapeutic
development.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [371] [Energy Dissipation Rate Guided Adaptive Sampling for Physics-Informed Neural Networks: Resolving Surface-Bulk Dynamics in Allen-Cahn Systems](https://arxiv.org/abs/2507.09757)
*Chunyan Li,Wenkai Yu,Qi Wang*

Key words: EDRAS, PINN, 自适应采样, 热力学一致PDE, 能量耗散率

TL;DR: EDRAS是一种新方法，通过局部能量耗散率密度指导自适应采样，显著提升PINNs在任意域上求解热力学一致PDE的性能，相比传统方法误差降低六倍。

<details>
  <summary>Details</summary>

Main category: math.NA

Motivation: 提升PINNs在求解热力学一致PDE时的性能，特别是在复杂几何域中的准确性和效率。

Method: 利用局部能量耗散率密度作为指标，动态采样关键点，改进PINN的训练过程。

Result: 相比传统RAR方法，相对均方误差降低六倍，计算效率更高，能识别高影响力点。

Conclusion: EDRAS为PINN提供了一个物理驱动的增强框架，使其成为研究复杂热力学过程的强大工具。

Abstract: We introduce the Energy Dissipation Rate guided Adaptive Sampling (EDRAS)
strategy, a novel method that substantially enhances the performance of
Physics-Informed Neural Networks (PINNs) in solving thermodynamically
consistent partial differential equations (PDEs) over arbitrary domains. EDRAS
leverages the local energy dissipation rate density as a guiding metric to
identify and adaptively re-sample critical collocation points from both the
interior and boundary of the computational domain. This dynamical sampling
approach improves the accuracy of residual-based PINNs by aligning the training
process with the underlying physical structure of the system. In this study, we
demonstrate the effectiveness of EDRAS using the Allen-Cahn phase field model
in irregular geometries, achieving up to a sixfold reduction in the relative
mean square error compared to traditional residual-based adaptive refinement
(RAR) methods. Moreover, we compare EDRAS with other residual-based adaptive
sampling approaches and show that EDRAS is not only computationally more
efficient but also more likely to identify high-impact collocation points.
Through numerical solutions of the Allen-Cahn equation with both static
(Neumann) and dynamic boundary conditions in 2D disk- and ellipse-shaped
domains solved using PINN coupled with EDRAS, we gain significant insights into
how dynamic boundary conditions influence bulk phase evolution and
thermodynamic behavior. The proposed approach offers an effective, physically
informed enhancement to PINN frameworks for solving thermodynamically
consistent models, making PINN a robust and versatile computational tool for
investigating complex thermodynamic processes in arbitrary geometries.

</details>


### [372] [Physics-informed neural networks for high-dimensional solutions and snaking bifurcations in nonlinear lattices](https://arxiv.org/abs/2507.09782)
*Muhammad Luthfi Shahab,Fidya Almira Suheri,Rudy Kusdiantara,Hadi Susanto*

Key words: PINNs, 非线性晶格, 分岔图, 线性稳定性分析, 高维计算

TL;DR: 本文提出了一个基于物理信息神经网络（PINNs）的框架，用于解决非线性晶格中的关键问题，包括解近似、分岔图构建和线性稳定性分析。通过结合优化算法和随机采样策略，提高了计算效率和精度。数值实验表明，该方法在高维情况下表现优于传统方法。

<details>
  <summary>Details</summary>

Main category: math.NA

Motivation: 研究旨在利用PINNs解决非线性晶格中的计算挑战，特别是高维情况下的效率和精度问题。通过将神经网络与物理知识结合，提供了一种更高效的解决方案。

Method: 采用PINNs近似非线性晶格模型的解，结合Levenberg-Marquardt算法优化网络权重，并引入随机采样策略提高效率。进一步将PINNs与延续法结合以构建分岔图，并通过输出约束进行线性稳定性分析。

Result: 实验表明，该方法在精度和效率上与传统方法相当或更优，尤其在高维情况下展示出显著优势。

Conclusion: PINNs为研究复杂非线性晶格系统提供了可扩展且高效的工具，尤其在高维情况下具有巨大潜力。

Abstract: This paper introduces a framework based on physics-informed neural networks
(PINNs) for addressing key challenges in nonlinear lattices, including solution
approximation, bifurcation diagram construction, and linear stability analysis.
We first employ PINNs to approximate solutions of nonlinear systems arising
from lattice models, using the Levenberg-Marquardt algorithm to optimize
network weights for greater accuracy. To enhance computational efficiency in
high-dimensional settings, we integrate a stochastic sampling strategy. We then
extend the method by coupling PINNs with a continuation approach to compute
snaking bifurcation diagrams, incorporating an auxiliary equation to
effectively track successive solution branches. For linear stability analysis,
we adapt PINNs to compute eigenvectors, introducing output constraints to
enforce positivity, in line with Sturm-Liouville theory. Numerical experiments
are conducted on the discrete Allen-Cahn equation with cubic and quintic
nonlinearities in one to five spatial dimensions. The results demonstrate that
the proposed approach achieves accuracy comparable to, or better than,
traditional numerical methods, especially in high-dimensional regimes where
computational resources are a limiting factor. These findings highlight the
potential of neural networks as scalable and efficient tools for the study of
complex nonlinear lattice systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [373] [ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching](https://arxiv.org/abs/2507.09318)
*Han Zhu,Wei Kang,Liyong Guo,Zengwei Yao,Fangjun Kuang,Weiji Zhuang,Zhaoqing Li,Zhifeng Han,Dong Zhang,Xin Zhang,Xingchen Song,Long Lin,Daniel Povey*

Key words: 文本-语音对话生成，非自回归模型，流匹配，零样本学习，OpenDialog数据集

TL;DR: ZipVoice-Dialog是一个基于流匹配的非自回归零样本文本-语音对话生成模型，通过引入说话人轮换嵌入和课程学习策略，解决了传统自回归模型推理慢和不稳定的问题，并在多个评估指标上表现优异。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 由于现有自回归模型在生成对话时推理速度慢且不稳定，且缺乏大规模开源对话数据集，作者提出ZipVoice-Dialog和OpenDialog数据集，以解决这些问题。

Method: 1) 说话人轮换嵌入实现精确的轮换控制；2) 课程学习策略稳定语音-文本对齐；3) 立体声对话生成的专用策略。

Result: ZipVoice-Dialog在可懂度、说话人轮换准确性、说话人相似性和推理速度方面表现优异。

Conclusion: ZipVoice-Dialog解决了传统模型的局限性，并通过OpenDialog数据集和基准评测推动了领域发展。

Abstract: Generating spoken dialogue is more challenging than monologue text-to-speech
(TTS) due to the need for realistic turn-taking and distinct speaker timbres.
Existing spoken dialogue generation models, being auto-regressive, suffer from
slow and unstable inference. To overcome these limitations, we introduce
ZipVoice-Dialog, a non-autoregressive zero-shot spoken dialogue generation
model built upon flow matching. Key designs include: 1) speaker-turn embeddings
for precise speaker turn-taking; 2) a curriculum learning strategy for stable
speech-text alignment; 3) specialized strategies to enable stereo dialogue
generation. Additionally, recognizing the lack of open-source large-scale
spoken dialogue datasets, we curated OpenDialog, a 6.8k-hour spoken dialogue
dataset from in-the-wild speech data. Furthermore, we established a benchmark
to comprehensively evaluate various models. Experimental results demonstrate
that ZipVoice-Dialog achieves superior performance in intelligibility, speaker
turn-taking accuracy, speaker similarity, and inference speed. Our codes, model
checkpoints, demo samples, and the OpenDialog dataset are all publicly
available at https://github.com/k2-fsa/ZipVoice.

</details>


### [374] [Natural Language-based Assessment of L2 Oral Proficiency using LLMs](https://arxiv.org/abs/2507.10200)
*Stefano Bannò,Rao Ma,Mengjie Qian,Siyuan Tang,Kate Knill,Mark Gales*

Key words: 自然语言评估, 大语言模型, 零样本学习, can-do描述符

TL;DR: NLA方法利用语言描述符评估语言模型能力，开箱即用的Qwen 2.5 72B表现接近最佳语音模型，超越专用BERT模型。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 探索语言模型是否能像人类一样解释和应用语言描述符进行第二语言评估。

Method: 使用Qwen 2.5 72B模型零样本评估S&I Corpus的回答，仅依赖文本信息。

Result: NLA表现接近最优语音模型，超越专用BERT模型，尤其在任务不匹配时表现出色。

Conclusion: NLA基于明确的语言描述符，可泛化到其他数据类型和语言，具有更高可解释性。

Abstract: Natural language-based assessment (NLA) is an approach to second language
assessment that uses instructions - expressed in the form of can-do descriptors
- originally intended for human examiners, aiming to determine whether large
language models (LLMs) can interpret and apply them in ways comparable to human
assessment. In this work, we explore the use of such descriptors with an
open-source LLM, Qwen 2.5 72B, to assess responses from the publicly available
S&I Corpus in a zero-shot setting. Our results show that this approach -
relying solely on textual information - achieves competitive performance: while
it does not outperform state-of-the-art speech LLMs fine-tuned for the task, it
surpasses a BERT-based model trained specifically for this purpose. NLA proves
particularly effective in mismatched task settings, is generalisable to other
data types and languages, and offers greater interpretability, as it is
grounded in clearly explainable, widely applicable language descriptors.

</details>


### [375] [Aligning Generative Speech Enhancement with Human Preferences via Direct Preference Optimization](https://arxiv.org/abs/2507.09929)
*Haoyang Li,Nana Hou,Yuchen Hu,Jixun Yao,Sabato Marco Siniscalchi,Eng Siong Chng*

Key words: 语音增强, 语言模型, DPO, 感知质量, UTMOS

TL;DR: 本文提出了一种基于语言模型的语音增强方法，利用DPO优化听觉质量，并引入UTMOS作为代理评估，实验表明其质量指标显著提升。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 现有基于语言模型的语音增强方法过于关注干净语音令牌的最大似然性，可能与人耳感知不一致，导致质量下降。本文旨在弥补这一差距。

Method: 采用DPO直接优化感知偏好，利用UTMOS神经网络模型替代人类评分作为反馈，指导模型训练。

Result: 在2020年深度噪声抑制挑战测试集上，使用DPO的预训练模型在多项语音质量指标上提升高达56%。

Conclusion: 本文首次将DPO应用于语音增强，并通过代理感知反馈实现了更高的听觉对齐效果，为感知优化的语音增强指明了新方向。

Abstract: This work investigates speech enhancement (SE) from the perspective of
language models (LMs). We propose a novel method that leverages Direct
Preference Optimization (DPO) to improve the perceptual quality of enhanced
speech. Using UTMOS, a neural MOS prediction model, as a proxy for human
ratings, our approach guides optimization toward perceptually preferred
outputs. This differs from existing LM-based SE methods that focus on
maximizing the likelihood of clean speech tokens, which may misalign with human
perception and degrade quality despite low prediction error. Experiments on the
2020 Deep Noise Suppression Challenge test sets demonstrate that applying DPO
to a pretrained LM-based SE model yields consistent improvements across various
speech quality metrics, with relative gains of up to 56%. To our knowledge,
this is the first application of DPO to SE and the first to incorporate proxy
perceptual feedback into LM-based SE training, pointing to a promising
direction for perceptually aligned SE.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [376] [Sequence-Model-Guided Measurement Selection for Quantum State Learning](https://arxiv.org/abs/2507.09891)
*Jiaxin Huang,Yan Zhu,Giulio Chiribella,Ya-Dong Wu*

Key words: 量子系统, 深度学习, 测量优化, 序列模型, 拓扑量子系统

TL;DR: 论文提出了一种使用深度学习神经网络的方法，通过序列模型架构自适应地选择高效测量方式，解决量子系统表征中的测量选择问题。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 量子系统的实验数据表征是量子科学与技术中的核心问题，但在大规模量子系统中优化测量选择变得难以处理。

Method: 使用深度学习神经网络结合序列模型架构，以数据驱动和自适应的方式寻找高效测量选择。

Result: 神经网络选择的测量方式在多种任务中均优于随机选择，且在拓扑量子系统中倾向于推荐边界测量。

Conclusion: 神经网络能够自适应地优化测量选择，甚至在无量子物理知识的情况下发现边界与体之间的关系。

Abstract: Characterization of quantum systems from experimental data is a central
problem in quantum science and technology. But which measurements should be
used to gather data in the first place? While optimal measurement choices can
be worked out for small quantum systems, the optimization becomes intractable
as the system size grows large. To address this problem, we introduce a deep
neural network with a sequence model architecture that searches for efficient
measurement choices in a data-driven, adaptive manner. The model can be applied
to a variety of tasks, including the prediction of linear and nonlinear
properties of quantum states, as well as state clustering and state tomography
tasks. In all these tasks, we find that the measurement choices identified by
our neural network consistently outperform the uniformly random choice.
Intriguingly, for topological quantum systems, our model tends to recommend
measurements at the system's boundaries, even when the task is to predict bulk
properties. This behavior suggests that the neural network may have
independently discovered a connection between boundaries and bulk, without
having been provided any built-in knowledge of quantum physics.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [377] [Physics-Based Machine Learning Closures and Wall Models for Hypersonic Transition-Continuum Boundary Layer Predictions](https://arxiv.org/abs/2507.08986)
*Ashish S. Nair,Narendra Singh,Marco Panesi,Justin Sirignano,Jonathan F. MacArt*

Key words: 稀薄高超声速流, 机器学习, 偏态高斯分布, 非平衡效应, 克努森数

TL;DR: 提出了一种基于物理约束的机器学习框架，用于改进稀薄高超声速流的模拟，解决了传统连续介质模型在高克努森数和高马赫数下的不足。

<details>
  <summary>Details</summary>

Main category: physics.flu-dyn

Motivation: 传统Navier-Stokes-Fourier (NSF) 模型在高克努森数（0.1-10）范围内无法准确预测非平衡效应（如速度滑移、温度跳跃等），亟需改进。

Method: 采用深度学习PDE模型（DPMs）嵌入控制方程，通过伴随优化训练；引入基于偏态高斯分布的壁面模型，替代经验滑移条件。

Result: 结合偏态高斯分布壁面模型的无迹各向异性粘度模型显著提高了预测精度，尤其是在高马赫数和高克努森数下。

Conclusion: 该工作为传统连续介质方法失效的流态提供了一种数据驱动且物理一致的新型建模策略。

Abstract: Modeling rarefied hypersonic flows remains a fundamental challenge due to the
breakdown of classical continuum assumptions in the transition-continuum
regime, where the Knudsen number ranges from approximately 0.1 to 10.
Conventional Navier-Stokes-Fourier (NSF) models with empirical slip-wall
boundary conditions fail to accurately predict nonequilibrium effects such as
velocity slip, temperature jump, and shock structure deviations. We develop a
physics-constrained machine learning framework that augments transport models
and boundary conditions to extend the applicability of continuum solvers in
nonequilibrium hypersonic regimes. We employ deep learning PDE models (DPMs)
for the viscous stress and heat flux embedded in the governing PDEs and trained
via adjoint-based optimization. We evaluate these for two-dimensional
supersonic flat-plate flows across a range of Mach and Knudsen numbers.
Additionally, we introduce a wall model based on a mixture of skewed Gaussian
approximations of the particle velocity distribution function. This wall model
replaces empirical slip conditions with physically informed, data-driven
boundary conditions for the streamwise velocity and wall temperature. Our
results show that a trace-free anisotropic viscosity model, paired with the
skewed-Gaussian distribution function wall model, achieves significantly
improved accuracy, particularly at high-Mach and high-Knudsen number regimes.
Strategies such as parallel training across multiple Knudsen numbers and
inclusion of high-Mach data during training are shown to enhance model
generalization. Increasing model complexity yields diminishing returns for
out-of-sample cases, underscoring the need to balance degrees of freedom and
overfitting. This work establishes data-driven, physics-consistent strategies
for improving hypersonic flow modeling for regimes in which conventional
continuum approaches are invalid.

</details>


### [378] [WellPINN: Accurate Well Representation for Transient Fluid Pressure Diffusion in Subsurface Reservoirs with Physics-Informed Neural Networks](https://arxiv.org/abs/2507.09330)
*Linus Walter,Qingkai Kong,Sara Hanson-Hedgecock,Víctor Vilarrasa*

Key words: Physics-informed neural networks, Well representation, Reservoir modeling, Inverse modeling, Operational scenarios

TL;DR: 论文提出了WellPINN工作流，通过迭代训练多个PINN模型精确模拟井附近流体压力。

<details>
  <summary>Details</summary>

Main category: physics.flu-dyn

Motivation: 现有PINN方法难以准确模拟井附近流体压力，尤其在注水早期阶段。

Method: 采用分解域和逐步缩小子域的方法，迭代逼近等效井半径。

Result: WellPINN能准确推断注水期间流体压力，显著提升PINN在反演建模中的应用潜力。

Conclusion: WellPINN为井附近压力模拟提供了有效解决方案，推动了PINN在油藏模拟中的应用。

Abstract: Accurate representation of wells is essential for reliable reservoir
characterization and simulation of operational scenarios in subsurface flow
models. Physics-informed neural networks (PINNs) have recently emerged as a
promising method for reservoir modeling, offering seamless integration of
monitoring data and governing physical equations. However, existing PINN-based
studies face major challenges in capturing fluid pressure near wells,
particularly during the early stage after injection begins. To address this, we
propose WellPINN, a modeling workflow that combines the outputs of multiple
sequentially trained PINN models to accurately represent wells. This workflow
iteratively approximates the radius of the equivalent well to match the actual
well dimensions by decomposing the domain into stepwise shrinking subdomains
with a simultaneously reducing equivalent well radius. Our results demonstrate
that sequential training of superimposing networks around the pumping well is
the first workflow that focuses on accurate inference of fluid pressure from
pumping rates throughout the entire injection period, significantly advancing
the potential of PINNs for inverse modeling and operational scenario
simulations. All data and code for this paper will be made openly available at
https://github.com/linuswalter/WellPINN.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [379] [Investigating the Robustness of Extreme Precipitation Super-Resolution Across Climates](https://arxiv.org/abs/2507.09166)
*Louise Largeau,Erwan Koch,David Leutwyler,Gregoire Mariethoz,Valerie Chavez-Demoulin,Tom Beucler*

Key words: 极端降水, 降尺度, 稳健性, 广义极值分布, 气候变化

TL;DR: 该论文提出了一种直接超分辨率参数化的方法，以改进极端降水预测在气候变化下的稳健性，并通过瑞士的完美模型框架验证了其有效性。

<details>
  <summary>Details</summary>

Main category: physics.ao-ph

Motivation: 由于气候模型的空间分辨率较低，直接用于极端降水等社会相关变量的预测存在局限性，现有方法难以评估分布变化的稳健性。

Method: 使用向量广义线性和可加模型，直接从粗分辨率的降水场和地形数据中超分辨率广义极值分布参数。

Result: 在瑞士的完美模型框架下，该方法成功超分辨率了夏季小时极端降水的分布，并提出了“稳健性差距”概念以评估模型结构对气候变化下分位数泛化的影响。

Conclusion: 该方法适用于参数化分布变量，并提供了一种模型无关的诊断工具，用于理解经验降尺度在气候变化和极端条件下的泛化能力。

Abstract: The coarse spatial resolution of gridded climate models, such as general
circulation models, limits their direct use in projecting socially relevant
variables like extreme precipitation. Most downscaling methods estimate the
conditional distributions of extremes by generating large ensembles,
complicating the assessment of robustness under distributional shifts, such as
those induced by climate change. To better understand and potentially improve
robustness, we propose super-resolving the parameters of the target variable's
probability distribution directly using analytically tractable mappings. Within
a perfect-model framework over Switzerland, we demonstrate that vector
generalized linear and additive models can super-resolve the generalized
extreme value distribution of summer hourly precipitation extremes from coarse
precipitation fields and topography. We introduce the notion of a "robustness
gap", defined as the difference in predictive error between present-trained and
future-trained models, and use it to diagnose how model structure affects the
generalization of each quantile to a pseudo-global warming scenario. By
evaluating multiple model configurations, we also identify an upper limit on
the super-resolution factor based on the spatial auto- and cross-correlation of
precipitation and elevation, beyond which coarse precipitation loses predictive
value. Our framework is broadly applicable to variables governed by parametric
distributions and offers a model-agnostic diagnostic for understanding when and
why empirical downscaling generalizes to climate change and extremes.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [380] [THOR: Transformer Heuristics for On-Demand Retrieval](https://arxiv.org/abs/2507.09592)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Key words: THOR, 文本转SQL, 企业数据库, 自然语言处理, 安全分析

TL;DR: THOR模块是一个由eSapiens设计的文本转SQL引擎，通过模块化架构实现自然语言问题到SQL查询的转换，支持企业数据库的安全、可扩展分析。

<details>
  <summary>Details</summary>

Main category: cs.DB

Motivation: 旨在为非技术用户提供零SQL的简单方式安全访问实时数据，同时确保企业级的安全性和可扩展性。

Method: 采用解耦的编排/执行架构，包括Supervisor Agent路由查询、Schema Retrieval动态注入元数据、SQL Generation Agent生成受保护的查询，并集成自纠正与评级循环。

Result: 在金融、销售和运营场景中表现出可靠的即席查询和自动定期报告能力。

Conclusion: THOR模块通过嵌入模式感知、容错执行和合规保护，为非技术用户提供了简单且安全的数据访问方式。

Abstract: We introduce the THOR (Transformer Heuristics for On-Demand Retrieval)
Module, designed and implemented by eSapiens, a secure, scalable engine that
transforms natural-language questions into verified, read-only SQL analytics
for enterprise databases. The Text-to-SQL module follows a decoupled
orchestration/execution architecture: a Supervisor Agent routes queries, Schema
Retrieval dynamically injects table and column metadata, and a SQL Generation
Agent emits single-statement SELECT queries protected by a read-only guardrail.
An integrated Self-Correction & Rating loop captures empty results, execution
errors, or low-quality outputs and triggers up to five LLM-driven regeneration
attempts. Finally, a Result Interpretation Agent produces concise,
human-readable insights and hands raw rows to the Insight & Intelligence engine
for visualization or forecasting.
  Smoke tests across finance, sales, and operations scenarios demonstrate
reliable ad-hoc querying and automated periodic reporting. By embedding schema
awareness, fault-tolerant execution, and compliance guardrails, the THOR Module
empowers non-technical users to access live data with zero-SQL simplicity and
enterprise-grade safety.

</details>


### [381] [HedraRAG: Coordinating LLM Generation and Database Retrieval in Heterogeneous RAG Serving](https://arxiv.org/abs/2507.09138)
*Zhengding Hu,Vibha Murthy,Zaifeng Pan,Wanlu Li,Xiaoyi Fang,Yufei Ding,Yuke Wang*

Key words: 检索增强生成；图抽象；动态优化；混合计算；资源利用

TL;DR: 该论文提出HedraRAG系统，通过图抽象优化异构RAG服务中的多阶段工作流，提升执行效率。

<details>
  <summary>Details</summary>

Main category: cs.DB

Motivation: 解决异构检索增强生成（RAG）服务中多阶段工作流和多样化请求模式带来的系统级挑战。

Method: 使用基于图的抽象和动态图变换技术（如节点分裂、重新排序、边添加等）优化执行计划，结合混合CPU-GPU管道提升资源利用。

Result: 在多种RAG工作流中实现1.5倍至5倍的加速，显著优于现有框架。

Conclusion: HedraRAG通过协调生成和检索优化了服务环境中的执行效率。

Abstract: This paper addresses emerging system-level challenges in heterogeneous
retrieval-augmented generation (RAG) serving, where complex multi-stage
workflows and diverse request patterns complicate efficient execution. We
present HedraRAG, a runtime system built on a graph-based abstraction that
exposes optimization opportunities across stage-level parallelism,
intra-request similarity, and inter-request skewness. These opportunities are
realized through dynamic graph transformations, such as node splitting,
reordering, edge addition, and dependency rewiring, applied to wavefronts of
subgraphs spanning concurrent requests. The resulting execution plans are
mapped onto hybrid CPU-GPU pipelines to improve resource utilization and reduce
latency. Evaluations across a wide range of RAG workflows demonstrate speedups
exceeding 1.5x and reaching up to 5x over existing frameworks, showcasing the
effectiveness of coordinated generation and retrieval in serving environments.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [382] [An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion for Long-tailed Multispectral Point Clouds](https://arxiv.org/abs/2412.11407)
*TianZhu Liu,BangYan Hu,YanFeng Gu,Xian Li,Aleksandra Pižurica*

Key words: 多光谱点云, 室外分类, 长尾分布, 自适应多尺度融合

TL;DR: 该论文提出了一种针对具有长尾分布的多光谱点云的增强分类方法，通过自适应多尺度融合解决了室外数据集中的稀疏标签、尺度差异和长尾分布问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决多光谱点云在室外场景中的分类问题，尤其是稀疏标签、尺度差异和长尾分布带来的挑战。

Method: 1. 网格平衡采样策略生成训练样本；2. 多尺度特征融合模块解决尺度变化问题；3. 自适应混合损失模块平衡不同类别的学习能力。

Result: 在三个多光谱点云数据集上验证了方法的有效性，优于现有方法。

Conclusion: 提出的方法在多光谱点云分类中表现优异，有效解决了室外场景中的多个挑战。

Abstract: Multispectral point cloud (MPC) captures 3D spatial-spectral information from
the observed scene, which can be used for scene understanding and has a wide
range of applications. However, most of the existing classification methods
were extensively tested on indoor datasets, and when applied to outdoor
datasets they still face problems including sparse labeled targets, differences
in land-covers scales, and long-tailed distributions. To address the above
issues, an enhanced classification method based on adaptive multi-scale fusion
for MPCs with long-tailed distributions is proposed. In the training set
generation stage, a grid-balanced sampling strategy is designed to reliably
generate training samples from sparse labeled datasets. In the feature learning
stage, a multi-scale feature fusion module is proposed to fuse shallow features
of land-covers at different scales, addressing the issue of losing fine
features due to scale variations in land-covers. In the classification stage,
an adaptive hybrid loss module is devised to utilize multi-classification heads
with adaptive weights to balance the learning ability of different classes,
improving the classification performance of small classes due to various-scales
and long-tailed distributions in land-covers. Experimental results on three MPC
datasets demonstrate the effectiveness of the proposed method compared with the
state-of-the-art methods.

</details>


### [383] [Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging](https://arxiv.org/abs/2507.08052)
*Mazen Ali,António Pereira,Fabio Gentile,Aser Cortines,Sam Mugel,Román Orús,Stelios P. Neophytides,Michalis Mavrovouniotis*

Key words: 高光谱图像, 机器学习, CNN, 梯度提升, 云掩模

TL;DR: 评估了多种机器学习方法用于高光谱卫星图像云和云影掩模，CNN结合特征压缩表现最佳，实现高精度和低计算成本。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 高光谱卫星图像中云和云影的掩模是提取高质量数据的关键预处理步骤。

Method: 比较了梯度提升方法（XGBoost、LightGBM）和卷积神经网络（CNN）。

Result: 所有模型精度超过93%，CNN特征压缩版本综合表现最优。

Conclusion: 轻量级AI模型适合实时高光谱图像处理，支持星载AI系统。

Abstract: Cloud and cloud shadow masking is a crucial preprocessing step in
hyperspectral satellite imaging, enabling the extraction of high-quality,
analysis-ready data. This study evaluates various machine learning approaches,
including gradient boosting methods such as XGBoost and LightGBM as well as
convolutional neural networks (CNNs). All boosting and CNN models achieved
accuracies exceeding 93%. Among the investigated models, the CNN with feature
reduction emerged as the most efficient, offering a balance of high accuracy,
low storage requirements, and rapid inference times on both CPUs and GPUs.
Variations of this version, with only up to 597 trainable parameters,
demonstrated the best trade-off in terms of deployment feasibility, accuracy,
and computational efficiency. These results demonstrate the potential of
lightweight artificial intelligence (AI) models for real-time hyperspectral
image processing, supporting the development of on-board satellite AI systems
for space-based applications.

</details>


### [384] [Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models](https://arxiv.org/abs/2507.09279)
*Anita Kriz,Elizabeth Laura Janes,Xing Shen,Tal Arbel*

Key words: 多模态大语言模型, 强化学习, 置信度校准, 医疗决策, 视觉问答

TL;DR: 提出了一个强化学习框架Prompt4Trust，用于提升多模态大语言模型在医疗场景中的置信度校准和任务准确性，同时展示了在PMC-VQA基准测试中的优异表现和零样本泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 医疗领域中多模态大语言模型的部署受限于其对提示设计的敏感性和高置信度错误回答的倾向，需要一种能够提升模型可信度的方法。

Method: 利用强化学习训练一个轻量级语言模型，生成上下文相关的辅助提示，以优化下游任务模型的置信度校准和回答准确性。

Result: 在PMC-VQA基准测试中取得了最先进的性能，同时展示了零样本泛化能力，表明无需额外计算成本即可扩展校准效果。

Conclusion: 强化学习驱动的提示工程可以有效提升多模态大语言模型在安全关键场景中的可信度，为医疗决策提供可靠支持。

Abstract: Multimodal large language models (MLLMs) hold considerable promise for
applications in healthcare. However, their deployment in safety-critical
settings is hindered by two key limitations: (i) sensitivity to prompt design,
and (ii) a tendency to generate incorrect responses with high confidence. As
clinicians may rely on a model's stated confidence to gauge the reliability of
its predictions, it is especially important that when a model expresses high
confidence, it is also highly accurate. We introduce Prompt4Trust, the first
reinforcement learning (RL) framework for prompt augmentation targeting
confidence calibration in MLLMs. A lightweight LLM is trained to produce
context-aware auxiliary prompts that guide a downstream task MLLM to generate
responses in which the expressed confidence more accurately reflects predictive
accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically
prioritizes aspects of calibration most critical for safe and trustworthy
clinical decision-making. Beyond improvements driven by this clinically
motivated calibration objective, our proposed method also improves task
accuracy, achieving state-of-the-art medical visual question answering (VQA)
performance on the PMC-VQA benchmark, which is composed of multiple-choice
questions spanning diverse medical imaging modalities. Moreover, our framework
trained with a small downstream task MLLM showed promising zero-shot
generalization to larger MLLMs in our experiments, suggesting the potential for
scalable calibration without the associated computational costs. This work
demonstrates the potential of automated yet human-aligned prompt engineering
for improving the the trustworthiness of MLLMs in safety critical settings. Our
codebase can be found at https://github.com/xingbpshen/vccrl-llm.

</details>


### [385] [MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models](https://arxiv.org/abs/2507.09574)
*Haozhe Zhao,Zefan Cai,Shuzheng Si,Liang Chen,Jiuxiang Gu,Wen Xiao,Junjie Hu*

Key words: 多模态图像生成、自回归框架、两阶段训练、视觉控制、训练效率

TL;DR: MENTOR是一种新型自回归框架，通过两阶段训练实现多模态输入的细粒度对齐，无需额外适配器或交叉注意力模块。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决现有文本-图像模型在多模态输入平衡、精确视觉控制和复杂图像生成中的训练效率问题。

Method: 结合自回归图像生成器和两阶段训练范式：多模态对齐阶段和指令调优阶段。

Result: 在DreamBench++基准测试中表现优异，尤其在概念保持和提示跟随上超越基线方法。

Conclusion: MENTOR在图像重建保真度、任务适应性和训练效率上优于基于扩散的方法。

Abstract: Recent text-to-image models produce high-quality results but still struggle
with precise visual control, balancing multimodal inputs, and requiring
extensive training for complex multimodal image generation. To address these
limitations, we propose MENTOR, a novel autoregressive (AR) framework for
efficient Multimodal-conditioned Tuning for Autoregressive multimodal image
generation. MENTOR combines an AR image generator with a two-stage training
paradigm, enabling fine-grained, token-level alignment between multimodal
inputs and image outputs without relying on auxiliary adapters or
cross-attention modules. The two-stage training consists of: (1) a multimodal
alignment stage that establishes robust pixel- and semantic-level alignment,
followed by (2) a multimodal instruction tuning stage that balances the
integration of multimodal inputs and enhances generation controllability.
Despite modest model size, suboptimal base components, and limited training
resources, MENTOR achieves strong performance on the DreamBench++ benchmark,
outperforming competitive baselines in concept preservation and prompt
following. Additionally, our method delivers superior image reconstruction
fidelity, broad task adaptability, and improved training efficiency compared to
diffusion-based methods. Dataset, code, and models are available at:
https://github.com/HaozheZhao/MENTOR

</details>


### [386] [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://arxiv.org/abs/2507.09876)
*Yongheng Zhang,Xu Liu,Ruihan Tao,Qiguang Chen,Hao Fei,Wanxiang Che,Libo Qin*

Key words: 视频理解, 链式思维, 多模态学习, MLLMs, 视频推理

TL;DR: 该论文提出了视频-文本交错链式思维（ViTCoT）范式，通过结合视觉和文本信息，显著提升了视频理解的性能，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前视频推理方法主要依赖文本信息，忽视了视觉模态的重要性，而人类在推理过程中会自然回顾视觉内容。因此，论文提出ViTCoT范式以更接近人类认知的方式进行视频推理。

Method: 论文首先构建了视频-文本交错基准（ViTIB），并利用多模态大模型（MLLMs）筛选关键视频片段并手动验证。随后，系统地探索了ViTCoT范式在视频理解领域的潜力。

Result: 实验表明，ViTCoT显著优于传统的纯文本链式思维范式，并有效激活了多模态大模型中更多的神经元值。

Conclusion: ViTCoT通过结合视觉和文本信息，提供了一种更直观且符合认知的视频推理方法，为视频理解领域提供了新的研究方向。

Abstract: Video understanding plays a vital role in bridging low-level visual signals
with high-level cognitive reasoning, and is fundamental to applications such as
autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid
development of large language models (LLMs), particularly those utilizing
Chain-of-Thought (CoT) technology, has significantly advanced video reasoning
capabilities. However, current approaches primarily depend on textual
information for reasoning, overlooking the visual modality in the actual video
reasoning process. In contrast, humans naturally re-examine visual content
while reasoning. Motivated by this, we introduce a novel video reasoning
paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive
and cognitively aligned reasoning. To the end, first, we construct the
Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for
key-video selection and manually verified. Furthermore, we extensively explore
the potential of the ViTCoT paradigm in the video understanding field.
Extensive experiments demonstrate that ViTCoT significantly enhances
performance compared to the traditional text-only CoT paradigm and effectively
activates more neuron values in MLLMs.

</details>


### [387] [Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect](https://arxiv.org/abs/2507.10013)
*Tom Kouwenhoven,Kiana Shahrasbi,Tessa Verhoef*

Key words: 视觉与语言模型, bouba-kiki效应, 跨模态整合, CLIP, 人类认知

TL;DR: 本文重新评估了视觉与语言模型（VLMs）是否能够像人类一样整合跨模态信息，特别是在bouba-kiki效应中的表现。研究发现，CLIP的两个变体（ResNet和ViT）未能表现出与人类一致的效果。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探究视觉与语言模型是否能够像人类一样整合跨模态信息，特别是在bouba-kiki效应中的表现，以评估这些模型的认知对齐性。

Method: 使用两种方法：基于提示的概率评估和Grad-CAM视觉注意力分析，分别在ResNet和ViT模型上进行测试。

Result: 模型未能一致表现出bouba-kiki效应，与人类行为的匹配性较低，显示出它们在跨模态理解上的局限性。

Conclusion: 视觉与语言模型在跨模态概念的内部表示和人类直觉对齐方面存在限制。

Abstract: Recent advances in multimodal models have raised questions about whether
vision-and-language models (VLMs) integrate cross-modal information in ways
that reflect human cognition. One well-studied test case in this domain is the
bouba-kiki effect, where humans reliably associate pseudowords like "bouba"
with round shapes and "kiki" with jagged ones. Given the mixed evidence found
in prior studies for this effect in VLMs, we present a comprehensive
re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer
(ViT), given their centrality in many state-of-the-art VLMs. We apply two
complementary methods closely modelled after human experiments: a prompt-based
evaluation that uses probabilities as model preference, and we use Grad-CAM as
a novel way to interpret visual attention in shape-word matching tasks. Our
findings show that these models do not consistently exhibit the bouba-kiki
effect. While ResNet shows a preference for round shapes, overall performance
across both models lacks the expected associations. Moreover, direct comparison
with prior human data on the same task shows that the models' responses fall
markedly short of the robust, modality-integrated behaviour characteristic of
human cognition. These results contribute to the ongoing debate about the
extent to which VLMs truly understand cross-modal concepts, highlighting
limitations in their internal representations and alignment with human
intuitions.

</details>


### [388] [FaceLLM: A Multimodal Large Language Model for Face Understanding](https://arxiv.org/abs/2507.10300)
*Hatef Otroshi Shahreza,Sébastien Marcel*

Key words: FaceLLM, 多模态大语言模型, ChatGPT, FairFaceGPT, 面部图像理解

TL;DR: 提出了一个专门用于面部图像理解的多模态大语言模型FaceLLM，通过ChatGPT生成高质量问答对构建数据集FairFaceGPT，显著提升了MLLMs在面部相关任务中的表现。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有MLLMs在通用数据集上训练，难以理解面部图像的领域特定视觉线索，如表情、情感和人口统计特征，缺乏大规模标注数据集。

Method: 利用ChatGPT和属性感知提示生成基于FairFace数据集的问答对，构建FairFaceGPT数据集，训练专门的FaceLLM模型。

Result: FaceLLM在多种面部中心任务中表现优异，达到最先进水平。

Conclusion: 展示了语言模型生成的合成监督在构建领域专用MLLMs中的潜力，为可信赖的人本多模态AI系统奠定了基础。

Abstract: Multimodal large language models (MLLMs) have shown remarkable performance in
vision-language tasks. However, existing MLLMs are primarily trained on generic
datasets, limiting their ability to reason on domain-specific visual cues such
as those in facial images. In particular, tasks that require detailed
understanding of facial structure, expression, emotion, and demographic
features remain underexplored by MLLMs due to the lack of large-scale annotated
face image-text datasets. In this work, we introduce FaceLLM, a multimodal
large language model trained specifically for facial image understanding. To
construct the training data, we propose a novel weakly supervised pipeline that
uses ChatGPT with attribute-aware prompts to generate high-quality
question-answer pairs based on images from the FairFace dataset. The resulting
corpus, called FairFaceGPT, covers a diverse set of attributes including
expression, pose, skin texture, and forensic information. Our experiments
demonstrate that FaceLLM improves the performance of MLLMs on various
face-centric tasks and achieves state-of-the-art performance. This work
highlights the potential of synthetic supervision via language models for
building domain-specialized MLLMs, and sets a precedent for trustworthy,
human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM
models are publicly available in the project page.

</details>


### [389] [Devanagari Handwritten Character Recognition using Convolutional Neural Network](https://arxiv.org/abs/2507.10398)
*Diksha Mehta,Prateek Mehta*

Key words: 手写字符识别, 梵文, 深度卷积神经网络, 自动化处理, DHCD数据集

TL;DR: 该研究提出了一种使用双层深度卷积神经网络识别手写梵文字符的方法，旨在提高识别率和实现自动化处理。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 梵文是印度最古老的语言之一，缺乏数字化工具，因此研究旨在通过自动化方法提取手写梵文字符，以节省时间并解决数据过时问题。

Method: 研究采用双层深度卷积神经网络，并利用包含36类梵文字符的开源数据集（DHCD）进行训练和测试，每类包含1700张图像。

Result: 方法在测试和训练中分别达到了96.36%和99.55%的准确率。

Conclusion: 该方法在手写梵文字符识别任务中表现出色，为梵文的自动化数字化提供了有效解决方案。

Abstract: Handwritten character recognition is getting popular among researchers
because of its possible applications in facilitating technological search
engines, social media, recommender systems, etc. The Devanagari script is one
of the oldest language scripts in India that does not have proper digitization
tools. With the advancement of computing and technology, the task of this
research is to extract handwritten Hindi characters from an image of Devanagari
script with an automated approach to save time and obsolete data. In this
paper, we present a technique to recognize handwritten Devanagari characters
using two deep convolutional neural network layers. This work employs a
methodology that is useful to enhance the recognition rate and configures a
convolutional neural network for effective Devanagari handwritten text
recognition (DHTR). This approach uses the Devanagari handwritten character
dataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each
of these classes has 1700 images for training and testing purposes. This
approach obtains promising results in terms of accuracy by achieving 96.36%
accuracy in testing and 99.55% in training time.

</details>


### [390] [Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources](https://arxiv.org/abs/2507.10403)
*Daniele Rege Cambrin,Lorenzo Vaiani,Giuseppe Gallipoli,Luca Cagliero,Paolo Garza*

Key words: 遥感, SAR, 多光谱, 检索, 对比学习

TL;DR: 论文提出了一种名为CrisisLandMark的大规模卫星图像语料库，以及CLOSP框架，通过文本对齐光学和SAR图像，显著提升了检索性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有文本到图像检索系统仅适用于RGB数据，未能充分利用其他传感器（如SAR和多光谱）的独特物理信息。

Method: 提出CrisisLandMark语料库和CLOSP框架，利用对比学习将光学和SAR图像对齐到统一嵌入空间；GeoCLOSP进一步整合地理坐标。

Result: CLOSP将检索nDGC提升54%，GeoCLOSP在位置依赖任务中表现优异。

Conclusion: 多样传感器数据与地理上下文的整合是释放遥感档案潜力的关键。

Abstract: Retrieving relevant imagery from vast satellite archives is crucial for
applications like disaster response and long-term climate monitoring. However,
most text-to-image retrieval systems are limited to RGB data, failing to
exploit the unique physical information captured by other sensors, such as the
all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the
spectral signatures in optical multispectral data. To bridge this gap, we
introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1
SAR and Sentinel-2 multispectral images paired with structured textual
annotations for land cover, land use, and crisis events harmonized from
authoritative land cover systems (CORINE and Dynamic World) and crisis-specific
sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),
a novel framework that uses text as a bridge to align unpaired optical and SAR
images into a unified embedding space. Our experiments show that CLOSP achieves
a new state-of-the-art, improving retrieval nDGC by 54% over existing models.
Additionally, we find that the unified training strategy overcomes the inherent
difficulty of interpreting SAR imagery by transferring rich semantic knowledge
from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which
integrates geographic coordinates into our framework, creates a powerful
trade-off between generality and specificity: while the CLOSP excels at general
semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving
location-dependent crisis events and rare geographic features. This work
highlights that the integration of diverse sensor data and geographic context
is essential for unlocking the full potential of remote sensing archives.

</details>


### [391] [BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis](https://arxiv.org/abs/2507.09036)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Hendrik Möller,Ilhem Isra Mekki,Josef A. Buchner,Anton Schmick,Arianna Pfiffer,Eva Oswald,Lucas Zimmer,Ezequiel de la Rosa,Sarthak Pati,Julian Canisius,Arianna Piffer,Ujjwal Baid,Mahyar Valizadeh,Akis Linardos,Jan C. Peeken,Surprosanna Shit,Felix Steinbauer,Daniel Rueckert,Rolf Heckemann,Spyridon Bakas,Jan Kirschke,Constantin von See,Ivan Ezhov,Marie Piraud,Benedikt Wiestler,Bjoern Menze*

Key words: BrainLesion Suite, 脑部病变, 图像分析, Python, 模块化

TL;DR: BrainLesion Suite 是一个基于 Python 的模块化工具包，专注于简化脑部病变图像分析流程的开发。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 旨在为临床和科研提供一个易于使用的工具，减少认知负担并支持复杂工作流的创建。

Method: 包含预处理模块，支持多模态图像配准、图谱注册、颅骨剥离等功能，并利用 BraTS 挑战赛算法进行模态合成和病变分割。

Result: 提供了从图像处理到分割模型评估的完整工具链，可应用于多种脑部病变分析。

Conclusion: BrainLesion Suite 不仅适用于脑部病变分析，还可扩展至其他生物医学图像分析领域。

Abstract: BrainLesion Suite is a versatile toolkit for building modular brain lesion
image analysis pipelines in Python. Following Pythonic principles, BrainLesion
Suite is designed to provide a 'brainless' development experience, minimizing
cognitive effort and streamlining the creation of complex workflows for
clinical and scientific practice. At its core is an adaptable preprocessing
module that performs co-registration, atlas registration, and optional
skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion
Suite leverages algorithms from the BraTS challenge to synthesize missing
modalities, inpaint lesions, and generate pathology-specific tumor
segmentations. BrainLesion Suite also enables quantifying segmentation model
performance, with tools such as panoptica to compute lesion-wise metrics.
Although BrainLesion Suite was originally developed for image analysis
pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,
it can be adapted for other biomedical image analysis applications. The
individual BrainLesion Suite packages and tutorials are accessible on GitHub.

</details>


### [392] [EmbRACE-3K: Embodied Reasoning and Action in Complex Environments](https://arxiv.org/abs/2507.10548)
*Mingxian Lin,Wei Huang,Yitang Li,Chengjie Jiang,Kui Wu,Fangwei Zhong,Shengju Qian,Xin Wang,Xiaojuan Qi*

Key words: 视觉语言模型,具身推理,EmRACE-3K,空间推理,多阶段任务

TL;DR: EmRACE-3K是一个包含3000多个语言引导任务的数据集，旨在评估和改进视觉语言模型在具身环境中的推理能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前视觉语言模型在被动图像和视频理解任务中表现优异，但在需要在线交互和主动场景理解的具身环境中表现有限。

Method: 通过Unreal Engine和UnrealCV-Zoo框架构建多样化的真实环境，设计多步轨迹任务，并评估模型在探索、动态空间-语义推理和多阶段目标执行方面的能力。

Result: 零样本测试中，所有模型的成功率低于20%，而经过监督学习和强化学习微调的Qwen2.5-VL-7B在各方面均取得显著提升。

Conclusion: EmRACE-3K为开发具身推理能力提供了有效工具，揭示了当前模型的局限性，并为未来改进指明了方向。

Abstract: Recent advanced vision-language models(VLMs) have demonstrated strong
performance on passive, offline image and video understanding tasks. However,
their effectiveness in embodied settings, which require online interaction and
active scene understanding remains limited. In such scenarios, an agent
perceives the environment from a first-person perspective, with each action
dynamically shaping subsequent observations. Even state-of-the-art models such
as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment
interactions, exhibiting clear limitations in spatial reasoning and
long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset
of over 3,000 language-guided tasks situated in diverse, photorealistic
environments constructed using Unreal Engine and the UnrealCV-Zoo framework.
The tasks encompass a wide range of embodied challenges, including navigation,
object manipulation, and multi-stage goal execution. Each task unfolds as a
multi-step trajectory, pairing first-person visual observations with high-level
instructions, grounded actions, and natural language rationales that express
the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to
evaluate the embodied reasoning capabilities of VLMs across three key
dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage
Goal Execution. In zero-shot settings, all models achieve success rates below
20%, underscoring the challenge posed by our benchmark and the current
limitations of VLMs in interactive environments. To demonstrate the utility of
EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning
followed by reinforcement learning. This approach yields substantial
improvements across all three challenge categories, highlighting the dataset's
effectiveness in enabling the development of embodied reasoning capabilities.

</details>


### [393] [Infinite Video Understanding](https://arxiv.org/abs/2507.09068)
*Dell Zhang,Xiangyu Chen,Jixiang Luo,Mengxi Jia,Changzhi Sun,Ruilong Ren,Jingren Liu,Hao Sun,Xuelong Li*

Key words: 无限视频理解；多模态；长视频；AI；流式架构

TL;DR: 当前大语言模型及其多模态扩展在视频理解方面取得显著进展，但仍面临长时间视频内容的处理和理解的挑战，提出‘无限视频理解’作为新的研究目标。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决当前模型在长时间视频内容处理中的计算、内存和时间一致性等挑战，推动多媒体和AI领域的创新。

Method: 提出‘无限视频理解’的概念，并围绕流式架构、持久记忆机制、分层自适应表示、事件中心推理和新型评估方法等方向展开研究。

Result: 明确了实现‘无限视频理解’的核心挑战和研究方向，为未来研究提供了框架。

Conclusion: ‘无限视频理解’是一个具有挑战性但充满前景的研究目标，将推动多媒体和AI领域的进一步发展。

Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.

</details>


### [394] [AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition](https://arxiv.org/abs/2507.09248)
*Varsha Devi,Amine Bohi,Pardeep Kumar*

Key words: 情感识别,上下文去偏,因果干预,注意力机制

TL;DR: AGCD-Net通过注意力引导的上下文去偏模型，结合Hybrid ConvNeXt和因果干预模块，有效减少情感识别中的上下文偏见。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统情感识别方法存在上下文偏见问题，AGCD-Net旨在解决这一问题以提高真实场景下的情感识别准确性。

Method: 采用Hybrid ConvNeXt作为编码器，结合空间变换网络和Squeeze-and-Excitation层，并设计注意力引导-因果干预模块（AG-CIM）。

Result: 在CAER-S数据集上表现优异，达到最先进水平。

Conclusion: AGCD-Net通过因果去偏技术显著提升了复杂场景中的情感识别性能。

Abstract: Context-aware emotion recognition (CAER) enhances affective computing in
real-world scenarios, but traditional methods often suffer from context
bias-spurious correlation between background context and emotion labels (e.g.
associating ``garden'' with ``happy''). In this paper, we propose
\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces
\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the
ConvNeXt backbone by integrating Spatial Transformer Network and
Squeeze-and-Excitation layers for enhanced feature recalibration. At the core
of AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),
which applies causal theory, perturbs context features, isolates spurious
correlations, and performs an attention-driven correction guided by face
features to mitigate context bias. Experimental results on the CAER-S dataset
demonstrate the effectiveness of AGCD-Net, achieving state-of-the-art
performance and highlighting the importance of causal debiasing for robust
emotion recognition in complex settings.

</details>


### [395] [Cross Knowledge Distillation between Artificial and Spiking Neural Networks](https://arxiv.org/abs/2507.09269)
*Shuhan Ye,Yuanbin Qian,Chong Wang,Sunqi Lin,Jiazhen Xu,Jiangbo Qian,Yuqi Li*

Key words: 脉冲神经网络（SNNs）, 知识蒸馏, 跨模态学习, 事件数据

TL;DR: 该论文提出了一种称为跨知识蒸馏（CKD）的方法，旨在通过结合RGB数据和人工神经网络（ANNs）的知识来提升脉冲神经网络（SNNs）在事件数据（DVS）上的性能，解决了跨模态和跨架构的挑战。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: SNNs在计算机视觉领域具有高生物可信度、事件驱动和节能优势，但受限于标注数据不足和架构不成熟，其性能不及ANNs。通过知识蒸馏结合RGB数据和ANNs的能力，可以提升SNNs的性能。

Method: 提出了跨知识蒸馏（CKD），利用语义相似性和滑动替换解决跨模态挑战，并通过间接分阶段知识蒸馏应对跨架构挑战。

Result: 在主流神经形态数据集（如N-Caltech101和CEP-DVS）上的实验表明，CKD优于当前最先进方法。

Conclusion: CKD方法有效提升了SNNs在事件数据上的性能，验证了其解决跨模态和跨架构问题的能力。

Abstract: Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in
computer vision domain due to their high biological plausibility, event-driven
characteristic and energy-saving efficiency. Still, limited annotated
event-based datasets and immature SNN architectures result in their performance
inferior to that of Artificial Neural Networks (ANNs). To enhance the
performance of SNNs on their optimal data format, DVS data, we explore using
RGB data and well-performing ANNs to implement knowledge distillation. In this
case, solving cross-modality and cross-architecture challenges is necessary. In
this paper, we propose cross knowledge distillation (CKD), which not only
leverages semantic similarity and sliding replacement to mitigate the
cross-modality challenge, but also uses an indirect phased knowledge
distillation to mitigate the cross-architecture challenge. We validated our
method on main-stream neuromorphic datasets, including N-Caltech101 and
CEP-DVS. The experimental results show that our method outperforms current
State-of-the-Art methods. The code will be available at
https://github.com/ShawnYE618/CKD

</details>


### [396] [ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation](https://arxiv.org/abs/2507.09299)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Key words: Vision Transformers, Prototypical Network, Few-shot Learning, Meta-learning

TL;DR: ViT-ProtoNet结合ViT-Small和原型网络，显著提升了少样本图像分类性能，尤其在5-shot设置下表现优于传统CNN方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 利用ViT的强大表征能力，优化少样本分类任务中的原型网络性能。

Method: 通过平均支持样本的token嵌入构建原型，在Mini-ImageNet等四个基准数据集上进行评估。

Result: 在5-shot准确率上提升3.2%，且特征可分性优于CNN方法，甚至与更复杂的Transformer方法竞争。

Conclusion: ViT-ProtoNet为基于Transformer的元学习设定了新基准。

Abstract: The remarkable representational power of Vision Transformers (ViTs) remains
underutilized in few-shot image classification. In this work, we introduce
ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical
Network framework. By averaging class conditional token embeddings from a
handful of support examples, ViT-ProtoNet constructs robust prototypes that
generalize to novel categories under 5-shot settings. We conduct an extensive
empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,
CUB-200, and CIFAR-FS, including overlapped support variants to assess
robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based
prototypical counterparts, achieving up to a 3.2\% improvement in 5-shot
accuracy and demonstrating superior feature separability in latent space.
Furthermore, it outperforms or is competitive with transformer-based
competitors using a more lightweight backbone. Comprehensive ablations examine
the impact of transformer depth, patch size, and fine-tuning strategy. To
foster reproducibility, we release code and pretrained weights. Our results
establish ViT-ProtoNet as a powerful, flexible approach for few-shot
classification and set a new baseline for transformer-based meta-learners.

</details>


### [397] [AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning](https://arxiv.org/abs/2507.09308)
*Zile Wang,Hao Yu,Jiabo Zhan,Chun Yuan*

Key words: RGBA图像,潜在扩散模型,VAE,透明图像生成

TL;DR: 论文提出了ALPHA基准和ALPHAVAE模型，用于生成透明或分层内容（RGBA图像），并显著提升了重建性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前潜在扩散模型在高保真RGB图像合成方面取得了显著成果，但透明或分层内容的生成由于缺乏大规模基准而未被充分探索。

Method: 提出了ALPHA基准，将标准RGB指标扩展到四通道图像，并设计了ALPHAVAE模型，通过结合多种损失函数训练统一的RGBA VAE。

Result: ALPHAVAE在仅使用8K图像训练的情况下，PSNR提升了4.9 dB，SSIM提高了3.2%，且在透明图像生成中表现优异。

Conclusion: ALPHAVAE在RGBA图像生成和重建方面优于现有方法，具有高效性和可扩展性。

Abstract: Recent advances in latent diffusion models have achieved remarkable results
in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress
and reconstruct pixel data at low computational cost. However, the generation
of transparent or layered content (RGBA image) remains largely unexplored, due
to the lack of large-scale benchmarks. In this work, we propose ALPHA, the
first comprehensive RGBA benchmark that adapts standard RGB metrics to
four-channel images via alpha blending over canonical backgrounds. We further
introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB
VAE by incorporating a dedicated alpha channel. The model is trained with a
composite objective that combines alpha-blended pixel reconstruction,
patch-level fidelity, perceptual consistency, and dual KL divergence
constraints to ensure latent fidelity across both RGB and alpha
representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used
by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase
in SSIM over LayerDiffuse in reconstruction. It also enables superior
transparent image generation when fine-tuned within a latent diffusion
framework. Our code, data, and models are released on
https://github.com/o0o0o00o0/AlphaVAE for reproducibility.

</details>


### [398] [Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data](https://arxiv.org/abs/2507.09420)
*Timothy Chase Jr,Karthik Dantu*

Key words: 天体表面特征, 实时跟踪, 轻量级神经网络, 域适应, 注意力对齐

TL;DR: 论文提出了一种新型的轻量级神经网络架构，用于实时检测和描述天体表面地形特征，解决了传统方法计算量大和训练数据稀缺的问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统的光度立体方法依赖于大量先验成像和离线处理，成本高、速度慢且泛化能力有限。学习型计算机视觉技术虽有潜力，但计算需求大且缺乏标注数据。

Method: 提出轻量级神经网络架构，结合改进的域适应方法和注意力对齐机制，实现实时检测和描述天体地形特征。

Result: 新系统在性能上优于现有最先进技术。

Conclusion: 该研究为天体表面地形特征的实时跟踪提供了一种高效解决方案。

Abstract: The detection and tracking of celestial surface terrain features are crucial
for autonomous spaceflight applications, including Terrain Relative Navigation
(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data
collection. Traditional photoclinometry-based pipelines often rely on extensive
a priori imaging and offline processing, constrained by the computational
limitations of radiation-hardened systems. While historically effective, these
approaches typically increase mission costs and duration, operate at low
processing rates, and have limited generalization. Recently, learning-based
computer vision has gained popularity to enhance spacecraft autonomy and
overcome these limitations. While promising, emerging techniques frequently
impose computational demands exceeding the capabilities of typical spacecraft
hardware for real-time operation and are further challenged by the scarcity of
labeled training data for diverse extraterrestrial environments. In this work,
we present novel formulations for in-situ landmark tracking via detection and
description. We utilize lightweight, computationally efficient neural network
architectures designed for real-time execution on current-generation spacecraft
flight processors. For landmark detection, we propose improved domain
adaptation methods that enable the identification of celestial terrain features
with distinct, cheaply acquired training data. Concurrently, for landmark
description, we introduce a novel attention alignment formulation that learns
robust feature representations that maintain correspondence despite significant
landmark viewpoint variations. Together, these contributions form a unified
system for landmark tracking that demonstrates superior performance compared to
existing state-of-the-art techniques.

</details>


### [399] [HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space](https://arxiv.org/abs/2507.09487)
*Changli Wang,Fang Yin,Jiafeng Liu,Rui Wu*

Key words: 视觉-语义层次结构, 双曲空间, HMID-Net, 掩码图像建模, 知识蒸馏

TL;DR: 提出了HMID-Net方法，结合掩码图像建模和知识蒸馏在双曲空间中训练高效模型，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探索如何在双曲空间中更高效地训练模型以捕捉和利用视觉-语义层次结构。

Method: HMID-Net结合了掩码图像建模（MIM）和知识蒸馏技术，并在双曲空间中设计了专用的蒸馏损失函数。

Result: 实验表明该方法在下游任务中表现优异，显著优于MERU和CLIP等模型。

Conclusion: 双曲空间中的MIM和知识蒸馏技术能实现与欧几里得空间相同的成功，且效率更高。

Abstract: Visual and semantic concepts are often structured in a hierarchical manner.
For instance, textual concept `cat' entails all images of cats. A recent study,
MERU, successfully adapts multimodal learning techniques from Euclidean space
to hyperbolic space, effectively capturing the visual-semantic hierarchy.
However, a critical question remains: how can we more efficiently train a model
to capture and leverage this hierarchy? In this paper, we propose the
\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel
and efficient method that integrates Masked Image Modeling (MIM) and knowledge
distillation techniques within hyperbolic space. To the best of our knowledge,
this is the first approach to leverage MIM and knowledge distillation in
hyperbolic space to train highly efficient models. In addition, we introduce a
distillation loss function specifically designed to facilitate effective
knowledge transfer in hyperbolic space. Our experiments demonstrate that MIM
and knowledge distillation techniques in hyperbolic space can achieve the same
remarkable success as in Euclidean space. Extensive evaluations show that our
method excels across a wide range of downstream tasks, significantly
outperforming existing models like MERU and CLIP in both image classification
and retrieval.

</details>


### [400] [SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification](https://arxiv.org/abs/2507.09492)
*Fuyin Ye,Erwen Yao,Jianyong Chen,Fengmei He,Junxiang Zhang,Lihao Ni*

Key words: 高光谱图像分类,张量分解,正则化,轻量级网络,PaviaU数据集

TL;DR: 该论文提出了一种自适应的张量正则化网络（SDTN）和轻量级的张量正则化网络（TRN），用于高光谱图像分类，解决了高维数据、谱-空间冗余和标签样本稀缺的问题，并在PaviaU数据集上验证了其优越性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统方法在处理高维高光谱图像时面临数据冗余、标签样本稀缺等问题，导致分类性能不佳。论文旨在通过动态调整张量秩和轻量化网络设计，提升分类精度和计算效率。

Method: 提出SDTN网络，结合张量分解和正则化机制动态调整张量秩；进一步提出TRN网络，利用SDTN提取的特征，设计轻量化多尺度谱-空间特征提取网络。

Result: 在PaviaU数据集上，该方法显著提升了分类精度，同时减少了模型参数量，优于现有技术。

Conclusion: SDTN和TRN的组合解决了高光谱图像分类中的关键挑战，适用于资源受限的实时部署场景。

Abstract: Hyperspectral image classification plays a pivotal role in precision
agriculture, providing accurate insights into crop health monitoring, disease
detection, and soil analysis. However, traditional methods struggle with
high-dimensional data, spectral-spatial redundancy, and the scarcity of labeled
samples, often leading to suboptimal performance. To address these challenges,
we propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines
tensor decomposition with regularization mechanisms to dynamically adjust
tensor ranks, ensuring optimal feature representation tailored to the
complexity of the data. Building upon SDTN, we propose the Tensor-Regularized
Network (TRN), which integrates the features extracted by SDTN into a
lightweight network capable of capturing spectral-spatial features at multiple
scales. This approach not only maintains high classification accuracy but also
significantly reduces computational complexity, making the framework highly
suitable for real-time deployment in resource-constrained environments.
Experiments on PaviaU datasets demonstrate significant improvements in accuracy
and reduced model parameters compared to state-of-the-art methods.

</details>


### [401] [QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models](https://arxiv.org/abs/2507.09514)
*Tien-Yu Chi,Hung-Yueh Chiang,Diana Marculescu,Kai-Chiang Wu*

Key words: 状态空间模型, 激活剪枝, 后训练优化, 计算效率, 视觉骨干网络

TL;DR: QuarterMap是一种针对状态空间模型（SSMs）的后训练激活剪枝方法，通过去除空间冗余并恢复维度来提升效率，无需重新训练。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 为了解决VMamba等基于SSM的视觉骨干网络中空间冗余导致的效率瓶颈问题，提出一种无需重新训练的高效解决方案。

Method: 在后训练阶段，通过剪除冗余的空间激活，并使用最近邻上采样恢复维度，从而减少计算量。

Result: 在ImageNet-1K上，QuarterMap实现了11%的速度提升且精度下降小于0.9%；在ADE20K分割和医疗成像任务中也表现优异。

Conclusion: QuarterMap为SSM提供了一种即插即用的高效部署工具，无需牺牲模型的可迁移性。

Abstract: State space models (SSMs) reduce the quadratic complexity of transformers by
leveraging linear recurrence. Recently, VMamba has emerged as a strong
SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in
its four-directional scan. We propose QuarterMap, a post-training activation
pruning method that removes redundant spatial activations before scanning and
restores dimensions via nearest-neighbor upsampling. Our method improves
throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%
speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains
on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a
domain-specific model that shares the same four-directional scanning structure,
where it consistently improves throughput while preserving accuracy across
multiple medical imaging tasks. Compared to token merging methods like ToMe,
QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our
method offers a plug-and-play tool for deployment-time efficiency without
compromising transferability.

</details>


### [402] [VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization](https://arxiv.org/abs/2507.09531)
*Son Nguyen,Giang Nguyen,Hung Dao,Thao Do,Daeyoung Kim*

Key words: Key Information Extraction, multimodal large language models, content-aware tokenization, document understanding

TL;DR: VDInstruct通过内容感知的分词策略和显式布局建模，提高了密集文档的信息提取效率，并在KIE基准测试中取得了SOTA结果。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决现有MLLMs在密集文档上表现不佳以及视觉分词方法带来的计算冗余和内存效率低下的问题。

Method: 采用内容感知的分词策略，根据文档复杂度生成token，并结合三阶段训练范式。

Result: 在KIE基准测试中达到SOTA，减少了约3.6倍的图像token，并在零样本评估中显著优于基线方法。

Conclusion: 内容感知分词与显式布局建模为文档理解提供了有前景的方向。

Abstract: Key Information Extraction (KIE) underpins the understanding of visual
documents (e.g., receipts and contracts) by extracting precise semantic content
and accurately capturing spatial structure. Yet existing multimodal large
language models (MLLMs) often perform poorly on dense documents and rely on
vision tokenization approaches that scale with image size, leading to redundant
computation and memory inefficiency. To address these challenges, we introduce
VDInstruct, an MLLM that separates spatial region detection from semantic
feature extraction. Central to our model is a content-aware tokenization
strategy: rather than fragmenting the entire image uniformly, it generates
tokens in proportion to document complexity, preserving critical structure
while eliminating wasted tokens. Leveraging a three-stage training paradigm,
our model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching
or exceeding the accuracy of leading approaches while reducing the number of
image tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses
strong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its
robustness to unseen documents. These findings show that content-aware
tokenization combined with explicit layout modeling offers a promising
direction forward for document understanding. Data, source code, and model
weights will be made publicly available.

</details>


### [403] [Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges](https://arxiv.org/abs/2507.09562)
*Yidong Jiang*

Key words: Segment Anything Model, 提示工程, 图像分割, 多模态, 应用领域

TL;DR: 本文首次全面调查了针对Segment Anything Model（SAM）及其变体的提示工程技术，系统地组织和分析了这一新兴领域的快速发展的研究。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 尽管基于提示的方法在SAM的成功中发挥了关键作用，但提示工程的重要性尚未得到充分探索。本文旨在填补这一空白。

Method: 通过系统梳理和分类现有研究，涵盖了从基础的几何输入到多模态方法的各种提示工程技术及其实际应用。

Result: 研究发现提示工程已从简单几何输入发展为复杂多模态方法，使SAM能够广泛应用于医疗影像和遥感等领域。

Conclusion: 本文提出了一个结构化框架，为理解和推进分割基础模型中的提示工程提供了重要参考，并指出了未来的研究方向。

Abstract: The Segment Anything Model (SAM) has revolutionized image segmentation
through its innovative prompt-based approach, yet the critical role of prompt
engineering in its success remains underexplored. This paper presents the first
comprehensive survey focusing specifically on prompt engineering techniques for
SAM and its variants. We systematically organize and analyze the rapidly
growing body of work in this emerging field, covering fundamental
methodologies, practical applications, and key challenges. Our review reveals
how prompt engineering has evolved from simple geometric inputs to
sophisticated multimodal approaches, enabling SAM's adaptation across diverse
domains including medical imaging and remote sensing. We identify unique
challenges in prompt optimization and discuss promising research directions.
This survey fills an important gap in the literature by providing a structured
framework for understanding and advancing prompt engineering in foundation
models for segmentation.

</details>


### [404] [Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI](https://arxiv.org/abs/2507.09630)
*Shomukh Qari,Maha A. Thafar*

Key words: 脑卒中分类, CT扫描, MaxViT, 可解释AI, 数据增强

TL;DR: 该研究提出了一种基于AI的多类脑卒中分类框架，使用CT扫描图像结合MaxViT等Transformer模型，通过数据增强和XAI技术实现高精度和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 早期和准确诊断脑卒中对改善患者预后至关重要，尤其是在紧急情况下。研究旨在开发高效、可信的AI辅助诊断工具。

Method: 采用MaxViT作为主要深度学习模型，辅以其他Transformer变体和数据增强技术，集成Grad-CAM++提供可视化解释。

Result: MaxViT模型在数据增强后达到98.00%的准确率和F1分数，优于其他模型。

Conclusion: 该研究为临床实践提供了一种高精度、可解释的AI辅助诊断工具，有助于提升紧急情况下的脑卒中诊断效率。

Abstract: Stroke is one of the leading causes of death globally, making early and
accurate diagnosis essential for improving patient outcomes, particularly in
emergency settings where timely intervention is critical. CT scans are the key
imaging modality because of their speed, accessibility, and cost-effectiveness.
This study proposed an artificial intelligence framework for multiclass stroke
classification (ischemic, hemorrhagic, and no stroke) using CT scan images from
a dataset provided by the Republic of Turkey's Ministry of Health. The proposed
method adopted MaxViT, a state-of-the-art Vision Transformer, as the primary
deep learning model for image-based stroke classification, with additional
transformer variants (vision transformer, transformer-in-transformer, and
ConvNext). To enhance model generalization and address class imbalance, we
applied data augmentation techniques, including synthetic image generation. The
MaxViT model trained with augmentation achieved the best performance, reaching
an accuracy and F1-score of 98.00%, outperforming all other evaluated models
and the baseline methods. The primary goal of this study was to distinguish
between stroke types with high accuracy while addressing crucial issues of
transparency and trust in artificial intelligence models. To achieve this,
Explainable Artificial Intelligence (XAI) was integrated into the framework,
particularly Grad-CAM++. It provides visual explanations of the model's
decisions by highlighting relevant stroke regions in the CT scans and
establishing an accurate, interpretable, and clinically applicable solution for
early stroke detection. This research contributed to the development of a
trustworthy AI-assisted diagnostic tool for stroke, facilitating its
integration into clinical practice and enhancing access to timely and optimal
stroke diagnosis in emergency departments, thereby saving more lives.

</details>


### [405] [View Invariant Learning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2507.08831)
*Josh Qixuan Sun,Xiaoying Xing,Huaiyuan Weng,Chul Min Yeum,Mark Crowley*

Key words: VLNCE, V2-VLNCE, 视角不变学习, 对比学习, 师生框架

TL;DR: 提出了V2-VLNCE（带变化视角的连续环境视觉语言导航）问题和VIL（视角不变学习）方法，通过对比学习和师生框架提升导航策略的视角不变性，取得了SOTA性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有导航策略对视角变化敏感，需要提升其鲁棒性。

Method: 提出VIL方法，结合对比学习和师生框架，学习稀疏且视角不变的特征，并采用端到端训练优化。

Result: 在V2-VLNCE上优于SOTA方法8-15%，在标准VLNCE设置下仍表现良好，RxR-CE数据集上全面领先。

Conclusion: VIL方法在不影响标准视角性能的前提下，可作为即插即用的后训练策略。

Abstract: Vision-Language Navigation in Continuous Environments (VLNCE), where an agent
follows instructions and moves freely to reach a destination, is a key research
problem in embodied AI. However, most navigation policies are sensitive to
viewpoint changes, i.e., variations in camera height and viewing angle that
alter the agent's observation. In this paper, we introduce a generalized
scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View
Invariant Learning), a view-invariant post-training strategy that enhances the
robustness of existing navigation policies to changes in camera viewpoint. VIL
employs a contrastive learning framework to learn sparse and view-invariant
features. Additionally, we introduce a teacher-student framework for the
Waypoint Predictor Module, a core component of most VLNCE baselines, where a
view-dependent teacher model distills knowledge into a view-invariant student
model. We employ an end-to-end training paradigm to jointly optimize these
components, thus eliminating the cost for individual module training. Empirical
results show that our method outperforms state-of-the-art approaches on
V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets
R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE
setting and find that, despite being trained for varied viewpoints, it often
still improves performance. On the more challenging RxR-CE dataset, our method
also achieved state-of-the-art performance across all metrics when compared to
other map-free methods. This suggests that adding VIL does not diminish the
standard viewpoint performance and can serve as a plug-and-play post-training
method.

</details>


### [406] [A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends](https://arxiv.org/abs/2507.09861)
*Yihao Ding,Siwen Luo,Yue Dai,Yanbei Jiang,Zechuan Li,Geoffrey Martin,Yifan Peng*

Key words: 视觉丰富文档理解, 多模态大语言模型, OCR, 特征融合, 训练范式

TL;DR: 该调查综述了基于多模态大语言模型（MLLMs）的视觉丰富文档理解（VRDU）的进展，涵盖特征编码、训练范式及数据集，并探讨了未来方向与挑战。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: VRDU的重要性日益凸显，而MLLMs在结合OCR依赖与非OCR框架处理文档方面展现出潜力，推动了该领域的综述需求。

Method: 重点分析了三种核心内容：文本、视觉和布局特征的编码与融合方法；训练范式（预训练、指令调优等）；以及相关数据集。

Result: 总结了MLLMs在VRDU中的关键技术和应用，并指出当前研究的局限性。

Conclusion: 未来需提升VRDU系统的效率、泛化性和鲁棒性，提出了进一步研究方向。

Abstract: Visually-Rich Document Understanding (VRDU) has emerged as a critical field,
driven by the need to automatically process documents containing complex
visual, textual, and layout information. Recently, Multimodal Large Language
Models (MLLMs) have shown remarkable potential in this domain, leveraging both
Optical Character Recognition (OCR)-dependent and OCR-free frameworks to
extract and interpret information in document images. This survey reviews
recent advancements in MLLM-based VRDU, highlighting three core components: (1)
methods for encoding and fusing textual, visual, and layout features; (2)
training paradigms, including pretraining strategies, instruction-response
tuning, and the trainability of different model modules; and (3) datasets
utilized for pretraining, instruction-tuning, and supervised fine-tuning.
Finally, we discuss the challenges and opportunities in this evolving field and
propose future directions to advance the efficiency, generalizability, and
robustness of VRDU systems.

</details>


### [407] [PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection](https://arxiv.org/abs/2507.08979)
*Mahdiyar Molahasani,Azadeh Motamedi,Michael Greenspan,Il-Min Kim,Ali Etemad*

Key words: PRISM, 视觉语言模型, 去偏, 伪相关性, CLIP

TL;DR: PRISM是一种无需数据和任务无关的视觉语言模型（VLM）去偏方法，通过生成场景描述并学习投影来减少伪相关性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决视觉语言模型（如CLIP）因训练数据中的偏见而导致的预测偏差问题。

Method: 1. 使用LLM生成包含伪相关性的场景描述；2. 通过对比式去偏损失学习投影，减少伪相关性并保留图像与文本嵌入的对齐。

Result: 在Waterbirds和CelebA数据集上优于现有去偏方法。

Conclusion: PRISM是一种高效且无需外部数据的去偏方法，效果好。

Abstract: We introduce Projection-based Reduction of Implicit Spurious bias in
vision-language Models (PRISM), a new data-free and task-agnostic solution for
bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in
their training data, leading to skewed predictions. PRISM is designed to debias
VLMs without relying on predefined bias categories or additional external data.
It operates in two stages: first, an LLM is prompted with simple class prompts
to generate scene descriptions that contain spurious correlations. Next, PRISM
uses our novel contrastive-style debiasing loss to learn a projection that maps
the embeddings onto a latent space that minimizes spurious correlations while
preserving the alignment between image and text embeddings.Extensive
experiments demonstrate that PRISM outperforms current debiasing methods on the
commonly used Waterbirds and CelebA datasets We make our code public at:
https://github.com/MahdiyarMM/PRISM.

</details>


### [408] [Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?](https://arxiv.org/abs/2507.09052)
*Fang Chen,Alex Villa,Gongbo Liang,Xiaoyi Lu,Meng Tang*

Key words: 扩散模型、长尾分布、对比学习、多样性提升

TL;DR: 提出了两种对比损失函数，用于解决长尾分布数据中尾部类图像的多样性问题，提升扩散模型的生成多样性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 长尾分布数据中尾部类样本不足导致图像合成多样性下降，需要提升尾部类多样性同时不损害头部类质量。

Method: 使用无监督InfoNCE损失和基于MSE的对比损失，增强尾部类多样性；提出条件与无条件生成对齐策略。

Result: 在多个长尾数据集（如CIFAR10/100-LT等）上表现优于标准DDPM和其他方法。

Conclusion: 通过对比学习框架有效提升长尾数据扩散模型的尾部类多样性，方法简单且通用。

Abstract: Training data for class-conditional image synthesis often exhibit a
long-tailed distribution with limited images for tail classes. Such an
imbalance causes mode collapse and reduces the diversity of synthesized images
for tail classes. For class-conditional diffusion models trained on imbalanced
data, we aim to improve the diversity of tail class images without compromising
the fidelity and diversity of head class images. We achieve this by introducing
two deceptively simple but highly effective contrastive loss functions.
Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to
increase the distance/dissimilarity among synthetic images, particularly for
tail classes. To further enhance the diversity of tail classes, our second loss
is an MSE loss that contrasts class-conditional generation with unconditional
generation at large timesteps. This second loss makes the denoising process
insensitive to class conditions for the initial steps, which enriches tail
classes through knowledge sharing from head classes. Conditional-unconditional
alignment has been shown to enhance the performance of long-tailed GAN. We are
the first to adapt such alignment to diffusion models. We successfully
leveraged contrastive learning for class-imbalanced diffusion models. Our
contrastive learning framework is easy to implement and outperforms standard
DDPM and alternative methods for class-imbalanced diffusion models across
various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and
ImageNetLT.

</details>


### [409] [MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks](https://arxiv.org/abs/2507.09092)
*Ram S Iyer,Narayan S Iyer,Rugmini Ammal P*

Key words: MI CAM, 卷积神经网络, 解释性, 显著图, 互信息

TL;DR: 本文提出了一种名为MI CAM的新型后处理视觉解释方法，基于激活映射和互信息权重，生成显著图，并提供因果解释。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 随着机器视觉在医疗和自动化电厂等关键领域的应用，理解和解释卷积神经网络的内部机制变得尤为重要。

Method: MI CAM通过计算每个特征图与输入图像的互信息进行加权，再通过权重与激活映射的线性组合生成显著图，并通过反事实分析验证因果解释。

Result: MI CAM在定性和定量指标上表现与现有最佳方法相当，甚至在某些方面更优。

Conclusion: MI CAM提供了一种有效的视觉解释方法，能够生成无偏见的模型推理过程解释。

Abstract: With the intervention of machine vision in our crucial day to day necessities
including healthcare and automated power plants, attention has been drawn to
the internal mechanisms of convolutional neural networks, and the reason why
the network provides specific inferences. This paper proposes a novel post-hoc
visual explanation method called MI CAM based on activation mapping. Differing
from previous class activation mapping based approaches, MI CAM produces
saliency visualizations by weighing each feature map through its mutual
information with the input image and the final result is generated by a linear
combination of weights and activation maps. It also adheres to producing causal
interpretations as validated with the help of counterfactual analysis. We aim
to exhibit the visual performance and unbiased justifications for the model
inferencing procedure achieved by MI CAM. Our approach works at par with all
state-of-the-art methods but particularly outperforms some in terms of
qualitative and quantitative measures. The implementation of proposed method
can be found on https://anonymous.4open.science/r/MI-CAM-4D27

</details>


### [410] [Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis](https://arxiv.org/abs/2507.09950)
*Shubham Shukla,Kunal Sonalkar*

Key words: 时尚零售, 属性识别, 大型语言模型, 零样本学习, DeepFashion-MultiModal

TL;DR: 该论文研究了大型语言模型（LLMs）在细粒度时尚属性识别中的零样本性能，比较了GPT-4o-mini和Gemini 2.0 Flash的表现，发现后者在F1分数上表现更优。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探索LLMs在时尚零售中产品属性识别的潜力，以提升客户发现体验和产品目录组织效率。

Method: 使用DeepFashion-MultiModal数据集，仅以图像为输入，评估两款LLMs在18类时尚属性上的零样本性能。

Result: Gemini 2.0 Flash的宏观F1分数为56.79%，优于GPT-4o-mini的43.28%，表明其在属性识别中的优势。

Conclusion: LLMs在时尚属性识别中表现良好，但需领域特定微调；Gemini 2.0 Flash更适合电商任务。

Abstract: The fashion retail business is centered around the capacity to comprehend
products. Product attribution helps in comprehending products depending on the
business process. Quality attribution improves the customer experience as they
navigate through millions of products offered by a retail website. It leads to
well-organized product catalogs. In the end, product attribution directly
impacts the 'discovery experience' of the customer. Although large language
models (LLMs) have shown remarkable capabilities in understanding multimodal
data, their performance on fine-grained fashion attribute recognition remains
under-explored. This paper presents a zero-shot evaluation of state-of-the-art
LLMs that balance performance with speed and cost efficiency, mainly
GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset
DeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to
evaluate these models in the attribution tasks of fashion products. Our study
evaluates these models across 18 categories of fashion attributes, offering
insight into where these models excel. We only use images as the sole input for
product information to create a constrained environment. Our analysis shows
that Gemini 2.0 Flash demonstrates the strongest overall performance with a
macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a
macro F1 score of 43.28%. Through detailed error analysis, our findings provide
practical insights for deploying these LLMs in production e-commerce product
attribution-related tasks and highlight the need for domain-specific
fine-tuning approaches. This work also lays the groundwork for future research
in fashion AI and multimodal attribute extraction.

</details>


### [411] [Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning](https://arxiv.org/abs/2507.09118)
*Linlan Huang,Xusheng Cao,Haori Lu,Yifan Meng,Fei Yang,Xialei Liu*

Key words: 持续学习、CLIP模型、模态间隙、类增量学习

TL;DR: 本文提出了一种基于模态间隙的持续学习方法MG-CLIP，通过分析CLIP模型在微调过程中的模态间隙变化，利用模态间隙保持和补偿技术提升其在新数据学习时的性能并减少遗忘。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: CLIP模型在多任务中表现出色，但在持续学习中其模态间隙问题常被忽视。本文旨在通过模态间隙分析提升CLIP在持续学习中的表现。

Method: 提出MG-CLIP方法，通过模态间隙保持减少遗忘，通过模态间隙补偿增强新数据学习能力。

Result: 在多个基准测试中，MG-CLIP表现优于现有方法，且无需额外的回放数据。

Conclusion: 模态间隙是CLIP模型中持续学习的关键因素，MG-CLIP方法为未来研究提供了新方向。

Abstract: Continual learning aims to enable models to learn sequentially from
continuously incoming data while retaining performance on previously learned
tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting
strong capabilities across various downstream tasks, there has been growing
interest in leveraging CLIP for continual learning in such scenarios. Most
existing works overlook the inherent modality gap in CLIP, a key factor in its
generalization and adaptability. In this paper, we analyze the variations in
the modality gap during the fine-tuning of vision-language pre-trained models.
Our observations reveal that the modality gap effectively reflects the extent
to which pre-trained knowledge is preserved. Based on these insights, we
propose a simple yet effective method, MG-CLIP, that improves CLIP's
performance in class-incremental learning. Our approach leverages modality gap
preservation to mitigate forgetting and modality gap compensation to enhance
the capacity for new data, introducing a novel modality-gap-based perspective
for continual learning. Extensive experiments on multiple benchmarks
demonstrate that our method outperforms existing approaches without requiring
additional replay data. Our code is available at
https://github.com/linlany/MindtheGap.

</details>


### [412] [(Almost) Free Modality Stitching of Foundation Models](https://arxiv.org/abs/2507.10015)
*Jaisidh Singh,Diganta Misra,Boris Knyazev,Antonio Orvieto*

Key words: 多模态模型,超网络,模型对齐,连接器训练

TL;DR: 论文提出Hypernetwork Model Alignment (Hyma)，一种利用超网络实现最优单模态模型选择和连接器训练的一体化解决方案，显著降低了搜索成本。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的多模态基础模型通常通过拼接单模态模型实现，但模型选择和连接器训练在大规模数据集上计算成本高昂，需要解决这一问题。

Method: 利用超网络的参数预测能力，为N×M种单模态模型组合联合训练连接器模块。

Result: Hyma将最优单模态模型对搜索成本降低10倍，同时在多个多模态基准测试中表现与网格搜索相当。

Conclusion: Hyma是一种高效且性能优异的解决方案，适用于多模态基础模型的设计。

Abstract: Foundation multi-modal models are often designed by stitching of multiple
existing pretrained uni-modal models: for example, an image classifier with an
autoregressive text model. This stitching process is performed by training a
connector module that aims to align the representation-representation or
representation-input spaces of these uni-modal models. However, given the
complexity of training such connectors on large scale web-based datasets
coupled with the ever-increasing number of available pretrained uni-modal
models, the task of uni-modal models selection and subsequent connector module
training becomes computationally demanding. To address this under-studied
critical problem, we propose Hypernetwork Model Alignment (Hyma), a novel
all-in-one solution for optimal uni-modal model selection and connector
training by leveraging hypernetworks. Specifically, our framework utilizes the
parameter prediction capability of a hypernetwork to obtain jointly trained
connector modules for $N \times M$ combinations of uni-modal models. In our
experiments, Hyma reduces the optimal uni-modal model pair search cost by
$10\times$ (averaged across all experiments), while matching the ranking and
trained connector performance obtained via grid search across a suite of
diverse multi-modal benchmarks.

</details>


### [413] [Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning](https://arxiv.org/abs/2507.10056)
*A. K. M. Shoriful Islam,Md. Rakib Hassan,Macbah Uddin,Md. Shahidur Rahman*

Key words: 家禽疾病检测, 机器学习, 图像分析, 资源高效

TL;DR: 该研究提出一种轻量级机器学习方法，通过分析家禽粪便图像检测疾病，实现了高准确性且低资源消耗。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 家禽养殖是全球食品供应链的重要组成部分，但易受疾病影响，需高效低成本检测方法。

Method: 利用多色空间特征提取和多种描述符，结合PCA和XGBoost降维，训练ANN分类器。

Result: ANN分类器达到95.85%准确率，无需GPU，执行时间仅638秒。

Conclusion: 该方法为低资源农业环境提供了一种经济高效、可解释的疾病检测替代方案。

Abstract: Poultry farming is a vital component of the global food supply chain, yet it
remains highly vulnerable to infectious diseases such as coccidiosis,
salmonellosis, and Newcastle disease. This study proposes a lightweight machine
learning-based approach to detect these diseases by analyzing poultry fecal
images. We utilize multi-color space feature extraction (RGB, HSV, LAB) and
explore a wide range of color, texture, and shape-based descriptors, including
color histograms, local binary patterns (LBP), wavelet transforms, and edge
detectors. Through a systematic ablation study and dimensionality reduction
using PCA and XGBoost feature selection, we identify a compact global feature
set that balances accuracy and computational efficiency. An artificial neural
network (ANN) classifier trained on these features achieved 95.85% accuracy
while requiring no GPU and only 638 seconds of execution time in Google Colab.
Compared to deep learning models such as Xception and MobileNetV3, our proposed
model offers comparable accuracy with drastically lower resource usage. This
work demonstrates a cost-effective, interpretable, and scalable alternative to
deep learning for real-time poultry disease detection in low-resource
agricultural settings.

</details>


### [414] [Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift](https://arxiv.org/abs/2507.09222)
*Behraj Khan,Tahir Syed*

Key words: Foundation models, distribution shift, confidence misalignment, Fisher information penalty, confidence misalignment penalty, StaRFM

TL;DR: 论文提出了一种统一的框架StaRFM，解决了基础模型在部署时遇到的数据分布偏移和置信度不对齐问题，通过Fisher信息惩罚和置信度对齐惩罚，在多个任务和数据集上表现出色。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 基础模型（如CLIP和SAM）在低样本迁移学习中表现出色，但在部署时面临两个主要挑战：训练与测试数据之间的分布偏移和导致错误预测的置信度不对齐。这些问题在不同任务中表现不同，但现有解决方案仍局限于特定领域。

Method: 提出统一框架StaRFM，引入Fisher信息惩罚（FIP）减少分布偏移，并通过置信度对齐惩罚（CMP）校准分割任务中的不确定性。FIP通过Fisher-Rao范数控制泛化，CMP通过Brier分数优化最小化校准误差。

Result: StaRFM在19个视觉数据集上表现出色（如准确率提升3.5%，ECE降低28%），在医学分割任务中（如BraTS、ATLAS）达到84.7% DSC和4.8mm HD95，跨域性能差距降低40%。

Conclusion: StaRFM是一种即插即用的框架，能够无缝集成到基础模型中，显著提升性能并解决关键挑战。

Abstract: Foundation models like CLIP and SAM have transformed computer vision and
medical imaging via low-shot transfer learning. However, deployment of these
models hindered by two key challenges: \textit{distribution shift} between
training and test data, and \textit{confidence misalignment} that leads to
overconfident incorrect predictions. These issues manifest differently in
vision-language classification and medical segmentation tasks, yet existing
solutions remain domain-specific. We propose \textit{StaRFM}, a unified
framework addressing both challenges. It introduces a Fisher information
penalty (FIP), extended to 3D medical data via patch-wise regularization, to
reduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence
misalignment penalty (CMP), reformulated for voxel-level predictions,
calibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes
bounds showing FIP controls generalization via the Fisher-Rao norm, while CMP
minimizes calibration error through Brier score optimization. StaRFM shows
consistent performance like \texttt{+}3.5\% accuracy and 28\% lower ECE on 19
vision datasets (e.g., ImageNet, Office-Home), 84.7\% DSC and 4.8mm HD95 in
medical segmentation (e.g., BraTS, ATLAS), and 40\% lower cross-domain
performance gap compared to prior benchmarking methods. The framework is
plug-and-play, requiring minimal architectural changes for seamless integration
with foundation models. Code and models will be released at
https://anonymous.4open.science/r/StaRFM-C0CD/README.md

</details>


### [415] [Supercharging Floorplan Localization with Semantic Rays](https://arxiv.org/abs/2507.09291)
*Yuval Grader,Hadar Averbuch-Elor*

Key words: 地板图定位, 语义感知, 概率体积, 深度估计, 语义光线

TL;DR: 该论文提出了一种语义感知的定位框架，通过联合估计深度和语义光线，构建结构-语义概率体积，显著提升了地板图定位的准确性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前的地板图定位技术主要依赖基于深度的结构信息，忽略了地板图中丰富的语义信息，如窗户和门的位置。

Method: 提出了一种从粗到细的概率体积构建方法，先通过少量光线采样获得低分辨率概率体积，再在概率高的区域进行密集采样以优化预测。

Result: 在两个标准地板图定位基准上，该方法显著优于现有技术，召回率显著提升，并能轻松整合额外元数据如房间标签。

Conclusion: 该方法通过结合深度和语义信息，显著提升了地板图定位的精度和效率。

Abstract: Floorplans provide a compact representation of the building's structure,
revealing not only layout information but also detailed semantics such as the
locations of windows and doors. However, contemporary floorplan localization
techniques mostly focus on matching depth-based structural cues, ignoring the
rich semantics communicated within floorplans. In this work, we introduce a
semantic-aware localization framework that jointly estimates depth and semantic
rays, consolidating over both for predicting a structural-semantic probability
volume. Our probability volume is constructed in a coarse-to-fine manner: We
first sample a small set of rays to obtain an initial low-resolution
probability volume. We then refine these probabilities by performing a denser
sampling only in high-probability regions and process the refined values for
predicting a 2D location and orientation angle. We conduct an evaluation on two
standard floorplan localization benchmarks. Our experiments demonstrate that
our approach substantially outperforms state-of-the-art methods, achieving
significant improvements in recall metrics compared to prior works. Moreover,
we show that our framework can easily incorporate additional metadata such as
room labels, enabling additional gains in both accuracy and efficiency.

</details>


### [416] [Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion](https://arxiv.org/abs/2507.10127)
*Md Abulkalam Azad,John Nyberg,Håvard Dalen,Bjørnar Grenne,Lasse Lovstakken,Andreas Østvik*

Key words: 超声心动图,点跟踪,运动估计,心脏功能,轻量级网络

TL;DR: 论文探讨了超声心动图中先进点跟踪方法的潜力，发现现有方法因方向性运动偏差而受限，并提出改进策略和轻量级网络以提升跟踪性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 准确跟踪超声心动图中可变形组织的运动对心脏功能测量至关重要，但传统方法和现代点跟踪方法在该领域表现不佳。

Method: 通过分析心脏运动偏差，改进了训练策略并引入定制增强方法，提出了一种轻量级网络。

Result: 实验显示，改进策略显著提升了模型性能，EchoTracker在整体位置准确性上提高了60.7%。

Conclusion: 改进的点跟踪方法在临床评估中提升了GLS测量，与专家验证的工具更接近，显示出更好的实际应用潜力。

Abstract: Accurate motion estimation for tracking deformable tissues in
echocardiography is essential for precise cardiac function measurements. While
traditional methods like block matching or optical flow struggle with intricate
cardiac motion, modern point tracking approaches remain largely underexplored
in this domain. This work investigates the potential of state-of-the-art (SOTA)
point tracking methods for ultrasound, with a focus on echocardiography.
Although these novel approaches demonstrate strong performance in general
videos, their effectiveness and generalizability in echocardiography remain
limited. By analyzing cardiac motion throughout the heart cycle in real B-mode
ultrasound videos, we identify that a directional motion bias across different
views is affecting the existing training strategies. To mitigate this, we
refine the training procedure and incorporate a set of tailored augmentations
to reduce the bias and enhance tracking robustness and generalization through
impartial cardiac motion. We also propose a lightweight network leveraging
multi-scale cost volumes from spatial context alone to challenge the advanced
spatiotemporal point tracking models. Experiments demonstrate that fine-tuning
with our strategies significantly improves models' performances over their
baselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker
boosts overall position accuracy by 60.7% and reduces median trajectory error
by 61.5% across heart cycle phases. Interestingly, several point tracking
models fail to outperform our proposed simple model in terms of tracking
accuracy and generalization, reflecting their limitations when applied to
echocardiography. Nevertheless, clinical evaluation reveals that these methods
improve GLS measurements, aligning more closely with expert-validated,
semi-automated tools and thus demonstrating better reproducibility in
real-world applications.

</details>


### [417] [DAA*: Deep Angular A Star for Image-based Path Planning](https://arxiv.org/abs/2507.09305)
*Zhiwei Xu*

Key words: 路径模仿学习, 路径平滑性, 深度角度A*, PAF, 路径最优性

TL;DR: 论文提出了一种称为深度角度A*（DAA*）的新方法，通过引入路径角度自由度（PAF）来提升路径平滑性，从而改善路径模仿学习的效果。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 路径平滑性在专家示范的路径模仿学习中常被忽视，本文旨在通过自适应路径平滑性提升路径相似性。

Method: 结合路径角度自由度（PAF）到A*算法中，通过优化路径缩短和平滑性来提升路径最优性，其中PAF探索移动角度对路径节点扩展的影响。

Result: 在7个数据集上的评估显示，DAA*在路径相似性和路径长度方面显著优于神经A*和TransPath，分别提升了9.0% SPR、6.9% ASIM和3.9% PSIM。

Conclusion: DAA*通过联合优化路径缩短和平滑性，显著提升了路径模仿学习的性能，仅在路径最优性和搜索效率之间存在微小权衡。

Abstract: Path smoothness is often overlooked in path imitation learning from expert
demonstrations. In this paper, we introduce a novel learning method, termed
deep angular A* (DAA*), by incorporating the proposed path angular freedom
(PAF) into A* to improve path similarity through adaptive path smoothness. The
PAF aims to explore the effect of move angles on path node expansion by finding
the trade-off between their minimum and maximum values, allowing for high
adaptiveness for imitation learning. DAA* improves path optimality by closely
aligning with the reference path through joint optimization of path shortening
and smoothing, which correspond to heuristic distance and PAF, respectively.
Throughout comprehensive evaluations on 7 datasets, including 4 maze datasets,
2 video-game datasets, and a real-world drone-view dataset containing 2
scenarios, we demonstrate remarkable improvements of our DAA* over neural A* in
path similarity between the predicted and reference paths with a shorter path
length when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,
and 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path
loss and path probability map loss, DAA* significantly outperforms the
state-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also
discuss the minor trade-off between path optimality and search efficiency where
applicable.

</details>


### [418] [GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups](https://arxiv.org/abs/2507.09410)
*Bernie Boscoe,Shawn Johnson,Andrea Osborn,Chandler Campbell,Karen Mager*

Key words: 相机陷阱, ML/AI, 数据处理, 低资源, 研究工具

TL;DR: 论文提出了一种低资源处理相机陷阱数据的流程，结合了ML/AI能力，适用于资源有限的小型研究团队。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 相机陷阱数据处理的挑战包括数据量大、标注准确性、环境条件多变以及ML/AI工具的集成困难，本文旨在为资源有限的研究团队提供解决方案。

Method: 开发了一种低资源处理流程，支持本地数据管理，结合ML/AI功能，并针对数据传输、推理和评估提供实用方法。

Result: 流程提供了高效的数据处理方式，帮助研究人员从大规模相机陷阱数据中提取有价值的信息。

Conclusion: 该流程为资源有限的研究团队提供了一个可行的解决方案，提升了数据处理效率和实用性。

Abstract: Camera traps have long been used by wildlife researchers to monitor and study
animal behavior, population dynamics, habitat use, and species diversity in a
non-invasive and efficient manner. While data collection from the field has
increased with new tools and capabilities, methods to develop, process, and
manage the data, especially the adoption of ML/AI tools, remain challenging.
These challenges include the sheer volume of data generated, the need for
accurate labeling and annotation, variability in environmental conditions
affecting data quality, and the integration of ML/AI tools into existing
workflows that often require domain-specific customization and computational
resources. This paper provides a guide to a low-resource pipeline to process
camera trap data on-premise, incorporating ML/AI capabilities tailored for
small research groups with limited resources and computational expertise. By
focusing on practical solutions, the pipeline offers accessible approaches for
data transmission, inference, and evaluation, enabling researchers to discover
meaningful insights from their ever-increasing camera trap datasets.

</details>


### [419] [A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images](https://arxiv.org/abs/2507.10202)
*Jaeseong Lee,Yeeun Choi,Heechan Choi,Hanjung Kim,Seonjoo Kim*

Key words: 多模态大语言模型（MLLMs）、高分辨率图像、细粒度定位、任务无关框架、ECP

TL;DR: 论文提出了一种无需训练、任务无关的两阶段框架（ECP），通过提取候选区域再预测，解决了多模态大语言模型（MLLMs）在高分辨率图像上性能受限的问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 多模态大语言模型在视觉-语言任务中表现优异，但在高分辨率图像的细粒度定位和推理任务中表现不佳。这主要是因为模型的图像分辨率是固定的，与预训练图像编码器对齐，输入高分辨率图像会导致泛化性能下降。

Method: 提出了ECP框架，分为两个阶段：首先从下采样图像中提取候选区域（隐含定位线索），然后在候选区域上进行最终预测，从而保留细粒度细节并避免高分辨率带来的挑战。

Result: 在4K GUI grounding和4K、8K MLLM感知任务中，ECP分别实现了21.3%、5.8%和5.2%的绝对性能提升。

Conclusion: ECP框架有效提升了MLLMs在高分辨率图像任务中的性能，且无需额外训练，具有通用性。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in vision-language understanding, reasoning, and generation.
However, they struggle with tasks requiring fine-grained localization and
reasoning in high-resolution images. This constraint stems from the fact that
MLLMs are fine-tuned with fixed image resolution to align with the pre-trained
image encoder used in MLLM. Consequently, feeding high-resolution images
directly into MLLMs leads to poor generalization due to a train-test resolution
discrepancy, while downsampling these images-although ensuring
consistency-compromises fine-grained visual details and ultimately degrades
performance. To address this challenge, we propose Extract Candidate then
Predict (ECP), a novel training-free, task-agnostic two-stage framework
designed to enhance MLLM performance on high-resolution images. The key
intuition behind ECP is that while MLLMs struggle with high-resolution images,
their predictions on downsampled images still contain implicit localization
cues. By first identifying candidate region using the coarse prediction and
then predicting the final output based on candidate region, ECP effectively
preserves fine-grained details while mitigating the challenges posed by
high-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K
MLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared
to baseline respectively, demonstrating its effectiveness. Code is available at
https://github.com/yenncye/ECP.

</details>


### [420] [ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users](https://arxiv.org/abs/2507.10223)
*Xiangyu Yin,Boyuan Yang,Weichen Liu,Qiyao Xue,Abrar Alamri,Goeran Fiedler,Wei Gao*

Key words: 步态分析, 假肢, 视觉学习, 数据集, 机器学习

TL;DR: 提出名为ProGait的多用途数据集，用于支持视频对象分割、2D人体姿态估计和步态分析，并通过基准任务和微调模型展示其应用价值。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决基于视觉的机器学习方法在假肢检测和分析中的挑战，优化假肢设计和步态分析。

Method: 构建包含412个视频片段的ProGait数据集，提供基准任务和微调模型。

Result: 与预训练视觉模型相比，ProGait数据集提高了假肢相关任务的通用性。

Conclusion: ProGait数据集为假肢步态分析提供了有效工具，具有实际应用价值。

Abstract: Prosthetic legs play a pivotal role in clinical rehabilitation, allowing
individuals with lower-limb amputations the ability to regain mobility and
improve their quality of life. Gait analysis is fundamental for optimizing
prosthesis design and alignment, directly impacting the mobility and life
quality of individuals with lower-limb amputations. Vision-based machine
learning (ML) methods offer a scalable and non-invasive solution to gait
analysis, but face challenges in correctly detecting and analyzing prosthesis,
due to their unique appearances and new movement patterns. In this paper, we
aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait,
to support multiple vision tasks including Video Object Segmentation, 2D Human
Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from
four above-knee amputees when testing multiple newly-fitted prosthetic legs
through walking trials, and depicts the presence, contours, poses, and gait
patterns of human subjects with transfemoral prosthetic legs. Alongside the
dataset itself, we also present benchmark tasks and fine-tuned baseline models
to illustrate the practical application and performance of the ProGait dataset.
We compared our baseline models against pre-trained vision models,
demonstrating improved generalizability when applying the ProGait dataset for
prosthesis-specific tasks. Our code is available at
https://github.com/pittisl/ProGait and dataset at
https://huggingface.co/datasets/ericyxy98/ProGait.

</details>


### [421] [Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams](https://arxiv.org/abs/2507.09640)
*Leonor Fernandes,Tiago Gonçalves,João Matos,Luis Filipe Nakayama,Jaime S. Cardoso*

Key words: 糖尿病视网膜病变, AI公平性, 解耦技术, 医学影像, mBRSET数据集

TL;DR: 该论文研究了AI模型在糖尿病视网膜病变（DR）预测中的公平性和性能，以及通过解耦技术减轻偏见的有效性。使用mBRSET数据集评估了三种模型，结果显示解耦效果因模型而异。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统DR筛查成本高且不易获取，AI算法提供可扩展解决方案，但公平性和泛化能力存疑，研究旨在评估模型的公平性和偏见缓解效果。

Method: 使用mBRSET数据集训练ConvNeXt V2、DINOv2和Swin V2模型，预测DR及敏感属性（如年龄、性别），评估公平性并应用解耦技术。

Result: 模型DR预测性能高（AUROC达94%），但对敏感属性的预测存在差异。解耦技术对DINOv2提升2%性能，但对其他模型性能下降。

Conclusion: 研究突出了医学影像AI中公平性的重要性，解耦技术效果因模型而异，需进一步优化。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss in working-age
adults. While screening reduces the risk of blindness, traditional imaging is
often costly and inaccessible. Artificial intelligence (AI) algorithms present
a scalable diagnostic solution, but concerns regarding fairness and
generalization persist. This work evaluates the fairness and performance of
image-trained models in DR prediction, as well as the impact of disentanglement
as a bias mitigation technique, using the diverse mBRSET fundus dataset. Three
models, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to
predict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness
was assessed between subgroups of SAs, and disentanglement was applied to
reduce bias. All models achieved high DR prediction performance in diagnosing
(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%
AUROC, respectively). Fairness assessment suggests disparities, such as a 10%
AUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction
had varying results, depending on the model selected. Disentanglement improved
DINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2
and Swin V2 (7% and 3%, respectively). These findings highlight the complexity
of disentangling fine-grained features in fundus imaging and emphasize the
importance of fairness in medical imaging AI to ensure equitable and reliable
healthcare solutions.

</details>


### [422] [CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding](https://arxiv.org/abs/2507.10449)
*Hongyong Han,Wei Wang,Gaowei Zhang,Mingjie Li,Yi Wang*

Key words: 珊瑚礁监测、视觉问答、大规模视觉语言模型、数据集、生态保护

TL;DR: 提出首个大规模珊瑚礁视觉问答数据集CoralVQA，包含12,805张珊瑚图像和277,653个问答对，为珊瑚礁监测提供专业基准。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 珊瑚礁是重要但脆弱的生态系统，需要专业监测，但传统方法依赖专家知识，VQA技术可提供用户友好的交互方式。

Method: 开发半自动数据构建流程，结合海洋生物学家专业知识，创建包含多维问题的珊瑚礁VQA数据集。

Result: 数据集CoralVQA为珊瑚礁图像提供全面评估，揭示当前LVLM在珊瑚礁分析中的局限与机遇。

Conclusion: CoralVQA为珊瑚保护提供重要工具，并为LVLM的未来发展指明方向。

Abstract: Coral reefs are vital yet vulnerable ecosystems that require continuous
monitoring to support conservation. While coral reef images provide essential
information in coral monitoring, interpreting such images remains challenging
due to the need for domain expertise. Visual Question Answering (VQA), powered
by Large Vision-Language Models (LVLMs), has great potential in user-friendly
interaction with coral reef images. However, applying VQA to coral imagery
demands a dedicated dataset that addresses two key challenges: domain-specific
annotations and multidimensional questions. In this work, we introduce
CoralVQA, the first large-scale VQA dataset for coral reef analysis. It
contains 12,805 real-world coral images from 67 coral genera collected from 3
oceans, along with 277,653 question-answer pairs that comprehensively assess
ecological and health-related conditions. To construct this dataset, we develop
a semi-automatic data construction pipeline in collaboration with marine
biologists to ensure both scalability and professional-grade data quality.
CoralVQA presents novel challenges and provides a comprehensive benchmark for
studying vision-language reasoning in the context of coral reef images. By
evaluating several state-of-the-art LVLMs, we reveal key limitations and
opportunities. These insights form a foundation for future LVLM development,
with a particular emphasis on supporting coral conservation efforts.

</details>


### [423] [RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening](https://arxiv.org/abs/2507.10461)
*Tao Tang,Chengxu Yang*

Key words: Pansharpening, CNN, RAPNet, RAPConv, PAN-DFF

TL;DR: RAPNet提出了一种基于内容自适应卷积的新架构，通过RAPConv和PAN-DFF模块提升空间细节和光谱保真度，在遥感图像融合中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有卷积神经网络在遥感图像融合中因忽略局部内容变化而受限，RAPNet旨在解决这一问题。

Method: 采用RAPConv生成空间自适应卷积核，结合PAN-DFF模块的注意力机制优化特征融合。

Result: 在公开数据集上的评估显示，RAPNet在定量和定性指标上均优于现有方法，并通过消融实验验证了自适应组件的有效性。

Conclusion: RAPNet通过自适应设计显著提升了遥感图像融合的性能。

Abstract: Pansharpening refers to the process of integrating a high resolution
panchromatic (PAN) image with a lower resolution multispectral (MS) image to
generate a fused product, which is pivotal in remote sensing. Despite the
effectiveness of CNNs in addressing this challenge, they are inherently
constrained by the uniform application of convolutional kernels across all
spatial positions, overlooking local content variations. To overcome this
issue, we introduce RAPNet, a new architecture that leverages content-adaptive
convolution. At its core, RAPNet employs the Receptive-field Adaptive
Pansharpening Convolution (RAPConv), designed to produce spatially adaptive
kernels responsive to local feature context, thereby enhancing the precision of
spatial detail extraction. Additionally, the network integrates the
Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an
attention mechanism to achieve an optimal balance between spatial detail
enhancement and spectral fidelity. Comprehensive evaluations on publicly
available datasets confirm that RAPNet delivers superior performance compared
to existing approaches, as demonstrated by both quantitative metrics and
qualitative assessments. Ablation analyses further substantiate the
effectiveness of the proposed adaptive components.

</details>


### [424] [NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection](https://arxiv.org/abs/2507.09795)
*Amirhossein Ansari,Ke Wang,Pulei Xiong*

Key words: 零样本OOD检测, 负标签, CLIP, NegRefine, 多匹配感知

TL;DR: NegRefine提出了一种新的负标签细化框架，用于零样本OOD检测，通过过滤机制和动态评分函数提升性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有负标签方法存在将分布内样本误判为OOD的缺陷，尤其是在处理子类别标签和专有名词时表现不佳。

Method: NegRefine引入了一个过滤机制来优化负标签集，并结合多匹配感知的评分函数动态调整多个标签的贡献。

Result: NegRefine在大规模基准测试（如ImageNet-1K）中表现出更鲁棒的分布内外样本区分能力。

Conclusion: NegRefine通过优化负标签集和动态评分，显著提升了零样本OOD检测的准确性。

Abstract: Recent advancements in Vision-Language Models like CLIP have enabled
zero-shot OOD detection by leveraging both image and textual label information.
Among these, negative label-based methods such as NegLabel and CSP have shown
promising results by utilizing a lexicon of words to define negative labels for
distinguishing OOD samples. However, these methods suffer from detecting
in-distribution samples as OOD due to negative labels that are subcategories of
in-distribution labels or proper nouns. They also face limitations in handling
images that match multiple in-distribution and negative labels. We propose
NegRefine, a novel negative label refinement framework for zero-shot OOD
detection. By introducing a filtering mechanism to exclude subcategory labels
and proper nouns from the negative label set and incorporating a
multi-matching-aware scoring function that dynamically adjusts the
contributions of multiple labels matching an image, NegRefine ensures a more
robust separation between in-distribution and OOD samples. We evaluate
NegRefine on large-scale benchmarks, including ImageNet-1K. Source code is
available at https://github.com/ah-ansari/NegRefine.

</details>


### [425] [Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation](https://arxiv.org/abs/2507.10474)
*Seyed Alireza Rahimi Azghadi,Truong-Thanh-Hung Nguyen,Helene Fournier,Monica Wachowicz,Rene Richard,Francis Palma,Hung Cao*

Key words: 跌倒检测, 联邦学习, 室内定位, 隐私保护, 边缘计算

TL;DR: 提出了一个结合半监督联邦学习、室内定位导航和基于视觉的跌倒检测框架，以高准确率和隐私保护检测老年人跌倒。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 针对老年人口增长及跌倒风险增加的问题，需开发高效、可靠且隐私保护的跌倒检测系统。

Method: 结合SF2D（半监督联邦学习跌倒检测系统）、室内定位导航系统和基于视觉的跌倒识别系统，通过可穿戴设备和边缘设备实现多系统互补检测。

Result: SF2D准确率99.19%，视觉检测准确率96.3%，导航系统成功率95%，整体框架准确率达99.99%。

Conclusion: 该框架不仅高效可靠，还能保护用户隐私，适用于老年人跌倒检测。

Abstract: The aging population is growing rapidly, and so is the danger of falls in
older adults. A major cause of injury is falling, and detection in time can
greatly save medical expenses and recovery time. However, to provide timely
intervention and avoid unnecessary alarms, detection systems must be effective
and reliable while addressing privacy concerns regarding the user. In this
work, we propose a framework for detecting falls using several complementary
systems: a semi-supervised federated learning-based fall detection system
(SF2D), an indoor localization and navigation system, and a vision-based human
fall recognition system. A wearable device and an edge device identify a fall
scenario in the first system. On top of that, the second system uses an indoor
localization technique first to localize the fall location and then navigate a
robot to inspect the scenario. A vision-based detection system running on an
edge device with a mounted camera on a robot is used to recognize fallen
people. Each of the systems of this proposed framework achieves different
accuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to
99.19% accuracy, while the vision-based fallen people detection achieves 96.3%
accuracy. However, when we combine the accuracy of these two systems with the
accuracy of the navigation system (95% success rate), our proposed framework
creates a highly reliable performance for fall detection, with an overall
accuracy of 99.99%. Not only is the proposed framework safe for older adults,
but it is also a privacy-preserving solution for detecting falls.

</details>


### [426] [Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models](https://arxiv.org/abs/2507.09830)
*Shuhao Fu,Philip J. Kellman,Hongjing Lu*

Key words: 3D形状识别, 点云, 人类视觉, 深度学习模型, 视觉变换器, 卷积神经网络

TL;DR: 论文探讨了人类和深度学习模型在识别3D形状（如点云）时的表现差异，发现视觉变换器（point transformer）模型能更好地模拟人类表现，归因于其对3D形状的分层抽象能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 研究旨在了解深度学习模型是否形成与人类视觉相似的3D形状表征，以及这些表征在对象识别中的作用。

Method: 通过两组人类实验（操纵点密度、对象方向和局部几何结构）对比DGCNN和point transformer模型的表现。

Result: point transformer模型比卷积神经网络（DGCNN）更好地模拟人类表现，优势源于其分层抽象机制。

Conclusion: 视觉变换器模型在3D形状表征上更接近人类，为未来研究提供了方向。

Abstract: Both humans and deep learning models can recognize objects from 3D shapes
depicted with sparse visual information, such as a set of points randomly
sampled from the surfaces of 3D objects (termed a point cloud). Although deep
learning models achieve human-like performance in recognizing objects from 3D
shapes, it remains unclear whether these models develop 3D shape
representations similar to those used by human vision for object recognition.
We hypothesize that training with 3D shapes enables models to form
representations of local geometric structures in 3D shapes. However, their
representations of global 3D object shapes may be limited. We conducted two
human experiments systematically manipulating point density and object
orientation (Experiment 1), and local geometric structure (Experiment 2).
Humans consistently performed well across all experimental conditions. We
compared two types of deep learning models, one based on a convolutional neural
network (DGCNN) and the other on visual transformers (point transformer), with
human performance. We found that the point transformer model provided a better
account of human performance than the convolution-based model. The advantage
mainly results from the mechanism in the point transformer model that supports
hierarchical abstraction of 3D shapes.

</details>


### [427] [BenchReAD: A systematic benchmark for retinal anomaly detection](https://arxiv.org/abs/2507.10492)
*Chenyu Lian,Hong-Yu Zhou,Zhanli Hu,Jing Qin*

Key words: 视网膜异常检测，基准评估，DRA，NFM-DRA，泛化性

TL;DR: 该论文提出了一个全面的视网膜异常检测基准，填补了现有研究的数据和方法局限性，并通过改进的DRA方法（NFM-DRA）实现了最先进的性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 由于缺乏全面且公开的视网膜异常检测基准，现有研究在异常类型、测试集和泛化性评估方面受限，影响了方法的发展和评估。

Method: 提出了一个系统性基准，并改进DRA方法，引入Normal Feature Memory（NFM）机制，形成NFM-DRA。

Result: NFM-DRA在性能上超过现有方法，解决了DRA对未见异常性能下降的问题。

Conclusion: 论文的基准和方法为视网膜异常检测领域提供了标准化工具，推动了该领域的研究进展。

Abstract: Retinal anomaly detection plays a pivotal role in screening ocular and
systemic diseases. Despite its significance, progress in the field has been
hindered by the absence of a comprehensive and publicly available benchmark,
which is essential for the fair evaluation and advancement of methodologies.
Due to this limitation, previous anomaly detection work related to retinal
images has been constrained by (1) a limited and overly simplistic set of
anomaly types, (2) test sets that are nearly saturated, and (3) a lack of
generalization evaluation, resulting in less convincing experimental setups.
Furthermore, existing benchmarks in medical anomaly detection predominantly
focus on one-class supervised approaches (training only with negative samples),
overlooking the vast amounts of labeled abnormal data and unlabeled data that
are commonly available in clinical practice. To bridge these gaps, we introduce
a benchmark for retinal anomaly detection, which is comprehensive and
systematic in terms of data and algorithm. Through categorizing and
benchmarking previous methods, we find that a fully supervised approach
leveraging disentangled representations of abnormalities (DRA) achieves the
best performance but suffers from significant drops in performance when
encountering certain unseen anomalies. Inspired by the memory bank mechanisms
in one-class supervised learning, we propose NFM-DRA, which integrates DRA with
a Normal Feature Memory to mitigate the performance degradation, establishing a
new SOTA. The benchmark is publicly available at
https://github.com/DopamineLcy/BenchReAD.

</details>


### [428] [Cameras as Relative Positional Encoding](https://arxiv.org/abs/2507.10496)
*Ruilong Li,Brent Yi,Junchen Liu,Hang Gao,Yi Ma,Angjoo Kanazawa*

Key words: 多视图Transformer, 相机几何, PRoPE, 3D感知, 新视图合成

TL;DR: 该论文比较了多视图Transformer中相机几何条件化的方法，提出了一种新的相对编码（PRoPE），在多种任务和场景中显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 多视图计算机视觉任务中，相机几何关系对3D感知至关重要，但现有方法未能完全利用这些关系。

Method: 比较了三种相机条件化方法：token级光线图编码、attention级相对姿态编码，以及提出的PRoPE（投影位置编码），覆盖相机内外参数。

Result: 实验表明，PRoPE在多个任务和场景（如新视图合成、深度估计和空间认知）中显著提升了性能，且具有泛化能力。

Conclusion: PRoPE是一种有效的相机几何编码方法，适用于多视图Transformer任务。

Abstract: Transformers are increasingly prevalent for multi-view computer vision tasks,
where geometric relationships between viewpoints are critical for 3D
perception. To leverage these relationships, multi-view transformers must use
camera geometry to ground visual tokens in 3D space. In this work, we compare
techniques for conditioning transformers on cameras: token-level raymap
encodings, attention-level relative pose encodings, and a new relative encoding
we propose -- Projective Positional Encoding (PRoPE) -- that captures complete
camera frustums, both intrinsics and extrinsics, as a relative positional
encoding. Our experiments begin by showing how relative camera conditioning
improves performance in feedforward novel view synthesis, with further gains
from PRoPE. This holds across settings: scenes with both shared and varying
intrinsics, when combining token- and attention-level conditioning, and for
generalization to inputs with out-of-distribution sequence lengths and camera
intrinsics. We then verify that these benefits persist for different tasks,
stereo depth estimation and discriminative spatial cognition, as well as larger
model sizes.

</details>


### [429] [Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies](https://arxiv.org/abs/2507.10029)
*Seokeon Choi,Sunghyun Park,Hyoungwoo Park,Jeongho Kim,Sungrack Yun*

Key words: 文本到图像扩散模型,内存效率,个性化,设备端优化,零阶优化

TL;DR: 提出了一种选择性优化框架，结合低分辨率反向传播(BP-low)和高分辨率零阶优化(ZO-high)，以实现内存高效、高质量的文本到图像扩散模型个性化。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决在边缘设备上适应文本到图像扩散模型时的内存效率和隐私保护问题。

Method: 通过时间步感知的概率函数动态选择优化策略，结合BP-low和ZO-high的优势。

Result: 实验表明，该方法在减少内存消耗的同时保持高质量输出，适用于设备端个性化。

Conclusion: 该框架成功平衡了内存效率与模型性能，适用于实际部署。

Abstract: Memory-efficient personalization is critical for adapting text-to-image
diffusion models while preserving user privacy and operating within the limited
computational resources of edge devices. To this end, we propose a selective
optimization framework that adaptively chooses between backpropagation on
low-resolution images (BP-low) and zeroth-order optimization on high-resolution
images (ZO-high), guided by the characteristics of the diffusion process. As
observed in our experiments, BP-low efficiently adapts the model to
target-specific features, but suffers from structural distortions due to
resolution mismatch. Conversely, ZO-high refines high-resolution details with
minimal memory overhead but faces slow convergence when applied without prior
adaptation. By complementing both methods, our framework leverages BP-low for
effective personalization while using ZO-high to maintain structural
consistency, achieving memory-efficient and high-quality fine-tuning. To
maximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware
probabilistic function that dynamically selects the appropriate optimization
strategy based on diffusion timesteps. This function mitigates the overfitting
from BP-low at high timesteps, where structural information is critical, while
ensuring ZO-high is applied more effectively as training progresses.
Experimental results demonstrate that our method achieves competitive
performance while significantly reducing memory consumption, enabling scalable,
high-quality on-device personalization without increasing inference latency.

</details>


### [430] [Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder](https://arxiv.org/abs/2507.10552)
*Vladimir Iashin,Horace Lee,Dan Schofield,Andrew Zisserman*

Key words: 相机陷阱,自监督学习,黑猩猩面部识别,DINOv2,Vision Transformers

TL;DR: 该研究提出了一种完全自监督的方法，利用DINOv2框架从无标记的相机陷阱视频中学习黑猩猩面部嵌入，无需人工标注身份标签。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决野生动物监测中人工识别个体动物的效率瓶颈，推动非侵入性种群研究的可扩展性。

Method: 基于DINOv2框架，利用自动提取的面部图像训练Vision Transformers，实现自监督学习。

Result: 在Bossou等挑战性基准测试中，表现出优于监督基线的开放集重识别性能。

Conclusion: 自监督学习在生物多样性监测中具有巨大潜力，为可扩展的非侵入性种群研究提供了新途径。

Abstract: Camera traps are revolutionising wildlife monitoring by capturing vast
amounts of visual data; however, the manual identification of individual
animals remains a significant bottleneck. This study introduces a fully
self-supervised approach to learning robust chimpanzee face embeddings from
unlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision
Transformers on automatically mined face crops, eliminating the need for
identity labels. Our method demonstrates strong open-set re-identification
performance, surpassing supervised baselines on challenging benchmarks such as
Bossou, despite utilising no labelled data during training. This work
underscores the potential of self-supervised learning in biodiversity
monitoring and paves the way for scalable, non-invasive population studies.

</details>


### [431] [A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area](https://arxiv.org/abs/2507.10084)
*Haonan Chen,Xin Tong*

Key words: 迁移学习,遥感图像,水域分割,SegFormer,领域偏移

TL;DR: 该研究提出了一种基于SegFormer模型的两阶段迁移学习策略，用于解决遥感图像水域分割中的领域偏移和小样本问题，显著提升了分割性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 针对遥感图像水域分割领域中普遍存在的领域偏移和小样本问题，研究旨在提出一种有效的解决方案。

Method: 研究采用两阶段迁移学习策略，先在多样源域上训练基础分割模型，然后在特定目标域上进行微调。

Result: 实验结果显示，此策略将水域分割任务的IoU从25.50%提升至64.84%，有效解决了模型性能下降问题。

Conclusion: 该策略不仅解决了领域差异导致的性能下降问题，还为数据稀缺和环境独特的遥感场景提供了高精度主题信息提取的技术范例。

Abstract: To address the prevalent challenges of domain shift and small sample sizes in
remote sensing image water body segmentation, this study proposes and validates
a two-stage transfer learning strategy based on the SegFormer model. The
approach begins by training a foundational segmentation model on a diverse
source domain, where it achieves an Intersection over Union (IoU) of 68.80% on
its validation set, followed by fine-tuning on data from the distinct target
domain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by
highly complex topography and spectral features -- the experimental results
demonstrate that this strategy significantly boosts the IoU for the water body
segmentation task from 25.50% (for direct transfer) to 64.84%. This not only
effectively resolves the model performance degradation caused by domain
discrepancy but also provides an effective technical paradigm for
high-precision thematic information extraction in data-scarce and
environmentally unique remote sensing scenarios.

</details>


### [432] [Deep Recurrence for Dynamical Segmentation Models](https://arxiv.org/abs/2507.10143)
*David Calhas,Arlindo L. Oliveira*

Key words: 反馈机制, U-Net, 预测编码, 生物启发, 数据效率

TL;DR: 论文提出了一种受预测编码启发的反馈机制，通过在标准U-Net架构中引入递归循环，提升了模型在噪声环境下的性能和数据效率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 生物视觉系统依赖反馈连接迭代优化感知，而大多数人工神经网络仍是前馈的。为此，作者希望探索反馈机制在人工神经网络中的应用。

Method: 在标准U-Net架构中引入反馈循环，并采用softmax投影和指数衰减两种生物启发操作以确保稳定性。

Result: 实验表明，反馈模型在噪声条件下显著优于前馈模型，且在有限监督下泛化能力更强，仅需2个训练样本即可表现优于随机。

Conclusion: 反馈机制增强了鲁棒性和数据效率，为更具适应性和生物启发性的神经网络架构提供了路径。

Abstract: While biological vision systems rely heavily on feedback connections to
iteratively refine perception, most artificial neural networks remain purely
feedforward, processing input in a single static pass. In this work, we propose
a predictive coding inspired feedback mechanism that introduces a recurrent
loop from output to input, allowing the model to refine its internal state over
time. We implement this mechanism within a standard U-Net architecture and
introduce two biologically motivated operations, softmax projection and
exponential decay, to ensure stability of the feedback loop. Through controlled
experiments on a synthetic segmentation task, we show that the feedback model
significantly outperforms its feedforward counterpart in noisy conditions and
generalizes more effectively with limited supervision. Notably, feedback
achieves above random performance with just two training examples, while the
feedforward model requires at least four. Our findings demonstrate that
feedback enhances robustness and data efficiency, and offer a path toward more
adaptive and biologically inspired neural architectures. Code is available at:
github.com/DCalhas/feedback_segmentation.

</details>


### [433] [Spatial Lifting for Dense Prediction](https://arxiv.org/abs/2507.10222)
*Mingzhi Xu,Yizhe Zhang*

Key words: 密集预测, Spatial Lifting, 维度提升, 模型效率, 语义分割, 深度估计

TL;DR: 提出了一种名为Spatial Lifting (SL)的新方法，通过将二维输入（如图像）提升到更高维空间进行处理，显著减少了模型参数和推理成本，同时在密集预测任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统密集预测方法通常需要大量参数和高计算成本，SL旨在通过维度提升技术实现高效且准确的预测。

Method: SL将标准2D输入提升至高维空间（如3D），并利用对应维度网络（如3D U-Net）处理，输出具有内在结构的高维结果。

Result: 在19个基准数据集（13个语义分割，6个深度估计）上验证，SL性能与常规方法相当，但参数减少98%以上，推理成本显著降低。

Conclusion: SL为新视觉建模范式，为高效、准确、可靠的密集预测任务提供了新方向。

Abstract: We present Spatial Lifting (SL), a novel methodology for dense prediction
tasks. SL operates by lifting standard inputs, such as 2D images, into a
higher-dimensional space and subsequently processing them using networks
designed for that higher dimension, such as a 3D U-Net. Counterintuitively,
this dimensionality lifting allows us to achieve good performance on benchmark
tasks compared to conventional approaches, while reducing inference costs and
significantly lowering the number of model parameters. The SL framework
produces intrinsically structured outputs along the lifted dimension. This
emergent structure facilitates dense supervision during training and enables
robust, near-zero-additional-cost prediction quality assessment at test time.
We validate our approach across 19 benchmark datasets (13 for semantic
segmentation and 6 for depth estimation), demonstrating competitive dense
prediction performance while reducing the model parameter count by over 98% (in
the U-Net case) and lowering inference costs. Spatial Lifting introduces a new
vision modeling paradigm that offers a promising path toward more efficient,
accurate, and reliable deep networks for dense prediction tasks in vision.

</details>


### [434] [Test-Time Canonicalization by Foundation Models for Robust Perception](https://arxiv.org/abs/2507.10375)
*Utkarsh Singhal,Ryan Feng,Stella X. Yu,Atul Prakash*

Key words: FOCAL, 视觉感知, 鲁棒性, 基础模型, 数据驱动

TL;DR: FOCAL是一种测试时、数据驱动的框架，利用基础模型的互联网级视觉先验，通过生成和优化候选变换实现鲁棒性，无需重新训练或架构修改。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法依赖专用架构或预定义增强训练，泛化能力有限，FOCAL旨在通过数据驱动方法提升视觉感知的鲁棒性。

Method: 生成并优化候选变换，使其朝视觉典型的“规范”视图发展，利用基础模型的视觉先验增强鲁棒性。

Result: 在CLIP和SAM上验证了FOCAL对2D/3D旋转、光照变化（对比度和颜色）及昼夜变化的鲁棒性提升。

Conclusion: FOCAL通过数据驱动方法挑战了需依赖变换特定训练的假设，为鲁棒性提供了可扩展解决方案。

Abstract: Real-world visual perception requires invariance to diverse transformations,
yet current methods rely heavily on specialized architectures or training on
predefined augmentations, limiting generalization. We propose FOCAL, a
test-time, data-driven framework that achieves robust perception by leveraging
internet-scale visual priors from foundation models. By generating and
optimizing candidate transformations toward visually typical, "canonical"
views, FOCAL enhances robustness without re-training or architectural changes.
Our experiments demonstrate improved robustness of CLIP and SAM across
challenging transformations, including 2D/3D rotations, illumination shifts
(contrast and color), and day-night variations. We also highlight potential
applications in active vision. Our approach challenges the assumption that
transform-specific training is necessary, instead offering a scalable path to
invariance. Our code is available at: https://github.com/sutkarsh/focal.

</details>


### [435] [Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks](https://arxiv.org/abs/2507.10381)
*Aaryam Sharma*

Key words: 拓扑数据分析（TDA）、深度学习、遥感分类、卷积神经网络（CNNs）、持久同调

TL;DR: 论文提出了一种将拓扑数据分析（TDA）特征与深度学习模型结合的简单方法，提升遥感分类性能，在EuroSAT数据集上达到99.33%的准确率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 卷积神经网络（CNNs）在图像分类中偏向于纹理特征，作者希望通过TDA提取的几何特征弥补这一不足。

Method: 设计了TDA特征工程流程，将拓扑特征与ResNet18等深度学习模型结合。

Result: 在EuroSAT数据集上，方法比基线ResNet18提升1.44%，准确率达到99.33%；在RESISC45数据集上比基线提升1.82%。

Conclusion: TDA特征可与深度学习模型有效结合，即使在没有明显拓扑结构的数据集上也能提升性能，扩展了TDA的适用性。

Abstract: Topological data analysis (TDA) is a relatively new field that is gaining
rapid adoption due to its robustness and ability to effectively describe
complex datasets by quantifying geometric information. In imaging contexts, TDA
typically models data as filtered cubical complexes from which we can extract
discriminative features using persistence homology. Meanwhile, convolutional
neural networks (CNNs) have been shown to be biased towards texture based local
features. To address this limitation, we propose a TDA feature engineering
pipeline and a simple method to integrate topological features with deep
learning models on remote sensing classification. Our method improves the
performance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving
99.33% accuracy, which surpasses all previously reported single-model
accuracies, including those with larger architectures, such as ResNet50 (2x
larger) and XL Vision Transformers (197x larger). We additionally show that our
method's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45
dataset. To our knowledge, this is the first application of TDA features in
satellite scene classification with deep learning. This demonstrates that TDA
features can be integrated with deep learning models, even on datasets without
explicit topological structures, thereby increasing the applicability of TDA. A
clean implementation of our method will be made publicly available upon
publication.

</details>


### [436] [National level satellite-based crop field inventories in smallholder landscapes](https://arxiv.org/abs/2507.10499)
*Philippe Rufin,Pauline Lucie Hammer,Leon-Friedrich Thomas,Sá Nogueira Lisboa,Natasha Ribeiro,Almeida Sitoe,Patrick Hostert,Patrick Meyfroidt*

Key words: 小农农业、高分辨率遥感、深度学习、莫桑比克、农田碎片化

TL;DR: 该研究利用高分辨率地球观测数据和深度学习技术，首次绘制了莫桑比克全国2100万个农田地块的分布图，准确率达93%，揭示了复杂小农系统中的农田碎片化特点及其与经济社会环境的关系。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 小农农业的可持续发展政策设计受限于对农田分布和地块规模等基本系统属性的认识不足，研究旨在通过先进技术填补这一空白。

Method: 结合极高分辨率（1.5米）地球观测数据和深度迁移学习技术，实现了全国尺度复杂农田系统的自动划分，且减少了对参考数据的依赖。

Result: 生成莫桑比克全国2100万块农田的地图，总体准确率93%；地块中位数面积0.16公顷，83%小于0.5公顷，揭示了农田碎片化与周边社会经济环境因素的关联。

Conclusion: 农田地块大小是影响农业生产、生计和环境的关键指标，映射了小农、中等规模和大规模农业的多样化分布及其潜在影响。

Abstract: The design of science-based policies to improve the sustainability of
smallholder agriculture is challenged by a limited understanding of fundamental
system properties, such as the spatial distribution of active cropland and
field size. We integrate very high spatial resolution (1.5 m) Earth observation
data and deep transfer learning to derive crop field delineations in complex
agricultural systems at the national scale, while maintaining minimum reference
data requirements and enhancing transferability. We provide the first
national-level dataset of 21 million individual fields for Mozambique (covering
~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural
land use with an overall accuracy of 93% and balanced omission and commission
errors. Field-level spatial agreement reached median intersection over union
(IoU) scores of 0.81, advancing the state-of-the-art in large-area field
delineation in complex smallholder systems. The active cropland maps capture
fragmented rural regions with low cropland shares not yet identified in global
land cover or cropland maps. These regions are mostly located in agricultural
frontier regions which host 7-9% of the Mozambican population. Field size in
Mozambique is very low overall, with half of the fields being smaller than 0.16
ha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial
resolution (0.05{\deg}) is 0.32 ha, but it varies strongly across gradients of
accessibility, population density, and net forest cover change. This variation
reflects a diverse set of actors, ranging from semi-subsistence smallholder
farms to medium-scale commercial farming, and large-scale farming operations.
Our results highlight that field size is a key indicator relating to
socio-economic and environmental outcomes of agriculture (e.g., food
production, livelihoods, deforestation, biodiversity), as well as their
trade-offs.

</details>


### [437] [Quantize-then-Rectify: Efficient VQ-VAE Training](https://arxiv.org/abs/2507.10547)
*Borui Zhang,Qihang Rao,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Key words: VQ-VAE, 量化噪声, 通道多组量化, 后整流器, 训练效率

TL;DR: ReVQ利用预训练的VAE快速训练高压缩率的VQ-VAE，显著降低计算成本。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有高压缩率VQ-VAE训练计算成本高昂，需数千GPU小时。

Method: 通过控制量化噪声在VAE容忍阈值内，结合通道多组量化和后整流器。

Result: ReVQ在22小时内完成训练，保持高质量重建（rFID = 1.06）。

Conclusion: ReVQ显著提升效率与重建质量的平衡，降低训练成本。

Abstract: Visual tokenizers are pivotal in multimodal large models, acting as bridges
between continuous inputs and discrete tokens. Nevertheless, training
high-compression-rate VQ-VAEs remains computationally demanding, often
necessitating thousands of GPU hours. This work demonstrates that a pre-trained
VAE can be efficiently transformed into a VQ-VAE by controlling quantization
noise within the VAE's tolerance threshold. We present
\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs
to enable rapid VQ-VAE training with minimal computational overhead. By
integrating \textbf{channel multi-group quantization} to enlarge codebook
capacity and a \textbf{post rectifier} to mitigate quantization errors, ReVQ
compresses ImageNet images into at most 512 tokens while sustaining competitive
reconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training
costs by over two orders of magnitude relative to state-of-the-art approaches:
ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,
whereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental
results show that ReVQ achieves superior efficiency-reconstruction trade-offs.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [438] [Physics-informed machine learning: A mathematical framework with applications to time series forecasting](https://arxiv.org/abs/2507.08906)
*Nathan Doumèche*

Key words: 物理信息机器学习, 核方法, 神经网络, 工业应用, 能源预测

TL;DR: 该论文探讨了物理信息机器学习（PIML）的统计特性及其在工业中的应用，包括基于核方法的理论分析和实际能源信号的预测。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 研究物理知识如何与机器学习结合，以提高模型的性能和可解释性，尤其在复杂系统（如PDE系统）中的应用。

Method: 采用物理信息神经网络（PINNs）和核岭回归工具分析PIML的统计特性，并开发了高效的GPU实现算法；在工业应用中，通过实际数据验证了物理约束框架的有效性。

Result: 理论分析表明PIML具有较好的近似性和收敛性；实际应用中，成功预测了电动汽车充电占用率和电力需求。

Conclusion: PIML在理论和应用上均表现出色，能够有效整合物理知识，提升预测精度。

Abstract: Physics-informed machine learning (PIML) is an emerging framework that
integrates physical knowledge into machine learning models. This physical prior
often takes the form of a partial differential equation (PDE) system that the
regression function must satisfy. In the first part of this dissertation, we
analyze the statistical properties of PIML methods. In particular, we study the
properties of physics-informed neural networks (PINNs) in terms of
approximation, consistency, overfitting, and convergence. We then show how PIML
problems can be framed as kernel methods, making it possible to apply the tools
of kernel ridge regression to better understand their behavior. In addition, we
use this kernel formulation to develop novel physics-informed algorithms and
implement them efficiently on GPUs. The second part explores industrial
applications in forecasting energy signals during atypical periods. We present
results from the Smarter Mobility challenge on electric vehicle charging
occupancy and examine the impact of mobility on electricity demand. Finally, we
introduce a physics-constrained framework for designing and enforcing
constraints in time series, applying it to load forecasting and tourism
forecasting in various countries.

</details>


### [439] [The Bayesian Approach to Continual Learning: An Overview](https://arxiv.org/abs/2507.08922)
*Tameem Adel*

Key words: 贝叶斯持续学习, 任务增量学习, 类增量学习, 领域适应, 迁移学习, 元学习

TL;DR: 该论文探讨了贝叶斯持续学习的定义、分类及与心理学等领域的联系，并提出了未来研究方向。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 研究持续学习如何在不需要重新训练的情况下更新知识，并避免忘记过去的学习经验，以拓展深度学习在实际问题中的应用范围。

Method: 通过定义贝叶斯持续学习及其相关领域（如领域适应、迁移学习和元学习），提出分类法对现有算法进行系统归纳。

Result: 综述了最先进的贝叶斯持续学习算法，并揭示了其与心理学领域的潜在联系。

Conclusion: 贝叶斯持续学习在拓展深度学习应用方面具有潜力，但仍面临挑战，需进一步研究解决。

Abstract: Continual learning is an online paradigm where a learner continually
accumulates knowledge from different tasks encountered over sequential time
steps. Importantly, the learner is required to extend and update its knowledge
without forgetting about the learning experience acquired from the past, and
while avoiding the need to retrain from scratch. Given its sequential nature
and its resemblance to the way humans think, continual learning offers an
opportunity to address several challenges which currently stand in the way of
widening the range of applicability of deep models to further real-world
problems. The continual need to update the learner with data arriving
sequentially strikes inherent congruence between continual learning and
Bayesian inference which provides a principal platform to keep updating the
prior beliefs of a model given new data, without completely forgetting the
knowledge acquired from the old data. This survey inspects different settings
of Bayesian continual learning, namely task-incremental learning and
class-incremental learning. We begin by discussing definitions of continual
learning along with its Bayesian setting, as well as the links with related
fields, such as domain adaptation, transfer learning and meta-learning.
Afterwards, we introduce a taxonomy offering a comprehensive categorization of
algorithms belonging to the Bayesian continual learning paradigm. Meanwhile, we
analyze the state-of-the-art while zooming in on some of the most prominent
Bayesian continual learning algorithms to date. Furthermore, we shed some light
on links between continual learning and developmental psychology, and
correspondingly introduce analogies between both fields. We follow that with a
discussion of current challenges, and finally conclude with potential areas for
future research on Bayesian continual learning.

</details>


### [440] [Fixed-Confidence Multiple Change Point Identification under Bandit Feedback](https://arxiv.org/abs/2507.08994)
*Joseph Lazzaro,Ciara Pike-Burke*

Key words: 分段常数函数, 赌博机问题, 突变点识别, Track-and-Stop

TL;DR: 论文提出了一种固定置信度的分段常数赌博机问题，用于快速识别函数突变位置，并通过理论分析和实验验证了方法的有效性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 分段常数函数在多个领域广泛应用，但快速准确地识别其突变位置仍具挑战性，需要一种高效的方法来解决这一问题。

Method: 提出了一种基于Track-and-Stop的简单高效算法，通过集中采样突变点附近区域，并根据突变幅度调整采样数量，实现最优识别。

Result: 理论分析表明该方法是渐进最优的，实验结果表明其在合成环境中具有高效性。

Conclusion: 该方法在识别分段常数函数的突变位置时表现出色，为实际应用提供了可靠工具。

Abstract: Piecewise constant functions describe a variety of real-world phenomena in
domains ranging from chemistry to manufacturing. In practice, it is often
required to confidently identify the locations of the abrupt changes in these
functions as quickly as possible. For this, we introduce a fixed-confidence
piecewise constant bandit problem. Here, we sequentially query points in the
domain and receive noisy evaluations of the function under bandit feedback. We
provide instance-dependent lower bounds for the complexity of change point
identification in this problem. These lower bounds illustrate that an optimal
method should focus its sampling efforts adjacent to each of the change points,
and the number of samples around each change point should be inversely
proportional to the magnitude of the change. Building on this, we devise a
simple and computationally efficient variant of Track-and-Stop and prove that
it is asymptotically optimal in many regimes. We support our theoretical
findings with experimental results in synthetic environments demonstrating the
efficiency of our method.

</details>


### [441] [Optimal High-probability Convergence of Nonlinear SGD under Heavy-tailed Noise via Symmetrization](https://arxiv.org/abs/2507.09093)
*Aleksandar Armacki,Dragana Bajovic,Dusan Jakovetic,Soummya Kar*

Key words: SGD、重尾噪声、非凸优化、对称化、非线性框架

TL;DR: 研究在高概率下SGD类方法在非凸优化和重尾噪声下的收敛性，提出非线性框架和对称化噪声的新方法，实现最优收敛率。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 重尾噪声在非凸优化中影响SGD类方法的收敛性，当前方法对噪声分布的限制较严，需更通用的框架和对称化噪声的解决方案。

Method: 提出非线性SGD（N-SGD）框架，包含sign、clipping等非线性操作；针对非对称噪声，提出对称化梯度估计器（SGE）和mini-batch SGE（MSGE）。

Result: N-SGD和N-SGE/N-MSGE在重尾噪声下均实现O(t^{-1/2})的收敛率，且具有指数衰减的尾部性能。

Conclusion: 通过噪声对称化和非线性框架，放松了对噪声分布的限制，适用于更广泛的优化场景。

Abstract: We study convergence in high-probability of SGD-type methods in non-convex
optimization and the presence of heavy-tailed noise. To combat the heavy-tailed
noise, a general black-box nonlinear framework is considered, subsuming
nonlinearities like sign, clipping, normalization and their smooth
counterparts. Our first result shows that nonlinear SGD (N-SGD) achieves the
rate $\widetilde{\mathcal{O}}(t^{-1/2})$, for any noise with unbounded moments
and a symmetric probability density function (PDF). Crucially, N-SGD has
exponentially decaying tails, matching the performance of linear SGD under
light-tailed noise. To handle non-symmetric noise, we propose two novel
estimators, based on the idea of noise symmetrization. The first, dubbed
Symmetrized Gradient Estimator (SGE), assumes a noiseless gradient at any
reference point is available at the start of training, while the second, dubbed
Mini-batch SGE (MSGE), uses mini-batches to estimate the noiseless gradient.
Combined with the nonlinear framework, we get N-SGE and N-MSGE methods,
respectively, both achieving the same convergence rate and exponentially
decaying tails as N-SGD, while allowing for non-symmetric noise with unbounded
moments and PDF satisfying a mild technical condition, with N-MSGE additionally
requiring bounded noise moment of order $p \in (1,2]$. Compared to works
assuming noise with bounded $p$-th moment, our results: 1) are based on a novel
symmetrization approach; 2) provide a unified framework and relaxed moment
conditions; 3) imply optimal oracle complexity of N-SGD and N-SGE, strictly
better than existing works when $p < 2$, while the complexity of N-MSGE is
close to existing works. Compared to works assuming symmetric noise with
unbounded moments, we: 1) provide a sharper analysis and improved rates; 2)
facilitate state-dependent symmetric noise; 3) extend the strong guarantees to
non-symmetric noise.

</details>


### [442] [CoVAE: Consistency Training of Variational Autoencoders](https://arxiv.org/abs/2507.09103)
*Gianluigi Silvestri,Luca Ambrogioni*

Key words: CoVAE, 单阶段训练, 变分自编码器, 一致性模型, 生成模型

TL;DR: 提出了一种新颖的单阶段生成自编码框架CoVAE，通过一致性模型技术训练VAE架构，避免了传统两阶段方法的计算开销和采样时间增加的问题。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 传统生成方法依赖于两阶段训练，先通过自动编码器降维，再在潜在空间训练生成模型，但这种方法引入了计算开销和采样时间增加的缺点。因此，作者希望提出一种单阶段的解决方案。

Method: CoVAE采用一致性模型技术训练VAE架构，编码器学习一系列渐进增强噪声的潜在表示，并通过时间相关的β参数调整KL损失。解码器使用一致性损失和变分正则化进行训练。

Result: CoVAE能够在一步或少数步骤内生成高质量样本，无需学习先验分布，显著优于传统VAE和其他单阶段VAE方法。

Conclusion: CoVAE为自编码和扩散式生成建模提供了统一框架，并实现了一步生成高性能自编码的可行路径。

Abstract: Current state-of-the-art generative approaches frequently rely on a two-stage
training procedure, where an autoencoder (often a VAE) first performs
dimensionality reduction, followed by training a generative model on the
learned latent space. While effective, this introduces computational overhead
and increased sampling times. We challenge this paradigm by proposing
Consistency Training of Variational AutoEncoders (CoVAE), a novel single-stage
generative autoencoding framework that adopts techniques from consistency
models to train a VAE architecture. The CoVAE encoder learns a progressive
series of latent representations with increasing encoding noise levels,
mirroring the forward processes of diffusion and flow matching models. This
sequence of representations is regulated by a time dependent $\beta$ parameter
that scales the KL loss. The decoder is trained using a consistency loss with
variational regularization, which reduces to a conventional VAE loss at the
earliest latent time. We show that CoVAE can generate high-quality samples in
one or few steps without the use of a learned prior, significantly
outperforming equivalent VAEs and other single-stage VAEs methods. Our approach
provides a unified framework for autoencoding and diffusion-style generative
modeling and provides a viable route for one-step generative high-performance
autoencoding. Our code is publicly available at
https://github.com/gisilvs/covae.

</details>


### [443] [A Generalization Theory for Zero-Shot Prediction](https://arxiv.org/abs/2507.09128)
*Ronak Mehta,Zaid Harchaoui*

Key words: 零样本预测,自监督学习,多模态对比学习,任务无关模型,理论基础

TL;DR: 该论文提出了一个理论框架，用于理解零样本预测方法如何通过自监督和多模态对比学习预训练任务无关的基础模型。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 研究零样本预测方法的理论基础，以理解其泛化能力。

Method: 提出理论框架，识别零样本预测的目标量及其关键条件独立性关系。

Result: 框架揭示了零样本预测能够泛化的关键条件独立性关系。

Conclusion: 研究为零样本预测提供了理论支持，帮助理解其泛化机制。

Abstract: A modern paradigm for generalization in machine learning and AI consists of
pre-training a task-agnostic foundation model, generally obtained using
self-supervised and multimodal contrastive learning. The resulting
representations can be used for prediction on a downstream task for which no
labeled data is available. We present a theoretical framework to better
understand this approach, called zero-shot prediction. We identify the target
quantities that zero-shot prediction aims to learn, or learns in passing, and
the key conditional independence relationships that enable its generalization
ability.

</details>


### [444] [A Randomized Algorithm for Sparse PCA based on the Basic SDP Relaxation](https://arxiv.org/abs/2507.09148)
*Alberto Del Pia,Dekun Zhou*

Key words: 稀疏主成分分析,SDP松弛,随机近似算法,维度缩减

TL;DR: 提出了一种基于SDP松弛的随机近似算法解决稀疏主成分分析（SPCA）问题，证明了其近似比率上限及适用条件，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: SPCA是维度缩减的基本方法，但问题是NP难的，现有方法在近似比率和计算效率上存在局限。

Method: 采用基于SDP松弛的随机近似算法，算法在多次调用时能以高概率达到稀疏常数级别的近似比率，且在特定技术假设下平均比率进一步受限于对数级。

Result: 在低秩或指数衰减特征值的SDP解情况下，技术假设成立；算法在一类广义Wishart协方差模型中表现接近最优。

Conclusion: 提出的算法在理论和实验上均表现出色，适用于广泛的SPCA实例，是SPCA问题的有效解决方案。

Abstract: Sparse Principal Component Analysis (SPCA) is a fundamental technique for
dimensionality reduction, and is NP-hard. In this paper, we introduce a
randomized approximation algorithm for SPCA, which is based on the basic SDP
relaxation. Our algorithm has an approximation ratio of at most the sparsity
constant with high probability, if called enough times. Under a technical
assumption, which is consistently satisfied in our numerical tests, the average
approximation ratio is also bounded by $\mathcal{O}(\log{d})$, where $d$ is the
number of features. We show that this technical assumption is satisfied if the
SDP solution is low-rank, or has exponentially decaying eigenvalues. We then
present a broad class of instances for which this technical assumption holds.
We also demonstrate that in a covariance model, which generalizes the spiked
Wishart model, our proposed algorithm achieves a near-optimal approximation
ratio. We demonstrate the efficacy of our algorithm through numerical results
on real-world datasets.

</details>


### [445] [Uncovering symmetric and asymmetric species associations from community and environmental data](https://arxiv.org/abs/2507.09317)
*Sara Si-Moussi,Esther Galbrun,Mickael Hedde,Giovanni Poggiato,Matthias Rohr,Wilfried Thuiller*

Key words: 生物相互作用、机器学习、物种关联、非对称关系、群落组装

TL;DR: 本文提出了一种机器学习框架，用于从物种群落和环境数据中提取双向关联，解决了传统模型仅能处理对称关系的局限性。通过模拟和实证数据验证了其有效性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 生物相互作用（对称和非对称）对群落组装和物种空间共变有重要影响，但传统模型多假设对称关系，限制了其解释能力。

Method: 提出了一种基于机器学习的框架，将物种间关联建模为源物种对目标物种的直接影响，并嵌入物种特异性潜变量；同时结合多物种条件生成模型，区分环境驱动和生物关联的不同模式。

Result: 框架能够有效恢复已知的对称和非对称关联，并在关联网络中展现出优越性。与传统模型相比，其性能更优。

Conclusion: 该框架直观、模块化，适用于多种生物类群，为研究生物相互作用提供了新工具。

Abstract: There is no much doubt that biotic interactions shape community assembly and
ultimately the spatial co-variations between species. There is a hope that the
signal of these biotic interactions can be observed and retrieved by
investigating the spatial associations between species while accounting for the
direct effects of the environment. By definition, biotic interactions can be
both symmetric and asymmetric. Yet, most models that attempt to retrieve
species associations from co-occurrence or co-abundance data internally assume
symmetric relationships between species. Here, we propose and validate a
machine-learning framework able to retrieve bidirectional associations by
analyzing species community and environmental data.
  Our framework (1) models pairwise species associations as directed influences
from a source to a target species, parameterized with two species-specific
latent embeddings: the effect of the source species on the community, and the
response of the target species to the community; and (2) jointly fits these
associations within a multi-species conditional generative model with different
modes of interactions between environmental drivers and biotic associations.
Using both simulated and empirical data, we demonstrate the ability of our
framework to recover known asymmetric and symmetric associations and highlight
the properties of the learned association networks. By comparing our approach
to other existing models such as joint species distribution models and
probabilistic graphical models, we show its superior capacity at retrieving
symmetric and asymmetric interactions. The framework is intuitive, modular and
broadly applicable across various taxonomic groups.

</details>


### [446] [An Algorithm for Identifying Interpretable Subgroups With Elevated Treatment Effects](https://arxiv.org/abs/2507.09494)
*Albert Chiu*

Key words: CATE, 可解释性, 规则集, 亚组分析, Pareto最优

TL;DR: 提出一种算法，用于识别可解释的治疗效应提升亚组，通过规则集（如条件A与B或条件C）总结个体或条件平均治疗效应（CATE）估计结果。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现有CATE估计方法通常产生高维且难以解释的结果，本方法旨在提取关键信息以支持决策、政策实施和科学理解。

Method: 提出一个目标函数，平衡亚组规模和效应大小，并通过超参数控制生成Pareto最优规则集。采用样本分割实现有效推断。

Result: 通过模拟和实证研究验证了该方法的实用性和局限性。

Conclusion: 该方法为CATE估计提供了一种可解释的补充工具，适用于复杂的高阶交互场景。

Abstract: We introduce an algorithm for identifying interpretable subgroups with
elevated treatment effects, given an estimate of individual or conditional
average treatment effects (CATE). Subgroups are characterized by ``rule sets''
-- easy-to-understand statements of the form (Condition A AND Condition B) OR
(Condition C) -- which can capture high-order interactions while retaining
interpretability. Our method complements existing approaches for estimating the
CATE, which often produce high dimensional and uninterpretable results, by
summarizing and extracting critical information from fitted models to aid
decision making, policy implementation, and scientific understanding. We
propose an objective function that trades-off subgroup size and effect size,
and varying the hyperparameter that controls this trade-off results in a
``frontier'' of Pareto optimal rule sets, none of which dominates the others
across all criteria. Valid inference is achievable through sample splitting. We
demonstrate the utility and limitations of our method using simulated and
empirical examples.

</details>


### [447] [Signed Graph Learning: Algorithms and Theory](https://arxiv.org/abs/2507.09717)
*Abdullah Karaaslanli,Bisakh Banerjee,Tapabrata Maiti,Selin Aviyente*

Key words: 带符号图学习, 净拉普拉斯矩阵, 平滑信号, ADMM算法, 基因调控网络

TL;DR: 该论文提出了一种从平滑带符号图信号中学习带符号图结构的方法，利用净拉普拉斯矩阵作为图移位算子，并通过非凸优化问题最小化信号总变差。采用ADMM算法解决了优化问题，并提供了理论收敛性证明和误差界限。方法在模拟数据和基因调控网络推断中进行了验证。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现有的图学习研究主要关注无符号图，但许多生物和社会系统更适合用带符号图来描述，因此需要开发一种能够学习带符号图结构的方法。

Method: 使用净拉普拉斯矩阵作为图移位算子，定义平滑带符号图信号为一个低通带符号图滤波器的输出。通过非凸优化问题最小化信号总变差来学习图结构，采用ADMM算法，并提出了一个降低计算复杂度的快速算法。

Result: 提出了一个高效的带符号图学习方法，理论证明了算法的收敛性，并给出了学习误差的界限。在模拟数据和基因调控网络推断中验证了方法的有效性。

Conclusion: 该方法能够有效地学习带符号图结构，并且在计算效率和理论保障方面优于现有方法。

Abstract: Real-world data is often represented through the relationships between data
samples, forming a graph structure. In many applications, it is necessary to
learn this graph structure from the observed data. Current graph learning
research has primarily focused on unsigned graphs, which consist only of
positive edges. However, many biological and social systems are better
described by signed graphs that account for both positive and negative
interactions, capturing similarity and dissimilarity between samples. In this
paper, we develop a method for learning signed graphs from a set of smooth
signed graph signals. Specifically, we employ the net Laplacian as a graph
shift operator (GSO) to define smooth signed graph signals as the outputs of a
low-pass signed graph filter defined by the net Laplacian. The signed graph is
then learned by formulating a non-convex optimization problem where the total
variation of the observed signals is minimized with respect to the net
Laplacian. The proposed problem is solved using alternating direction method of
multipliers (ADMM) and a fast algorithm reducing the per-ADMM iteration
complexity from quadratic to linear in the number of nodes is introduced.
Furthermore, theoretical proofs of convergence for the algorithm and a bound on
the estimation error of the learned net Laplacian as a function of sample size,
number of nodes, and graph topology are provided. Finally, the proposed method
is evaluated on simulated data and gene regulatory network inference problem
and compared to existing signed graph learning methods.

</details>


### [448] [Discovering Governing Equations in the Presence of Uncertainty](https://arxiv.org/abs/2507.09740)
*Ridwan Olabiyi,Han Hu,Ashif Iquebal*

Key words: 随机逆物理发现,SIP,动力学系统,系统变异性,后验分布,Kullback-Leibler散度

TL;DR: 提出了一种新的随机逆物理发现（SIP）框架，通过将未知系数建模为随机变量，并利用Kullback-Leibler散度优化后验分布，有效处理系统变异性和测量噪声，显著提高了动力学系统建模的准确性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 传统方法在处理具有系统变异性和噪声的动态系统时表现不佳，因此需要一种能够同时考虑这些因素的新方法。

Method: 引入SIP框架，将未知系数视为随机变量，通过最小化Kullback-Leibler散度优化后验分布。

Result: 在多个基准问题上，SIP比现有方法（如SINDy）平均降低了82%的系数误差，并能生成具有量化不确定性的可解释模型。

Conclusion: SIP为噪声、变异性和数据有限的动态系统提供了一种鲁棒且数据高效的新方法。

Abstract: In the study of complex dynamical systems, understanding and accurately
modeling the underlying physical processes is crucial for predicting system
behavior and designing effective interventions. Yet real-world systems exhibit
pronounced input (or system) variability and are observed through noisy,
limited data conditions that confound traditional discovery methods that assume
fixed-coefficient deterministic models. In this work, we theorize that
accounting for system variability together with measurement noise is the key to
consistently discover the governing equations underlying dynamical systems. As
such, we introduce a stochastic inverse physics-discovery (SIP) framework that
treats the unknown coefficients as random variables and infers their posterior
distribution by minimizing the Kullback-Leibler divergence between the
push-forward of the posterior samples and the empirical data distribution.
Benchmarks on four canonical problems -- the Lotka-Volterra predator-prey
system (multi- and single-trajectory), the historical Hudson Bay lynx-hare
data, the chaotic Lorenz attractor, and fluid infiltration in porous media
using low- and high-viscosity liquids -- show that SIP consistently identifies
the correct equations and lowers coefficient root-mean-square error by an
average of 82\% relative to the Sparse Identification of Nonlinear Dynamics
(SINDy) approach and its Bayesian variant. The resulting posterior
distributions yield 95\% credible intervals that closely track the observed
trajectories, providing interpretable models with quantified uncertainty. SIP
thus provides a robust, data-efficient approach for consistent physics
discovery in noisy, variable, and data-limited settings.

</details>


### [449] [Regret Analysis of Posterior Sampling-Based Expected Improvement for Bayesian Optimization](https://arxiv.org/abs/2507.09828)
*Shion Takeno,Yu Inatsu,Masayuki Karasuyama,Ichiro Takeuchi*

Key words: 贝叶斯优化，期望改进，高斯过程，遗憾界

TL;DR: 本文分析了基于随机化的期望改进方法（EI），证明了其在高斯过程假设下的次线性贝叶斯累积遗憾界，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 期望改进（EI）在优化高成本黑箱函数中表现优异，但理论研究较少，本文旨在填补这一空白。

Method: 提出了一种随机化的EI变体，通过从后验样本路径的最大值中评估EI。

Result: 在高斯过程假设下，该方法实现了次线性贝叶斯累积遗憾界。

Conclusion: 随机化的EI方法在理论和实验中都表现出有效性。

Abstract: Bayesian optimization is a powerful tool for optimizing an
expensive-to-evaluate black-box function. In particular, the effectiveness of
expected improvement (EI) has been demonstrated in a wide range of
applications. However, theoretical analyses of EI are limited compared with
other theoretically established algorithms. This paper analyzes a randomized
variant of EI, which evaluates the EI from the maximum of the posterior sample
path. We show that this posterior sampling-based random EI achieves the
sublinear Bayesian cumulative regret bounds under the assumption that the
black-box function follows a Gaussian process. Finally, we demonstrate the
effectiveness of the proposed method through numerical experiments.

</details>


### [450] [Simulating Biases for Interpretable Fairness in Offline and Online Classifiers](https://arxiv.org/abs/2507.10154)
*Ricardo Inácio,Zafeiris Kokkinogenis,Vitor Cerqueira,Carlos Soares*

Key words: 偏见缓解,合成数据集,可解释性,代理模型,分类器

TL;DR: 该论文提出了一种框架，通过可控的偏见注入生成合成数据集，并评估偏见如何导致模型决策不公，同时引入了一种新的可解释性技术。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 预测模型常常因训练数据中的偏见而强化不公平决策，因此需要评估和缓解这种偏见。

Method: 开发了一个基于代理的模型（ABM）生成带有系统性偏见的合成数据集，并利用分类器预测贷款结果；同时提出了一种新的可解释性技术——二阶Shapley值。

Result: 研究表明，偏见数据会导致不公平的预测结果，并通过框架和可解释性技术揭示了偏见嵌入过程及其缓解效果。

Conclusion: 论文贡献了一种可控偏见注入的合成数据集生成框架和新的可解释性技术，为偏见缓解提供了工具。

Abstract: Predictive models often reinforce biases which were originally embedded in
their training data, through skewed decisions. In such cases, mitigation
methods are critical to ensure that, regardless of the prevailing disparities,
model outcomes are adjusted to be fair. To assess this, datasets could be
systematically generated with specific biases, to train machine learning
classifiers. Then, predictive outcomes could aid in the understanding of this
bias embedding process. Hence, an agent-based model (ABM), depicting a loan
application process that represents various systemic biases across two
demographic groups, was developed to produce synthetic datasets. Then, by
applying classifiers trained on them to predict loan outcomes, we can assess
how biased data leads to unfairness. This highlights a main contribution of
this work: a framework for synthetic dataset generation with controllable bias
injection. We also contribute with a novel explainability technique, which
shows how mitigations affect the way classifiers leverage data features, via
second-order Shapley values. In experiments, both offline and online learning
approaches are employed. Mitigations are applied at different stages of the
modelling pipeline, such as during pre-processing and in-processing.

</details>


### [451] [MF-GLaM: A multifidelity stochastic emulator using generalized lambda models](https://arxiv.org/abs/2507.10303)
*K. Giannoukou,X. Zhu,S. Marelli,B. Sudret*

Key words: 随机模拟器,多保真度建模,广义λ模型,条件分布

TL;DR: 提出了MF-GLaMs方法，利用低保真度随机模拟器数据高效模拟高保真度随机模拟器的条件响应分布。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 传统确定性替代建模技术无法处理随机模拟器的条件概率分布，且高保真度模拟器数据获取成本高，低保真度模拟器数据可用于补充。

Method: 基于广义λ模型（GLaM），通过四参数广义λ分布表示输入条件下的分布，非侵入式地结合多保真度数据。

Result: 实验表明，MF-GLaMs在相同成本下精度更高，或在显著降低成本下性能相当。

Conclusion: MF-GLaMs是一种高效的多保真度建模方法，适用于随机模拟器的条件分布预测。

Abstract: Stochastic simulators exhibit intrinsic stochasticity due to unobservable,
uncontrollable, or unmodeled input variables, resulting in random outputs even
at fixed input conditions. Such simulators are common across various scientific
disciplines; however, emulating their entire conditional probability
distribution is challenging, as it is a task traditional deterministic
surrogate modeling techniques are not designed for. Additionally, accurately
characterizing the response distribution can require prohibitively large
datasets, especially for computationally expensive high-fidelity (HF)
simulators. When lower-fidelity (LF) stochastic simulators are available, they
can enhance limited HF information within a multifidelity surrogate modeling
(MFSM) framework. While MFSM techniques are well-established for deterministic
settings, constructing multifidelity emulators to predict the full conditional
response distribution of stochastic simulators remains a challenge. In this
paper, we propose multifidelity generalized lambda models (MF-GLaMs) to
efficiently emulate the conditional response distribution of HF stochastic
simulators by exploiting data from LF stochastic simulators. Our approach
builds upon the generalized lambda model (GLaM), which represents the
conditional distribution at each input by a flexible, four-parameter
generalized lambda distribution. MF-GLaMs are non-intrusive, requiring no
access to the internal stochasticity of the simulators nor multiple
replications of the same input values. We demonstrate the efficacy of MF-GLaM
through synthetic examples of increasing complexity and a realistic earthquake
application. Results show that MF-GLaMs can achieve improved accuracy at the
same cost as single-fidelity GLaMs, or comparable performance at significantly
reduced cost.

</details>


### [452] [Information Must Flow: Recursive Bootstrapping for Information Bottleneck in Optimal Transport](https://arxiv.org/abs/2507.10443)
*Xin Li*

Key words: CCUP, 信息瓶颈, 最优传输, 递归推断, 熵最小化

TL;DR: 本文提出了一种名为CCUP的统一框架，将认知建模为高熵环境与低熵内容之间的信息定向流动，并解决了信息瓶颈问题。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 旨在通过CCUP框架统一建模认知过程中信息的双向流动，解决信息瓶颈问题，提升认知效率。

Method: 采用Rao-Blackwellized变分熵最小化方法，结合时空自举技术，实现信息的层级推断和记忆优化。

Result: 证明了Delta收敛定理，显示递归熵最小化可在潜在空间中形成稳定吸引子，从而优化感知和行动规划。

Conclusion: CCUP框架揭示了递归推断在个体认知和集体智能中的基础作用，为信息流的结构化建模提供了新视角。

Abstract: We present the Context-Content Uncertainty Principle (CCUP), a unified
framework that models cognition as the directed flow of information between
high-entropy context and low-entropy content. Inference emerges as a cycle of
bidirectional interactions, bottom-up contextual disambiguation paired with
top-down content reconstruction, which resolves the Information Bottleneck in
Optimal Transport (iBOT). Implemented via Rao-Blackwellized variational entropy
minimization, CCUP steers representations toward minimal joint uncertainty
while preserving inferential directionality. Local cycle completion underpins
temporal bootstrapping, chaining simulations to refine memory, and spatial
bootstrapping, enabling compositional hierarchical inference. We prove a Delta
Convergence Theorem showing that recursive entropy minimization yields
delta-like attractors in latent space, stabilizing perceptual schemas and motor
plans. Temporal bootstrapping through perception-action loops and sleep-wake
consolidation further transforms episodic traces into semantic knowledge.
Extending CCUP, each hierarchical level performs delta-seeded inference:
low-entropy content seeds diffuse outward along goal-constrained paths shaped
by top-down priors and external context, confining inference to task-relevant
manifolds and circumventing the curse of dimensionality. Building on this, we
propose that language emerges as a symbolic transport system, externalizing
latent content to synchronize inference cycles across individuals. Together,
these results establish iBOT as a foundational principle of information flow in
both individual cognition and collective intelligence, positioning recursive
inference as the structured conduit through which minds adapt, align, and
extend.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [453] [The Second Machine Turn: From Checking Proofs to Creating Concepts](https://arxiv.org/abs/2507.10179)
*Asvin G*

Key words: AI, 数学发现, 概念创造, 人机协作

TL;DR: 论文探讨了AI在数学概念创造中的潜在作用及其对数学发展的影响。

<details>
  <summary>Details</summary>

Main category: math.HO

Motivation: 研究AI如何从自动化证明检查发展到自动化创造数学概念，推动数学发现的进步。

Method: 分析了当前技术状态、障碍及解决方案，并尝试将概念创造过程数学化。

Result: 展示了AI在概念创造中的潜力，并探讨了其对数学和人类-机器协作的可能影响。

Conclusion: AI的数学概念创造能力可能重塑数学的未来发展，开启多种可能的未来场景。

Abstract: We identify a second machine turn in the process of mathematical discovery:
after automating proof-checking, AI is now poised to automate the *creation* of
mathematical concepts themselves. We discuss the current state of the art,
obstacles and potential solutions as well as a preliminary attempt at
mathematizing the creation of concepts itself. The paper ends with an
assessment of how these capabilities could reshape mathematics and
human-machine collaboration, and a few different futures we might find
ourselves in.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [454] [Optimizing External Sources for Controlled Burning Plasma in Tokamaks with Neural Ordinary Differential Equations](https://arxiv.org/abs/2507.09431)
*Zefang Liu,Weston M. Stacey*

Key words: Neural ODEs, 托卡马克, 等离子体控制, 逆向建模, 外部源剖面

TL;DR: 提出了一种基于神经普通微分方程（Neural ODEs）的多节点等离子体动力学逆向建模方法，用于精确调控外部粒子与能源源以实现受控燃烧等离子体的目标。

<details>
  <summary>Details</summary>

Main category: physics.plasm-ph

Motivation: 为了实现托卡马克中受控燃烧等离子体的精确调控，需要发展一种能够计算外部源剖面的方法，以驱动等离子体达到目标密度和温度。

Method: 采用逆向建模方法，基于Neural ODEs的多节点等离子体动力学模型，通过优化问题实现目标轨迹与实际轨迹的最小化差异。

Result: 开发了NeuralPlasmaODE框架，能够模拟多区域、多时间尺度的输运，并计算所需的外部源剖面。

Conclusion: 该方法将正向模拟工具转化为面向控制的模型，为现有及未来聚变装置提供了实用的外部源剖面计算方法。

Abstract: Achieving controlled burning plasma in tokamaks requires precise regulation
of external particle and energy sources to reach and maintain target core
densities and temperatures. This work presents an inverse modeling approach
using a multinodal plasma dynamics model based on neural ordinary differential
equations (Neural ODEs). Given a desired time evolution of nodal quantities
such as deuteron density or electron temperature, we compute the external
source profiles, such as neutral beam injection (NBI) power, that drive the
plasma toward the specified behavior. The approach is implemented within the
NeuralPlasmaODE framework, which models multi-region, multi-timescale transport
and incorporates physical mechanisms including radiation, auxiliary heating,
and internodal energy exchange. By formulating the control task as an
optimization problem, we use automatic differentiation through the Neural ODE
solver to minimize the discrepancy between simulated and target trajectories.
This framework transforms the forward simulation tool into a control-oriented
model and provides a practical method for computing external source profiles in
both current and future fusion devices.

</details>


### [455] [Sensitivity Analysis of Transport and Radiation in NeuralPlasmaODE for ITER Burning Plasmas](https://arxiv.org/abs/2507.09432)
*Zefang Liu,Weston M. Stacey*

Key words: NeuralPlasmaODE, ITER, 敏感性分析, 能量约束, 传输机制

TL;DR: 本文介绍了NeuralPlasmaODE模型的扩展应用，用于分析ITER等离子体中传输和辐射机制的敏感性，揭示了关键物理参数对能量约束的影响。

<details>
  <summary>Details</summary>

Main category: physics.plasm-ph

Motivation: 理解物理参数如何影响燃烧等离子体行为对ITER的可靠运行至关重要。

Method: 扩展NeuralPlasmaODE模型，进行传输和辐射机制的敏感性分析，计算核心和边缘温度及密度的归一化敏感性。

Result: 结果表明磁场强度、安全因子和杂质含量对能量约束有显著影响，还揭示了温度依赖性传输对自我调节行为的贡献。

Conclusion: NeuralPlasmaODE在燃烧等离子体环境的预测建模和情景优化中具有实用价值。

Abstract: Understanding how key physical parameters influence burning plasma behavior
is critical for the reliable operation of ITER. In this work, we extend
NeuralPlasmaODE, a multi-region, multi-timescale model based on neural ordinary
differential equations, to perform a sensitivity analysis of transport and
radiation mechanisms in ITER plasmas. Normalized sensitivities of core and edge
temperatures and densities are computed with respect to transport
diffusivities, electron cyclotron radiation (ECR) parameters, impurity
fractions, and ion orbit loss (IOL) timescales. The analysis focuses on
perturbations around a trained nominal model for the ITER inductive scenario.
Results highlight the dominant influence of magnetic field strength, safety
factor, and impurity content on energy confinement, while also revealing how
temperature-dependent transport contributes to self-regulating behavior. These
findings demonstrate the utility of NeuralPlasmaODE for predictive modeling and
scenario optimization in burning plasma environments.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [456] [CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design](https://arxiv.org/abs/2507.09792)
*Prashant Govindarajan,Davide Baldelli,Jay Pathak,Quentin Fournier,Sarath Chandar*

Key words: 计算机辅助设计（CAD）、大型语言模型（LLMs）、自然语言处理、自动化设计、几何度量

TL;DR: 该论文提出了一种基于大型语言模型（LLMs）的自动化CAD设计方法，通过构建大规模数据集和微调模型，实现了从自然语言描述生成CAD序列，显著提高了设计效率。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 当前CAD建模仍然是一项耗时且手动为主的任务，尽管有小规模基于Transformer的模型尝试自动化，但尚未充分利用大型语言模型的潜力。

Method: 作者构建了一个包含17万多个CAD模型的大规模数据集，并使用GPT-4生成人性化描述。通过微调代码-LLMs，将自然语言描述转化为基于JSON格式的CAD序列。提出了基于几何和拓扑的指标（如球度、平均曲率和欧拉特性）来评估生成对象的质量。

Result: 实验和消融研究表明，该方法能够高效自动化CAD设计，大幅提升新对象的设计速度。

Conclusion: 利用大型语言模型实现文本驱动的CAD生成是可行的，且能够显著提升设计效率。

Abstract: Computer-aided design (CAD) is the digital construction of 2D and 3D objects,
and is central to a wide range of engineering and manufacturing applications
like automobile and aviation. Despite its importance, CAD modeling remains
largely a time-intensive, manual task. Recent works have attempted to automate
this process with small transformer-based models and handcrafted CAD sequence
representations. However, there has been little effort to leverage the
potential of large language models (LLMs) for sequential CAD design. In this
work, we introduce a new large-scale dataset of more than 170k CAD models
annotated with high-quality, human-like descriptions generated with our
pipeline based on GPT-4.1. Using this dataset, we fine-tune powerful code-LLMs
to generate CAD sequences represented in a JSON-based format from natural
language descriptions, demonstrating the viability and effectiveness of this
approach for text-conditioned CAD generation. Because simple metrics often fail
to reflect the quality of generated objects, we introduce geometric and
topological metrics based on sphericity, mean curvature, and Euler
characteristic to provide richer structural insights. Our experiments and
ablation studies on both synthetic and human-annotated data demonstrate that
CADmium is able to automate CAD design, drastically speeding up the design of
new objects. The dataset, code, and fine-tuned models are available online.

</details>


### [457] [ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions](https://arxiv.org/abs/2507.10542)
*Shivangi Aneja,Sebastian Weiss,Irene Baeza,Prashanth Chandran,Gaspard Zoss,Matthias Nießner,Derek Bradley*

Key words: 3D头部头像, 高斯泼洒, 局部表情, 实时动画, 高保真渲染

TL;DR: 该论文提出了一种结合局部面部表情和3D高斯泼洒技术的方法，用于生成高保真、实时动画的3D头部头像。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 现有方法在全局表情空间操作，难以捕捉人脸细节和微表情。需要一种能表达局部皮肤褶皱和细微面部动作的高保真头像生成技术。

Method: 使用基于补丁的几何3D面部模型提取局部表情特征，结合Scaffold-GS的锚点动态合成3D高斯，并通过颜色密度化和渐进训练提升效果。

Result: ScaffoldAvatar在实时生成中实现了自然运动和多样表情的视觉效果，性能达到当前最优水平。

Conclusion: 该方法通过局部表情与3D高斯的结合，显著提升了头像的真实感和动态表现力。

Abstract: Generating high-fidelity real-time animated sequences of photorealistic 3D
head avatars is important for many graphics applications, including immersive
telepresence and movies. This is a challenging problem particularly when
rendering digital avatar close-ups for showing character's facial microfeatures
and expressions. To capture the expressive, detailed nature of human heads,
including skin furrowing and finer-scale facial movements, we propose to couple
locally-defined facial expressions with 3D Gaussian splatting to enable
creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In
contrast to previous works that operate on a global expression space, we
condition our avatar's dynamics on patch-based local expression features and
synthesize 3D Gaussians at a patch level. In particular, we leverage a
patch-based geometric 3D face model to extract patch expressions and learn how
to translate these into local dynamic skin appearance and motion by coupling
the patches with anchor points of Scaffold-GS, a recent hierarchical scene
representation. These anchors are then used to synthesize 3D Gaussians
on-the-fly, conditioned by patch-expressions and viewing direction. We employ
color-based densification and progressive training to obtain high-quality
results and faster convergence for high resolution 3K training images. By
leveraging patch-level expressions, ScaffoldAvatar consistently achieves
state-of-the-art performance with visually natural motion, while encompassing
diverse facial expressions and styles in real time.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [458] [KEN: Knowledge Augmentation and Emotion Guidance Network for Multimodal Fake News Detection](https://arxiv.org/abs/2507.09647)
*Peican Zhu,Yubo Jing,Le Cheng,Keke Tang,Yangming Guo*

Key words: 多模态假新闻检测, 知识增强, 情感引导, 平衡学习, LVLM

TL;DR: 本文提出了一种新颖的知识增强与情感引导网络（KEN），用于解决多模态假新闻检测中图像语义理解不足及情感类型处理不佳的问题，通过知识增强和情感平衡学习提升性能。

<details>
  <summary>Details</summary>

Main category: cs.MM

Motivation: 社交媒体的错误信息泛滥使得多模态假新闻检测成为研究重点，但现有研究在图像语义理解和新闻情感类型处理上存在不足。

Method: 提出KEN网络，利用LVLM的强大语义理解生成图像描述并检索文本证据，同时通过平衡学习对不同情感类型的新闻进行细粒度建模。

Result: 在两个真实数据集上的实验证明了KEN的优越性。

Conclusion: KEN通过知识增强和情感引导显著提升了多模态假新闻检测的性能。

Abstract: In recent years, the rampant spread of misinformation on social media has
made accurate detection of multimodal fake news a critical research focus.
However, previous research has not adequately understood the semantics of
images, and models struggle to discern news authenticity with limited textual
information. Meanwhile, treating all emotional types of news uniformly without
tailored approaches further leads to performance degradation. Therefore, we
propose a novel Knowledge Augmentation and Emotion Guidance Network (KEN). On
the one hand, we effectively leverage LVLM's powerful semantic understanding
and extensive world knowledge. For images, the generated captions provide a
comprehensive understanding of image content and scenes, while for text, the
retrieved evidence helps break the information silos caused by the closed and
limited text and context. On the other hand, we consider inter-class
differences between different emotional types of news through balanced
learning, achieving fine-grained modeling of the relationship between emotional
types and authenticity. Extensive experiments on two real-world datasets
demonstrate the superiority of our KEN.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [459] [Dynamical stability for dense patterns in discrete attractor neural networks](https://arxiv.org/abs/2507.10383)
*Uri Cohen,Máté Lengyel*

Key words: 神经网络, 离散吸引子, 动态稳定性, 临界负载, 激活函数

TL;DR: 该论文提出了一种理论，用于分析具有分级神经活动和噪声的网络中离散固定点的局部稳定性，确定了低于临界负载时所有固定点的稳定性，并强调了阈值线性激活和稀疏模式的优点。

<details>
  <summary>Details</summary>

Main category: cond-mat.dis-nn

Motivation: 研究生物记忆模型的神经网络中离散吸引子的动态稳定性问题，传统方法仅在高度限制条件下才能保证稳定性。

Method: 通过直接分析雅可比矩阵的体谱和离群值，推导出在网络中离散固定点局部稳定性的理论。

Result: 确定了临界负载，低于该负载时所有固定点稳定，且该临界负载受固定点中神经活动统计量和单神经元激活函数的影响。

Conclusion: 阈值线性激活和稀疏模式在计算上具有显著优势。

Abstract: Neural networks storing multiple discrete attractors are canonical models of
biological memory. Previously, the dynamical stability of such networks could
only be guaranteed under highly restrictive conditions. Here, we derive a
theory of the local stability of discrete fixed points in a broad class of
networks with graded neural activities and in the presence of noise. By
directly analyzing the bulk and outliers of the Jacobian spectrum, we show that
all fixed points are stable below a critical load that is distinct from the
classical \textit{critical capacity} and depends on the statistics of neural
activities in the fixed points as well as the single-neuron activation
function. Our analysis highlights the computational benefits of
threshold-linear activation and sparse-like patterns.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [460] [Learning from Synthetic Labs: Language Models as Auction Participants](https://arxiv.org/abs/2507.09083)
*Anand Shah,Kehang Zhu,Yanchen Jiang,Jeffrey G. Wang,Arif K. Dayi,John J. Horton,David C. Parkes*

Key words: 大型语言模型, 拍卖, 合成数据, 风险规避, 赢家诅咒

TL;DR: 论文研究了大型语言模型（LLMs）在拍卖中的行为，提出了一种合成数据生成方法，发现LLMs与实验结果一致，尤其在风险规避和策略证明拍卖中表现接近理论预测。

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: 探索LLMs作为人类行为代理在拍卖中的表现，降低实验成本并推动拍卖设计研究。

Method: 使用合成数据生成方法，运行超过1000次拍卖实验，结合链式思维推理能力分析LLMs行为。

Result: LLMs表现出与风险规避人类竞拍者一致的行为，在策略证明拍卖中接近理论预测，并受赢家诅咒影响。

Conclusion: LLMs可作为低成本拍卖实验的代理，其表现需依赖合适的心理模型提示。

Abstract: This paper investigates the behavior of simulated AI agents (large language
models, or LLMs) in auctions, introducing a novel synthetic data-generating
process to help facilitate the study and design of auctions. We find that LLMs
-- when endowed with chain of thought reasoning capacity -- agree with the
experimental literature in auctions across a variety of classic auction
formats. In particular, we find that LLM bidders produce results consistent
with risk-averse human bidders; that they perform closer to theoretical
predictions in obviously strategy-proof auctions; and, that they succumb to the
winner's curse in common value settings. On prompting, we find that LLMs are
not very sensitive to naive changes in prompts (e.g., language, currency) but
can improve dramatically towards theoretical predictions with the right mental
model (i.e., the language of Nash deviations). We run 1,000$+$ auctions for
less than $\$$400 with GPT-4 models (three orders of magnitude cheaper than
modern auction experiments) and develop a framework flexible enough to run
auction experiments with any LLM model and a wide range of auction design
specifications, facilitating further experimental study by decreasing costs and
serving as a proof-of-concept for the use of LLM proxies.

</details>


### [461] [Incentive-Aware Dynamic Resource Allocation under Long-Term Cost Constraints](https://arxiv.org/abs/2507.09473)
*Yan Dai,Negin Golrezaei,Patrick Jaillet*

Key words: 动态资源分配, 战略代理, 社会福利, 成本约束, 激励对齐

TL;DR: 论文研究了在战略代理环境下动态分配可重用资源的问题，提出了一个激励感知框架以确保社会福利最大化、满足长期成本约束并激励真实报告。

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: 研究动机源自实际应用，如云平台分配GPU给用户或政府部署移动医疗单位到竞争地区，需要解决战略代理的私有估值和资源分配的优化问题。

Method: 方法包括对传统原始-对偶方法的数值评估，发现了其在战略环境中的脆弱性，并提出了结合基于时代的延迟更新和随机探索轮的激励感知框架。

Result: 该机制实现了O~(√T)的社会福利遗憾，满足了所有成本约束，并确保了激励一致性，与非战略分配方法的性能相当。

Conclusion: 研究结论表明，提出的框架在战略代理环境下表现良好，具有实际应用潜力。

Abstract: Motivated by applications such as cloud platforms allocating GPUs to users or
governments deploying mobile health units across competing regions, we study
the dynamic allocation of a reusable resource to strategic agents with private
valuations. Our objective is to simultaneously (i) maximize social welfare,
(ii) satisfy multi-dimensional long-term cost constraints, and (iii)
incentivize truthful reporting. We begin by numerically evaluating primal-dual
methods widely used in constrained online optimization and find them to be
highly fragile in strategic settings -- agents can easily manipulate their
reports to distort future dual updates for future gain.
  To address this vulnerability, we develop an incentive-aware framework that
makes primal-dual methods robust to strategic behavior. Our design combines
epoch-based lazy updates -- where dual variables remain fixed within each epoch
-- with randomized exploration rounds that extract approximately truthful
signals for learning. Leveraging carefully designed online learning subroutines
that can be of independent interest for dual updates, our mechanism achieves
$\tilde{\mathcal{O}}(\sqrt{T})$ social welfare regret, satisfies all cost
constraints, and ensures incentive alignment. This matches the performance of
non-strategic allocation approaches while being robust to strategic agents.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [462] [Predictive Causal Inference via Spatio-Temporal Modeling and Penalized Empirical Likelihood](https://arxiv.org/abs/2507.08896)
*Byunghee Lee,Hye Yeon Sin,Joonsung Kang*

Key words: 预测因果推断,HMM,MTGCN,时空模型,生物医学数据

TL;DR: 提出一种集成框架，结合HMM和MTGCN，通过非对称处理时空信息，提升因果推断的预测准确性和偏差校正。

<details>
  <summary>Details</summary>

Main category: stat.ME

Motivation: 解决传统单一模型在时空复杂性生物医学数据中因果推断的局限性。

Method: 结合HMM（空间健康状态估计）和MTGCN（时间结果轨迹捕捉），非对称处理时空信息作为内外生变量。

Result: 框架在癌症、痴呆和帕金森病等临床领域中提升了预测准确性和偏差校正能力。

Conclusion: 该框架通过适应生物医学数据中的时空复杂性，推动了预测性因果推断的发展。

Abstract: This study introduces an integrated framework for predictive causal inference
designed to overcome limitations inherent in conventional single model
approaches. Specifically, we combine a Hidden Markov Model (HMM) for spatial
health state estimation with a Multi Task and Multi Graph Convolutional Network
(MTGCN) for capturing temporal outcome trajectories. The framework
asymmetrically treats temporal and spatial information regarding them as
endogenous variables in the outcome regression, and exogenous variables in the
propensity score model, thereby expanding the standard doubly robust treatment
effect estimation to jointly enhance bias correction and predictive accuracy.
To demonstrate its utility, we focus on clinical domains such as cancer,
dementia, and Parkinson disease, where treatment effects are challenging to
observe directly. Simulation studies are conducted to emulate latent disease
dynamics and evaluate the model performance under varying conditions. Overall,
the proposed framework advances predictive causal inference by structurally
adapting to spatiotemporal complexities common in biomedical data.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [463] [Less Stress, More Privacy: Stress Detection on Anonymized Speech of Air Traffic Controllers](https://arxiv.org/abs/2507.08882)
*Janaki Viswanathan,Alexander Blatt,Konrad Hagemann,Dietrich Klakow*

Key words: 关键词：ATC, 压力检测, 匿名化, 深度学习, GDPR

TL;DR: 摘要研究了在隐私保护条件下通过匿名化ATC语音数据进行压力检测的模型性能。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 研究动机是在遵守GDPR等隐私法规的同时，实现高效的ATC语音数据压力检测，以确保航空安全。

Method: 方法包括评估不同匿名化ATC语音数据的架构，并在匿名化的SUSAS数据集和ATC模拟数据集上进行测试。

Result: 最佳模型在匿名化SUSAS数据集上的压力检测准确率达93.6%，在模拟数据集上达80.1%。

Conclusion: 结论表明隐私保护不会妨碍构建高性能的深度学习模型。

Abstract: Air traffic control (ATC) demands multi-tasking under time pressure with high
consequences of an error. This can induce stress. Detecting stress is a key
point in maintaining the high safety standards of ATC. However, processing ATC
voice data entails privacy restrictions, e.g. the General Data Protection
Regulation (GDPR) law. Anonymizing the ATC voice data is one way to comply with
these restrictions. In this paper, different architectures for stress detection
for anonymized ATCO speech are evaluated. Our best networks reach a stress
detection accuracy of 93.6% on an anonymized version of the Speech Under
Simulated and Actual Stress (SUSAS) dataset and an accuracy of 80.1% on our
anonymized ATC simulation dataset. This shows that privacy does not have to be
an impediment in building well-performing deep-learning-based models.

</details>


### [464] [Voice Conversion for Lombard Speaking Style with Implicit and Explicit Acoustic Feature Conditioning](https://arxiv.org/abs/2507.09310)
*Dominika Woszczyk,Manuel Sam Ribeiro,Thomas Merritt,Daniel Korzekwa*

Key words: 文本转语音, Lombard风格, 语音转换, 隐式条件化, 清晰度

TL;DR: 研究提出了一种通过语音转换（VC）在缺乏目标说话者数据的情况下训练Lombard风格文本转语音（TTS）系统的方法，比较了隐式和显式声学特征条件化的效果。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: Lombard风格的TTS系统能提高语音清晰度，但录制大规模数据困难，研究探索通过VC技术解决这一问题。

Method: 比较了隐式和显式声学特征条件化的语音转换模型，提出了一种隐式条件化策略。

Result: 隐式条件化策略在保持说话者相似性的同时，取得了与显式条件化模型相当的清晰度提升。

Conclusion: 隐式条件化策略是Lombard风格语音转换的有效方法。

Abstract: Text-to-Speech (TTS) systems in Lombard speaking style can improve the
overall intelligibility of speech, useful for hearing loss and noisy
conditions. However, training those models requires a large amount of data and
the Lombard effect is challenging to record due to speaker and noise
variability and tiring recording conditions. Voice conversion (VC) has been
shown to be a useful augmentation technique to train TTS systems in the absence
of recorded data from the target speaker in the target speaking style. In this
paper, we are concerned with Lombard speaking style transfer. Our goal is to
convert speaker identity while preserving the acoustic attributes that define
the Lombard speaking style. We compare voice conversion models with implicit
and explicit acoustic feature conditioning. We observe that our proposed
implicit conditioning strategy achieves an intelligibility gain comparable to
the model conditioned on explicit acoustic features, while also preserving
speaker similarity.

</details>


### [465] [Evaluating Fake Music Detection Performance Under Audio Augmentations](https://arxiv.org/abs/2507.10447)
*Tomasz Sroka,Tomasz Wężowicz,Dominik Sidorczuk,Mateusz Modrzejewski*

Key words: 生成音频, 鲁棒性, 音乐深度伪造, 音频增强, 检测模型

TL;DR: 论文研究了音频增强对区分人工与生成音乐检测模型鲁棒性的影响，发现即使轻微增强也会显著降低模型性能。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 随着生成音频模型的快速发展，区分人工与生成音乐变得更具挑战性。为此，研究检测假音乐的模型鲁棒性。

Method: 构建真实与合成音乐数据集，应用多种音频变换，分析其对分类准确性的影响。测试最新音乐深度伪造检测模型的性能。

Result: 即使轻微音频增强也会显著降低模型的分类性能。

Conclusion: 当前的音乐深度伪造检测模型在音频增强下表现脆弱，需进一步改进。

Abstract: With the rapid advancement of generative audio models, distinguishing between
human-composed and generated music is becoming increasingly challenging. As a
response, models for detecting fake music have been proposed. In this work, we
explore the robustness of such systems under audio augmentations. To evaluate
model generalization, we constructed a dataset consisting of both real and
synthetic music generated using several systems. We then apply a range of audio
transformations and analyze how they affect classification accuracy. We test
the performance of a recent state-of-the-art musical deepfake detection model
in the presence of audio augmentations. The performance of the model decreases
significantly even with the introduction of light augmentations.

</details>


### [466] [MB-RIRs: a Synthetic Room Impulse Response Dataset with Frequency-Dependent Absorption Coefficients](https://arxiv.org/abs/2507.09750)
*Enric Gusó,Joanna Luberadzka,Umut Sayin,Xavier Serra*

Key words: 语音增强, 房间脉冲响应, 多频带吸收系数, 声学模拟

TL;DR: 研究四种策略对单耳语音增强中的合成房间脉冲响应（RIR）数据集生态有效性的影响，发现多频带吸收系数策略表现最佳。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 探讨如何通过多频带吸收系数、声源方向性和接收器方向性等策略提升合成RIR数据集的真实性，以改进语音增强性能。

Method: 在传统基于图像源方法的RIR基础上添加三项特征，并对比SoundSpaces数据集的网格RIR；使用DeepFilternet3模型训练并评估。

Result: 多频带吸收系数的RIR在真实RIR测试中表现最优，SDR提升0.51dB，MUSHRA评分提升8.9分。

Conclusion: 多频带吸收系数的RIR能显著提升语音增强模型的性能，数据集已公开下载。

Abstract: We investigate the effects of four strategies for improving the ecological
validity of synthetic room impulse response (RIR) datasets for monoaural Speech
Enhancement (SE). We implement three features on top of the traditional image
source method-based (ISM) shoebox RIRs: multiband absorption coefficients,
source directivity and receiver directivity. We additionally consider
mesh-based RIRs from the SoundSpaces dataset. We then train a DeepFilternet3
model for each RIR dataset and evaluate the performance on a test set of real
RIRs both objectively and subjectively. We find that RIRs which use
frequency-dependent acoustic absorption coefficients (MB-RIRs) can obtain
+0.51dB of SDR and a +8.9 MUSHRA score when evaluated on real RIRs. The MB-RIRs
dataset is publicly available for free download.

</details>


### [467] [AudioMAE++: learning better masked audio representations with SwiGLU FFNs](https://arxiv.org/abs/2507.10464)
*Sarthak Yadav,Sergios Theodoridis,Zheng-Hua Tan*

Key words: AudioMAE++, masked autoencoders, transformer, self-supervised learning, audio classification

TL;DR: AudioMAE++是一种改进的音频掩码自编码器，通过引入macaron风格的transformer块和门控线性单元，显著提升了音频分类和语音任务的性能。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 现有的音频MAE方法仍使用传统的transformer模块，而transformer领域已有许多新进展未被应用。本文旨在通过引入新架构提升音频MAE的性能。

Method: 提出AudioMAE++，使用macaron风格的transformer块和门控线性单元（GLUs）作为改进。

Result: 在AudioSet数据集上预训练后，AudioMAE++在10个下游任务中表现优于现有MAE方法，且具有优秀的扩展性。

Conclusion: AudioMAE++展示了通过新架构改进音频MAE的潜力，显著提升了性能。

Abstract: Masked Autoencoders (MAEs) trained on audio spectrogram patches have emerged
as a prominent approach for learning self-supervised audio representations.
While several recent papers have evaluated key aspects of training MAEs on
audio data, the majority of these approaches still leverage vanilla transformer
building blocks, whereas the transformer community has seen steady integration
of newer architectural advancements. In this work, we propose AudioMAE++, a
revamped audio masked autoencoder with two such enhancements, namely
macaron-style transformer blocks with gated linear units. When pretrained on
the AudioSet dataset, the proposed AudioMAE++ models outperform existing MAE
based approaches on 10 diverse downstream tasks, demonstrating excellent
performance on audio classification and speech-based benchmarks. The proposed
AudioMAE++ models also demonstrate excellent scaling characteristics,
outperforming directly comparable standard MAE baselines with up to 4x more
parameters.

</details>


### [468] [WildFX: A DAW-Powered Pipeline for In-the-Wild Audio FX Graph Modeling](https://arxiv.org/abs/2507.10534)
*Qihui Yang,Taylor Berg-Kirkpatrick,Julian McAuley,Zachary Novack*

Key words: AI音乐生成,数字信号处理,Docker,插件集成,多轨道音频混合

TL;DR: WildFX是一个基于Docker的管道，用于生成多轨道音频混合数据集，支持专业数字音频工作站的后端，并能无缝集成跨平台商业插件。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 解决AI在专业数字信号处理（DSP）工作流建模中的挑战，特别是对音频效果图（如混响、压缩、均衡）的神经黑盒建模问题。

Method: 使用Docker容器化的WildFX管道，支持VST/VST3/LV2/CLAP格式的插件，提供高效并行处理和简约的元数据接口。

Result: 实验验证了WildFX在混合图、插件/增益参数盲估计方面的有效性，并展示了其在AI研究与实际DSP需求之间的桥梁作用。

Conclusion: WildFX为AI音乐生成提供了实用的DSP工作流建模解决方案。

Abstract: Despite rapid progress in end-to-end AI music generation, AI-driven modeling
of professional Digital Signal Processing (DSP) workflows remains challenging.
In particular, while there is growing interest in neural black-box modeling of
audio effect graphs (e.g. reverb, compression, equalization), AI-based
approaches struggle to replicate the nuanced signal flow and parameter
interactions used in professional workflows. Existing differentiable plugin
approaches often diverge from real-world tools, exhibiting inferior performance
relative to simplified neural controllers under equivalent computational
constraints. We introduce WildFX, a pipeline containerized with Docker for
generating multi-track audio mixing datasets with rich effect graphs, powered
by a professional Digital Audio Workstation (DAW) backend. WildFX supports
seamless integration of cross-platform commercial plugins or any plugins in the
wild, in VST/VST3/LV2/CLAP formats, enabling structural complexity (e.g.,
sidechains, crossovers) and achieving efficient parallelized processing. A
minimalist metadata interface simplifies project/plugin configuration.
Experiments demonstrate the pipeline's validity through blind estimation of
mixing graphs, plugin/gain parameters, and its ability to bridge AI research
with practical DSP demands. The code is available on:
https://github.com/IsaacYQH/WildFX.

</details>
