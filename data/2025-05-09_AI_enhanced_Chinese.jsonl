{"id": "2505.04638", "pdf": "https://arxiv.org/pdf/2505.04638", "abs": "https://arxiv.org/abs/2505.04638", "authors": ["Tianyu Liu", "Simeng Han", "Xiao Luo", "Hanchen Wang", "Pan Lu", "Biqing Zhu", "Yuge Wang", "Keyi Li", "Jiapeng Chen", "Rihao Qu", "Yufeng Liu", "Xinyue Cui", "Aviv Yaish", "Yuhang Chen", "Minsheng Hao", "Chuhan Li", "Kexing Li", "Arman Cohan", "Hua Xu", "Mark Gerstein", "James Zou", "Hongyu Zhao"], "title": "Towards Artificial Intelligence Research Assistant for Expert-Involved Learning", "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "36 pages, 7 figures", "summary": "Large Language Models (LLMs) and Large Multi-Modal Models (LMMs) have emerged\nas transformative tools in scientific research, yet their reliability and\nspecific contributions to biomedical applications remain insufficiently\ncharacterized. In this study, we present \\textbf{AR}tificial\n\\textbf{I}ntelligence research assistant for \\textbf{E}xpert-involved\n\\textbf{L}earning (ARIEL), a multimodal dataset designed to benchmark and\nenhance two critical capabilities of LLMs and LMMs in biomedical research:\nsummarizing extensive scientific texts and interpreting complex biomedical\nfigures. To facilitate rigorous assessment, we create two open-source sets\ncomprising biomedical articles and figures with designed questions. We\nsystematically benchmark both open- and closed-source foundation models,\nincorporating expert-driven human evaluations conducted by doctoral-level\nexperts. Furthermore, we improve model performance through targeted prompt\nengineering and fine-tuning strategies for summarizing research papers, and\napply test-time computational scaling to enhance the reasoning capabilities of\nLMMs, achieving superior accuracy compared to human-expert corrections. We also\nexplore the potential of using LMM Agents to generate scientific hypotheses\nfrom diverse multimodal inputs. Overall, our results delineate clear strengths\nand highlight significant limitations of current foundation models, providing\nactionable insights and guiding future advancements in deploying large-scale\nlanguage and multi-modal models within biomedical research.", "AI": {"tldr": "This paper introduces ARIEL, a dataset for benchmarking LLMs and LMMs in biomedical tasks like summarizing texts and interpreting figures. It includes expert evaluations and proposes performance improvements through prompt engineering and fine-tuning.", "motivation": "The study aims to address the under-characterized reliability and contributions of LLMs and LMMs in biomedical research by providing a standardized benchmark.", "method": "Created open-source datasets of biomedical articles and figures, benchmarked models with expert evaluations, and improved performance via prompt engineering and fine-tuning.", "result": "Achieved superior accuracy in tasks like summarization and figure interpretation compared to human experts, while also exploring LMMs' potential for hypothesis generation.", "conclusion": "The work highlights both the capabilities and limitations of current models, offering insights for future advancements in biomedical AI applications."}}
{"id": "2505.04646", "pdf": "https://arxiv.org/pdf/2505.04646", "abs": "https://arxiv.org/abs/2505.04646", "authors": ["Poria Azadi"], "title": "Computational Irreducibility as the Foundation of Agency: A Formal Model Connecting Undecidability to Autonomous Behavior in Complex Systems", "categories": ["cs.AI", "cs.CC", "cs.IT", "math.IT"], "comment": null, "summary": "This article explores the emergence of autonomy and agency by connecting\nfundamental computational limits (decidability, completeness, computational\nirreducibility) with physical concepts. We introduce a formal model of a\n\"minimal agent\" operating within potentially Turing-complete environments.\nUsing algorithmic information theory, we argue that the inherent undecidability\nand computational irreducibility of agent-environment interaction lead to\nunpredictability and novel information generation, enabling agency (effective\ngoal-directed action). Computational irreducibility prevents full external\nprediction, creating necessary conditions for autonomous behavior. We relate\nthis to computational sourcehood, where an agent is the irreducible origin of\nits behavior, though formalizing this concept remains challenging. Our central\nthesis, formally proven, is that genuine autonomy necessarily implies\nundecidability from an external perspective, distinguishing autonomous systems\nfrom predictable ones. We propose that agency arises when agent-environment\ncoupling complexity allows mutual information between internal states and\nrelevant environmental variables to increase, particularly where analytical\nsolutions are absent and operational closure is needed for persistence. This\nframework links agency directly to the computational properties of interaction,\noffering implications for understanding consciousness, designing autonomous AI,\nand reconceptualizing free will in a deterministic yet computationally\nirreducible universe.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u8ba1\u7b97\u4e0d\u53ef\u7ea6\u6027\u7b49\u7406\u8bba\uff0c\u63d0\u51fa\u6700\u5c0f\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u8bc1\u660e\u771f\u6b63\u7684\u81ea\u4e3b\u6027\u5fc5\u7136\u4f34\u968f\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u5e76\u5c06\u667a\u80fd\u4f53\u884c\u4e3a\u4e0e\u8ba1\u7b97\u7279\u6027\u76f4\u63a5\u5173\u8054\u3002", "motivation": "\u63a2\u7d22\u81ea\u4e3b\u6027\u4e0e\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u8ba1\u7b97\u57fa\u7840\uff0c\u8bd5\u56fe\u901a\u8fc7\u8ba1\u7b97\u7406\u8bba\u89e3\u91ca\u81ea\u4e3b\u6027\u3001\u4e0d\u53ef\u9884\u6d4b\u6027\u548c\u65b0\u4fe1\u606f\u751f\u6210\u7684\u672c\u8d28\u3002", "method": "\u5f15\u5165\u6700\u5c0f\u667a\u80fd\u4f53\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u7ed3\u5408\u7b97\u6cd5\u4fe1\u606f\u7406\u8bba\u548c\u8ba1\u7b97\u4e0d\u53ef\u7ea6\u6027\uff0c\u5206\u6790\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u7279\u6027\u3002", "result": "\u8bc1\u660e\u81ea\u4e3b\u6027\u5fc5\u7136\u5bfc\u81f4\u5916\u90e8\u89c6\u89d2\u7684\u4e0d\u53ef\u5224\u5b9a\u6027\uff0c\u63d0\u51fa\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u8ba1\u7b97\u8d77\u6e90\u7406\u8bba\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7406\u89e3\u610f\u8bc6\u3001\u8bbe\u8ba1\u81ea\u4e3bAI\u53ca\u5728\u8ba1\u7b97\u4e0d\u53ef\u7ea6\u5b87\u5b99\u4e2d\u91cd\u65b0\u5b9a\u4e49\u81ea\u7531\u610f\u5fd7\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2505.04674", "pdf": "https://arxiv.org/pdf/2505.04674", "abs": "https://arxiv.org/abs/2505.04674", "authors": ["Enqiang Zhu", "Chenkai Hao", "Chanjuan Liu", "Yongsheng Rao"], "title": "Dynamic Location Search for Identifying Maximum Weighted Independent Sets in Complex Networks", "categories": ["cs.AI"], "comment": null, "summary": "While Artificial intelligence (AI), including Generative AI, are effective at\ngenerating high-quality traffic data and optimization solutions in intelligent\ntransportation systems (ITSs), these techniques often demand significant\ntraining time and computational resources, especially in large-scale and\ncomplex scenarios. To address this, we introduce a novel and efficient\nalgorithm for solving the maximum weighted independent set (MWIS) problem,\nwhich can be used to model many ITSs applications, such as traffic signal\ncontrol and vehicle routing. Given the NP-hard nature of the MWIS problem, our\nproposed algorithm, DynLS, incorporates three key innovations to solve it\neffectively. First, it uses a scores-based adaptive vertex perturbation (SAVP)\ntechnique to accelerate convergence, particularly in sparse graphs. Second, it\nincludes a region location mechanism (RLM) to help escape local optima by\ndynamically adjusting the search space. Finally, it employs a novel variable\nneighborhood descent strategy, ComLS, which combines vertex exchange strategies\nwith a reward mechanism to guide the search toward high-quality solutions. Our\nexperimental results demonstrate DynLS's superior performance, consistently\ndelivering high-quality solutions within 1000 seconds. DynLS outperformed five\nleading algorithms across 360 test instances, achieving the best solution for\n350 instances and surpassing the second-best algorithm, Cyclic-Fast, by 177\ninstances. Moreover, DynLS matched Cyclic-Fast's convergence speed,\nhighlighting its efficiency and practicality. This research represents a\nsignificant advancement in heuristic algorithms for the MWIS problem, offering\na promising approach to aid AI techniques in optimizing intelligent\ntransportation systems.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aDynLS\u7684\u65b0\u578b\u9ad8\u6548\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6700\u5927\u52a0\u6743\u72ec\u7acb\u96c6\uff08MWIS\uff09\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u53ef\u7528\u4e8e\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff08ITS\uff09\u5e94\u7528\u3002\u901a\u8fc7\u4e09\u9879\u521b\u65b0\u6280\u672f\uff0cDynLS\u5728360\u4e2a\u6d4b\u8bd5\u5b9e\u4f8b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u89e3\u51b3\u4e86AI\u5728ITS\u4e2d\u9ad8\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u7684\u95ee\u9898\u3002", "motivation": "AI\u548c\u751f\u6210\u5f0fAI\u5728ITS\u4e2d\u867d\u7136\u6709\u6548\uff0c\u4f46\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u65f6\u95f4\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u590d\u6742\u573a\u666f\u4e2d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86DynLS\u7b97\u6cd5\u3002", "method": "DynLS\u7ed3\u5408\u4e86\u4e09\u79cd\u5173\u952e\u6280\u672f\uff1a\u57fa\u4e8e\u5206\u6570\u7684\u81ea\u9002\u5e94\u9876\u70b9\u6270\u52a8\uff08SAVP\uff09\u3001\u533a\u57df\u5b9a\u4f4d\u673a\u5236\uff08RLM\uff09\u52a8\u6001\u8c03\u6574\u641c\u7d22\u7a7a\u95f4\uff0c\u4ee5\u53ca\u7ed3\u5408\u9876\u70b9\u4ea4\u6362\u7b56\u7565\u548c\u5956\u52b1\u673a\u5236\u7684ComLS\u7b56\u7565\u3002", "result": "\u5728360\u4e2a\u6d4b\u8bd5\u5b9e\u4f8b\u4e2d\uff0cDynLS\u5728350\u4e2a\u5b9e\u4f8b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u4e8e\u7b2c\u4e8c\u540d\u7b97\u6cd5Cyclic-Fast177\u4e2a\u5b9e\u4f8b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u4f3c\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "DynLS\u662fMWIS\u95ee\u9898\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u4e3aAI\u4f18\u5316ITS\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.04736", "pdf": "https://arxiv.org/pdf/2505.04736", "abs": "https://arxiv.org/abs/2505.04736", "authors": ["Sutapa Dey Tithi", "Arun Kumar Ramesh", "Clara DiMarco", "Xiaoyi Tian", "Nazia Alam", "Kimia Fazeli", "Tiffany Barnes"], "title": "The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems", "categories": ["cs.AI"], "comment": null, "summary": "Intelligent tutoring systems have demonstrated effectiveness in teaching\nformal propositional logic proofs, but their reliance on template-based\nexplanations limits their ability to provide personalized student feedback.\nWhile large language models (LLMs) offer promising capabilities for dynamic\nfeedback generation, they risk producing hallucinations or pedagogically\nunsound explanations. We evaluated the stepwise accuracy of LLMs in\nconstructing multi-step symbolic logic proofs, comparing six prompting\ntechniques across four state-of-the-art LLMs on 358 propositional logic\nproblems. Results show that DeepSeek-V3 achieved superior performance with\n84.4% accuracy on stepwise proof construction and excelled particularly in\nsimpler rules. We further used the best-performing LLM to generate explanatory\nhints for 1,050 unique student problem-solving states from a logic ITS and\nevaluated them on 4 criteria with both an LLM grader and human expert ratings\non a 20% sample. Our analysis finds that LLM-generated hints were 75% accurate\nand rated highly by human evaluators on consistency and clarity, but did not\nperform as well explaining why the hint was provided or its larger context. Our\nresults demonstrate that LLMs may be used to augment tutoring systems with\nlogic tutoring hints, but requires additional modifications to ensure accuracy\nand pedagogical appropriateness.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7b26\u53f7\u903b\u8f91\u8bc1\u660e\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0DeepSeek-V3\u5728\u9010\u6b65\u8bc1\u660e\u6784\u5efa\u4e2d\u51c6\u786e\u7387\u8fbe84.4%\uff0c\u4f46\u5728\u751f\u6210\u89e3\u91ca\u6027\u63d0\u793a\u65f6\u4ecd\u9700\u6539\u8fdb\u4ee5\u786e\u4fdd\u6559\u5b66\u9002\u7528\u6027\u3002", "motivation": "\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u5728\u547d\u9898\u903b\u8f91\u6559\u5b66\u4e2d\u4f9d\u8d56\u6a21\u677f\u5316\u89e3\u91ca\uff0c\u7f3a\u4e4f\u4e2a\u6027\u5316\u53cd\u9988\uff0c\u800cLLMs\u867d\u80fd\u52a8\u6001\u751f\u6210\u53cd\u9988\uff0c\u5374\u53ef\u80fd\u4ea7\u751f\u4e0d\u51c6\u786e\u6216\u6559\u5b66\u4e0a\u4e0d\u5408\u7406\u7684\u89e3\u91ca\u3002\u672c\u6587\u65e8\u5728\u9a8c\u8bc1LLMs\u5728\u903b\u8f91\u8bc1\u660e\u4e2d\u7684\u5b9e\u9645\u8868\u73b0\u3002", "method": "\u6bd4\u8f83\u4e86\u56db\u79cd\u524d\u6cbfLLMs\u7684\u516d\u79cd\u63d0\u793a\u6280\u672f\uff0c\u8bc4\u4f30\u5176\u5728358\u4e2a\u547d\u9898\u903b\u8f91\u95ee\u9898\u4e0a\u7684\u9010\u6b65\u51c6\u786e\u6027\uff0c\u5e76\u5229\u7528\u6700\u4f73LLM\u4e3a1,050\u4e2a\u5b66\u751f\u89e3\u9898\u72b6\u6001\u751f\u6210\u63d0\u793a\uff0c\u901a\u8fc7LLM\u8bc4\u5206\u548c20%\u6837\u672c\u7684\u4eba\u5de5\u8bc4\u4f30\u8fdb\u884c\u68c0\u9a8c\u3002", "result": "DeepSeek-V3\u5728\u9010\u6b65\u8bc1\u660e\u4e2d\u8868\u73b0\u6700\u4f73\uff0884.4%\u51c6\u786e\u7387\uff09\uff0c\u751f\u6210\u7684\u63d0\u793a\u5728\u4e00\u81f4\u6027\u548c\u6e05\u6670\u5ea6\u4e0a\u83b7\u9ad8\u5206\uff0c\u4f46\u5728\u89e3\u91ca\u4e0a\u4e0b\u6587\u548c\u539f\u56e0\u4e0a\u8f83\u5f31\uff0c\u603b\u4f53\u51c6\u786e\u7387\u4e3a75%\u3002", "conclusion": "LLMs\u53ef\u8f85\u52a9\u903b\u8f91\u8f85\u5bfc\u7cfb\u7edf\u7684\u63d0\u793a\u751f\u6210\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u63d0\u5347\u51c6\u786e\u6027\u548c\u6559\u5b66\u9002\u7528\u6027\u3002"}}
{"id": "2505.04634", "pdf": "https://arxiv.org/pdf/2505.04634", "abs": "https://arxiv.org/abs/2505.04634", "authors": ["Abhiroop Bhattacharya", "Sylvain G. Cloutier"], "title": "MatMMFuse: Multi-Modal Fusion model for Material Property Prediction", "categories": ["cs.LG", "cs.CE"], "comment": "Presented at AI for Accelerated Materials Design(AI4Mat), ICLR 2025\n  (https://openreview.net/forum?id=pN4Zg6HBlq#discussion)", "summary": "The recent progress of using graph based encoding of crystal structures for\nhigh throughput material property prediction has been quite successful.\nHowever, using a single modality model prevents us from exploiting the\nadvantages of an enhanced features space by combining different\nrepresentations. Specifically, pre-trained Large language models(LLMs) can\nencode a large amount of knowledge which is beneficial for training of models.\nMoreover, the graph encoder is able to learn the local features while the text\nencoder is able to learn global information such as space group and crystal\nsymmetry. In this work, we propose Material Multi-Modal Fusion(MatMMFuse), a\nfusion based model which uses a multi-head attention mechanism for the\ncombination of structure aware embedding from the Crystal Graph Convolution\nNetwork (CGCNN) and text embeddings from the SciBERT model. We train our model\nin an end-to-end framework using data from the Materials Project Dataset. We\nshow that our proposed model shows an improvement compared to the vanilla CGCNN\nand SciBERT model for all four key properties: formation energy, band gap,\nenergy above hull and fermi energy. Specifically, we observe an improvement of\n40% compared to the vanilla CGCNN model and 68% compared to the SciBERT model\nfor predicting the formation energy per atom. Importantly, we demonstrate the\nzero shot performance of the trained model on small curated datasets of\nPerovskites, Chalcogenides and the Jarvis Dataset. The results show that the\nproposed model exhibits better zero shot performance than the individual plain\nvanilla CGCNN and SciBERT model. This enables researchers to deploy the model\nfor specialized industrial applications where collection of training data is\nprohibitively expensive.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u878d\u5408\u6a21\u578bMatMMFuse\uff0c\u901a\u8fc7\u7ed3\u5408\u6676\u4f53\u56fe\u5377\u79ef\u7f51\u7edc\uff08CGCNN\uff09\u7684\u7ed3\u6784\u5d4c\u5165\u548cSciBERT\u6a21\u578b\u7684\u6587\u672c\u5d4c\u5165\uff0c\u63d0\u5347\u4e86\u6750\u6599\u5c5e\u6027\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u5e76\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5355\u4e00\u6a21\u6001\u6a21\u578b\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u7279\u5f81\u7a7a\u95f4\u7684\u4f18\u52bf\uff0c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08\u5982SciBERT\uff09\u80fd\u7f16\u7801\u4e30\u5bcc\u77e5\u8bc6\uff0c\u800c\u56fe\u7f16\u7801\u5668\u80fd\u5b66\u4e60\u5c40\u90e8\u7279\u5f81\uff0c\u7ed3\u5408\u4e24\u8005\u53ef\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u591a\u6ce8\u610f\u529b\u5934\u673a\u5236\u878d\u5408CGCNN\u7684\u7ed3\u6784\u5d4c\u5165\u548cSciBERT\u7684\u6587\u672c\u5d4c\u5165\uff0c\u57fa\u4e8eMaterials Project Dataset\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u76f8\u6bd4\u5355\u6a21\u6a21\u578bCGCNN\u548cSciBERT\uff0cMatMMFuse\u5728\u5f62\u6210\u80fd\u3001\u5e26\u9699\u7b49\u56db\u9879\u5173\u952e\u5c5e\u6027\u4e0a\u5206\u522b\u63d0\u534740%\u548c68%\uff0c\u5e76\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "MatMMFuse\u6a21\u578b\u5728\u591a\u6a21\u6001\u878d\u5408\u548c\u96f6\u6837\u672c\u9884\u6d4b\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u7684\u5de5\u4e1a\u573a\u666f\u3002"}}
{"id": "2505.04628", "pdf": "https://arxiv.org/pdf/2505.04628", "abs": "https://arxiv.org/abs/2505.04628", "authors": ["Yusen Wu", "Junwu Xiong", "Xiaotie Deng"], "title": "How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "Expanding the application of large language models (LLMs) to societal life,\ninstead of primary function only as auxiliary assistants to communicate with\nonly one person at a time, necessitates LLMs' capabilities to independently\nplay roles in multi-user, multi-turn social agent tasks within complex social\nsettings. However, currently the capability has not been systematically\nmeasured with available benchmarks. To address this gap, we first introduce an\nagent task leveling framework grounded in sociological principles.\nConcurrently, we propose a novel benchmark, How Social Is It (we call it HSII\nbelow), designed to assess LLM's social capabilities in comprehensive social\nagents tasks and benchmark representative models. HSII comprises four stages:\nformat parsing, target selection, target switching conversation, and stable\nconversation, which collectively evaluate the communication and task completion\ncapabilities of LLMs within realistic social interaction scenarios dataset,\nHSII-Dataset. The dataset is derived step by step from news dataset. We perform\nan ablation study by doing clustering to the dataset. Additionally, we\ninvestigate the impact of chain of thought (COT) method on enhancing LLMs'\nsocial performance. Since COT cost more computation, we further introduce a new\nstatistical metric, COT-complexity, to quantify the efficiency of certain LLMs\nwith COTs for specific social tasks and strike a better trade-off between\nmeasurement of correctness and efficiency. Various results of our experiments\ndemonstrate that our benchmark is well-suited for evaluating social skills in\nLLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHSII\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u793e\u4ea4\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u9636\u6bb5\u5168\u9762\u6d4b\u8bd5\u5176\u793e\u4ea4\u8868\u73b0\u3002\u540c\u65f6\u7814\u7a76\u4e86\u94fe\u5f0f\u601d\u7ef4\uff08COT\uff09\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86COT-complexity\u6307\u6807\u4ee5\u6743\u8861\u6b63\u786e\u6027\u4e0e\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u7cfb\u7edf\u5730\u8861\u91cfLLMs\u5728\u591a\u7528\u6237\u3001\u591a\u8f6e\u793e\u4ea4\u4efb\u52a1\u4e2d\u72ec\u7acb\u626e\u6f14\u89d2\u8272\u7684\u80fd\u529b\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u793e\u4f1a\u5b66\u539f\u7406\u7684\u4ee3\u7406\u4efb\u52a1\u5206\u7ea7\u6846\u67b6\u548cHSII\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u56db\u4e2a\u9636\u6bb5\uff1a\u683c\u5f0f\u89e3\u6790\u3001\u76ee\u6807\u9009\u62e9\u3001\u76ee\u6807\u5207\u6362\u5bf9\u8bdd\u548c\u7a33\u5b9a\u5bf9\u8bdd\u3002\u901a\u8fc7HSII-Dataset\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u7814\u7a76COT\u65b9\u6cd5\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHSII\u57fa\u51c6\u80fd\u6709\u6548\u8bc4\u4f30LLMs\u7684\u793e\u4ea4\u80fd\u529b\uff0c\u4e14COT\u65b9\u6cd5\u867d\u63d0\u5347\u6027\u80fd\u4f46\u589e\u52a0\u8ba1\u7b97\u6210\u672c\uff0cCOT-complexity\u6307\u6807\u80fd\u4f18\u5316\u6548\u7387\u4e0e\u6b63\u786e\u6027\u7684\u6743\u8861\u3002", "conclusion": "HSII\u662f\u8bc4\u4f30LLMs\u793e\u4ea4\u80fd\u529b\u7684\u5408\u9002\u5de5\u5177\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u590d\u6742\u7684\u793e\u4f1a\u573a\u666f\u3002"}}
{"id": "2505.04822", "pdf": "https://arxiv.org/pdf/2505.04822", "abs": "https://arxiv.org/abs/2505.04822", "authors": ["Lior Fox", "Yonatan Loewenstein"], "title": "Is there Value in Reinforcement Learning?", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted to The 6th Multidisciplinary Conference on Reinforcement\n  Learning and Decision Making (RLDM 2025)", "summary": "Action-values play a central role in popular Reinforcement Learing (RL)\nmodels of behavior. Yet, the idea that action-values are explicitly represented\nhas been extensively debated. Critics had therefore repeatedly suggested that\npolicy-gradient (PG) models should be favored over value-based (VB) ones, as a\npotential solution for this dilemma. Here we argue that this solution is\nunsatisfying. This is because PG methods are not, in fact, \"Value-free\" --\nwhile they do not rely on an explicit representation of Value for acting\n(stimulus-response mapping), they do require it for learning. Hence, switching\nto PG models is, per se, insufficient for eliminating Value from models of\nbehavior. More broadly, the requirement for a representation of Value stems\nfrom the underlying assumptions regarding the optimization objective posed by\nthe standard RL framework, not from the particular algorithm chosen to solve\nit. Previous studies mostly took these standard RL assumptions for granted, as\npart of their conceptualization or problem modeling, while debating the\ndifferent methods used to optimize it (i.e., PG or VB). We propose that,\ninstead, the focus of the debate should shift to critically evaluating the\nunderlying modeling assumptions. Such evaluation is particularly important from\nan experimental perspective. Indeed, the very notion of Value must be\nreconsidered when standard assumptions (e.g., risk neutrality,\nfull-observability, Markovian environment, exponential discounting) are\nrelaxed, as is likely in natural settings. Finally, we use the Value debate as\na case study to argue in favor of a more nuanced, algorithmic rather than\nstatistical, view of what constitutes \"a model\" in cognitive sciences. Our\nanalysis suggests that besides \"parametric\" statistical complexity, additional\naspects such as computational complexity must also be taken into account when\nevaluating model complexity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba8\u8bba\u4e86\u52a8\u4f5c\u503c\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6838\u5fc3\u5730\u4f4d\u53ca\u5176\u8868\u793a\u4e89\u8bae\uff0c\u63d0\u51fa\u7b56\u7565\u68af\u5ea6\u6a21\u578b\u5e76\u975e\u771f\u6b63\u201c\u65e0\u503c\u201d\uff0c\u5e76\u547c\u5401\u91cd\u65b0\u8bc4\u4f30\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u7684\u57fa\u672c\u5047\u8bbe\uff0c\u5f3a\u8c03\u6a21\u578b\u8bc4\u4f30\u5e94\u8003\u8651\u8ba1\u7b97\u590d\u6742\u6027\u3002", "motivation": "\u89e3\u51b3\u52a8\u4f5c\u503c\u8868\u793a\u5728\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u4e89\u8bae\uff0c\u6307\u51fa\u7b56\u7565\u68af\u5ea6\u6a21\u578b\u5e76\u975e\u771f\u6b63\u201c\u65e0\u503c\u201d\uff0c\u5e76\u63a8\u52a8\u5b66\u672f\u754c\u91cd\u65b0\u5ba1\u89c6\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u7684\u57fa\u672c\u5047\u8bbe\u3002", "method": "\u901a\u8fc7\u5206\u6790\u7b56\u7565\u68af\u5ea6\u4e0e\u57fa\u4e8e\u503c\u7684\u65b9\u6cd5\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u8bba\u8bc1\u52a8\u4f5c\u503c\u5728\u5b66\u4e60\u548c\u884c\u4e3a\u4e2d\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63a2\u8ba8\u653e\u677e\u6807\u51c6\u5047\u8bbe\u540e\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "result": "\u63ed\u793a\u4e86\u7b56\u7565\u68af\u5ea6\u6a21\u578b\u4ecd\u9700\u4f9d\u8d56\u503c\u7684\u8868\u793a\u8fdb\u884c\u5b66\u4e60\uff0c\u5f3a\u8c03\u9700\u91cd\u65b0\u8bc4\u4f30\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u7684\u5e95\u5c42\u5047\u8bbe\uff0c\u5c24\u5176\u662f\u5728\u81ea\u7136\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u63d0\u8bae\u5c06\u8ba8\u8bba\u7126\u70b9\u8f6c\u5411\u6a21\u578b\u7684\u57fa\u672c\u5047\u8bbe\uff0c\u5e76\u63d0\u5021\u5728\u8ba4\u77e5\u79d1\u5b66\u4e2d\u5bf9\u6a21\u578b\u590d\u6742\u6027\u7684\u8bc4\u4f30\u5e94\u7efc\u5408\u8003\u8651\u53c2\u6570\u7edf\u8ba1\u548c\u8ba1\u7b97\u590d\u6742\u6027\u3002"}}
{"id": "2505.04733", "pdf": "https://arxiv.org/pdf/2505.04733", "abs": "https://arxiv.org/abs/2505.04733", "authors": ["Shai Feldman", "Stephen Bates", "Yaniv Romano"], "title": "Conformal Prediction with Corrupted Labels: Uncertain Imputation and Robust Re-weighting", "categories": ["cs.LG"], "comment": null, "summary": "We introduce a framework for robust uncertainty quantification in situations\nwhere labeled training data are corrupted, through noisy or missing labels. We\nbuild on conformal prediction, a statistical tool for generating prediction\nsets that cover the test label with a pre-specified probability. The validity\nof conformal prediction, however, holds under the i.i.d assumption, which does\nnot hold in our setting due to the corruptions in the data. To account for this\ndistribution shift, the privileged conformal prediction (PCP) method proposed\nleveraging privileged information (PI) -- additional features available only\nduring training -- to re-weight the data distribution, yielding valid\nprediction sets under the assumption that the weights are accurate. In this\nwork, we analyze the robustness of PCP to inaccuracies in the weights. Our\nanalysis indicates that PCP can still yield valid uncertainty estimates even\nwhen the weights are poorly estimated. Furthermore, we introduce uncertain\nimputation (UI), a new conformal method that does not rely on weight\nestimation. Instead, we impute corrupted labels in a way that preserves their\nuncertainty. Our approach is supported by theoretical guarantees and validated\nempirically on both synthetic and real benchmarks. Finally, we show that these\ntechniques can be integrated into a triply robust framework, ensuring\nstatistically valid predictions as long as at least one underlying method is\nvalid.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8bad\u7ec3\u6570\u636e\u5b58\u5728\u566a\u58f0\u6216\u7f3a\u5931\u6807\u7b7e\u65f6\u8fdb\u884c\u7a33\u5065\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5e76\u5206\u6790\u4e86\u6743\u91cd\u4f30\u8ba1\u4e0d\u51c6\u786e\u65f6\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6807\u7b7e\u53ef\u80fd\u56e0\u566a\u58f0\u6216\u7f3a\u5931\u800c\u635f\u574f\uff0c\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u8bc1\u9884\u6d4b\u96c6\u7684\u7edf\u8ba1\u6709\u6548\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u9002\u5e94\u6570\u636e\u635f\u574f\u7684\u7a33\u5065\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u5206\u6790\u4e86\u7279\u6743\u5171\u5f62\u9884\u6d4b\uff08PCP\uff09\u5728\u6743\u91cd\u4f30\u8ba1\u4e0d\u51c6\u786e\u65f6\u7684\u7a33\u5065\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u4e0d\u786e\u5b9a\u63d2\u8865\uff08UI\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d2\u8865\u53d7\u635f\u6807\u7b7e\u6765\u4fdd\u6301\u5176\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cPCP\u5728\u6743\u91cd\u4e0d\u51c6\u786e\u65f6\u4ecd\u6709\u6548\uff0c\u800cUI\u65b9\u6cd5\u65e0\u9700\u4f9d\u8d56\u6743\u91cd\u4f30\u8ba1\u5373\u53ef\u4fdd\u6301\u9884\u6d4b\u96c6\u7684\u7edf\u8ba1\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u4e09\u91cd\u7a33\u5065\u6846\u67b6\u7ed3\u5408\u4e86PCP\u548cUI\u65b9\u6cd5\uff0c\u786e\u4fdd\u53ea\u8981\u81f3\u5c11\u4e00\u79cd\u65b9\u6cd5\u6709\u6548\uff0c\u5373\u53ef\u751f\u6210\u7edf\u8ba1\u6709\u6548\u7684\u9884\u6d4b\u96c6\u3002"}}
{"id": "2505.04637", "pdf": "https://arxiv.org/pdf/2505.04637", "abs": "https://arxiv.org/abs/2505.04637", "authors": ["Dongxing Yu"], "title": "Adaptive Token Boundaries: Integrating Human Chunking Mechanisms into Multimodal LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in multimodal large language models (MLLMs) have\ndemonstrated remarkable capabilities in processing diverse data types, yet\nsignificant disparities persist between human cognitive processes and\ncomputational approaches to multimodal information integration. This research\npresents a systematic investigation into the parallels between human\ncross-modal chunking mechanisms and token representation methodologies in\nMLLMs. Through empirical studies comparing human performance patterns with\nmodel behaviors across visual-linguistic tasks, we demonstrate that\nconventional static tokenization schemes fundamentally constrain current\nmodels' capacity to simulate the dynamic, context-sensitive nature of human\ninformation processing. We propose a novel framework for dynamic cross-modal\ntokenization that incorporates adaptive boundaries, hierarchical\nrepresentations, and alignment mechanisms grounded in cognitive science\nprinciples. Quantitative evaluations demonstrate that our approach yields\nstatistically significant improvements over state-of-the-art models on\nbenchmark tasks (+7.8% on Visual Question Answering, +5.3% on Complex Scene\nDescription) while exhibiting more human-aligned error patterns and attention\ndistributions. These findings contribute to the theoretical understanding of\nthe relationship between human cognition and artificial intelligence, while\nproviding empirical evidence for developing more cognitively plausible AI\nsystems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u7684\u5dee\u8ddd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8de8\u6a21\u6001\u6807\u8bb0\u5316\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u5e76\u66f4\u8d34\u8fd1\u4eba\u7c7b\u8ba4\u77e5\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u591a\u6a21\u6001\u4fe1\u606f\u6574\u5408\u4e2d\u5b58\u5728\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7684\u663e\u8457\u5dee\u5f02\uff0c\u5c24\u5176\u662f\u9759\u6001\u6807\u8bb0\u5316\u65b9\u6cd5\u9650\u5236\u4e86\u6a21\u578b\u6a21\u62df\u4eba\u7c7b\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u5904\u7406\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8de8\u6a21\u6001\u6807\u8bb0\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u8fb9\u754c\u3001\u5206\u5c42\u8868\u793a\u548c\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u539f\u7406\u7684\u5bf9\u9f50\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u7684\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u3002", "result": "\u65b0\u6846\u67b6\u5728VQA\u548c\u590d\u6742\u573a\u666f\u63cf\u8ff0\u4efb\u52a1\u4e0a\u5206\u522b\u5b9e\u73b0+7.8%\u548c+5.3%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u9519\u8bef\u6a21\u5f0f\u548c\u6ce8\u610f\u529b\u5206\u5e03\u66f4\u63a5\u8fd1\u4eba\u7c7b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4eba\u7c7b\u8ba4\u77e5\u4e0eAI\u7684\u7406\u8bba\u5173\u8054\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u7b26\u5408\u8ba4\u77e5\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2505.04843", "pdf": "https://arxiv.org/pdf/2505.04843", "abs": "https://arxiv.org/abs/2505.04843", "authors": ["Sebasti\u00e1n R. Castro", "Roberto Campbell", "Nancy Lau", "Octavio Villalobos", "Jiaqi Duan", "Alvaro A. Cardenas"], "title": "Large Language Models are Autonomous Cyber Defenders", "categories": ["cs.AI", "cs.CR", "I.2.0"], "comment": "Presented at IEEE CAI Workshop on Adaptive Cyber Defense 2025.\n  Proceedings to appear", "summary": "Fast and effective incident response is essential to prevent adversarial\ncyberattacks. Autonomous Cyber Defense (ACD) aims to automate incident response\nthrough Artificial Intelligence (AI) agents that plan and execute actions. Most\nACD approaches focus on single-agent scenarios and leverage Reinforcement\nLearning (RL). However, ACD RL-trained agents depend on costly training, and\ntheir reasoning is not always explainable or transferable. Large Language\nModels (LLMs) can address these concerns by providing explainable actions in\ngeneral security contexts. Researchers have explored LLM agents for ACD but\nhave not evaluated them on multi-agent scenarios or interacting with other ACD\nagents. In this paper, we show the first study on how LLMs perform in\nmulti-agent ACD environments by proposing a new integration to the CybORG CAGE\n4 environment. We examine how ACD teams of LLM and RL agents can interact by\nproposing a novel communication protocol. Our results highlight the strengths\nand weaknesses of LLMs and RL and help us identify promising research\ndirections to create, train, and deploy future teams of ACD agents.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLM\u5728\u591a\u667a\u80fd\u4f53\u81ea\u4e3b\u7f51\u7edc\u9632\u5fa1\uff08ACD\uff09\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684CybORG CAGE 4\u96c6\u6210\u548c\u901a\u4fe1\u534f\u8bae\uff0c\u8bc4\u4f30\u4e86LLM\u4e0eRL\u667a\u80fd\u4f53\u7684\u4e92\u52a8\u6548\u679c\u3002", "motivation": "\u5f53\u524dACD\u4e3b\u8981\u4f9d\u8d56\u5355\u667a\u80fd\u4f53RL\u65b9\u6cd5\uff0c\u5b58\u5728\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u5dee\u7684\u95ee\u9898\u3002LLM\u901a\u8fc7\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u52a8\u4f5c\u5728\u5b89\u5168\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5c1a\u672a\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e0b\u6d4b\u8bd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684CybORG CAGE 4\u73af\u5883\u96c6\u6210\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u901a\u4fe1\u534f\u8bae\uff0c\u4ee5\u8bc4\u4f30LLM\u4e0eRL\u667a\u80fd\u4f53\u5728\u591a\u667a\u80fd\u4f53ACD\u4e2d\u7684\u534f\u4f5c\u8868\u73b0\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u548cRL\u5728\u591a\u667a\u80fd\u4f53ACD\u4e2d\u7684\u4f18\u52a3\u52bf\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765ACD\u56e2\u961f\u521b\u5efa\u3001\u8bad\u7ec3\u548c\u90e8\u7f72\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "LLM\u5728\u591a\u667a\u80fd\u4f53ACD\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u4e0eRL\u7684\u7ed3\u5408\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.04738", "pdf": "https://arxiv.org/pdf/2505.04738", "abs": "https://arxiv.org/abs/2505.04738", "authors": ["Stepan Tretiakov", "Xingjian Li", "Krishna Kumar"], "title": "SetONet: A Deep Set-based Operator Network for Solving PDEs with permutation invariant variable input sampling", "categories": ["cs.LG", "I.2; G.1.8"], "comment": null, "summary": "Neural operators, particularly the Deep Operator Network (DeepONet), have\nshown promise in learning mappings between function spaces for solving\ndifferential equations. However, standard DeepONet requires input functions to\nbe sampled at fixed locations, limiting its applicability in scenarios with\nvariable sensor configurations, missing data, or irregular grids. We introduce\nthe Set Operator Network (SetONet), a novel architecture that integrates Deep\nSets principles into the DeepONet framework to address this limitation. The\ncore innovation lies in the SetONet branch network, which processes the input\nfunction as an unordered \\emph{set} of location-value pairs. This design\nensures permutation invariance with respect to the input points, making SetONet\ninherently robust to variations in the number and locations of sensors. SetONet\nlearns richer, spatially-aware input representations by explicitly processing\nspatial coordinates and function values. We demonstrate SetONet's effectiveness\non several benchmark problems, including derivative/anti-derivative operators,\n1D Darcy flow, and 2D elasticity. Results show that SetONet successfully learns\noperators under variable input sampling conditions where standard DeepONet\nfails. Furthermore, SetONet is architecturally robust to sensor drop-off;\nunlike standard DeepONet, which requires methods like interpolation to function\nwith missing data. Notably, SetONet can achieve comparable or improved accuracy\nover DeepONet on fixed grids, particularly for nonlinear problems, likely due\nto its enhanced input representation. SetONet provides a flexible and robust\nextension to the neural operator toolkit, significantly broadening the\napplicability of operator learning to problems with variable or incomplete\ninput data.", "AI": {"tldr": "SetONet\u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408DeepONet\u548cDeep Sets\u539f\u7406\uff0c\u89e3\u51b3\u4e86\u6807\u51c6DeepONet\u5728\u5904\u7406\u53ef\u53d8\u4f20\u611f\u5668\u914d\u7f6e\u3001\u7f3a\u5931\u6570\u636e\u6216\u4e0d\u89c4\u5219\u7f51\u683c\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6807\u51c6DeepONet\u5728\u5904\u7406\u8f93\u5165\u51fd\u6570\u65f6\u9700\u56fa\u5b9a\u91c7\u6837\u4f4d\u7f6e\uff0c\u9650\u5236\u4e86\u5176\u5728\u53ef\u53d8\u4f20\u611f\u5668\u914d\u7f6e\u3001\u7f3a\u5931\u6570\u636e\u6216\u4e0d\u89c4\u5219\u7f51\u683c\u573a\u666f\u4e0b\u7684\u5e94\u7528\u3002SetONet\u65e8\u5728\u514b\u670d\u8fd9\u4e00\u9650\u5236\u3002", "method": "SetONet\u91c7\u7528\u5206\u652f\u7f51\u7edc\u5904\u7406\u8f93\u5165\u51fd\u6570\u7684\u65e0\u5e8f\u4f4d\u7f6e-\u503c\u5bf9\u96c6\u5408\uff0c\u786e\u4fdd\u5bf9\u8f93\u5165\u70b9\u7684\u6392\u5217\u4e0d\u53d8\u6027\uff0c\u4ece\u800c\u589e\u5f3a\u5bf9\u4f20\u611f\u5668\u6570\u91cf\u548c\u4f4d\u7f6e\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSetONet\u80fd\u5728\u53d8\u91cf\u8f93\u5165\u91c7\u6837\u6761\u4ef6\u4e0b\u6210\u529f\u5b66\u4e60\u7b97\u5b50\uff0c\u4e14\u5728\u56fa\u5b9a\u7f51\u683c\u4e0a\u7684\u6027\u80fd\u4f18\u4e8e\u6216\u4e0eDeepONet\u76f8\u5f53\uff0c\u5c24\u5176\u5bf9\u975e\u7ebf\u6027\u95ee\u9898\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "SetONet\u4e3a\u795e\u7ecf\u7b97\u5b50\u5de5\u5177\u7bb1\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u9c81\u68d2\u7684\u6269\u5c55\uff0c\u663e\u8457\u62d3\u5bbd\u4e86\u7b97\u5b50\u5b66\u4e60\u5728\u53d8\u91cf\u6216\u4e0d\u5b8c\u6574\u8f93\u5165\u6570\u636e\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2505.04639", "pdf": "https://arxiv.org/pdf/2505.04639", "abs": "https://arxiv.org/abs/2505.04639", "authors": ["Abhishek Mishra", "Ritesh Sur Chowdhury", "Vartul Bahuguna", "Isha Pandey", "Ganesh Ramakrishnan"], "title": "Language translation, and change of accent for speech-to-speech task using diffusion model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speech-to-speech translation (S2ST) aims to convert spoken input in one\nlanguage to spoken output in another, typically focusing on either language\ntranslation or accent adaptation. However, effective cross-cultural\ncommunication requires handling both aspects simultaneously - translating\ncontent while adapting the speaker's accent to match the target language\ncontext. In this work, we propose a unified approach for simultaneous speech\ntranslation and change of accent, a task that remains underexplored in current\nliterature. Our method reformulates the problem as a conditional generation\ntask, where target speech is generated based on phonemes and guided by target\nspeech features. Leveraging the power of diffusion models, known for\nhigh-fidelity generative capabilities, we adapt text-to-image diffusion\nstrategies by conditioning on source speech transcriptions and generating Mel\nspectrograms representing the target speech with desired linguistic and\naccentual attributes. This integrated framework enables joint optimization of\ntranslation and accent adaptation, offering a more parameter-efficient and\neffective model compared to traditional pipelines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540c\u65f6\u8fdb\u884c\u8bed\u97f3\u7ffb\u8bd1\u548c\u53e3\u97f3\u8c03\u6574\u7684\u7edf\u4e00\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u76ee\u6807\u8bed\u97f3\u7684\u6885\u5c14\u9891\u8c31\u56fe\uff0c\u4f18\u5316\u7ffb\u8bd1\u548c\u53e3\u97f3\u9002\u5e94\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\uff08S2ST\uff09\u901a\u5e38\u53ea\u5173\u6ce8\u8bed\u8a00\u7ffb\u8bd1\u6216\u53e3\u97f3\u8c03\u6574\uff0c\u800c\u8de8\u6587\u5316\u6c9f\u901a\u9700\u8981\u540c\u65f6\u5904\u7406\u4e24\u8005\u3002", "method": "\u5c06\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6761\u4ef6\u751f\u6210\u4efb\u52a1\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u76ee\u6807\u8bed\u97f3\u7684\u6885\u5c14\u9891\u8c31\u56fe\uff0c\u5e76\u901a\u8fc7\u6e90\u8bed\u97f3\u8f6c\u5f55\u548c\u76ee\u6807\u8bed\u97f3\u7279\u5f81\u8fdb\u884c\u6307\u5bfc\u3002", "result": "\u63d0\u51fa\u7684\u96c6\u6210\u6846\u67b6\u5728\u7ffb\u8bd1\u548c\u53e3\u97f3\u9002\u5e94\u65b9\u9762\u66f4\u9ad8\u6548\u4e14\u6709\u6548\uff0c\u4f18\u4e8e\u4f20\u7edf\u6d41\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u540c\u65f6\u5904\u7406\u8bed\u97f3\u7ffb\u8bd1\u548c\u53e3\u97f3\u8c03\u6574\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u53c2\u6570\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2505.04851", "pdf": "https://arxiv.org/pdf/2505.04851", "abs": "https://arxiv.org/abs/2505.04851", "authors": ["Viacheslav Vasilev", "Vladimir Arkhipkin", "Julia Agafonova", "Tatiana Nikulina", "Evelina Mironova", "Alisa Shichanina", "Nikolai Gerasimenko", "Mikhail Shoytov", "Denis Dimitrov"], "title": "CRAFT: Cultural Russian-Oriented Dataset Adaptation for Focused Text-to-Image Generation", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.CY", "cs.LG"], "comment": "This is arxiv version of the paper which was accepted for the Doklady\n  Mathematics Journal in 2024", "summary": "Despite the fact that popular text-to-image generation models cope well with\ninternational and general cultural queries, they have a significant knowledge\ngap regarding individual cultures. This is due to the content of existing large\ntraining datasets collected on the Internet, which are predominantly based on\nWestern European or American popular culture. Meanwhile, the lack of cultural\nadaptation of the model can lead to incorrect results, a decrease in the\ngeneration quality, and the spread of stereotypes and offensive content. In an\neffort to address this issue, we examine the concept of cultural code and\nrecognize the critical importance of its understanding by modern image\ngeneration models, an issue that has not been sufficiently addressed in the\nresearch community to date. We propose the methodology for collecting and\nprocessing the data necessary to form a dataset based on the cultural code, in\nparticular the Russian one. We explore how the collected data affects the\nquality of generations in the national domain and analyze the effectiveness of\nour approach using the Kandinsky 3.1 text-to-image model. Human evaluation\nresults demonstrate an increase in the level of awareness of Russian culture in\nthe model.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u4e3b\u6d41\u6587\u751f\u56fe\u6a21\u578b\u5728\u56fd\u9645\u548c\u901a\u7528\u6587\u5316\u67e5\u8be2\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4e2a\u4f53\u6587\u5316\u4e0a\u5b58\u5728\u663e\u8457\u77e5\u8bc6\u7f3a\u53e3\uff0c\u4e3b\u8981\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u4ee5\u6b27\u7f8e\u6d41\u884c\u6587\u5316\u4e3a\u4e3b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u5316\u4ee3\u7801\uff08\u5c24\u5176\u662f\u4fc4\u7f57\u65af\u6587\u5316\uff09\u7684\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7Kandinsky 3.1\u6a21\u578b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u4fc4\u7f57\u65af\u6587\u5316\u7684\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u6587\u751f\u56fe\u6a21\u578b\u56e0\u8bad\u7ec3\u6570\u636e\u4ee5\u6b27\u7f8e\u6587\u5316\u4e3a\u4e3b\uff0c\u5bfc\u81f4\u5bf9\u5176\u4ed6\u6587\u5316\uff08\u5982\u4fc4\u7f57\u65af\uff09\u7684\u751f\u6210\u7ed3\u679c\u5b58\u5728\u504f\u89c1\u6216\u9519\u8bef\uff0c\u751a\u81f3\u53ef\u80fd\u4f20\u64ad\u5192\u72af\u6027\u5185\u5bb9\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6587\u5316\u4ee3\u7801\u6570\u636e\u96c6\u586b\u8865\u8fd9\u4e00\u7f3a\u53e3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u7279\u5b9a\u6587\u5316\uff08\u4fc4\u7f57\u65af\uff09\u7684\u6570\u636e\u6536\u96c6\u4e0e\u5904\u7406\u65b9\u6cd5\uff0c\u6784\u5efa\u6587\u5316\u4ee3\u7801\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528Kandinsky 3.1\u6a21\u578b\u8fdb\u884c\u751f\u6210\u8d28\u91cf\u4e0e\u6587\u5316\u9002\u5e94\u6027\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u4eba\u7c7b\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u4fc4\u7f57\u65af\u6587\u5316\u7684\u751f\u6210\u51c6\u786e\u6027\u4e0e\u6587\u5316\u654f\u611f\u6027\uff0c\u51cf\u5c11\u4e86\u523b\u677f\u5370\u8c61\u548c\u9519\u8bef\u5185\u5bb9\u3002", "conclusion": "\u6587\u5316\u4ee3\u7801\u6570\u636e\u96c6\u7684\u5f15\u5165\u80fd\u6709\u6548\u6539\u5584\u6587\u751f\u56fe\u6a21\u578b\u5728\u7279\u5b9a\u6587\u5316\u9886\u57df\u7684\u751f\u6210\u8d28\u91cf\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u6587\u5316\u4ee5\u4fc3\u8fdb\u591a\u6837\u6027\u3002"}}
{"id": "2505.04741", "pdf": "https://arxiv.org/pdf/2505.04741", "abs": "https://arxiv.org/abs/2505.04741", "authors": ["Kenneth Li", "Yida Chen", "Fernanda Vi\u00e9gas", "Martin Wattenberg"], "title": "When Bad Data Leads to Good Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025", "summary": "In large language model (LLM) pretraining, data quality is believed to\ndetermine model quality. In this paper, we re-examine the notion of \"quality\"\nfrom the perspective of pre- and post-training co-design. Specifically, we\nexplore the possibility that pre-training on more toxic data can lead to better\ncontrol in post-training, ultimately decreasing a model's output toxicity.\nFirst, we use a toy experiment to study how data composition affects the\ngeometry of features in the representation space. Next, through controlled\nexperiments with Olmo-1B models trained on varying ratios of clean and toxic\ndata, we find that the concept of toxicity enjoys a less entangled linear\nrepresentation as the proportion of toxic data increases. Furthermore, we show\nthat although toxic data increases the generational toxicity of the base model,\nit also makes the toxicity easier to remove. Evaluations on Toxigen and Real\nToxicity Prompts demonstrate that models trained on toxic data achieve a better\ntrade-off between reducing generational toxicity and preserving general\ncapabilities when detoxifying techniques such as inference-time intervention\n(ITI) are applied. Our findings suggest that, with post-training taken into\naccount, bad data may lead to good models.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u6bd2\u6027\u5185\u5bb9\u6bd4\u4f8b\u5bf9\u6a21\u578b\u63a7\u5236\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9002\u5f53\u7684\u6bd2\u6027\u6570\u636e\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u89e3\u6bd2\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u91cd\u65b0\u5b9a\u4e49\u6570\u636e\u2018\u8d28\u91cf\u2019\uff0c\u63a2\u7d22\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u52a0\u5165\u66f4\u591a\u6bd2\u6027\u6570\u636e\u662f\u5426\u80fd\u5728\u540e\u8bad\u7ec3\u9636\u6bb5\u63d0\u4f9b\u66f4\u597d\u7684\u63a7\u5236\u80fd\u529b\uff0c\u4ece\u800c\u964d\u4f4e\u6a21\u578b\u8f93\u51fa\u7684\u6bd2\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u901a\u8fc7\u5c0f\u89c4\u6a21\u5b9e\u9a8c\u5206\u6790\u6570\u636e\u7ec4\u6210\u5bf9\u7279\u5f81\u8868\u793a\u7a7a\u95f4\u7684\u5f71\u54cd\uff0c\u5e76\u63a7\u5236\u5b9e\u9a8c\u6bd4\u8f83\u4e0d\u540c\u6bd4\u4f8b\u6e05\u6d01\u4e0e\u6bd2\u6027\u6570\u636e\u8bad\u7ec3\u7684Olmo-1B\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6bd2\u6027\u6570\u636e\u4f7f\u6a21\u578b\u4e2d\u6bd2\u6027\u6982\u5ff5\u7ebf\u6027\u8868\u793a\u66f4\u6e05\u6670\uff0c\u867d\u7136\u589e\u52a0\u4e86\u57fa\u7840\u6a21\u578b\u7684\u6bd2\u6027\u8f93\u51fa\uff0c\u4f46\u540e\u5904\u7406\u89e3\u6bd2\u66f4\u6709\u6548\u3002\u5728Toxigen\u548cReal Toxicity Prompts\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6bd2\u6027\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u89e3\u6bd2\u4e0e\u6027\u80fd\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u8003\u8651\u540e\u8bad\u7ec3\u5904\u7406\u65f6\uff0c\u8d28\u91cf\u8f83\u5dee\u7684\u6570\u636e\u4e5f\u53ef\u80fd\u8bad\u7ec3\u51fa\u66f4\u597d\u7684\u6a21\u578b\u3002"}}
{"id": "2505.04640", "pdf": "https://arxiv.org/pdf/2505.04640", "abs": "https://arxiv.org/abs/2505.04640", "authors": ["Hicham Assoudi"], "title": "A Comparative Benchmark of a Moroccan Darija Toxicity Detection Model (Typica.ai) and Major LLM-Based Moderation APIs (OpenAI, Mistral, Anthropic)", "categories": ["cs.CL"], "comment": "GitHub repository with reproducibility materials and evaluation\n  notebook available at:\n  https://github.com/assoudi-typica-ai/darija-toxicity-benchmark", "summary": "This paper presents a comparative benchmark evaluating the performance of\nTypica.ai's custom Moroccan Darija toxicity detection model against major\nLLM-based moderation APIs: OpenAI (omni-moderation-latest), Mistral\n(mistral-moderation-latest), and Anthropic Claude (claude-3-haiku-20240307). We\nfocus on culturally grounded toxic content, including implicit insults,\nsarcasm, and culturally specific aggression often overlooked by general-purpose\nsystems. Using a balanced test set derived from the OMCD_Typica.ai_Mix dataset,\nwe report precision, recall, F1-score, and accuracy, offering insights into\nchallenges and opportunities for moderation in underrepresented languages. Our\nresults highlight Typica.ai's superior performance, underlining the importance\nof culturally adapted models for reliable content moderation.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86Typica.ai\u7684\u6469\u6d1b\u54e5\u65b9\u8a00\u6bd2\u6027\u68c0\u6d4b\u6a21\u578b\u4e0e\u4e3b\u6d41LLM\u5ba1\u6838API\u7684\u6027\u80fd\uff0c\u5f3a\u8c03\u4e86\u6587\u5316\u9002\u5e94\u6027\u6a21\u578b\u5728\u5185\u5bb9\u5ba1\u6838\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u8bc4\u4f30\u6587\u5316\u76f8\u5173\u6bd2\u6027\u5185\u5bb9\uff08\u5982\u9690\u6666\u4fae\u8fb1\u3001\u8bbd\u523a\uff09\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u586b\u8865\u901a\u7528\u7cfb\u7edf\u5728\u6b64\u9886\u57df\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528OMCD_Typica.ai_Mix\u6570\u636e\u96c6\u6d4b\u8bd5Typica.ai\u6a21\u578b\u4e0eOpenAI\u3001Mistral\u3001Anthropic Claude\u7684API\uff0c\u5bf9\u6bd4\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u503c\u548c\u51c6\u786e\u7387\u3002", "result": "Typica.ai\u8868\u73b0\u6700\u4f18\uff0c\u8bc1\u660e\u6587\u5316\u9002\u914d\u6a21\u578b\u5728\u5185\u5bb9\u5ba1\u6838\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "\u6587\u5316\u9002\u5e94\u6027\u6a21\u578b\u5bf9\u53ef\u9760\u7684\u5185\u5bb9\u5ba1\u6838\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u5728 underrepresented\u8bed\u8a00\u4e2d\u3002"}}
{"id": "2505.04914", "pdf": "https://arxiv.org/pdf/2505.04914", "abs": "https://arxiv.org/abs/2505.04914", "authors": ["John Hawkins"], "title": "Enigme: Generative Text Puzzles for Evaluating Reasoning in Language Models", "categories": ["cs.AI", "cs.CL", "I.2.7"], "comment": "To be published in the proceedings of The 2025 11th International\n  Conference on Engineering, Applied Sciences, and Technology (ICEAST)", "summary": "Transformer-decoder language models are a core innovation in text based\ngenerative artificial intelligence. These models are being deployed as\ngeneral-purpose intelligence systems in many applications. Central to their\nutility is the capacity to understand natural language commands and exploit the\nreasoning embedded in human text corpora to apply some form of reasoning\nprocess to a wide variety of novel tasks. To understand the limitations of this\napproach to generating reasoning we argue that we need to consider the\narchitectural constraints of these systems. Consideration of the latent\nvariable structure of transformer-decoder models allows us to design reasoning\ntasks that should probe the boundary of their capacity to reason. We present\nenigme, an open-source library for generating text-based puzzles to be used in\ntraining and evaluating reasoning skills within transformer-decoder models and\nfuture AI architectures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u8bbe\u8ba1\u6587\u672c\u8c1c\u9898\uff08enigme\uff09\u6765\u8bc4\u4f30\u548c\u8bad\u7ec3\u57fa\u4e8eTransformer-decoder\u7684\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63a2\u8ba8\u4e86\u5176\u67b6\u6784\u9650\u5236\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3Transformer-decoder\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8bbe\u8ba1\u4efb\u52a1\u6765\u63a2\u6d4b\u8fd9\u4e9b\u8fb9\u754c\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u5206\u6790\u6a21\u578b\u7684\u6f5c\u5728\u53d8\u91cf\u7ed3\u6784\uff0c\u8bbe\u8ba1\u6587\u672c\u8c1c\u9898\u5e93\uff08enigme\uff09\u6765\u751f\u6210\u8bc4\u4f30\u63a8\u7406\u80fd\u529b\u7684\u4efb\u52a1\u3002", "result": "\u7ed3\u679c\u662f\u5f00\u53d1\u4e86enigme\u8fd9\u4e00\u5f00\u6e90\u5e93\uff0c\u7528\u4e8e\u751f\u6210\u6587\u672c\u8c1c\u9898\u4ee5\u6d4b\u8bd5\u548c\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u6280\u80fd\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8fd9\u79cd\u65b9\u6cd5\u6709\u52a9\u4e8e\u63ed\u793aTransformer-decoder\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u8fb9\u754c\uff0c\u5e76\u4e3a\u672a\u6765AI\u67b6\u6784\u63d0\u4f9b\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2505.04757", "pdf": "https://arxiv.org/pdf/2505.04757", "abs": "https://arxiv.org/abs/2505.04757", "authors": ["Louis Bouvier", "Thibault Prunet", "Vincent Lecl\u00e8re", "Axel Parmentier"], "title": "Primal-dual algorithm for contextual stochastic combinatorial optimization", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "This paper introduces a novel approach to contextual stochastic optimization,\nintegrating operations research and machine learning to address decision-making\nunder uncertainty. Traditional methods often fail to leverage contextual\ninformation, which underscores the necessity for new algorithms. In this study,\nwe utilize neural networks with combinatorial optimization layers to encode\npolicies. Our goal is to minimize the empirical risk, which is estimated from\npast data on uncertain parameters and contexts. To that end, we present a\nsurrogate learning problem and a generic primal-dual algorithm that is\napplicable to various combinatorial settings in stochastic optimization. Our\napproach extends classic Fenchel-Young loss results and introduces a new\nregularization method using sparse perturbations on the distribution simplex.\nThis allows for tractable updates in the original space and can accommodate\ndiverse objective functions. We demonstrate the linear convergence of our\nalgorithm under certain conditions and provide a bound on the non-optimality of\nthe resulting policy in terms of the empirical risk. Experiments on a\ncontextual stochastic minimum weight spanning tree problem show that our\nalgorithm is efficient and scalable, achieving performance comparable to\nimitation learning of solutions computed using an expensive Lagrangian-based\nheuristic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fd0\u7b79\u5b66\u548c\u673a\u5668\u5b66\u4e60\u7684\u65b0\u9896\u4e0a\u4e0b\u6587\u968f\u673a\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u548c\u7ec4\u5408\u4f18\u5316\u5c42\u7f16\u7801\u7b56\u7565\uff0c\u5728\u591a\u79cd\u7ec4\u5408\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4e9f\u9700\u65b0\u7b97\u6cd5\u6765\u4f18\u5316\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7f51\u7edc\u4e0e\u7ec4\u5408\u4f18\u5316\u5c42\u7f16\u7801\u7b56\u7565\uff0c\u63d0\u51fa\u66ff\u4ee3\u5b66\u4e60\u95ee\u9898\u548c\u901a\u7528\u539f\u5bf9\u5076\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u7a00\u758f\u6270\u52a8\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "result": "\u7b97\u6cd5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u7ebf\u6027\u6536\u655b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u4e0a\u4e0b\u6587\u968f\u673a\u6700\u5c0f\u751f\u6210\u6811\u95ee\u9898\u4e2d\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\uff0c\u6027\u80fd\u63a5\u8fd1\u57fa\u4e8e\u62c9\u683c\u6717\u65e5\u542f\u53d1\u5f0f\u7684\u6a21\u4eff\u5b66\u4e60\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4e0a\u4e0b\u6587\u968f\u673a\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.04642", "pdf": "https://arxiv.org/pdf/2505.04642", "abs": "https://arxiv.org/abs/2505.04642", "authors": ["Nischal Mandal", "Yang Li"], "title": "Rethinking Multimodal Sentiment Analysis: A High-Accuracy, Simplified Fusion Architecture", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal sentiment analysis, a pivotal task in affective computing, seeks\nto understand human emotions by integrating cues from language, audio, and\nvisual signals. While many recent approaches leverage complex attention\nmechanisms and hierarchical architectures, we propose a lightweight, yet\neffective fusion-based deep learning model tailored for utterance-level emotion\nclassification. Using the benchmark IEMOCAP dataset, which includes aligned\ntext, audio-derived numeric features, and visual descriptors, we design a\nmodality-specific encoder using fully connected layers followed by dropout\nregularization. The modality-specific representations are then fused using\nsimple concatenation and passed through a dense fusion layer to capture\ncross-modal interactions. This streamlined architecture avoids computational\noverhead while preserving performance, achieving a classification accuracy of\n92% across six emotion categories. Our approach demonstrates that with careful\nfeature engineering and modular design, simpler fusion strategies can\noutperform or match more complex models, particularly in resource-constrained\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u878d\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\uff0c\u901a\u8fc7\u7b80\u5355\u4e32\u8054\u548c\u5bc6\u96c6\u878d\u5408\u5c42\u5b9e\u73b0\u9ad8\u6548\u60c5\u611f\u5206\u7c7b\uff0c\u51c6\u786e\u7387\u8fbe92%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u590d\u6742\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u5206\u5c42\u67b6\u6784\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u8f7b\u91cf\u4e14\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002", "method": "\u4f7f\u7528\u6a21\u6001\u7279\u5b9a\u7684\u7f16\u7801\u5668\uff08\u5168\u8fde\u63a5\u5c42\u548cDropout\uff09\u5904\u7406\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u89c9\u7279\u5f81\uff0c\u901a\u8fc7\u7b80\u5355\u4e32\u8054\u548c\u5bc6\u96c6\u878d\u5408\u5c42\u5b9e\u73b0\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u3002", "result": "\u5728IEMOCAP\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u516d\u7c7b\u60c5\u611f\u5206\u7c7b\u4e2d\u8fbe\u523092%\u7684\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u7b80\u5355\u878d\u5408\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7279\u5f81\u548c\u6a21\u5757\u5316\u7ed3\u6784\uff0c\u7b80\u5355\u878d\u5408\u65b9\u6cd5\u53ef\u5ab2\u7f8e\u6216\u8d85\u8d8a\u590d\u6742\u6a21\u578b\uff0c\u5c24\u5176\u9002\u5408\u8d44\u6e90\u53d7\u9650\u573a\u666f\u3002"}}
{"id": "2505.04927", "pdf": "https://arxiv.org/pdf/2505.04927", "abs": "https://arxiv.org/abs/2505.04927", "authors": ["Sebastian Dumbrava"], "title": "Belief Filtering for Epistemic Control in Linguistic State Space", "categories": ["cs.AI"], "comment": "18 pages", "summary": "We examine belief filtering as a mechanism for the epistemic control of\nartificial agents, focusing on the regulation of internal cognitive states\nrepresented as linguistic expressions. This mechanism is developed within the\nSemantic Manifold framework, where belief states are dynamic, structured\nensembles of natural language fragments. Belief filters act as content-aware\noperations on these fragments across various cognitive transitions. This paper\nillustrates how the inherent interpretability and modularity of such a\nlinguistically-grounded cognitive architecture directly enable belief\nfiltering, offering a principled approach to agent regulation. The study\nhighlights the potential for enhancing AI safety and alignment through\nstructured interventions in an agent's internal semantic space and points to\nnew directions for architecturally embedded cognitive governance.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4fe1\u5ff5\u8fc7\u6ee4\u4f5c\u4e3a\u4eba\u5de5\u667a\u80fd\u8ba4\u77e5\u63a7\u5236\u673a\u5236\u7684\u4f5c\u7528\uff0c\u901a\u8fc7\u8bed\u8a00\u8868\u8fbe\u7684\u52a8\u6001\u7ed3\u6784\u5316\u8c03\u63a7\u5185\u90e8\u8ba4\u77e5\u72b6\u6001\uff0c\u589e\u5f3aAI\u5b89\u5168\u6027\u548c\u5bf9\u9f50\u6027\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u8bed\u8a00\u57fa\u7840\u67b6\u6784\u5b9e\u73b0AI\u5185\u90e8\u8ba4\u77e5\u72b6\u6001\u7684\u52a8\u6001\u8c03\u63a7\uff0c\u63d0\u5347AI\u7684\u5b89\u5168\u6027\u548c\u8ba4\u77e5\u53ef\u63a7\u6027\u3002", "method": "\u57fa\u4e8e\u8bed\u4e49\u6d41\u5f62\u6846\u67b6\uff0c\u5bf9\u81ea\u7136\u8bed\u8a00\u7247\u6bb5\u8fdb\u884c\u5185\u5bb9\u611f\u77e5\u64cd\u4f5c\uff0c\u5b9e\u73b0\u4fe1\u5ff5\u8fc7\u6ee4\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u79cd\u8bed\u8a00\u5316\u8ba4\u77e5\u67b6\u6784\u80fd\u76f4\u63a5\u652f\u6301\u4fe1\u5ff5\u8fc7\u6ee4\uff0c\u4e3aAI\u8c03\u63a7\u63d0\u4f9b\u7ed3\u6784\u5316\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u673a\u5236\u4e3a\u5d4c\u5165\u5f0f\u8ba4\u77e5\u6cbb\u7406\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u5728AI\u5185\u90e8\u8bed\u4e49\u7a7a\u95f4\u8fdb\u884c\u5e72\u9884\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.04775", "pdf": "https://arxiv.org/pdf/2505.04775", "abs": "https://arxiv.org/abs/2505.04775", "authors": ["Amr Alkhatib", "Roman Bresson", "Henrik Bostr\u00f6m", "Michalis Vazirgiannis"], "title": "Prediction via Shapley Value Regression", "categories": ["cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Shapley values have several desirable, theoretically well-supported,\nproperties for explaining black-box model predictions. Traditionally, Shapley\nvalues are computed post-hoc, leading to additional computational cost at\ninference time. To overcome this, a novel method, called ViaSHAP, is proposed,\nthat learns a function to compute Shapley values, from which the predictions\ncan be derived directly by summation. Two approaches to implement the proposed\nmethod are explored; one based on the universal approximation theorem and the\nother on the Kolmogorov-Arnold representation theorem. Results from a\nlarge-scale empirical investigation are presented, showing that ViaSHAP using\nKolmogorov-Arnold Networks performs on par with state-of-the-art algorithms for\ntabular data. It is also shown that the explanations of ViaSHAP are\nsignificantly more accurate than the popular approximator FastSHAP on both\ntabular data and images.", "AI": {"tldr": "ViaSHAP\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u4e00\u4e2a\u8ba1\u7b97Shapley\u503c\u7684\u51fd\u6570\u6765\u76f4\u63a5\u63a8\u5bfc\u9884\u6d4b\u7ed3\u679c\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u7684\u989d\u5916\u8ba1\u7b97\u6210\u672c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cViaSHAP\u5728\u8868\u683c\u6570\u636e\u548c\u56fe\u50cf\u4e0a\u7684\u89e3\u91ca\u51c6\u786e\u6027\u663e\u8457\u4f18\u4e8eFastSHAP\u3002", "motivation": "\u4f20\u7edf\u7684Shapley\u503c\u8ba1\u7b97\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u4f1a\u4ea7\u751f\u989d\u5916\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e86ViaSHAP\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u4e00\u4e2a\u51fd\u6570\u76f4\u63a5\u8ba1\u7b97Shapley\u503c\uff0c\u4ece\u800c\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "ViaSHAP\u91c7\u7528\u4e24\u79cd\u5b9e\u73b0\u65b9\u6cd5\uff1a\u57fa\u4e8e\u901a\u7528\u8fd1\u4f3c\u5b9a\u7406\u548c\u57fa\u4e8eKolmogorov-Arnold\u8868\u793a\u5b9a\u7406\u3002\u5b9e\u9a8c\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cViaSHAP\u5728\u8868\u683c\u6570\u636e\u4e0a\u7684\u6027\u80fd\u4e0e\u73b0\u6709\u6700\u4f18\u7b97\u6cd5\u76f8\u5f53\uff0c\u4e14\u5728\u8868\u683c\u6570\u636e\u548c\u56fe\u50cf\u4e0a\u7684\u89e3\u91ca\u51c6\u786e\u6027\u663e\u8457\u4f18\u4e8eFastSHAP\u3002", "conclusion": "ViaSHAP\u901a\u8fc7\u5b66\u4e60\u76f4\u63a5\u8ba1\u7b97Shapley\u503c\u7684\u51fd\u6570\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u6548\u7387\u548c\u89e3\u91ca\u51c6\u786e\u6027\uff0c\u4e3a\u9ed1\u76d2\u6a21\u578b\u9884\u6d4b\u7684\u89e3\u91ca\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.04643", "pdf": "https://arxiv.org/pdf/2505.04643", "abs": "https://arxiv.org/abs/2505.04643", "authors": ["Hannes Waldetoft", "Jakob Torgander", "M\u00e5ns Magnusson"], "title": "Prediction-powered estimators for finite population statistics in highly imbalanced textual data: Public hate crime estimation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Estimating population parameters in finite populations of text documents can\nbe challenging when obtaining the labels for the target variable requires\nmanual annotation. To address this problem, we combine predictions from a\ntransformer encoder neural network with well-established survey sampling\nestimators using the model predictions as an auxiliary variable. The\napplicability is demonstrated in Swedish hate crime statistics based on Swedish\npolice reports. Estimates of the yearly number of hate crimes and the police's\nunder-reporting are derived using the Hansen-Hurwitz estimator, difference\nestimation, and stratified random sampling estimation. We conclude that if\nlabeled training data is available, the proposed method can provide very\nefficient estimates with reduced time spent on manual annotation.", "AI": {"tldr": "\u5229\u7528Transformer\u7f16\u7801\u5668\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u4e0e\u7ecf\u5178\u8c03\u67e5\u62bd\u6837\u65b9\u6cd5\u7ed3\u5408\uff0c\u6709\u6548\u4f30\u8ba1\u6587\u672c\u6570\u636e\u4e2d\u7684\u76ee\u6807\u53d8\u91cf\uff0c\u5e94\u7528\u4e8e\u745e\u5178\u4ec7\u6068\u72af\u7f6a\u7edf\u8ba1\uff0c\u663e\u8457\u51cf\u5c11\u624b\u52a8\u6807\u6ce8\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u5728\u6709\u9650\u6587\u672c\u6570\u636e\u96c6\u4e2d\u83b7\u53d6\u76ee\u6807\u53d8\u91cf\u6807\u7b7e\u9700\u624b\u52a8\u6807\u6ce8\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4f30\u8ba1\u6548\u7387\u3002", "method": "\u7ed3\u5408Transformer\u7f16\u7801\u5668\u9884\u6d4b\u4e0eHansen-Hurwitz\u4f30\u8ba1\u3001\u5dee\u5f02\u4f30\u8ba1\u53ca\u5206\u5c42\u968f\u673a\u62bd\u6837\u4f30\u8ba1\u65b9\u6cd5\u3002", "result": "\u5728\u745e\u5178\u4ec7\u6068\u72af\u7f6a\u7edf\u8ba1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u53c2\u6570\u4f30\u8ba1\u3002", "conclusion": "\u82e5\u5177\u5907\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u4f9b\u9ad8\u6548\u4f30\u8ba1\u5e76\u5927\u5e45\u964d\u4f4e\u624b\u52a8\u6807\u6ce8\u6210\u672c\u3002"}}
{"id": "2505.04950", "pdf": "https://arxiv.org/pdf/2505.04950", "abs": "https://arxiv.org/abs/2505.04950", "authors": ["Shireen Kudukkil Manchingal", "Fabio Cuzzolin"], "title": "Position: Epistemic Artificial Intelligence is Essential for Machine Learning Models to Know When They Do Not Know", "categories": ["cs.AI"], "comment": null, "summary": "Despite the impressive achievements of AI, including advancements in\ngenerative models and large language models, there remains a significant gap in\nthe ability of AI to handle uncertainty and generalize beyond the training\ndata. We argue that AI models, especially in autonomous systems, fail to make\nrobust predictions when faced with unfamiliar or adversarial data, as evidenced\nby incidents with autonomous vehicles. Traditional machine learning approaches\nstruggle to address these issues due to an overemphasis on data fitting and\ndomain adaptation. This position paper posits a paradigm shift towards\nepistemic artificial intelligence, emphasizing the need for models to learn not\nonly from what they know but also from their ignorance. This approach, which\nfocuses on recognizing and managing uncertainty, offers a potential solution to\nimprove the resilience and robustness of AI systems, ensuring that they can\nbetter handle unpredictable real-world environments.", "AI": {"tldr": "\u4f5c\u8005\u6307\u51fa\u4e86AI\u5728\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u4e0d\u719f\u6089\u6216\u5bf9\u6297\u6027\u6570\u636e\u65f6\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3AI\u5728\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u9762\u5bf9\u672a\u77e5\u6570\u636e\u65f6\u9884\u6d4b\u4e0d\u7a33\u5065\u7684\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u7684\u4eba\u5de5\u667a\u80fd\uff08epistemic AI\uff09\u65b9\u6cd5\uff0c\u5f3a\u8c03\u6a21\u578b\u4e0d\u4ec5\u5e94\u4ece\u5df2\u77e5\u4e2d\u5b66\u4e60\uff0c\u8fd8\u5e94\u4ece\u81ea\u8eab\u7684\u65e0\u77e5\u4e2d\u5b66\u4e60\u3002", "result": "\u8fd9\u79cd\u65b9\u6cd5\u6709\u671b\u63d0\u9ad8AI\u7cfb\u7edf\u7684\u5f39\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f7f\u5176\u80fd\u66f4\u597d\u5730\u5e94\u5bf9\u4e0d\u53ef\u9884\u6d4b\u7684\u73b0\u5b9e\u73af\u5883\u3002", "conclusion": "\u4f5c\u8005\u8ba4\u4e3a\u8ba4\u77e5AI\u662f\u89e3\u51b3\u5f53\u524dAI\u5c40\u9650\u6027\u7684\u6f5c\u5728\u65b9\u6848\uff0c\u5c24\u5176\u662f\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2505.04796", "pdf": "https://arxiv.org/pdf/2505.04796", "abs": "https://arxiv.org/abs/2505.04796", "authors": ["Jade Garcia Bourr\u00e9e", "Augustin Godinot", "Martijn De Vos", "Milos Vujasinovic", "Sayan Biswas", "Gilles Tredan", "Erwan Le Merrer", "Anne-Marie Kermarrec"], "title": "Robust ML Auditing using Prior Knowledge", "categories": ["cs.LG"], "comment": "Accepted to the 42nd International Conference on Machine Learning\n  ICML25", "summary": "The rapid adoption of ML decision-making systems across products and services\nhas led to a set of regulations on how such systems should behave and be built.\nAmong all the technical challenges to enforcing these regulations, one crucial,\nyet under-explored problem is the risk of manipulation while these systems are\nbeing audited for fairness. This manipulation occurs when a platform\ndeliberately alters its answers to a regulator to pass an audit without\nmodifying its answers to other users. In this paper, we introduce a novel\napproach to manipulation-proof auditing by taking into account the auditor's\nprior knowledge of the task solved by the platform. We first demonstrate that\nregulators must not rely on public priors (e.g. a public dataset), as platforms\ncould easily fool the auditor in such cases. We then formally establish the\nconditions under which an auditor can prevent audit manipulations using prior\nknowledge about the ground truth. Finally, our experiments with two standard\ndatasets exemplify the maximum level of unfairness a platform can hide before\nbeing detected as malicious. Our formalization and generalization of\nmanipulation-proof auditing with a prior opens up new research directions for\nmore robust fairness audits.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u64cd\u7eb5\u7684\u516c\u5e73\u6027\u5ba1\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5ba1\u8ba1\u8005\u7684\u5148\u9a8c\u77e5\u8bc6\u9632\u6b62\u5e73\u53f0\u5728\u5ba1\u8ba1\u4e2d\u4f5c\u5f0a\uff0c\u5e76\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u51b3\u7b56\u7cfb\u7edf\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u516c\u5e73\u6027\u5ba1\u8ba1\u6210\u4e3a\u5fc5\u8981\uff0c\u4f46\u5b58\u5728\u5e73\u53f0\u5728\u5ba1\u8ba1\u4e2d\u4f5c\u5f0a\u7684\u98ce\u9669\uff0c\u5f53\u524d\u65b9\u6cd5\u5bf9\u6b64\u9632\u62a4\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5ba1\u8ba1\u8005\u5148\u9a8c\u77e5\u8bc6\u7684\u9632\u64cd\u7eb5\u5ba1\u8ba1\u6846\u67b6\uff0c\u907f\u514d\u4f9d\u8d56\u516c\u5171\u5148\u9a8c\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u786e\u7acb\u4e86\u9632\u64cd\u7eb5\u7684\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u9650\u5236\u5e73\u53f0\u9690\u85cf\u4e0d\u516c\u5e73\u6027\u7684\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u6700\u5927\u53ef\u9690\u85cf\u7684\u4e0d\u516c\u5e73\u7a0b\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u516c\u5e73\u6027\u5ba1\u8ba1\u63d0\u4f9b\u4e86\u9632\u64cd\u7eb5\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u63a8\u52a8\u4e86\u66f4\u9c81\u68d2\u7684\u5ba1\u8ba1\u65b9\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.04645", "pdf": "https://arxiv.org/pdf/2505.04645", "abs": "https://arxiv.org/abs/2505.04645", "authors": ["Tejas Jade", "Alex Yartsev"], "title": "ChatGPT for automated grading of short answer questions in mechanical ventilation", "categories": ["cs.CL", "cs.LG", "stat.CO"], "comment": null, "summary": "Standardised tests using short answer questions (SAQs) are common in\npostgraduate education. Large language models (LLMs) simulate conversational\nlanguage and interpret unstructured free-text responses in ways aligning with\napplying SAQ grading rubrics, making them attractive for automated grading. We\nevaluated ChatGPT 4o to grade SAQs in a postgraduate medical setting using data\nfrom 215 students (557 short-answer responses) enrolled in an online course on\nmechanical ventilation (2020--2024). Deidentified responses to three case-based\nscenarios were presented to ChatGPT with a standardised grading prompt and\nrubric. Outputs were analysed using mixed-effects modelling, variance component\nanalysis, intraclass correlation coefficients (ICCs), Cohen's kappa, Kendall's\nW, and Bland--Altman statistics. ChatGPT awarded systematically lower marks\nthan human graders with a mean difference (bias) of -1.34 on a 10-point scale.\nICC values indicated poor individual-level agreement (ICC1 = 0.086), and\nCohen's kappa (-0.0786) suggested no meaningful agreement. Variance component\nanalysis showed minimal variability among the five ChatGPT sessions (G-value =\n0.87), indicating internal consistency but divergence from the human grader.\nThe poorest agreement was observed for evaluative and analytic items, whereas\nchecklist and prescriptive rubric items had less disagreement. We caution\nagainst the use of LLMs in grading postgraduate coursework. Over 60% of\nChatGPT-assigned grades differed from human grades by more than acceptable\nboundaries for high-stakes assessments.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86ChatGPT 4o\u5728\u7814\u7a76\u751f\u533b\u5b66\u6559\u80b2\u4e2d\u81ea\u52a8\u8bc4\u5206\u77ed\u7b54\u6848\u95ee\u9898\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5176\u8bc4\u5206\u4e0e\u4eba\u7c7b\u8bc4\u5206\u8005\u5dee\u5f02\u663e\u8457\uff0c\u4e00\u81f4\u6027\u4f4e\uff0c\u5c24\u5176\u5728\u9ad8\u9636\u8ba4\u77e5\u9898\u76ee\u4e0a\u8868\u73b0\u66f4\u5dee\uff0c\u5efa\u8bae\u5728\u9ad8\u98ce\u9669\u8bc4\u4f30\u4e2d\u8c28\u614e\u4f7f\u7528\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u81ea\u7531\u6587\u672c\u65f6\u8868\u73b0\u51fa\u4e0e\u4eba\u5de5\u8bc4\u5206\u76f8\u4f3c\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u88ab\u63a2\u7d22\u7528\u4e8e\u81ea\u52a8\u8bc4\u5206\uff0c\u4f46\u6b64\u524d\u5bf9\u5176\u5728\u7814\u7a76\u751f\u6559\u80b2\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528ChatGPT 4o\u5bf9215\u540d\u5b66\u751f\u7684557\u4efd\u77ed\u7b54\u6848\u8fdb\u884c\u8bc4\u5206\uff0c\u5bf9\u6bd4\u4eba\u7c7b\u8bc4\u5206\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u6548\u5e94\u6a21\u578b\u3001\u65b9\u5dee\u5206\u6790\u3001ICC\u3001Cohen's kappa\u7b49\u7edf\u8ba1\u65b9\u6cd5\u5206\u6790\u4e00\u81f4\u6027\u548c\u504f\u5dee\u3002", "result": "ChatGPT\u8bc4\u5206\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\uff08\u5e73\u5747\u504f\u5dee-1.34\uff09\uff0c\u4e2a\u4f53\u4e00\u81f4\u6027\u5dee\uff08ICC1=0.086\uff09\uff0c\u9ad8\u9636\u9898\u76ee\u5206\u6b67\u66f4\u5927\uff0c60%\u4ee5\u4e0a\u8bc4\u5206\u8d85\u51fa\u53ef\u63a5\u53d7\u8bef\u5dee\u8303\u56f4\u3002", "conclusion": "LLMs\u5728\u7814\u7a76\u751f\u8bfe\u7a0b\u8bc4\u5206\u4e2d\u8868\u73b0\u4e0d\u53ef\u9760\uff0c\u5c24\u5176\u5728\u9ad8\u98ce\u9669\u8bc4\u4f30\u4e2d\u9700\u8c28\u614e\u4f7f\u7528\uff0c\u5efa\u8bae\u4f18\u5148\u9009\u62e9\u660e\u786e\u89c4\u5219\u7684\u9898\u76ee\u7c7b\u578b\u4ee5\u51cf\u5c11\u5206\u6b67\u3002"}}
{"id": "2505.04966", "pdf": "https://arxiv.org/pdf/2505.04966", "abs": "https://arxiv.org/abs/2505.04966", "authors": ["Jaeho Kim", "Yunseok Lee", "Seulki Lee"], "title": "Position: The AI Conference Peer Review Crisis Demands Author Feedback and Reviewer Rewards", "categories": ["cs.AI", "cs.CY"], "comment": "ICML2025 Position Track Oral", "summary": "The peer review process in major artificial intelligence (AI) conferences\nfaces unprecedented challenges with the surge of paper submissions (exceeding\n10,000 submissions per venue), accompanied by growing concerns over review\nquality and reviewer responsibility. This position paper argues for the need to\ntransform the traditional one-way review system into a bi-directional feedback\nloop where authors evaluate review quality and reviewers earn formal\naccreditation, creating an accountability framework that promotes a\nsustainable, high-quality peer review system. The current review system can be\nviewed as an interaction between three parties: the authors, reviewers, and\nsystem (i.e., conference), where we posit that all three parties share\nresponsibility for the current problems. However, issues with authors can only\nbe addressed through policy enforcement and detection tools, and ethical\nconcerns can only be corrected through self-reflection. As such, this paper\nfocuses on reforming reviewer accountability with systematic rewards through\ntwo key mechanisms: (1) a two-stage bi-directional review system that allows\nauthors to evaluate reviews while minimizing retaliatory behavior, (2)a\nsystematic reviewer reward system that incentivizes quality reviewing. We ask\nfor the community's strong interest in these problems and the reforms that are\nneeded to enhance the peer review process.", "AI": {"tldr": "\u8be5\u7acb\u573a\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u53cc\u5411\u53cd\u9988\u673a\u5236\u548c\u7cfb\u7edf\u5316\u5956\u52b1\u6765\u6539\u9769\u540c\u884c\u8bc4\u5ba1\u7cfb\u7edf\uff0c\u4ee5\u63d0\u9ad8\u8bc4\u5ba1\u8d28\u91cf\u548c\u95ee\u8d23\u5236\u3002", "motivation": "\u968f\u7740AI\u4f1a\u8bae\u6295\u7a3f\u91cf\u6fc0\u589e\uff08\u6bcf\u573a\u4f1a\u8bae\u8d85\u8fc71\u4e07\u7bc7\uff09\uff0c\u4f20\u7edf\u5355\u5411\u8bc4\u5ba1\u7cfb\u7edf\u9762\u4e34\u8d28\u91cf\u548c\u8d23\u4efb\u95ee\u9898\uff0c\u4f5c\u8005\u547c\u5401\u5efa\u7acb\u53cc\u5411\u53cd\u9988\u548c\u95ee\u8d23\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e24\u9879\u5173\u952e\u673a\u5236\uff1a(1) \u53cc\u5411\u8bc4\u5ba1\u7cfb\u7edf\uff0c\u5141\u8bb8\u4f5c\u8005\u8bc4\u4ef7\u8bc4\u5ba1\u8d28\u91cf\u5e76\u51cf\u5c11\u62a5\u590d\u884c\u4e3a\uff1b(2) \u7cfb\u7edf\u5316\u7684\u8bc4\u5ba1\u5956\u52b1\u673a\u5236\uff0c\u6fc0\u52b1\u9ad8\u8d28\u91cf\u8bc4\u5ba1\u3002", "result": "\u901a\u8fc7\u6539\u9769\u8bc4\u5ba1\u8d23\u4efb\u548c\u5956\u52b1\u4f53\u7cfb\uff0c\u6709\u671b\u63d0\u5347\u8bc4\u5ba1\u8d28\u91cf\u5e76\u5efa\u7acb\u53ef\u6301\u7eed\u7684\u540c\u884c\u8bc4\u5ba1\u7cfb\u7edf\u3002", "conclusion": "\u8bba\u6587\u547c\u5401\u793e\u533a\u5173\u6ce8\u8bc4\u5ba1\u95ee\u9898\u5e76\u652f\u6301\u6539\u9769\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u548c\u516c\u5e73\u7684\u8bc4\u5ba1\u6d41\u7a0b\u3002"}}
{"id": "2505.04802", "pdf": "https://arxiv.org/pdf/2505.04802", "abs": "https://arxiv.org/abs/2505.04802", "authors": ["Xiao Wang", "Jong-Youl Choi", "Takuya Kurihaya", "Isaac Lyngaas", "Hong-Jun Yoon", "Ming Fan", "Nasik Muhammad Nafi", "Aristeidis Tsaris", "Ashwin M. Aji", "Maliha Hossain", "Mohamed Wahib", "Dali Wang", "Peter Thornton", "Prasanna Balaprakash", "Moetasim Ashfaq", "Dan Lu"], "title": "ORBIT-2: Scaling Exascale Vision Foundation Models for Weather and Climate Downscaling", "categories": ["cs.LG", "astro-ph.EP", "cs.AI", "cs.DC", "physics.ao-ph"], "comment": null, "summary": "Sparse observations and coarse-resolution climate models limit effective\nregional decision-making, underscoring the need for robust downscaling.\nHowever, existing AI methods struggle with generalization across variables and\ngeographies and are constrained by the quadratic complexity of Vision\nTransformer (ViT) self-attention. We introduce ORBIT-2, a scalable foundation\nmodel for global, hyper-resolution climate downscaling. ORBIT-2 incorporates\ntwo key innovations: (1) Residual Slim ViT (Reslim), a lightweight architecture\nwith residual learning and Bayesian regularization for efficient, robust\nprediction; and (2) TILES, a tile-wise sequence scaling algorithm that reduces\nself-attention complexity from quadratic to linear, enabling long-sequence\nprocessing and massive parallelism. ORBIT-2 scales to 10 billion parameters\nacross 32,768 GPUs, achieving up to 1.8 ExaFLOPS sustained throughput and\n92-98% strong scaling efficiency. It supports downscaling to 0.9 km global\nresolution and processes sequences up to 4.2 billion tokens. On 7 km resolution\nbenchmarks, ORBIT-2 achieves high accuracy with R^2 scores in the range of 0.98\nto 0.99 against observation data.", "AI": {"tldr": "ORBIT-2 is a scalable foundation model for hyper-resolution climate downscaling, using lightweight Reslim architecture and TILES algorithm to achieve high efficiency and accuracy.", "motivation": "To address the limitations of sparse observations and coarse-resolution models in regional decision-making, overcoming AI generalization issues and high computational complexity.", "method": "Introduces Residual Slim ViT (Reslim) and TILES algorithm for lightweight, efficient processing and linear complexity scaling.", "result": "Achieves strong scaling efficiency (92-98%), supports 0.9 km global resolution, and scores R^2 0.98-0.99 on benchmarks.", "conclusion": "ORBIT-2 provides a robust solution for global climate downscaling with high accuracy and scalability."}}
{"id": "2505.04649", "pdf": "https://arxiv.org/pdf/2505.04649", "abs": "https://arxiv.org/abs/2505.04649", "authors": ["Chengzhang Yu", "Yiming Zhang", "Zhixin Liu", "Zenghui Ding", "Yining Sun", "Zhanpeng Jin"], "title": "FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights", "categories": ["cs.CL"], "comment": "12 pages, 4 figures, 5 table", "summary": "The automation of scientific research through large language models (LLMs)\npresents significant opportunities but faces critical challenges in knowledge\nsynthesis and quality assurance. We introduce Feedback-Refined Agent\nMethodology (FRAME), a novel framework that enhances medical paper generation\nthrough iterative refinement and structured feedback. Our approach comprises\nthree key innovations: (1) A structured dataset construction method that\ndecomposes 4,287 medical papers into essential research components through\niterative refinement; (2) A tripartite architecture integrating Generator,\nEvaluator, and Reflector agents that progressively improve content quality\nthrough metric-driven feedback; and (3) A comprehensive evaluation framework\nthat combines statistical metrics with human-grounded benchmarks. Experimental\nresults demonstrate FRAME's effectiveness, achieving significant improvements\nover conventional approaches across multiple models (9.91% average gain with\nDeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation\ndimensions. Human evaluation confirms that FRAME-generated papers achieve\nquality comparable to human-authored works, with particular strength in\nsynthesizing future research directions. The results demonstrated our work\ncould efficiently assist medical research by building a robust foundation for\nautomated medical research paper generation while maintaining rigorous academic\nstandards.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFRAME\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u63d0\u5347\u533b\u5b66\u8bba\u6587\u751f\u6210\u8d28\u91cf\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u548c\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5316\u79d1\u7814\u9762\u4e34\u77e5\u8bc6\u5408\u6210\u548c\u8d28\u91cf\u4fdd\u8bc1\u7684\u6311\u6218\uff0cFRAME\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "1) \u7ed3\u6784\u5316\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\u5206\u89e3\u533b\u5b66\u8bba\u6587\uff1b2) \u751f\u6210-\u8bc4\u4f30-\u53cd\u5c04\u4e09\u90e8\u5206\u67b6\u6784\uff1b3) \u7ed3\u5408\u7edf\u8ba1\u6307\u6807\u548c\u4eba\u5de5\u57fa\u51c6\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u663e\u793aFRAME\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff08DeepSeek V3\u63d0\u53479.91%\uff09\uff0c\u751f\u6210\u8bba\u6587\u8d28\u91cf\u63a5\u8fd1\u4eba\u5de5\u64b0\u5199\u3002", "conclusion": "FRAME\u4e3a\u81ea\u52a8\u5316\u533b\u5b66\u8bba\u6587\u751f\u6210\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u540c\u65f6\u4fdd\u6301\u5b66\u672f\u4e25\u8c28\u6027\u3002"}}
{"id": "2505.04997", "pdf": "https://arxiv.org/pdf/2505.04997", "abs": "https://arxiv.org/abs/2505.04997", "authors": ["Ling Yue", "Nithin Somasekharan", "Yadi Cao", "Shaowu Pan"], "title": "Foam-Agent: Towards Automated Intelligent CFD Workflows", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Computational Fluid Dynamics (CFD) is an essential simulation tool in various\nengineering disciplines, but it often requires substantial domain expertise and\nmanual configuration, creating barriers to entry. We present Foam-Agent, a\nmulti-agent framework that automates complex OpenFOAM-based CFD simulation\nworkflows from natural language inputs. Our innovation includes (1) a\nhierarchical multi-index retrieval system with specialized indices for\ndifferent simulation aspects, (2) a dependency-aware file generation system\nthat provides consistency management across configuration files, and (3) an\niterative error correction mechanism that diagnoses and resolves simulation\nfailures without human intervention. Through comprehensive evaluation on the\ndataset of 110 simulation tasks, Foam-Agent achieves an 83.6% success rate with\nClaude 3.5 Sonnet, significantly outperforming existing frameworks (55.5% for\nMetaOpenFOAM and 37.3% for OpenFOAM-GPT). Ablation studies demonstrate the\ncritical contribution of each system component, with the specialized error\ncorrection mechanism providing a 36.4% performance improvement. Foam-Agent\nsubstantially lowers the CFD expertise threshold while maintaining modeling\naccuracy, demonstrating the potential of specialized multi-agent systems to\ndemocratize access to complex scientific simulation tools. The code is public\nat https://github.com/csml-rpi/Foam-Agent", "AI": {"tldr": "Foam-Agent\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u81ea\u52a8\u5316OpenFOAM CFD\u6a21\u62df\u6d41\u7a0b\uff0c\u6781\u5927\u964d\u4f4e\u4e86CFD\u7684\u4e13\u4e1a\u95e8\u69db\u3002", "motivation": "CFD\u6a21\u62df\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u77e5\u8bc6\u548c\u624b\u52a8\u914d\u7f6e\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002Foam-Agent\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u964d\u4f4e\u8fd9\u4e9b\u95e8\u69db\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u7d22\u5f15\u68c0\u7d22\u7cfb\u7edf\u3001\u4f9d\u8d56\u611f\u77e5\u6587\u4ef6\u751f\u6210\u548c\u8fed\u4ee3\u9519\u8bef\u4fee\u6b63\u673a\u5236\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u3002", "result": "\u5728110\u4e2a\u6a21\u62df\u4efb\u52a1\u4e2d\uff0cFoam-Agent\u6210\u529f\u7387\u8fbe83.6%\uff0c\u8fdc\u8d85\u73b0\u6709\u6846\u67b6\uff08MetaOpenFOAM 55.5%\uff0cOpenFOAM-GPT 37.3%\uff09\u3002\u9519\u8bef\u4fee\u6b63\u673a\u5236\u8d21\u732e\u4e8636.4%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "Foam-Agent\u663e\u8457\u964d\u4f4e\u4e86CFD\u7684\u4e13\u4e1a\u95e8\u69db\uff0c\u5c55\u793a\u4e86\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u590d\u6742\u79d1\u5b66\u5de5\u5177\u6c11\u4e3b\u5316\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.04808", "pdf": "https://arxiv.org/pdf/2505.04808", "abs": "https://arxiv.org/abs/2505.04808", "authors": ["Vahan Martirosyan", "Jhony H. Giraldo", "Fragkiskos D. Malliaros"], "title": "Piecewise Constant Spectral Graph Neural Network", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to TMLR 2025", "summary": "Graph Neural Networks (GNNs) have achieved significant success across various\ndomains by leveraging graph structures in data. Existing spectral GNNs, which\nuse low-degree polynomial filters to capture graph spectral properties, may not\nfully identify the graph's spectral characteristics because of the polynomial's\nsmall degree. However, increasing the polynomial degree is computationally\nexpensive and beyond certain thresholds leads to performance plateaus or\ndegradation. In this paper, we introduce the Piecewise Constant Spectral Graph\nNeural Network(PieCoN) to address these challenges. PieCoN combines constant\nspectral filters with polynomial filters to provide a more flexible way to\nleverage the graph structure. By adaptively partitioning the spectrum into\nintervals, our approach increases the range of spectral properties that can be\neffectively learned. Experiments on nine benchmark datasets, including both\nhomophilic and heterophilic graphs, demonstrate that PieCoN is particularly\neffective on heterophilic datasets, highlighting its potential for a wide range\nof applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86PieCoN\uff0c\u4e00\u79cd\u7ed3\u5408\u5e38\u6570\u8c31\u6ee4\u6ce2\u5668\u548c\u591a\u9879\u5f0f\u6ee4\u6ce2\u5668\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u66f4\u7075\u6d3b\u5730\u5229\u7528\u56fe\u7ed3\u6784\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5f02\u8d28\u6027\u56fe\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4f4e\u9636\u591a\u9879\u5f0f\u6ee4\u6ce2\u5668\u7684\u8c31GNN\u53ef\u80fd\u56e0\u591a\u9879\u5f0f\u9636\u6570\u4e0d\u8db3\u800c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u56fe\u7684\u9891\u8c31\u7279\u6027\uff0c\u800c\u589e\u52a0\u9636\u6570\u53c8\u5e26\u6765\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002", "method": "\u63d0\u51faPieCoN\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5212\u5206\u9891\u8c31\u533a\u95f4\u5e76\u7ed3\u5408\u5e38\u6570\u8c31\u6ee4\u6ce2\u5668\u4e0e\u591a\u9879\u5f0f\u6ee4\u6ce2\u5668\uff0c\u589e\u5f3a\u5bf9\u9891\u8c31\u7279\u6027\u7684\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u57289\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5305\u62ec\u540c\u8d28\u6027\u548c\u5f02\u8d28\u6027\u56fe\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPieCoN\u5728\u5f02\u8d28\u6027\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "PieCoN\u4e3a\u56fe\u7ed3\u6784\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u590d\u6742\u9891\u8c31\u7279\u6027\u7684\u573a\u666f\uff0c\u5c55\u73b0\u4e86\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.04651", "pdf": "https://arxiv.org/pdf/2505.04651", "abs": "https://arxiv.org/abs/2505.04651", "authors": ["Adithya Kulkarni", "Fatimah Alotaibi", "Xinyue Zeng", "Longfeng Wu", "Tong Zeng", "Barry Menglong Yao", "Minqian Liu", "Shuaicheng Zhang", "Lifu Huang", "Dawei Zhou"], "title": "Scientific Hypothesis Generation and Validation: Methods, Datasets, and Future Directions", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are transforming scientific hypothesis\ngeneration and validation by enabling information synthesis, latent\nrelationship discovery, and reasoning augmentation. This survey provides a\nstructured overview of LLM-driven approaches, including symbolic frameworks,\ngenerative models, hybrid systems, and multi-agent architectures. We examine\ntechniques such as retrieval-augmented generation, knowledge-graph completion,\nsimulation, causal inference, and tool-assisted reasoning, highlighting\ntrade-offs in interpretability, novelty, and domain alignment. We contrast\nearly symbolic discovery systems (e.g., BACON, KEKADA) with modern LLM\npipelines that leverage in-context learning and domain adaptation via\nfine-tuning, retrieval, and symbolic grounding. For validation, we review\nsimulation, human-AI collaboration, causal modeling, and uncertainty\nquantification, emphasizing iterative assessment in open-world contexts. The\nsurvey maps datasets across biomedicine, materials science, environmental\nscience, and social science, introducing new resources like AHTech and\nCSKG-600. Finally, we outline a roadmap emphasizing novelty-aware generation,\nmultimodal-symbolic integration, human-in-the-loop systems, and ethical\nsafeguards, positioning LLMs as agents for principled, scalable scientific\ndiscovery.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u79d1\u5b66\u5047\u8bbe\u751f\u6210\u4e0e\u9a8c\u8bc1\u4e2d\u7684\u591a\u79cd\u65b9\u6cd5\uff0c\u5bf9\u6bd4\u4e86\u4f20\u7edf\u7b26\u53f7\u7cfb\u7edf\u4e0e\u73b0\u4ee3LLM\u6d41\u7a0b\uff0c\u5e76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u96c6\u6210\u4e0e\u4f26\u7406\u4fdd\u969c\u7684\u91cd\u8981\u6027\u3002", "motivation": "LLMs\u901a\u8fc7\u4fe1\u606f\u5408\u6210\u3001\u6f5c\u5728\u5173\u7cfb\u53d1\u73b0\u548c\u63a8\u7406\u589e\u5f3a\uff0c\u6b63\u5728\u6539\u53d8\u79d1\u5b66\u5047\u8bbe\u7684\u751f\u6210\u4e0e\u9a8c\u8bc1\u65b9\u5f0f\uff0c\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u68b3\u7406\u76f8\u5173\u65b9\u6cd5\u4e0e\u5e94\u7528\u3002", "method": "\u6587\u7ae0\u5206\u6790\u4e86\u7b26\u53f7\u6846\u67b6\u3001\u751f\u6210\u6a21\u578b\u3001\u6df7\u5408\u7cfb\u7edf\u3001\u591a\u667a\u80fd\u4f53\u67b6\u6784\u7b49\u6280\u672f\uff0c\u5305\u62ec\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u3001\u56e0\u679c\u63a8\u65ad\u7b49\uff0c\u5e76\u5bf9\u6bd4\u4e86\u4f20\u7edf\u7b26\u53f7\u7cfb\u7edf\u4e0e\u73b0\u4ee3LLM\u6d41\u7a0b\u3002", "result": "\u7efc\u8ff0\u8986\u76d6\u4e86\u751f\u7269\u533b\u5b66\u3001\u6750\u6599\u79d1\u5b66\u7b49\u9886\u57df\u7684\u591a\u7c7b\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u9896\u6027\u751f\u6210\u3001\u591a\u6a21\u6001-\u7b26\u53f7\u96c6\u6210\u7b49\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "LLMs\u6709\u671b\u6210\u4e3a\u79d1\u5b66\u53d1\u73b0\u7684\u89c4\u6a21\u5316\u5de5\u5177\uff0c\u4f46\u9700\u7ed3\u5408\u4eba\u7c7b\u53c2\u4e0e\u4e0e\u4f26\u7406\u4fdd\u969c\uff0c\u63a8\u52a8\u66f4\u5177\u539f\u5219\u6027\u7684\u7814\u7a76\u3002"}}
{"id": "2505.05029", "pdf": "https://arxiv.org/pdf/2505.05029", "abs": "https://arxiv.org/abs/2505.05029", "authors": ["Siyue Ren", "Wanli Fu", "Xinkun Zou", "Chen Shen", "Yi Cai", "Chen Chu", "Zhen Wang", "Shuyue Hu"], "title": "A Reputation System for Large Language Model-based Multi-agent Systems to Avoid the Tragedy of the Commons", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "The tragedy of the commons, where individual self-interest leads to\ncollectively disastrous outcomes, is a pervasive challenge in human society.\nRecent studies have demonstrated that similar phenomena can arise in generative\nmulti-agent systems (MASs). To address this challenge, this paper explores the\nuse of reputation systems as a remedy. We propose RepuNet, a dynamic,\ndual-level reputation framework that models both agent-level reputation\ndynamics and system-level network evolution. Specifically, driven by direct\ninteractions and indirect gossip, agents form reputations for both themselves\nand their peers, and decide whether to connect or disconnect other agents for\nfuture interactions. Through two distinct scenarios, we show that RepuNet\neffectively mitigates the 'tragedy of the commons', promoting and sustaining\ncooperation in generative MASs. Moreover, we find that reputation systems can\ngive rise to rich emergent behaviors in generative MASs, such as the formation\nof cooperative clusters, the social isolation of exploitative agents, and the\npreference for sharing positive gossip rather than negative ones.", "AI": {"tldr": "\u63d0\u51faRepuNet\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u53cc\u5c42\u4fe1\u8a89\u7cfb\u7edf\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684'\u516c\u5730\u60b2\u5267'\u95ee\u9898\uff0c\u4fc3\u8fdb\u5e76\u7ef4\u6301\u5408\u4f5c\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u56e0\u4e2a\u4f53\u81ea\u5229\u884c\u4e3a\u5bfc\u81f4\u7684\u96c6\u4f53\u707e\u96be\u6027\u540e\u679c\uff08'\u516c\u5730\u60b2\u5267'\uff09\uff0c\u63a2\u7d22\u4fe1\u8a89\u7cfb\u7edf\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faRepuNet\u6846\u67b6\uff0c\u7ed3\u5408\u667a\u80fd\u4f53\u7ea7\u4fe1\u8a89\u52a8\u6001\u548c\u7cfb\u7edf\u7ea7\u7f51\u7edc\u6f14\u5316\uff0c\u901a\u8fc7\u76f4\u63a5\u4ea4\u4e92\u548c\u95f4\u63a5'\u516b\u5366'\u5f62\u6210\u4fe1\u8a89\uff0c\u51b3\u5b9a\u8fde\u63a5\u6216\u65ad\u5f00\u5176\u4ed6\u667a\u80fd\u4f53\u3002", "result": "RepuNet\u6709\u6548\u7f13\u89e3'\u516c\u5730\u60b2\u5267'\uff0c\u4fc3\u8fdb\u5408\u4f5c\uff0c\u5e76\u89c2\u5bdf\u5230\u5408\u4f5c\u96c6\u7fa4\u5f62\u6210\u3001\u5265\u524a\u6027\u667a\u80fd\u4f53\u793e\u4ea4\u9694\u79bb\u7b49\u6d8c\u73b0\u884c\u4e3a\u3002", "conclusion": "\u4fe1\u8a89\u7cfb\u7edf\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u4e0d\u4ec5\u80fd\u89e3\u51b3\u5408\u4f5c\u95ee\u9898\uff0c\u8fd8\u80fd\u5f15\u53d1\u4e30\u5bcc\u7684\u6d8c\u73b0\u884c\u4e3a\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.04823", "pdf": "https://arxiv.org/pdf/2505.04823", "abs": "https://arxiv.org/abs/2505.04823", "authors": ["Junhao Xiong", "Hunter Nisonoff", "Ishan Gaur", "Jennifer Listgarten"], "title": "Guide your favorite protein sequence generative model", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Generative machine learning models have begun to transform protein\nengineering, yet no principled framework for conditioning on auxiliary\ninformation in a plug-and-play manner exists; one may want to iteratively\nincorporate experimental feedback, or make use of an existing classifier --\nsuch as for predicting enzyme commission number -- in order to guide the\nsampling of the generative model to generate sequences with desired properties.\nHerein, we present ProteinGuide, a rigorous and general framework to achieve\njust that: through unifying a broad class of protein generative models that\nincludes masked language, (order-agnostic) autoregressive, diffusion and\nflow-matching models, we provide an approach to statistically condition\npre-trained protein generative models. We demonstrate applicability of our\napproach by guiding each of two commonly used protein generative models,\nProteinMPNN and ESM3, to generate amino acid and structure token sequences\nconditioned on several user-specified properties, namely, enhanced stability\nand CATH-labeled fold generation.", "AI": {"tldr": "ProteinGuide\u662f\u4e00\u4e2a\u901a\u7528\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u9884\u8bad\u7ec3\u7684\u86cb\u767d\u8d28\u751f\u6210\u6a21\u578b\uff08\u5982ProteinMPNN\u548cESM3\uff09\u4e0e\u8f85\u52a9\u4fe1\u606f\uff08\u5982\u5b9e\u9a8c\u53cd\u9988\u6216\u9176\u5206\u7c7b\u9884\u6d4b\uff09\u7ed3\u5408\uff0c\u4ee5\u751f\u6210\u5177\u6709\u7279\u5b9a\u6027\u8d28\u7684\u86cb\u767d\u8d28\u5e8f\u5217\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4e00\u79cd\u7075\u6d3b\u3001\u53ef\u63d2\u62d4\u7684\u65b9\u6cd5\u6765\u5728\u86cb\u767d\u8d28\u5de5\u7a0b\u4e2d\u7ed3\u5408\u5916\u90e8\u4fe1\u606f\uff08\u5982\u5b9e\u9a8c\u6570\u636e\u6216\u5206\u7c7b\u5668\u9884\u6d4b\uff09\uff0c\u4ee5\u6307\u5bfc\u751f\u6210\u6a21\u578b\u4ea7\u751f\u5177\u5907\u7279\u5b9a\u6027\u8d28\u7684\u86cb\u767d\u8d28\u5e8f\u5217\u3002", "method": "ProteinGuide\u901a\u8fc7\u7edf\u4e00\u591a\u79cd\u86cb\u767d\u8d28\u751f\u6210\u6a21\u578b\uff08\u5982\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u3001\u81ea\u56de\u5f52\u6a21\u578b\u3001\u6269\u6563\u6a21\u578b\u548c\u6d41\u5339\u914d\u6a21\u578b\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u8ba1\u6761\u4ef6\u5316\u7684\u65b9\u6cd5\uff0c\u4ee5\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u8f85\u52a9\u4fe1\u606f\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06ProteinMPNN\u548cESM3\u6a21\u578b\u4e0e\u7528\u6237\u6307\u5b9a\u7684\u6027\u8d28\uff08\u5982\u589e\u5f3a\u7a33\u5b9a\u6027\u548c\u7279\u5b9a\u6298\u53e0\u7ed3\u6784\uff09\u7ed3\u5408\uff0c\u751f\u6210\u4e86\u7b26\u5408\u6761\u4ef6\u7684\u6c28\u57fa\u9178\u548c\u7ed3\u6784\u5e8f\u5217\u3002", "conclusion": "ProteinGuide\u4e3a\u86cb\u767d\u8d28\u5de5\u7a0b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u901a\u7528\u7684\u6761\u4ef6\u5316\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u6574\u5408\u591a\u79cd\u5916\u90e8\u4fe1\u606f\u4ee5\u6307\u5bfc\u86cb\u767d\u8d28\u8bbe\u8ba1\u3002"}}
{"id": "2505.04653", "pdf": "https://arxiv.org/pdf/2505.04653", "abs": "https://arxiv.org/abs/2505.04653", "authors": ["Khaled Saab", "Jan Freyberg", "Chunjong Park", "Tim Strother", "Yong Cheng", "Wei-Hung Weng", "David G. T. Barrett", "David Stutz", "Nenad Tomasev", "Anil Palepu", "Valentin Li\u00e9vin", "Yash Sharma", "Roma Ruparel", "Abdullah Ahmed", "Elahe Vedadi", "Kimberly Kanada", "Cian Hughes", "Yun Liu", "Geoff Brown", "Yang Gao", "Sean Li", "S. Sara Mahdavi", "James Manyika", "Katherine Chou", "Yossi Matias", "Avinatan Hassidim", "Dale R. Webster", "Pushmeet Kohli", "S. M. Ali Eslami", "Jo\u00eblle Barral", "Adam Rodman", "Vivek Natarajan", "Mike Schaekermann", "Tao Tu", "Alan Karthikesalingam", "Ryutaro Tanno"], "title": "Advancing Conversational Diagnostic AI with Multimodal Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated great potential for conducting\ndiagnostic conversations but evaluation has been largely limited to\nlanguage-only interactions, deviating from the real-world requirements of\nremote care delivery. Instant messaging platforms permit clinicians and\npatients to upload and discuss multimodal medical artifacts seamlessly in\nmedical consultation, but the ability of LLMs to reason over such data while\npreserving other attributes of competent diagnostic conversation remains\nunknown. Here we advance the conversational diagnosis and management\nperformance of the Articulate Medical Intelligence Explorer (AMIE) through a\nnew capability to gather and interpret multimodal data, and reason about this\nprecisely during consultations. Leveraging Gemini 2.0 Flash, our system\nimplements a state-aware dialogue framework, where conversation flow is\ndynamically controlled by intermediate model outputs reflecting patient states\nand evolving diagnoses. Follow-up questions are strategically directed by\nuncertainty in such patient states, leading to a more structured multimodal\nhistory-taking process that emulates experienced clinicians. We compared AMIE\nto primary care physicians (PCPs) in a randomized, blinded, OSCE-style study of\nchat-based consultations with patient actors. We constructed 105 evaluation\nscenarios using artifacts like smartphone skin photos, ECGs, and PDFs of\nclinical documents across diverse conditions and demographics. Our rubric\nassessed multimodal capabilities and other clinically meaningful axes like\nhistory-taking, diagnostic accuracy, management reasoning, communication, and\nempathy. Specialist evaluation showed AMIE to be superior to PCPs on 7/9\nmultimodal and 29/32 non-multimodal axes (including diagnostic accuracy). The\nresults show clear progress in multimodal conversational diagnostic AI, but\nreal-world translation needs further research.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86Articulate Medical Intelligence Explorer (AMIE)\u5728\u8fdc\u7a0b\u533b\u7597\u4e2d\u7684\u591a\u6a21\u6001\u8bca\u65ad\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5728\u591a\u6570\u4e34\u5e8a\u6307\u6807\u4e0a\u4f18\u4e8e\u521d\u7ea7\u4fdd\u5065\u533b\u751f\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8bca\u65ad\u5bf9\u8bdd\u8bc4\u4f30\u591a\u5c40\u9650\u4e8e\u7eaf\u6587\u672c\u4ea4\u4e92\uff0c\u800c\u5b9e\u9645\u8fdc\u7a0b\u533b\u7597\u9700\u5904\u7406\u591a\u6a21\u6001\u533b\u7597\u6570\u636e\uff0c\u56e0\u6b64\u7814\u7a76\u65e8\u5728\u63d0\u5347AI\u5728\u591a\u6a21\u6001\u73af\u5883\u4e0b\u7684\u8bca\u65ad\u80fd\u529b\u3002", "method": "\u901a\u8fc7Gemini 2.0 Flash\u5b9e\u73b0\u72b6\u6001\u611f\u77e5\u5bf9\u8bdd\u6846\u67b6\uff0c\u52a8\u6001\u63a7\u5236\u95ee\u8bca\u6d41\u7a0b\uff0c\u5e76\u5229\u7528\u60a3\u8005\u72b6\u6001\u4e0d\u786e\u5b9a\u6027\u6307\u5bfc\u540e\u7eed\u95ee\u9898\u3002\u5728OSCE\u5f0f\u7814\u7a76\u4e2d\uff0c\u4e0e\u521d\u7ea7\u4fdd\u5065\u533b\u5e08\u5bf9\u6bd4\u8bc4\u4f30105\u4e2a\u591a\u6a21\u6001\u573a\u666f\u3002", "result": "AMIE\u57287/9\u591a\u6a21\u6001\u6307\u6807\u548c29/32\u975e\u591a\u6a21\u6001\u6307\u6807\uff08\u5305\u62ec\u8bca\u65ad\u51c6\u786e\u6027\uff09\u4e0a\u4f18\u4e8e\u521d\u7ea7\u4fdd\u5065\u533b\u5e08\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u591a\u6a21\u6001\u5bf9\u8bdd\u8bca\u65adAI\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.05059", "pdf": "https://arxiv.org/pdf/2505.05059", "abs": "https://arxiv.org/abs/2505.05059", "authors": ["Sandro Junior Della Rovere", "Davide Basso", "Luca Bortolussi", "Mirjana Videnovic-Misic", "Husni Habal"], "title": "Enhancing Reinforcement Learning for the Floorplanning of Analog ICs with Beam Search", "categories": ["cs.AI", "cs.LG"], "comment": "Published in Proceedings of the 21st International Conference on\n  Synthesis, Modeling, Analysis and Simulation Methods, and Applications to\n  Circuit Design (SMACD 2025). 4 pages, 3 figures", "summary": "The layout of analog ICs requires making complex trade-offs, while addressing\ndevice physics and variability of the circuits. This makes full automation with\nlearning-based solutions hard to achieve. However, reinforcement learning (RL)\nhas recently reached significant results, particularly in solving the\nfloorplanning problem. This paper presents a hybrid method that combines RL\nwith a beam (BS) strategy. The BS algorithm enhances the agent's inference\nprocess, allowing for the generation of flexible floorplans by accomodating\nvarious objective weightings, and addressing congestion without without the\nneed for policy retraining or fine-tuning. Moreover, the RL agent's\ngeneralization ability stays intact, along with its efficient handling of\ncircuit features and constraints. Experimental results show approx. 5-85%\nimprovement in area, dead space and half-perimeter wire length compared to a\nstandard RL application, along with higher rewards for the agent. Moreover,\nperformance and efficiency align closely with those of existing\nstate-of-the-art techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u675f\u641c\u7d22\uff08BS\uff09\u7b56\u7565\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6a21\u62df\u96c6\u6210\u7535\u8def\u5e03\u5c40\u95ee\u9898\uff0c\u5728\u9762\u79ef\u3001\u6b7b\u533a\u548c\u7ebf\u957f\u7b49\u6307\u6807\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u6a21\u62dfIC\u5e03\u5c40\u9700\u8981\u5728\u8bbe\u5907\u7269\u7406\u548c\u7535\u8def\u53d8\u5f02\u6027\u4e4b\u95f4\u8fdb\u884c\u590d\u6742\u6743\u8861\uff0c\u4f20\u7edf\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5b8c\u5168\u81ea\u52a8\u5316\u3002RL\u867d\u6709\u8fdb\u5c55\uff0c\u4f46\u5728\u7075\u6d3b\u6027\u548c\u6548\u7387\u4e0a\u4ecd\u6709\u5c40\u9650\u3002", "method": "\u6df7\u5408\u65b9\u6cd5\u7ed3\u5408RL\u548cBS\u7b56\u7565\uff0cBS\u589e\u5f3a\u4e86\u63a8\u7406\u8fc7\u7a0b\uff0c\u652f\u6301\u591a\u76ee\u6807\u52a0\u6743\u5e76\u5904\u7406\u62e5\u585e\u95ee\u9898\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u9762\u79ef\u3001\u6b7b\u533a\u548c\u7ebf\u957f\u4e0a\u6bd4\u6807\u51c6RL\u63d0\u53475-85%\uff0c\u4e14\u6027\u80fd\u4e0e\u73b0\u6709\u5148\u8fdb\u6280\u672f\u76f8\u5f53\u3002", "conclusion": "\u6df7\u5408\u65b9\u6cd5\u4e0d\u4ec5\u4fdd\u7559\u4e86RL\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8fd8\u901a\u8fc7BS\u63d0\u5347\u4e86\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u4e3a\u81ea\u52a8\u5316\u5e03\u5c40\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.04842", "pdf": "https://arxiv.org/pdf/2505.04842", "abs": "https://arxiv.org/abs/2505.04842", "authors": ["Kusha Sareen", "Morgane M Moss", "Alessandro Sordoni", "Rishabh Agarwal", "Arian Hosseini"], "title": "Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners,\nsuch as GRPO or Leave-one-out PPO, abandon the learned value function in favor\nof empirically estimated returns. This hinders test-time compute scaling that\nrelies on using the value-function for verification. In this work, we propose\nRL$^V$ that augments any ``value-free'' RL method by jointly training the LLM\nas both a reasoner and a generative verifier using RL-generated data, adding\nverification capabilities without significant overhead. Empirically, RL$^V$\nboosts MATH accuracy by over 20\\% with parallel sampling and enables\n$8-32\\times$ efficient test-time compute scaling compared to the base RL\nmethod. RL$^V$ also exhibits strong generalization capabilities for both\neasy-to-hard and out-of-domain tasks. Furthermore, RL$^V$ achieves\n$1.2-1.6\\times$ higher performance when jointly scaling parallel and sequential\ntest-time compute with a long reasoning R1 model.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faRL$^V$\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3LLM\u4f5c\u4e3a\u63a8\u7406\u5668\u548c\u751f\u6210\u9a8c\u8bc1\u5668\uff0c\u63d0\u5347\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6548\u7387\uff0c\u5b9e\u73b020%\u4ee5\u4e0a\u7684MATH\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5e76\u5c55\u793a\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684RL\u65b9\u6cd5\uff08\u5982GRPO\u6216Leave-one-out PPO\uff09\u5ffd\u7565\u4e86\u5b66\u4e60\u5230\u7684\u4ef7\u503c\u51fd\u6570\uff0c\u5bfc\u81f4\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u53d7\u9650\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u9a8c\u8bc1\u80fd\u529b\u63d0\u5347\u6548\u7387\u3002", "method": "RL$^V$\u901a\u8fc7\u8054\u5408\u8bad\u7ec3LLM\u4f5c\u4e3a\u63a8\u7406\u5668\u548c\u751f\u6210\u9a8c\u8bc1\u5668\uff0c\u5229\u7528RL\u751f\u6210\u7684\u6570\u636e\u6dfb\u52a0\u9a8c\u8bc1\u80fd\u529b\uff0c\u65e0\u9700\u663e\u8457\u589e\u52a0\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRL$^V$\u5728\u5e76\u884c\u91c7\u6837\u4e0b\u5c06MATH\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc720%\uff0c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6548\u7387\u6bd4\u57fa\u7840RL\u65b9\u6cd5\u9ad8$8-32\u00d7$\uff0c\u4e14\u5bf9\u96be\u6613\u4efb\u52a1\u548c\u8de8\u57df\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "RL$^V$\u663e\u8457\u63d0\u5347\u4e86RL\u65b9\u6cd5\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5c24\u5176\u5728\u8054\u5408\u6269\u5c55\u5e76\u884c\u548c\u987a\u5e8f\u8ba1\u7b97\u65f6\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2505.04654", "pdf": "https://arxiv.org/pdf/2505.04654", "abs": "https://arxiv.org/abs/2505.04654", "authors": ["Yehor Tereshchenko", "Mika H\u00e4m\u00e4l\u00e4inen"], "title": "A Comparative Analysis of Ethical and Safety Gaps in LLMs using Relative Danger Coefficient", "categories": ["cs.CL"], "comment": null, "summary": "Artificial Intelligence (AI) and Large Language Models (LLMs) have rapidly\nevolved in recent years, showcasing remarkable capabilities in natural language\nunderstanding and generation. However, these advancements also raise critical\nethical questions regarding safety, potential misuse, discrimination and\noverall societal impact. This article provides a comparative analysis of the\nethical performance of various AI models, including the brand new\nDeepSeek-V3(R1 with reasoning and without), various GPT variants (4o, 3.5\nTurbo, 4 Turbo, o1/o3 mini) and Gemini (1.5 flash, 2.0 flash and 2.0 flash exp)\nand highlights the need for robust human oversight, especially in situations\nwith high stakes. Furthermore, we present a new metric for calculating harm in\nLLMs called Relative Danger Coefficient (RDC).", "AI": {"tldr": "\u8bba\u6587\u5bf9AI\u548cLLM\u7684\u4f26\u7406\u6027\u80fd\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u65b0\u6307\u6807RDC\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u9ad8\u98ce\u9669\u60c5\u51b5\u4e0b\u4eba\u7c7b\u76d1\u7763\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u8ba8AI\u548cLLM\u5728\u5feb\u901f\u53d1\u5c55\u4e2d\u5f15\u53d1\u7684\u4f26\u7406\u95ee\u9898\uff0c\u5982\u5b89\u5168\u6027\u3001\u6ee5\u7528\u3001\u6b67\u89c6\u548c\u793e\u4f1a\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u65b0\u6307\u6807RDC\uff08Relative Danger Coefficient\uff09\u8bc4\u4f30\u5305\u62ecDeepSeek-V3\u3001GPT\u53d8\u4f53\u548cGemini\u5728\u5185\u7684\u591a\u6b3eAI\u6a21\u578b\u3002", "result": "\u6bd4\u8f83\u5206\u6790\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u4f26\u7406\u8868\u73b0\uff0c\u53d1\u73b0\u9700\u8981\u5f3a\u5316\u4eba\u7c7b\u76d1\u7763\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u98ce\u9669\u573a\u666f\u3002", "conclusion": "AI\u4f26\u7406\u95ee\u9898\u4e9f\u5f85\u89e3\u51b3\uff0cRDC\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u5de5\u5177\uff0c\u540c\u65f6\u4eba\u7c7b\u5728\u9ad8\u98ce\u9669\u60c5\u51b5\u4e0b\u7684\u76d1\u7763\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.05106", "pdf": "https://arxiv.org/pdf/2505.05106", "abs": "https://arxiv.org/abs/2505.05106", "authors": ["Luca Salvatore Lorello", "Marco Lippi", "Stefano Melacci"], "title": "A Neuro-Symbolic Framework for Sequence Classification with Relational and Temporal Knowledge", "categories": ["cs.AI"], "comment": null, "summary": "One of the goals of neuro-symbolic artificial intelligence is to exploit\nbackground knowledge to improve the performance of learning tasks. However,\nmost of the existing frameworks focus on the simplified scenario where\nknowledge does not change over time and does not cover the temporal dimension.\nIn this work we consider the much more challenging problem of knowledge-driven\nsequence classification where different portions of knowledge must be employed\nat different timesteps, and temporal relations are available. Our experimental\nevaluation compares multi-stage neuro-symbolic and neural-only architectures,\nand it is conducted on a newly-introduced benchmarking framework. Results\ndemonstrate the challenging nature of this novel setting, and also highlight\nunder-explored shortcomings of neuro-symbolic methods, representing a precious\nreference for future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u65f6\u95f4\u7ef4\u5ea6\u53d8\u5316\u7684\u80cc\u666f\u4e0b\uff0c\u5982\u4f55\u5229\u7528\u52a8\u6001\u77e5\u8bc6\u8fdb\u884c\u795e\u7ecf\u7b26\u53f7\u5e8f\u5217\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\u4e86\u591a\u9636\u6bb5\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u548c\u7eaf\u795e\u7ecf\u67b6\u6784\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u7b26\u53f7AI\u6846\u67b6\u5927\u591a\u5047\u8bbe\u77e5\u8bc6\u662f\u9759\u6001\u7684\uff0c\u5ffd\u7565\u4e86\u65f6\u95f4\u7ef4\u5ea6\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u77e5\u8bc6\u968f\u65f6\u95f4\u53d8\u5316\u7684\u5e8f\u5217\u5206\u7c7b\u95ee\u9898\uff0c\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u77e5\u8bc6\u9a71\u52a8\u7684\u5e8f\u5217\u5206\u7c7b\u65b9\u6cd5\uff0c\u8003\u8651\u4e86\u4e0d\u540c\u65f6\u95f4\u6b65\u7684\u77e5\u8bc6\u52a8\u6001\u5229\u7528\u548c\u65f6\u95f4\u5173\u7cfb\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e00\u65b0\u8bbe\u5b9a\u6781\u5177\u6311\u6218\u6027\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u53c2\u8003\u3002", "conclusion": "\u672c\u6587\u7684\u5de5\u4f5c\u4e0d\u4ec5\u63d0\u51fa\u4e86\u52a8\u6001\u77e5\u8bc6\u9a71\u52a8\u7684\u5e8f\u5217\u5206\u7c7b\u95ee\u9898\uff0c\u8fd8\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u590d\u6742\u6027\uff0c\u4e3a\u63a8\u52a8\u795e\u7ecf\u7b26\u53f7AI\u5728\u65f6\u5e8f\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.04873", "pdf": "https://arxiv.org/pdf/2505.04873", "abs": "https://arxiv.org/abs/2505.04873", "authors": ["Minh K. Quan", "Pubudu N. Pathirana", "Mayuri Wijayasundara", "Sujeeva Setunge", "Dinh C. Nguyen", "Christopher G. Brinton", "David J. Love", "H. Vincent Poor"], "title": "Federated Learning for Cyber Physical Systems: A Comprehensive Survey", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "comment": "This work has been accepted by IEEE Communications Surveys &\n  Tutorials", "summary": "The integration of machine learning (ML) in cyber physical systems (CPS) is a\ncomplex task due to the challenges that arise in terms of real-time decision\nmaking, safety, reliability, device heterogeneity, and data privacy. There are\nalso open research questions that must be addressed in order to fully realize\nthe potential of ML in CPS. Federated learning (FL), a distributed approach to\nML, has become increasingly popular in recent years. It allows models to be\ntrained using data from decentralized sources. This approach has been gaining\npopularity in the CPS field, as it integrates computer, communication, and\nphysical processes. Therefore, the purpose of this work is to provide a\ncomprehensive analysis of the most recent developments of FL-CPS, including the\nnumerous application areas, system topologies, and algorithms developed in\nrecent years. The paper starts by discussing recent advances in both FL and\nCPS, followed by their integration. Then, the paper compares the application of\nFL in CPS with its applications in the internet of things (IoT) in further\ndepth to show their connections and distinctions. Furthermore, the article\nscrutinizes how FL is utilized in critical CPS applications, e.g., intelligent\ntransportation systems, cybersecurity services, smart cities, and smart\nhealthcare solutions. The study also includes critical insights and lessons\nlearned from various FL-CPS implementations. The paper's concluding section\ndelves into significant concerns and suggests avenues for further research in\nthis fast-paced and dynamic era.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u4fe1\u606f\u7269\u7406\u7cfb\u7edf\uff08CPS\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u5176\u4f18\u52bf\u3001\u6311\u6218\u53ca\u672a\u6765\u53d1\u5c55\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u5728CPS\u4e2d\u7684\u96c6\u6210\u9700\u6c42\u589e\u52a0\uff0c\u89e3\u51b3\u5b9e\u65f6\u51b3\u7b56\u3001\u5b89\u5168\u6027\u548c\u9690\u79c1\u7b49\u95ee\u9898\u6210\u4e3a\u5173\u952e\u3002\u8054\u90a6\u5b66\u4e60\u4f5c\u4e3a\u4e00\u79cd\u5206\u5e03\u5f0f\u65b9\u6cd5\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "method": "\u6587\u7ae0\u7efc\u8ff0\u4e86FL\u4e0eCPS\u7684\u96c6\u6210\uff0c\u5305\u62ec\u5e94\u7528\u9886\u57df\u3001\u7cfb\u7edf\u62d3\u6251\u548c\u7b97\u6cd5\uff0c\u5e76\u5bf9\u6bd4\u4e86FL\u5728CPS\u4e0e\u7269\u8054\u7f51\uff08IoT\uff09\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u5c55\u793a\u4e86FL\u5728\u667a\u80fd\u4ea4\u901a\u3001\u7f51\u7edc\u5b89\u5168\u3001\u667a\u6167\u57ce\u5e02\u548c\u533b\u7597\u7b49CPS\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\u6848\u4f8b\u53ca\u7ecf\u9a8c\u6559\u8bad\u3002", "conclusion": "\u6587\u7ae0\u603b\u7ed3\u4e86FL-CPS\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u8fd9\u4e00\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\u7279\u6027\u3002"}}
{"id": "2505.04655", "pdf": "https://arxiv.org/pdf/2505.04655", "abs": "https://arxiv.org/abs/2505.04655", "authors": ["Paul Landes", "Jimeng Sun", "Adam Cross"], "title": "Integration of Large Language Models and Traditional Deep Learning for Social Determinants of Health Prediction", "categories": ["cs.CL"], "comment": null, "summary": "Social Determinants of Health (SDoH) are economic, social and personal\ncircumstances that affect or influence an individual's health status. SDoHs\nhave shown to be correlated to wellness outcomes, and therefore, are useful to\nphysicians in diagnosing diseases and in decision-making. In this work, we\nautomatically extract SDoHs from clinical text using traditional deep learning\nand Large Language Models (LLMs) to find the advantages and disadvantages of\neach on an existing publicly available dataset. Our models outperform a\nprevious reference point on a multilabel SDoH classification by 10 points, and\nwe present a method and model to drastically speed up classification (12X\nexecution time) by eliminating expensive LLM processing. The method we present\ncombines a more nimble and efficient solution that leverages the power of the\nLLM for precision and traditional deep learning methods for efficiency. We also\nshow highly performant results on a dataset supplemented with synthetic data\nand several traditional deep learning models that outperform LLMs. Our models\nand methods offer the next iteration of automatic prediction of SDoHs that\nimpact at-risk patients.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u4ece\u4e34\u5e8a\u6587\u672c\u4e2d\u63d0\u53d6\u793e\u4f1a\u5065\u5eb7\u51b3\u5b9a\u56e0\u7d20\uff08SDoH\uff09\uff0c\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u5904\u7406\u65f6\u95f4\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u5408\u6210\u6570\u636e\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u3002", "motivation": "\u793e\u4f1a\u5065\u5eb7\u51b3\u5b9a\u56e0\u7d20\uff08SDoH\uff09\u5bf9\u5065\u5eb7\u72b6\u51b5\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u5176\u81ea\u52a8\u63d0\u53d6\u5728\u4e34\u5e8a\u6587\u672c\u4e2d\u7684\u6548\u7387\u4e0e\u7cbe\u5ea6\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002\u8bba\u6587\u65e8\u5728\u6bd4\u8f83\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u548cLLMs\u7684\u4f18\u52bf\u4e0e\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u548cLLMs\uff0c\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u5206\u7c7b\u65b9\u6cd5\uff0c\u6d88\u9664\u6602\u8d35\u7684LLM\u5904\u7406\u6b65\u9aa4\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u589e\u5f3a\u7684\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u591a\u4e2a\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u591a\u6807\u7b7eSDoH\u5206\u7c7b\u4e0a\u6bd4\u57fa\u51c6\u63d0\u5347\u4e8610\u5206\uff0c\u4e14\u6267\u884c\u901f\u5ea6\u63d0\u5347\u4e8612\u500d\uff0c\u540c\u65f6\u5728\u5408\u6210\u6570\u636e\u4e0a\u5c55\u793a\u4e86\u9ad8\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3a\u9ad8\u98ce\u9669\u60a3\u8005\u7684SDoH\u81ea\u52a8\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u4e00\u4ee3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05108", "pdf": "https://arxiv.org/pdf/2505.05108", "abs": "https://arxiv.org/abs/2505.05108", "authors": ["Zhaohan Feng", "Ruiqi Xue", "Lei Yuan", "Yang Yu", "Ning Ding", "Meiqin Liu", "Bingzhao Gao", "Jian Sun", "Gang Wang"], "title": "Multi-agent Embodied AI: Advances and Future Directions", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Embodied artificial intelligence (Embodied AI) plays a pivotal role in the\napplication of advanced technologies in the intelligent era, where AI systems\nare integrated with physical bodies that enable them to perceive, reason, and\ninteract with their environments. Through the use of sensors for input and\nactuators for action, these systems can learn and adapt based on real-world\nfeedback, allowing them to perform tasks effectively in dynamic and\nunpredictable environments. As techniques such as deep learning (DL),\nreinforcement learning (RL), and large language models (LLMs) mature, embodied\nAI has become a leading field in both academia and industry, with applications\nspanning robotics, healthcare, transportation, and manufacturing. However, most\nresearch has focused on single-agent systems that often assume static, closed\nenvironments, whereas real-world embodied AI must navigate far more complex\nscenarios. In such settings, agents must not only interact with their\nsurroundings but also collaborate with other agents, necessitating\nsophisticated mechanisms for adaptation, real-time learning, and collaborative\nproblem-solving. Despite increasing interest in multi-agent systems, existing\nresearch remains narrow in scope, often relying on simplified models that fail\nto capture the full complexity of dynamic, open environments for multi-agent\nembodied AI. Moreover, no comprehensive survey has systematically reviewed the\nadvancements in this area. As embodied AI rapidly evolves, it is crucial to\ndeepen our understanding of multi-agent embodied AI to address the challenges\npresented by real-world applications. To fill this gap and foster further\ndevelopment in the field, this paper reviews the current state of research,\nanalyzes key contributions, and identifies challenges and future directions,\nproviding insights to guide innovation and progress in this field.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u591a\u667a\u80fd\u4f53\u5177\u8eabAI\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u5206\u6790\u4e86\u5173\u952e\u8d21\u732e\u5e76\u6307\u51fa\u4e86\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5177\u8eabAI\u9700\u8981\u5e94\u5bf9\u590d\u6742\u591a\u53d8\u7684\u52a8\u6001\u5f00\u653e\u73af\u5883\uff0c\u4f46\u76ee\u524d\u7814\u7a76\u591a\u96c6\u4e2d\u5728\u5047\u8bbe\u9759\u6001\u3001\u5c01\u95ed\u73af\u5883\u7684\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u4e0a\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u5e76\u63a8\u52a8\u53d1\u5c55\uff0c\u672c\u6587\u8fdb\u884c\u4e86\u7cfb\u7edf\u7efc\u8ff0\u3002", "method": "\u901a\u8fc7\u56de\u987e\u73b0\u6709\u7814\u7a76\u3001\u5206\u6790\u5173\u952e\u8d21\u732e\uff0c\u8bc6\u522b\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "result": "\u6307\u51fa\u4e86\u5f53\u524d\u591a\u667a\u80fd\u4f53\u5177\u8eabAI\u7814\u7a76\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u52a8\u6001\u5f00\u653e\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u5e76\u63d0\u4f9b\u4e86\u8be5\u9886\u57df\u7684\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u5177\u8eabAI\u7684\u7814\u7a76\u4ecd\u5904\u4e8e\u53d1\u5c55\u9636\u6bb5\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u521b\u65b0\u6765\u89e3\u51b3\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u6311\u6218\uff0c\u8be5\u7efc\u8ff0\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2505.04881", "pdf": "https://arxiv.org/pdf/2505.04881", "abs": "https://arxiv.org/abs/2505.04881", "authors": ["Ziqing Qiao", "Yongheng Deng", "Jiali Zeng", "Dong Wang", "Lai Wei", "Fandong Meng", "Jie Zhou", "Ju Ren", "Yaoxue Zhang"], "title": "ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) perform strongly in complex reasoning tasks via\nChain-of-Thought (CoT) prompting, but often suffer from verbose outputs caused\nby redundant content, increasing computational overhead, and degrading user\nexperience. Existing compression methods either operate post-hoc pruning,\nrisking disruption to reasoning coherence, or rely on sampling-based selection,\nwhich fails to intervene effectively during generation. In this work, we\nintroduce a confidence-guided perspective to explain the emergence of redundant\nreflection in LRMs, identifying two key patterns: Confidence Deficit, where the\nmodel reconsiders correct steps due to low internal confidence, and Termination\nDelay, where reasoning continues even after reaching a confident answer. Based\non this analysis, we propose ConCISE (Confidence-guided Compression In\nStep-by-step Efficient Reasoning), a framework that simplifies reasoning chains\nby reinforcing the model's confidence during inference, thus preventing the\ngeneration of redundant reflection steps. It integrates Confidence Injection to\nstabilize intermediate steps and Early Stopping to terminate reasoning when\nconfidence is sufficient. Extensive experiments demonstrate that fine-tuning\nLRMs on ConCISE-generated data yields significantly shorter outputs, reducing\nlength by up to approximately 50% under SimPO, while maintaining high task\naccuracy. ConCISE consistently outperforms existing baselines across multiple\nreasoning benchmarks.", "AI": {"tldr": "ConCISE\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u7f6e\u4fe1\u5ea6\uff0c\u51cf\u5c11\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\uff0c\u4ece\u800c\u663e\u8457\u7f29\u77ed\u8f93\u51fa\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u56e0\u5197\u4f59\u5185\u5bb9\u5bfc\u81f4\u7684\u8f93\u51fa\u5197\u957f\u95ee\u9898\uff0c\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u63d0\u51faConCISE\u6846\u67b6\uff0c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u5206\u6790\uff08\u5982\u4fe1\u5fc3\u4e0d\u8db3\u548c\u7ec8\u6b62\u5ef6\u8fdf\uff09\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u6ce8\u5165\u548c\u63d0\u524d\u7ec8\u6b62\u6280\u672f\u4f18\u5316\u63a8\u7406\u94fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cConCISE\u80fd\u51cf\u5c11\u7ea650%\u7684\u8f93\u51fa\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u4efb\u52a1\u51c6\u786e\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ConCISE\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u538b\u7f29\u6280\u672f\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\uff0c\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.04660", "pdf": "https://arxiv.org/pdf/2505.04660", "abs": "https://arxiv.org/abs/2505.04660", "authors": ["Sana Alamgeer", "Yasine Souissi", "Anne H. H. Ngu"], "title": "AI-Generated Fall Data: Assessing LLMs and Diffusion Model for Wearable Fall Detection", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Training fall detection systems is challenging due to the scarcity of\nreal-world fall data, particularly from elderly individuals. To address this,\nwe explore the potential of Large Language Models (LLMs) for generating\nsynthetic fall data. This study evaluates text-to-motion (T2M, SATO, ParCo) and\ntext-to-text models (GPT4o, GPT4, Gemini) in simulating realistic fall\nscenarios. We generate synthetic datasets and integrate them with four\nreal-world baseline datasets to assess their impact on fall detection\nperformance using a Long Short-Term Memory (LSTM) model. Additionally, we\ncompare LLM-generated synthetic data with a diffusion-based method to evaluate\ntheir alignment with real accelerometer distributions. Results indicate that\ndataset characteristics significantly influence the effectiveness of synthetic\ndata, with LLM-generated data performing best in low-frequency settings (e.g.,\n20Hz) while showing instability in high-frequency datasets (e.g., 200Hz). While\ntext-to-motion models produce more realistic biomechanical data than\ntext-to-text models, their impact on fall detection varies. Diffusion-based\nsynthetic data demonstrates the closest alignment to real data but does not\nconsistently enhance model performance. An ablation study further confirms that\nthe effectiveness of synthetic data depends on sensor placement and fall\nrepresentation. These findings provide insights into optimizing synthetic data\ngeneration for fall detection models.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u5408\u6210\u8dcc\u5012\u6570\u636e\u4ee5\u89e3\u51b3\u771f\u5b9e\u8dcc\u5012\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5206\u6790\u4e86\u591a\u79cd\u6587\u672c\u5230\u8fd0\u52a8\uff08T2M\uff09\u548c\u6587\u672c\u5230\u6587\u672c\u6a21\u578b\u7684\u6548\u679c\uff0c\u5e76\u8bc4\u4f30\u4e86\u5408\u6210\u6570\u636e\u5bf9\u8dcc\u5012\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8e\u73b0\u5b9e\u4e2d\u8001\u5e74\u4eba\u8dcc\u5012\u6570\u636e\u7a00\u7f3a\uff0c\u8bad\u7ec3\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\u5177\u6709\u6311\u6218\u6027\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5408\u6210\u6570\u636e\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u6587\u672c\u5230\u8fd0\u52a8\uff08T2M, SATO, ParCo\uff09\u548c\u6587\u672c\u5230\u6587\u672c\u6a21\u578b\uff08GPT4o, GPT4, Gemini\uff09\u751f\u6210\u5408\u6210\u8dcc\u5012\u6570\u636e\uff0c\u5e76\u4e0e\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u7ed3\u5408\uff0c\u91c7\u7528LSTM\u6a21\u578b\u8bc4\u4f30\u6027\u80fd\u3002\u8fd8\u6bd4\u8f83\u4e86LLM\u751f\u6210\u6570\u636e\u4e0e\u57fa\u4e8e\u6269\u6563\u65b9\u6cd5\u7684\u6570\u636e\u5bf9\u9f50\u6548\u679c\u3002", "result": "\u5408\u6210\u6570\u636e\u7684\u6548\u679c\u53d7\u6570\u636e\u96c6\u7279\u6027\u5f71\u54cd\u663e\u8457\uff1aLLM\u751f\u6210\u6570\u636e\u5728\u4f4e\u9891\uff08\u598220Hz\uff09\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5728\u9ad8\u9891\uff08\u5982200Hz\uff09\u4e0d\u7a33\u5b9a\uff1b\u6587\u672c\u5230\u8fd0\u52a8\u6a21\u578b\u80fd\u751f\u6210\u66f4\u771f\u5b9e\u7684\u751f\u7269\u529b\u5b66\u6570\u636e\uff0c\u4f46\u5bf9\u68c0\u6d4b\u6548\u679c\u7684\u5f71\u54cd\u4e0d\u4e00\uff1b\u6269\u6563\u6a21\u578b\u6570\u636e\u6700\u63a5\u8fd1\u771f\u5b9e\u5206\u5e03\uff0c\u4f46\u672a\u4e00\u81f4\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u7684\u6548\u679c\u4f9d\u8d56\u4e8e\u4f20\u611f\u5668\u4f4d\u7f6e\u548c\u8dcc\u5012\u8868\u73b0\u65b9\u5f0f\u3002\u7814\u7a76\u4e3a\u4f18\u5316\u8dcc\u5012\u68c0\u6d4b\u6a21\u578b\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u6d1e\u89c1\u3002"}}
{"id": "2505.05115", "pdf": "https://arxiv.org/pdf/2505.05115", "abs": "https://arxiv.org/abs/2505.05115", "authors": ["Toby Ord"], "title": "Is there a half-life for the success rates of AI agents?", "categories": ["cs.AI", "68T42", "I.2.8"], "comment": "9 pages, 5 figures", "summary": "Building on the recent empirical work of Kwa et al. (2025), I show that\nwithin their suite of research-engineering tasks the performance of AI agents\non longer-duration tasks can be explained by an extremely simple mathematical\nmodel -- a constant rate of failing during each minute a human would take to do\nthe task. This implies an exponentially declining success rate with the length\nof the task and that each agent could be characterised by its own half-life.\nThis empirical regularity allows us to estimate the success rate for an agent\nat different task lengths. And the fact that this model is a good fit for the\ndata is suggestive of the underlying causes of failure on longer tasks -- that\nthey involve increasingly large sets of subtasks where failing any one fails\nthe task. Whether this model applies more generally on other suites of tasks is\nunknown and an important subject for further work.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793aAI\u4ee3\u7406\u5728\u957f\u65f6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u53ef\u7528\u7b80\u5355\u6570\u5b66\u6a21\u578b\u89e3\u91ca\uff0c\u5373\u6bcf\u5355\u4f4d\u65f6\u95f4\u5931\u8d25\u7387\u4e3a\u5e38\u6570\uff0c\u5bfc\u81f4\u6210\u529f\u7387\u968f\u4efb\u52a1\u65f6\u957f\u6307\u6570\u4e0b\u964d\u3002", "motivation": "\u57fa\u4e8eKwa\u7b49\uff082025\uff09\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u63a2\u8ba8AI\u4ee3\u7406\u5728\u957f\u65f6\u4efb\u52a1\u4e2d\u5931\u8d25\u7387\u7684\u4e00\u81f4\u6027\u89c4\u5f8b\u53ca\u5176\u6f5c\u5728\u539f\u56e0\u3002", "method": "\u63d0\u51fa\u5e38\u6570\u65f6\u95f4\u5931\u8d25\u7387\u6a21\u578b\uff0c\u5047\u8bbe\u6bcf\u5206\u949f\u4efb\u52a1\u5931\u8d25\u6982\u7387\u6052\u5b9a\uff0c\u9a8c\u8bc1\u5176\u5bf9\u4e0d\u540c\u4efb\u52a1\u65f6\u957f\u7684\u9002\u7528\u6027\u3002", "result": "\u6a21\u578b\u4e0e\u6570\u636e\u9ad8\u5ea6\u543b\u5408\uff0c\u8868\u660e\u957f\u65f6\u4efb\u52a1\u7684\u5931\u8d25\u53ef\u80fd\u6e90\u4e8e\u5b50\u4efb\u52a1\u96c6\u5408\u589e\u5927\u5bfc\u81f4\u7684\u7d2f\u79ef\u5931\u6548\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u7406\u89e3AI\u957f\u65f6\u4efb\u52a1\u5931\u8d25\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u4f46\u5176\u666e\u9002\u6027\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002"}}
{"id": "2505.04889", "pdf": "https://arxiv.org/pdf/2505.04889", "abs": "https://arxiv.org/abs/2505.04889", "authors": ["Tianzhe Xiao", "Yichen Li", "Yu Zhou", "Yining Qi", "Yi Liu", "Wei Wang", "Haozhao Wang", "Yi Wang", "Ruixuan Li"], "title": "FedRE: Robust and Effective Federated Learning with Privacy Preference", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Despite Federated Learning (FL) employing gradient aggregation at the server\nfor distributed training to prevent the privacy leakage of raw data, private\ninformation can still be divulged through the analysis of uploaded gradients\nfrom clients. Substantial efforts have been made to integrate local\ndifferential privacy (LDP) into the system to achieve a strict privacy\nguarantee. However, existing methods fail to take practical issues into account\nby merely perturbing each sample with the same mechanism while each client may\nhave their own privacy preferences on privacy-sensitive information (PSI),\nwhich is not uniformly distributed across the raw data. In such a case,\nexcessive privacy protection from private-insensitive information can\nadditionally introduce unnecessary noise, which may degrade the model\nperformance. In this work, we study the PSI within data and develop FedRE, that\ncan simultaneously achieve robustness and effectiveness benefits with LDP\nprotection. More specifically, we first define PSI with regard to the privacy\npreferences of each client. Then, we optimize the LDP by allocating less\nprivacy budget to gradients with higher PSI in a layer-wise manner, thus\nproviding a stricter privacy guarantee for PSI. Furthermore, to mitigate the\nperformance degradation caused by LDP, we design a parameter aggregation\nmechanism based on the distribution of the perturbed information. We conducted\nexperiments with text tamper detection on T-SROIE and DocTamper datasets, and\nFedRE achieves competitive performance compared to state-of-the-art methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedRE\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u672c\u5730\u5dee\u5206\u9690\u79c1\uff08LDP\uff09\u673a\u5236\uff0c\u9488\u5bf9\u4e0d\u540c\u5ba2\u6237\u7aef\u5bf9\u9690\u79c1\u654f\u611f\u4fe1\u606f\uff08PSI\uff09\u7684\u4e0d\u540c\u9700\u6c42\u5206\u5c42\u5206\u914d\u9690\u79c1\u9884\u7b97\uff0c\u4ece\u800c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u566a\u58f0\u5e72\u6270\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u5e94\u7528LDP\u65f6\u672a\u80fd\u8003\u8651\u4e0d\u540c\u5ba2\u6237\u7aef\u5bf9\u9690\u79c1\u654f\u611f\u4fe1\u606f\uff08PSI\uff09\u7684\u4e0d\u540c\u9700\u6c42\uff0c\u5bfc\u81f4\u5bf9\u975e\u9690\u79c1\u654f\u611f\u4fe1\u606f\u7684\u8fc7\u5ea6\u4fdd\u62a4\u5f15\u5165\u4e86\u4e0d\u5fc5\u8981\u7684\u566a\u58f0\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faFedRE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4e49PSI\u5e76\u5206\u5c42\u4f18\u5316LDP\u9690\u79c1\u9884\u7b97\u5206\u914d\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u6270\u52a8\u4fe1\u606f\u5206\u5e03\u7684\u53c2\u6570\u805a\u5408\u673a\u5236\u3002", "result": "\u5728T-SROIE\u548cDocTamper\u6570\u636e\u96c6\u4e0a\u7684\u6587\u672c\u7be1\u6539\u68c0\u6d4b\u5b9e\u9a8c\u4e2d\uff0cFedRE\u8868\u73b0\u51fa\u4e0e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\u3002", "conclusion": "FedRE\u901a\u8fc7\u5206\u5c42LDP\u4f18\u5316\u548c\u53c2\u6570\u805a\u5408\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u6709\u6548\u5e73\u8861\u3002"}}
{"id": "2505.04665", "pdf": "https://arxiv.org/pdf/2505.04665", "abs": "https://arxiv.org/abs/2505.04665", "authors": ["Haoyang Feng", "Yanjun Dai", "Yuan Gao"], "title": "Personalized Risks and Regulatory Strategies of Large Language Models in Digital Advertising", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although large language models have demonstrated the potential for\npersonalized advertising recommendations in experimental environments, in\nactual operations, how advertising recommendation systems can be combined with\nmeasures such as user privacy protection and data security is still an area\nworthy of in-depth discussion. To this end, this paper studies the personalized\nrisks and regulatory strategies of large language models in digital\nadvertising. This study first outlines the principles of Large Language Model\n(LLM), especially the self-attention mechanism based on the Transformer\narchitecture, and how to enable the model to understand and generate natural\nlanguage text. Then, the BERT (Bidirectional Encoder Representations from\nTransformers) model and the attention mechanism are combined to construct an\nalgorithmic model for personalized advertising recommendations and user factor\nrisk protection. The specific steps include: data collection and preprocessing,\nfeature selection and construction, using large language models such as BERT\nfor advertising semantic embedding, and ad recommendations based on user\nportraits. Then, local model training and data encryption are used to ensure\nthe security of user privacy and avoid the leakage of personal data. This paper\ndesigns an experiment for personalized advertising recommendation based on a\nlarge language model of BERT and verifies it with real user data. The\nexperimental results show that BERT-based advertising push can effectively\nimprove the click-through rate and conversion rate of advertisements. At the\nsame time, through local model training and privacy protection mechanisms, the\nrisk of user privacy leakage can be reduced to a certain extent.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b57\u5e7f\u544a\u4e2d\u7684\u4e2a\u6027\u5316\u98ce\u9669\u4e0e\u76d1\u7ba1\u7b56\u7565\uff0c\u7ed3\u5408BERT\u6a21\u578b\u4e0e\u6ce8\u610f\u529b\u673a\u5236\u6784\u5efa\u5e7f\u544a\u63a8\u8350\u7b97\u6cd5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u63d0\u9ad8\u70b9\u51fb\u7387\u5e76\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u5e7f\u544a\u63a8\u8350\u4e2d\u5982\u4f55\u7ed3\u5408\u7528\u6237\u9690\u79c1\u4fdd\u62a4\u4e0e\u6570\u636e\u5b89\u5168\uff0c\u586b\u8865\u5b9e\u9645\u64cd\u4f5c\u4e2d\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u91c7\u7528BERT\u6a21\u578b\u4e0e\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u6570\u636e\u52a0\u5bc6\u548c\u5c40\u90e8\u8bad\u7ec3\uff0c\u6784\u5efa\u4e2a\u6027\u5316\u63a8\u8350\u4e0e\u9690\u79c1\u4fdd\u62a4\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eBERT\u7684\u63a8\u8350\u80fd\u663e\u8457\u63d0\u5347\u5e7f\u544a\u70b9\u51fb\u7387\u4e0e\u8f6c\u5316\u7387\uff0c\u540c\u65f6\u964d\u4f4e\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5e7f\u544a\u63a8\u8350\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u9700\u7ed3\u5408\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u4ee5\u5b9e\u73b0\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2505.05177", "pdf": "https://arxiv.org/pdf/2505.05177", "abs": "https://arxiv.org/abs/2505.05177", "authors": ["Anish Ganguli", "Prabal Deb", "Debleena Banerjee"], "title": "MARK: Memory Augmented Refinement of Knowledge", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) assist in specialized tasks but struggle to\nalign with evolving domain knowledge without costly fine-tuning. Domain\nknowledge consists of: Knowledge: Immutable facts (e.g., 'A stone is solid')\nand generally accepted principles (e.g., ethical standards); Refined Memory:\nEvolving insights shaped by business needs and real-world changes. However, a\nsignificant gap often exists between a domain expert's deep, nuanced\nunderstanding and the system's domain knowledge, which can hinder accurate\ninformation retrieval and application. Our Memory-Augmented Refinement of\nKnowledge (MARK) framework enables LLMs to continuously learn without\nretraining by leveraging structured refined memory, inspired by the Society of\nMind. MARK operates through specialized agents, each serving a distinct role:\nResidual Refined Memory Agent: Stores and retrieves domain-specific insights to\nmaintain context over time; User Question Refined Memory Agent: Captures\nuser-provided facts, abbreviations, and terminology for better comprehension;\nLLM Response Refined Memory Agent: Extracts key elements from responses for\nrefinement and personalization. These agents analyse stored refined memory,\ndetect patterns, resolve contradictions, and improve response accuracy.\nTemporal factors like recency and frequency prioritize relevant information\nwhile discarding outdated insights. MARK enhances LLMs in multiple ways: Ground\nTruth Strategy: Reduces hallucinations by establishing a structured reference;\nDomain-Specific Adaptation: Essential for fields like healthcare, law, and\nmanufacturing, where proprietary insights are absent from public datasets;\nPersonalized AI Assistants: Improves virtual assistants by remembering user\npreferences, ensuring coherent responses over time.", "AI": {"tldr": "\u603b\u7ed3\u4e86\u8bba\u6587\u7684\u80cc\u666f\u3001\u52a8\u673a\u3001\u65b9\u6cd5\u548c\u7ed3\u679c", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9886\u57df\u77e5\u8bc6\u66f4\u65b0\u65f6\u7684\u9ad8\u6210\u672c\u5fae\u8c03\u95ee\u9898\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMARK\u7684\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u7ed3\u6784\u5316\u7ec6\u5316\u8bb0\u5fc6\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u3002", "method": "\u8bba\u6587\u91c7\u7528\u4e86Memory-Augmented Refinement of Knowledge (MARK)\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e13\u7528\u4ee3\u7406\uff08\u5982Residual Refined Memory Agent\u7b49\uff09\u6765\u5b58\u50a8\u3001\u68c0\u7d22\u548c\u4f18\u5316\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u3002", "result": "MARK\u6846\u67b6\u901a\u8fc7\u51cf\u5c11\u5e7b\u89c9\u3001\u63d0\u9ad8\u9886\u57df\u9002\u5e94\u6027\u548c\u4f18\u5316\u4e2a\u6027\u5316AI\u52a9\u624b\uff0c\u6210\u529f\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u548c\u51c6\u786e\u6027\u3002", "conclusion": "MARK\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4f7f\u5f97\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u65e0\u9700\u6602\u8d35\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u6301\u7eed\u5b66\u4e60\u548c\u9002\u5e94\u53d8\u5316\u7684\u9886\u57df\u77e5\u8bc6\u3002"}}
{"id": "2505.04891", "pdf": "https://arxiv.org/pdf/2505.04891", "abs": "https://arxiv.org/abs/2505.04891", "authors": ["Cong Qi", "Yeqing Chen", "Jie Zhang", "Wei Zhi"], "title": "Clustering with Communication: A Variational Framework for Single Cell Representation Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) has revealed complex cellular\nheterogeneity, but recent studies emphasize that understanding biological\nfunction also requires modeling cell-cell communication (CCC), the signaling\ninteractions mediated by ligand-receptor pairs that coordinate cellular\nbehavior. Tools like CellChat have demonstrated that CCC plays a critical role\nin processes such as cell differentiation, tissue regeneration, and immune\nresponse, and that transcriptomic data inherently encodes rich information\nabout intercellular signaling. We propose CCCVAE, a novel variational\nautoencoder framework that incorporates CCC signals into single-cell\nrepresentation learning. By leveraging a communication-aware kernel derived\nfrom ligand-receptor interactions and a sparse Gaussian process, CCCVAE encodes\nbiologically informed priors into the latent space. Unlike conventional VAEs\nthat treat each cell independently, CCCVAE encourages latent embeddings to\nreflect both transcriptional similarity and intercellular signaling context.\nEmpirical results across four scRNA-seq datasets show that CCCVAE improves\nclustering performance, achieving higher evaluation scores than standard VAE\nbaselines. This work demonstrates the value of embedding biological priors into\ndeep generative models for unsupervised single-cell analysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCCCVAE\u7684\u65b0\u578b\u53d8\u5206\u81ea\u7f16\u7801\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u7ec6\u80de\u95f4\u901a\u8baf\uff08CCC\uff09\u4fe1\u53f7\u6539\u8fdb\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u7684\u8868\u793a\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u805a\u7c7b\u6027\u80fd\u3002", "motivation": "\u7406\u89e3\u7ec6\u80de\u95f4\u901a\u8baf\uff08CCC\uff09\u5bf9\u63ed\u793a\u7ec6\u80de\u529f\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edfVAE\u6a21\u578b\u672a\u80fd\u6709\u6548\u5229\u7528CCC\u4fe1\u53f7\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u5c06\u8fd9\u4e9b\u751f\u7269\u5148\u9a8c\u77e5\u8bc6\u878d\u5165\u5355\u7ec6\u80de\u5206\u6790\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86CCCVAE\uff0c\u901a\u8fc7\u7ed3\u5408\u914d\u4f53-\u53d7\u4f53\u76f8\u4e92\u4f5c\u7528\u7684\u901a\u8baf\u611f\u77e5\u6838\u548c\u7a00\u758f\u9ad8\u65af\u8fc7\u7a0b\uff0c\u5c06CCC\u4fe1\u53f7\u5d4c\u5165\u6f5c\u7a7a\u95f4\uff0c\u4f7f\u6f5c\u8868\u793a\u80fd\u540c\u65f6\u53cd\u6620\u8f6c\u5f55\u76f8\u4f3c\u6027\u548c\u7ec6\u80de\u95f4\u901a\u8baf\u80cc\u666f\u3002", "result": "\u5728\u56db\u4e2ascRNA-seq\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCCCVAE\u7684\u805a\u7c7b\u6027\u80fd\u4f18\u4e8e\u6807\u51c6VAE\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86\u751f\u7269\u5148\u9a8c\u5d4c\u5165\u5bf9\u5355\u7ec6\u80de\u5206\u6790\u7684\u4ef7\u503c\u3002", "conclusion": "CCCVAE\u901a\u8fc7\u6574\u5408CCC\u4fe1\u53f7\u6539\u8fdb\u4e86\u5355\u7ec6\u80de\u6570\u636e\u7684\u6f5c\u8868\u793a\u5b66\u4e60\uff0c\u8bc1\u660e\u4e86\u5728\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u4e2d\u5d4c\u5165\u751f\u7269\u5148\u9a8c\u5bf9\u65e0\u76d1\u7763\u5355\u7ec6\u80de\u5206\u6790\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.04666", "pdf": "https://arxiv.org/pdf/2505.04666", "abs": "https://arxiv.org/abs/2505.04666", "authors": ["Mohammad Aqib", "Mohd Hamza", "Qipei Mei", "Ying Hei Chui"], "title": "Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Building codes are regulations that establish standards for the design,\nconstruction, and safety of buildings to ensure structural integrity, fire\nprotection, and accessibility. They are often extensive, complex, and subject\nto frequent updates, making manual querying challenging and time-consuming. Key\ndifficulties include navigating large volumes of text, interpreting technical\nlanguage, and identifying relevant clauses across different sections. A\npotential solution is to build a Question-Answering (QA) system that answers\nuser queries based on building codes. Among the various methods for building a\nQA system, Retrieval-Augmented Generation (RAG) stands out in performance. RAG\nconsists of two components: a retriever and a language model. This study\nfocuses on identifying a suitable retriever method for building codes and\noptimizing the generational capability of the language model using fine-tuning\ntechniques. We conducted a detailed evaluation of various retrieval methods by\nperforming the retrieval on the National Building Code of Canada (NBCC) and\nexplored the impact of domain-specific fine-tuning on several language models\nusing the dataset derived from NBCC. Our analysis included a comparative\nassessment of different retrievers and the performance of both pre-trained and\nfine-tuned models to determine the efficacy and domain-specific adaptation of\nlanguage models using fine-tuning on the NBCC dataset. Experimental results\nshowed that Elasticsearch proved to be the most robust retriever among all. The\nfindings also indicate that fine-tuning language models on an NBCC-specific\ndataset can enhance their ability to generate contextually relevant responses.\nWhen combined with context retrieved by a powerful retriever like\nElasticsearch, this improvement in LLM performance can optimize the RAG system,\nenabling it to better navigate the complexities of the NBCC.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6784\u5efa\u4e00\u4e2a\u57fa\u4e8e\u5efa\u7b51\u89c4\u8303\u7684\u95ee\u7b54\u7cfb\u7edf\uff0c\u91cd\u70b9\u4f18\u5316\u68c0\u7d22\u5668\u548c\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660eElasticsearch\u662f\u9ad8\u6548\u68c0\u7d22\u5668\uff0c\u4e14\u8bed\u8a00\u6a21\u578b\u7ecf\u9886\u57df\u5fae\u8c03\u540e\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u5efa\u7b51\u89c4\u8303\u5185\u5bb9\u7e41\u6742\u4e14\u66f4\u65b0\u9891\u7e41\uff0c\u624b\u5de5\u67e5\u8be2\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u901a\u8fc7\u81ea\u52a8\u5316\u6280\u672f\uff08\u5982QA\u7cfb\u7edf\uff09\u63d0\u5347\u67e5\u8be2\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528RAG\u6846\u67b6\uff08\u68c0\u7d22\u5668+\u8bed\u8a00\u6a21\u578b\uff09\uff0c\u5bf9\u6bd4\u591a\u79cd\u68c0\u7d22\u65b9\u6cd5\uff08\u4ee5\u52a0\u62ff\u5927\u56fd\u5bb6\u5efa\u7b51\u89c4\u8303NBCC\u4e3a\u6570\u636e\u96c6\uff09\uff0c\u5e76\u5bf9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9886\u57df\u5fae\u8c03\u3002", "result": "Elasticsearch\u4e3a\u6700\u4f73\u68c0\u7d22\u5668\uff1b\u8bed\u8a00\u6a21\u578b\u7ecfNBCC\u6570\u636e\u96c6\u5fae\u8c03\u540e\u751f\u6210\u80fd\u529b\u663e\u8457\u63d0\u5347\uff0c\u7ed3\u5408\u9ad8\u6548\u68c0\u7d22\u5668\u53ef\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8eRAG\u7684QA\u7cfb\u7edf\u80fd\u6709\u6548\u5e94\u5bf9\u5efa\u7b51\u89c4\u8303\u590d\u6742\u6027\uff0c\u68c0\u7d22\u5668\u9009\u62e9\u548c\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u662f\u5173\u952e\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2505.05197", "pdf": "https://arxiv.org/pdf/2505.05197", "abs": "https://arxiv.org/abs/2505.05197", "authors": ["Joel Z. Leibo", "Alexander Sasha Vezhnevets", "William A. Cunningham", "S\u00e9bastien Krier", "Manfred Diaz", "Simon Osindero"], "title": "Societal and technological progress as sewing an ever-growing, ever-changing, patchy, and polychrome quilt", "categories": ["cs.AI", "cs.CY"], "comment": "16 pages", "summary": "Artificial Intelligence (AI) systems are increasingly placed in positions\nwhere their decisions have real consequences, e.g., moderating online spaces,\nconducting research, and advising on policy. Ensuring they operate in a safe\nand ethically acceptable fashion is thus critical. However, most solutions have\nbeen a form of one-size-fits-all \"alignment\". We are worried that such systems,\nwhich overlook enduring moral diversity, will spark resistance, erode trust,\nand destabilize our institutions. This paper traces the underlying problem to\nan often-unstated Axiom of Rational Convergence: the idea that under ideal\nconditions, rational agents will converge in the limit of conversation on a\nsingle ethics. Treating that premise as both optional and doubtful, we propose\nwhat we call the appropriateness framework: an alternative approach grounded in\nconflict theory, cultural evolution, multi-agent systems, and institutional\neconomics. The appropriateness framework treats persistent disagreement as the\nnormal case and designs for it by applying four principles: (1) contextual\ngrounding, (2) community customization, (3) continual adaptation, and (4)\npolycentric governance. We argue here that adopting these design principles is\na good way to shift the main alignment metaphor from moral unification to a\nmore productive metaphor of conflict management, and that taking this step is\nboth desirable and urgent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a'appropriateness framework'\u7684\u66ff\u4ee3\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3AI\u51b3\u7b56\u4e2d\u7684\u4f26\u7406\u591a\u6837\u6027\u95ee\u9898\uff0c\u5f3a\u8c03\u51b2\u7a81\u7ba1\u7406\u4e0e\u6301\u7eed\u9002\u5e94\u7684\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "\u5f53\u524dAI\u4f26\u7406\u5bf9\u9f50\u65b9\u6848\u901a\u5e38\u5047\u8bbe\u9053\u5fb7\u7edf\u4e00\u6027\uff0c\u5ffd\u89c6\u4e86\u9053\u5fb7\u591a\u6837\u6027\u53ef\u80fd\u5bfc\u81f4\u7684\u793e\u4f1a\u4fe1\u4efb\u5371\u673a\u548c\u5236\u5ea6\u4e0d\u7a33\u5b9a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9002\u5e94\u591a\u5143\u4f26\u7406\u7684\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u51b2\u7a81\u7406\u8bba\u3001\u6587\u5316\u8fdb\u5316\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u5236\u5ea6\u7ecf\u6d4e\u5b66\uff0c\u63d0\u51fa\u4e86\u5305\u542b\u56db\u4e2a\u539f\u5219\u7684appropriateness framework\uff1a\u4e0a\u4e0b\u6587\u57fa\u7840\u3001\u793e\u533a\u5b9a\u5236\u3001\u6301\u7eed\u9002\u5e94\u548c\u591a\u4e2d\u5fc3\u6cbb\u7406\u3002", "result": "\u8be5\u6846\u67b6\u5c06\u4f26\u7406\u5bf9\u9f50\u7684\u9690\u55bb\u4ece\u9053\u5fb7\u7edf\u4e00\u8f6c\u5411\u51b2\u7a81\u7ba1\u7406\uff0c\u66f4\u9002\u5e94\u73b0\u5b9e\u4e2d\u7684\u6301\u7eed\u5206\u6b67\uff0c\u63d0\u4f9b\u4e86\u66f4\u5177\u9002\u5e94\u6027\u548c\u5305\u5bb9\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u4f5c\u8005\u8ba4\u4e3a\u91c7\u7528\u8fd9\u79cd\u8bbe\u8ba1\u539f\u5219\u4e0d\u4ec5\u662f\u53ef\u53d6\u7684\uff0c\u800c\u4e14\u662f\u7d27\u8feb\u7684\uff0c\u4ee5\u9002\u5e94\u591a\u5143\u4f26\u7406\u73af\u5883\u5e76\u589e\u5f3a\u793e\u4f1a\u4fe1\u4efb\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2505.04894", "pdf": "https://arxiv.org/pdf/2505.04894", "abs": "https://arxiv.org/abs/2505.04894", "authors": ["Nazanin Mehregan", "Robson E. De Grande"], "title": "GCN-Based Throughput-Oriented Handover Management in Dense 5G Vehicular Networks", "categories": ["cs.LG"], "comment": "Accepted at IEEE DCOSS-IoT 2025", "summary": "The rapid advancement of 5G has transformed vehicular networks, offering high\nbandwidth, low latency, and fast data rates essential for real-time\napplications in smart cities and vehicles. These improvements enhance traffic\nsafety and entertainment services. However, the limited coverage and frequent\nhandovers in 5G networks cause network instability, especially in high-mobility\nenvironments due to the ping-pong effect. This paper presents TH-GCN\n(Throughput-oriented Graph Convolutional Network), a novel approach for\noptimizing handover management in dense 5G networks. Using graph neural\nnetworks (GNNs), TH-GCN models vehicles and base stations as nodes in a dynamic\ngraph enriched with features such as signal quality, throughput, vehicle speed,\nand base station load. By integrating both user equipment and base station\nperspectives, this dual-centric approach enables adaptive, real-time handover\ndecisions that improve network stability. Simulation results show that TH-GCN\nreduces handovers by up to 78 percent and improves signal quality by 10\npercent, outperforming existing methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTH-GCN\u7684\u65b0\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u5bc6\u96c65G\u7f51\u7edc\u4e2d\u7684\u5207\u6362\u7ba1\u7406\uff0c\u51cf\u5c11\u5207\u6362\u6b21\u6570\u5e76\u63d0\u9ad8\u4fe1\u53f7\u8d28\u91cf\u3002", "motivation": "5G\u7f51\u7edc\u7684\u5feb\u901f\u53d1\u5c55\u4e3a\u8f66\u8f7d\u7f51\u7edc\u5e26\u6765\u4e86\u9ad8\u5e26\u5bbd\u548c\u4f4e\u5ef6\u8fdf\uff0c\u4f46\u8986\u76d6\u8303\u56f4\u6709\u9650\u548c\u9891\u7e41\u5207\u6362\u5bfc\u81f4\u7f51\u7edc\u4e0d\u7a33\u5b9a\uff0c\u7279\u522b\u662f\u5728\u9ad8\u79fb\u52a8\u6027\u73af\u5883\u4e2d\u3002", "method": "TH-GCN\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\uff0c\u5c06\u8f66\u8f86\u548c\u57fa\u7ad9\u5efa\u6a21\u4e3a\u52a8\u6001\u56fe\u4e2d\u7684\u8282\u70b9\uff0c\u7ed3\u5408\u4fe1\u53f7\u8d28\u91cf\u3001\u541e\u5410\u91cf\u3001\u8f66\u901f\u548c\u57fa\u7ad9\u8d1f\u8f7d\u7b49\u7279\u5f81\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u5b9e\u65f6\u5207\u6362\u51b3\u7b56\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cTH-GCN\u5c06\u5207\u6362\u6b21\u6570\u51cf\u5c11\u4e8678%\uff0c\u4fe1\u53f7\u8d28\u91cf\u63d0\u9ad8\u4e8610%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TH-GCN\u901a\u8fc7\u53cc\u4e2d\u5fc3\u65b9\u6cd5\u4f18\u5316\u5207\u6362\u7ba1\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e865G\u7f51\u7edc\u5728\u9ad8\u79fb\u52a8\u6027\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2505.04671", "pdf": "https://arxiv.org/pdf/2505.04671", "abs": "https://arxiv.org/abs/2505.04671", "authors": ["Yuxin Zhang", "Meihao Fan", "Ju Fan", "Mingyang Yi", "Yuyu Luo", "Jian Tan", "Guoliang Li"], "title": "Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and Process-Supervised Rewards", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly improved\nperformance on the Text-to-SQL task by leveraging their powerful reasoning\ncapabilities. To enhance accuracy during the reasoning process, external\nProcess Reward Models (PRMs) can be introduced during training and inference to\nprovide fine-grained supervision. However, if misused, PRMs may distort the\nreasoning trajectory and lead to suboptimal or incorrect SQL generation.To\naddress this challenge, we propose Reward-SQL, a framework that systematically\nexplores how to incorporate PRMs into the Text-to-SQL reasoning process\neffectively. Our approach follows a \"cold start, then PRM supervision\"\nparadigm. Specifically, we first train the model to decompose SQL queries into\nstructured stepwise reasoning chains using common table expressions\n(Chain-of-CTEs), establishing a strong and interpretable reasoning baseline.\nThen, we investigate four strategies for integrating PRMs, and find that\ncombining PRM as an online training signal (GRPO) with PRM-guided inference\n(e.g., best-of-N sampling) yields the best results. Empirically, on the BIRD\nbenchmark, Reward-SQL enables models supervised by a 7B PRM to achieve a 13.1%\nperformance gain across various guidance strategies. Notably, our GRPO-aligned\npolicy model based on Qwen2.5-Coder-7B-Instruct achieves 68.9% accuracy on the\nBIRD development set, outperforming all baseline methods under the same model\nsize. These results demonstrate the effectiveness of Reward-SQL in leveraging\nreward-based supervision for Text-to-SQL reasoning. Our code is publicly\navailable.", "AI": {"tldr": "Reward-SQL\u6846\u67b6\u901a\u8fc7\u7ed3\u5408Process Reward Models\uff08PRMs\uff09\u63d0\u5347Text-to-SQL\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u91c7\u7528\u2018\u51b7\u542f\u52a8\u540ePRM\u76d1\u7763\u2019\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\u3002\u5728BIRD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c7B PRM\u76d1\u7763\u7684\u6a21\u578b\u6027\u80fd\u63d0\u5347\u4e8613.1%\uff0c\u6700\u7ec8\u6a21\u578b\u5728\u5f00\u53d1\u96c6\u4e0a\u8fbe\u523068.9%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728Text-to-SQL\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46PRMs\u7684\u8bef\u7528\u53ef\u80fd\u4f1a\u5bfc\u81f4\u63a8\u7406\u8f68\u8ff9\u626d\u66f2\uff0c\u8fdb\u800c\u751f\u6210\u6b21\u4f18\u6216\u9519\u8bef\u7684SQL\u3002\u4e3a\u4e86\u6709\u6548\u5229\u7528PRMs\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\uff0c\u63d0\u51fa\u4e86Reward-SQL\u6846\u67b6\u3002", "method": "Reward-SQL\u91c7\u7528\u2018\u51b7\u542f\u52a8\u540ePRM\u76d1\u7763\u2019\u7b56\u7565\uff0c\u9996\u5148\u5229\u7528Chain-of-CTEs\u5206\u89e3SQL\u67e5\u8be2\u4e3a\u7ed3\u6784\u5316\u9010\u6b65\u63a8\u7406\u94fe\u4ee5\u5efa\u7acb\u57fa\u7ebf\uff0c\u968f\u540e\u63a2\u7d22\u56db\u79cdPRM\u6574\u5408\u7b56\u7565\uff0c\u6700\u7ec8\u53d1\u73b0GRPO\u4e0ePRM\u5f15\u5bfc\u63a8\u65ad\uff08\u5982best-of-N\u91c7\u6837\uff09\u7ec4\u5408\u6548\u679c\u6700\u4f73\u3002", "result": "\u5728BIRD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReward-SQL\u4f7f7B PRM\u76d1\u7763\u7684\u6a21\u578b\u6027\u80fd\u63d0\u5347\u4e8613.1%\uff0c\u91c7\u7528GRPO\u7b56\u7565\u7684Qwen2.5-Coder-7B-Instruct\u6a21\u578b\u5728\u5f00\u53d1\u96c6\u4e0a\u51c6\u786e\u7387\u8fbe\u523068.9%\uff0c\u4f18\u4e8e\u540c\u89c4\u6a21\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Reward-SQL\u8bc1\u660e\u4e86\u5956\u52b1\u76d1\u7763\u5728Text-to-SQL\u63a8\u7406\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u6574\u5408PRMs\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.05232", "pdf": "https://arxiv.org/pdf/2505.05232", "abs": "https://arxiv.org/abs/2505.05232", "authors": ["Mahmoud Amiri", "Thomas Bocklitz"], "title": "ChemRxivQuest: A Curated Chemistry Question-Answer Database Extracted from ChemRxiv Preprints", "categories": ["cs.AI"], "comment": null, "summary": "The rapid expansion of chemistry literature poses significant challenges for\nresearchers seeking to efficiently access domain-specific knowledge. To support\nadvancements in chemistry-focused natural language processing (NLP), we present\nChemRxivQuest, a curated dataset of 970 high-quality question-answer (QA) pairs\nderived from 155 ChemRxiv preprints across 17 subfields of chemistry. Each QA\npair is explicitly linked to its source text segment to ensure traceability and\ncontextual accuracy. ChemRxivQuest was constructed using an automated pipeline\nthat combines optical character recognition (OCR), GPT-4o-based QA generation,\nand a fuzzy matching technique for answer verification. The dataset emphasizes\nconceptual, mechanistic, applied, and experimental questions, enabling\napplications in retrieval-based QA systems, search engine development, and\nfine-tuning of domain-adapted large language models. We analyze the dataset's\nstructure, coverage, and limitations, and outline future directions for\nexpansion and expert validation. ChemRxivQuest provides a foundational resource\nfor chemistry NLP research, education, and tool development.", "AI": {"tldr": "ChemRxivQuest\u662f\u4e00\u4e2a\u7531970\u4e2a\u9ad8\u8d28\u91cf\u95ee\u7b54\u5bf9\u7ec4\u6210\u7684\u5316\u5b66\u9886\u57df\u6570\u636e\u96c6\uff0c\u6e90\u81ea155\u7bc7ChemRxiv\u9884\u5370\u672c\uff0c\u6db5\u76d617\u4e2a\u5b50\u9886\u57df\uff0c\u65e8\u5728\u652f\u6301\u5316\u5b66NLP\u7814\u7a76\u3002", "motivation": "\u5316\u5b66\u6587\u732e\u7684\u5feb\u901f\u6269\u5c55\u4f7f\u5f97\u7814\u7a76\u8005\u96be\u4ee5\u9ad8\u6548\u83b7\u53d6\u9886\u57df\u77e5\u8bc6\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u53ef\u8ffd\u6eaf\u7684\u95ee\u7b54\u6570\u636e\u96c6\u6765\u63a8\u52a8\u5316\u5b66NLP\u7684\u53d1\u5c55\u3002", "method": "\u4f7f\u7528\u81ea\u52a8\u5316\u6d41\u7a0b\uff08\u5305\u62ecOCR\u3001GPT-4o\u751f\u6210\u95ee\u7b54\u548c\u6a21\u7cca\u5339\u914d\u6280\u672f\uff09\u6784\u5efa\u6570\u636e\u96c6\uff0c\u786e\u4fdd\u95ee\u7b54\u5bf9\u7684\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u5173\u8054\u3002", "result": "\u6570\u636e\u96c6\u8986\u76d617\u4e2a\u5316\u5b66\u5b50\u9886\u57df\uff0c\u5f3a\u8c03\u6982\u5ff5\u3001\u673a\u7406\u3001\u5e94\u7528\u548c\u5b9e\u9a8c\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u68c0\u7d22\u5f0f\u95ee\u7b54\u7cfb\u7edf\u3001\u641c\u7d22\u5f15\u64ce\u5f00\u53d1\u548c\u9886\u57df\u9002\u914d\u5927\u6a21\u578b\u5fae\u8c03\u3002", "conclusion": "ChemRxivQuest\u4e3a\u5316\u5b66NLP\u7814\u7a76\u3001\u6559\u80b2\u548c\u5de5\u5177\u5f00\u53d1\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u672a\u6765\u6269\u5c55\u548c\u4e13\u5bb6\u9a8c\u8bc1\u7684\u65b9\u5411\u3002"}}
{"id": "2505.04898", "pdf": "https://arxiv.org/pdf/2505.04898", "abs": "https://arxiv.org/abs/2505.04898", "authors": ["Qiyang Han", "Masaaki Imaizumi"], "title": "Precise gradient descent training dynamics for finite-width multi-layer neural networks", "categories": ["cs.LG", "cs.AI", "math.OC", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "In this paper, we provide the first precise distributional characterization\nof gradient descent iterates for general multi-layer neural networks under the\ncanonical single-index regression model, in the `finite-width proportional\nregime' where the sample size and feature dimension grow proportionally while\nthe network width and depth remain bounded. Our non-asymptotic state evolution\ntheory captures Gaussian fluctuations in first-layer weights and concentration\nin deeper-layer weights, and remains valid for non-Gaussian features.\n  Our theory differs from existing neural tangent kernel (NTK), mean-field (MF)\ntheories and tensor program (TP) in several key aspects. First, our theory\noperates in the finite-width regime whereas these existing theories are\nfundamentally infinite-width. Second, our theory allows weights to evolve from\nindividual initializations beyond the lazy training regime, whereas NTK and MF\nare either frozen at or only weakly sensitive to initialization, and TP relies\non special initialization schemes. Third, our theory characterizes both\ntraining and generalization errors for general multi-layer neural networks\nbeyond the uniform convergence regime, whereas existing theories study\ngeneralization almost exclusively in two-layer settings.\n  As a statistical application, we show that vanilla gradient descent can be\naugmented to yield consistent estimates of the generalization error at each\niteration, which can be used to guide early stopping and hyperparameter tuning.\nAs a further theoretical implication, we show that despite model\nmisspecification, the model learned by gradient descent retains the structure\nof a single-index function with an effective signal determined by a linear\ncombination of the true signal and the initialization.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cbe\u786e\u63cf\u8ff0\u4e86\u5728\u6709\u9650\u5bbd\u5ea6\u6bd4\u4f8b\u673a\u5236\u4e0b\uff0c\u591a\u5c42\u795e\u7ecf\u7f51\u7edc\u68af\u5ea6\u4e0b\u964d\u8fed\u4ee3\u7684\u5206\u5e03\u7279\u6027\uff0c\u63d0\u51fa\u4e86\u975e\u6e10\u8fd1\u72b6\u6001\u6f14\u5316\u7406\u8bba\uff0c\u5e76\u4e0e\u73b0\u6709\u7406\u8bba\uff08\u5982NTK\u3001MF\u548cTP\uff09\u5728\u591a\u4e2a\u5173\u952e\u65b9\u9762\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u591a\u5c42\u795e\u7ecf\u7f51\u7edc\u5728\u6709\u9650\u5bbd\u5ea6\u6bd4\u4f8b\u673a\u5236\u4e0b\u7684\u68af\u5ea6\u4e0b\u964d\u884c\u4e3a\uff0c\u586b\u8865\u73b0\u6709\u7406\u8bba\uff08\u5982NTK\u3001MF\u548cTP\uff09\u5728\u65e0\u9650\u5bbd\u5ea6\u548c\u521d\u59cb\u5316\u654f\u611f\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u6269\u5c55\u5bf9\u8bad\u7ec3\u548c\u6cdb\u5316\u8bef\u5dee\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u975e\u6e10\u8fd1\u72b6\u6001\u6f14\u5316\u7406\u8bba\uff0c\u5206\u6790\u68af\u5ea6\u4e0b\u964d\u8fed\u4ee3\u4e2d\u7b2c\u4e00\u5c42\u6743\u91cd\u7684Gaussian\u6ce2\u52a8\u548c\u6df1\u5c42\u6743\u91cd\u7684\u96c6\u4e2d\u6027\uff0c\u9002\u7528\u4e8e\u975eGaussian\u7279\u5f81\u3002", "result": "\u7406\u8bba\u9a8c\u8bc1\u4e86\u68af\u5ea6\u4e0b\u964d\u7684\u6743\u91cd\u6f14\u5316\u884c\u4e3a\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5b66\u4e60\u4fdd\u7559\u5355\u6307\u6570\u51fd\u6570\u7ed3\u6784\u7684\u7279\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6cdb\u5316\u8bef\u5dee\u7684\u5b9e\u65f6\u4f30\u8ba1\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7406\u8bba\u4e3a\u6709\u9650\u5bbd\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u68af\u5ea6\u4e0b\u964d\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u6846\u67b6\uff0c\u5e76\u5728\u7edf\u8ba1\u5e94\u7528\u548c\u6a21\u578b\u7ed3\u6784\u7406\u89e3\u4e0a\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2505.04673", "pdf": "https://arxiv.org/pdf/2505.04673", "abs": "https://arxiv.org/abs/2505.04673", "authors": ["Madhur Jindal", "Saurabh Deshpande"], "title": "REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages (8 main), to be published in IJCAI 2025", "summary": "Vision Large Language Models (VLLMs) represent a significant advancement in\nartificial intelligence by integrating image-processing capabilities with\ntextual understanding, thereby enhancing user interactions and expanding\napplication domains. However, their increased complexity introduces novel\nsafety and ethical challenges, particularly in multi-modal and multi-turn\nconversations. Traditional safety evaluation frameworks, designed for\ntext-based, single-turn interactions, are inadequate for addressing these\ncomplexities. To bridge this gap, we introduce the REVEAL (Responsible\nEvaluation of Vision-Enabled AI LLMs) Framework, a scalable and automated\npipeline for evaluating image-input harms in VLLMs. REVEAL includes automated\nimage mining, synthetic adversarial data generation, multi-turn conversational\nexpansion using crescendo attack strategies, and comprehensive harm assessment\nthrough evaluators like GPT-4o.\n  We extensively evaluated five state-of-the-art VLLMs, GPT-4o, Llama-3.2,\nQwen2-VL, Phi3.5V, and Pixtral, across three important harm categories: sexual\nharm, violence, and misinformation. Our findings reveal that multi-turn\ninteractions result in significantly higher defect rates compared to\nsingle-turn evaluations, highlighting deeper vulnerabilities in VLLMs. Notably,\nGPT-4o demonstrated the most balanced performance as measured by our\nSafety-Usability Index (SUI) followed closely by Pixtral. Additionally,\nmisinformation emerged as a critical area requiring enhanced contextual\ndefenses. Llama-3.2 exhibited the highest MT defect rate ($16.55 \\%$) while\nQwen2-VL showed the highest MT refusal rate ($19.1 \\%$).", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86REVEAL\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\uff08VLLMs\uff09\u7684\u5b89\u5168\u6027\u548c\u4f26\u7406\u6311\u6218\uff0c\u53d1\u73b0\u591a\u8f6e\u4ea4\u4e92\u7684\u7f3a\u9677\u7387\u663e\u8457\u9ad8\u4e8e\u5355\u8f6e\u4ea4\u4e92\uff0cGPT-4o\u548cPixtral\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7531\u4e8e\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u4f20\u7edf\u8bc4\u4f30\u6846\u67b6\u65e0\u6cd5\u5e94\u5bf9\u591a\u6a21\u6001\u548c\u591a\u8f6e\u5bf9\u8bdd\u7684\u5b89\u5168\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faREVEAL\u6846\u67b6\uff0c\u5305\u542b\u81ea\u52a8\u5316\u56fe\u50cf\u6316\u6398\u3001\u5408\u6210\u5bf9\u6297\u6570\u636e\u751f\u6210\u3001\u591a\u8f6e\u5bf9\u8bdd\u6269\u5c55\uff08\u4f7f\u7528crescendo\u653b\u51fb\u7b56\u7565\uff09\u4ee5\u53ca\u901a\u8fc7GPT-4o\u7b49\u8bc4\u4f30\u5668\u8fdb\u884c\u7efc\u5408\u5371\u5bb3\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u4e86\u4e94\u79cdVLLMs\uff0c\u53d1\u73b0\u591a\u8f6e\u4ea4\u4e92\u7f3a\u9677\u7387\u663e\u8457\u66f4\u9ad8\uff1bGPT-4o\u5728\u5b89\u5168-\u53ef\u7528\u6027\u6307\u6570\uff08SUI\uff09\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u865a\u5047\u4fe1\u606f\u662f\u4e3b\u8981\u95ee\u9898\u9886\u57df\u3002", "conclusion": "REVEAL\u6846\u67b6\u80fd\u6709\u6548\u8bc4\u4f30VLLMs\u7684\u5b89\u5168\u6027\u548c\u4f26\u7406\u98ce\u9669\uff0c\u591a\u8f6e\u4ea4\u4e92\u66b4\u9732\u4e86\u6a21\u578b\u7684\u6df1\u5c42\u8106\u5f31\u6027\uff0c\u9700\u52a0\u5f3a\u4e0a\u4e0b\u6587\u9632\u5fa1\u3002"}}
{"id": "2505.05235", "pdf": "https://arxiv.org/pdf/2505.05235", "abs": "https://arxiv.org/abs/2505.05235", "authors": ["Luca Marzari", "Isabella Mastroeni", "Alessandro Farinelli"], "title": "Advancing Neural Network Verification through Hierarchical Safety Abstract Interpretation", "categories": ["cs.AI"], "comment": null, "summary": "Traditional methods for formal verification (FV) of deep neural networks\n(DNNs) are constrained by a binary encoding of safety properties, where a model\nis classified as either safe or unsafe (robust or not robust). This binary\nencoding fails to capture the nuanced safety levels within a model, often\nresulting in either overly restrictive or too permissive requirements. In this\npaper, we introduce a novel problem formulation called Abstract\nDNN-Verification, which verifies a hierarchical structure of unsafe outputs,\nproviding a more granular analysis of the safety aspect for a given DNN.\nCrucially, by leveraging abstract interpretation and reasoning about output\nreachable sets, our approach enables assessing multiple safety levels during\nthe FV process, requiring the same (in the worst case) or even potentially less\ncomputational effort than the traditional binary verification approach.\nSpecifically, we demonstrate how this formulation allows rank adversarial\ninputs according to their abstract safety level violation, offering a more\ndetailed evaluation of the model's safety and robustness. Our contributions\ninclude a theoretical exploration of the relationship between our novel\nabstract safety formulation and existing approaches that employ abstract\ninterpretation for robustness verification, complexity analysis of the novel\nproblem introduced, and an empirical evaluation considering both a complex deep\nreinforcement learning task (based on Habitat 3.0) and standard\nDNN-Verification benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Abstract DNN-Verification \u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u7ed3\u6784\u5206\u6790 DNN \u7684\u5b89\u5168\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u4e8c\u5143\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u66f4\u7ec6\u7c92\u5ea6\u7684\u5b89\u5168\u6027\u8bc4\u4f30\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u76f8\u5f53\u6216\u66f4\u9ad8\u3002", "motivation": "\u4f20\u7edf\u7684 DNN \u5f62\u5f0f\u5316\u9a8c\u8bc1\u65b9\u6cd5\u4ec5\u80fd\u8fdb\u884c\u4e8c\u5143\u5224\u65ad\uff08\u5b89\u5168\u6216\u4e0d\u5b89\u5168\uff09\uff0c\u65e0\u6cd5\u6355\u6349\u6a21\u578b\u5185\u90e8\u7684\u5b89\u5168\u7b49\u7ea7\u5dee\u5f02\uff0c\u5bfc\u81f4\u9a8c\u8bc1\u7ed3\u679c\u53ef\u80fd\u8fc7\u4e8e\u4e25\u82db\u6216\u5bbd\u677e\u3002", "method": "\u8bba\u6587\u91c7\u7528\u62bd\u8c61\u89e3\u91ca\u548c\u8f93\u51fa\u53ef\u8fbe\u96c6\u5206\u6790\uff0c\u6784\u5efa\u4e86\u5206\u5c42\u4e0d\u5b89\u5168\u8f93\u51fa\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u652f\u6301\u591a\u7ea7\u5b89\u5168\u6027\u8bc4\u4f30\uff0c\u5e76\u5206\u6790\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728 Habitat 3.0 \u548c\u6807\u51c6 DNN \u9a8c\u8bc1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u6709\u6548\uff0c\u80fd\u591f\u6839\u636e\u62bd\u8c61\u5b89\u5168\u7ea7\u522b\u5bf9\u5bf9\u6297\u8f93\u5165\u6392\u5e8f\uff0c\u63d0\u4f9b\u66f4\u7ec6\u81f4\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u3002", "conclusion": "Abstract DNN-Verification \u4e3a DNN \u5b89\u5168\u6027\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u7406\u8bba\u4e0a\u517c\u5bb9\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u8df5\u4e0a\u63d0\u5347\u4e86\u9a8c\u8bc1\u7684\u7cbe\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2505.04907", "pdf": "https://arxiv.org/pdf/2505.04907", "abs": "https://arxiv.org/abs/2505.04907", "authors": ["Soham Khisa", "Avijoy Chakma"], "title": "VaCDA: Variational Contrastive Alignment-based Scalable Human Activity Recognition", "categories": ["cs.LG"], "comment": null, "summary": "Technological advancements have led to the rise of wearable devices with\nsensors that continuously monitor user activities, generating vast amounts of\nunlabeled data. This data is challenging to interpret, and manual annotation is\nlabor-intensive and error-prone. Additionally, data distribution is often\nheterogeneous due to device placement, type, and user behavior variations. As a\nresult, traditional transfer learning methods perform suboptimally, making it\ndifficult to recognize daily activities. To address these challenges, we use a\nvariational autoencoder (VAE) to learn a shared, low-dimensional latent space\nfrom available sensor data. This space generalizes data across diverse sensors,\nmitigating heterogeneity and aiding robust adaptation to the target domain. We\nintegrate contrastive learning to enhance feature representation by aligning\ninstances of the same class across domains while separating different classes.\nWe propose Variational Contrastive Domain Adaptation (VaCDA), a multi-source\ndomain adaptation framework combining VAEs and contrastive learning to improve\nfeature representation and reduce heterogeneity between source and target\ndomains. We evaluate VaCDA on multiple publicly available datasets across three\nheterogeneity scenarios: cross-person, cross-position, and cross-device. VaCDA\noutperforms the baselines in cross-position and cross-device scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u591a\u6e90\u57df\u81ea\u9002\u5e94\u6846\u67b6VaCDA\uff0c\u7528\u4e8e\u89e3\u51b3\u53ef\u7a7f\u6234\u8bbe\u5907\u4f20\u611f\u5668\u6570\u636e\u4e2d\u7684\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u8de8\u57df\u6d3b\u52a8\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u53ef\u7a7f\u6234\u8bbe\u5907\u4ea7\u751f\u7684\u5927\u91cf\u672a\u6807\u8bb0\u6570\u636e\u56e0\u8bbe\u5907\u548c\u7528\u6237\u7684\u5dee\u5f02\u5448\u73b0\u9ad8\u5ea6\u5f02\u8d28\u6027\uff0c\u4f20\u7edf\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u6709\u6548\u5904\u7406\u548c\u9002\u5e94\u5f02\u8d28\u6027\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faVaCDA\u6846\u67b6\uff0c\u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u5171\u4eab\u7684\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\u4ee5\u51cf\u5c11\u5f02\u8d28\u6027\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff0c\u5b9e\u73b0\u8de8\u57df\u5bf9\u9f50\u3002", "result": "\u5728\u8de8\u4f4d\u7f6e\u548c\u8de8\u8bbe\u5907\u573a\u666f\u4e0b\uff0cVaCDA\u7684\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "VaCDA\u80fd\u6709\u6548\u7f13\u89e3\u4f20\u611f\u5668\u6570\u636e\u7684\u5f02\u8d28\u6027\uff0c\u63d0\u5347\u8de8\u57df\u6d3b\u52a8\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2505.04678", "pdf": "https://arxiv.org/pdf/2505.04678", "abs": "https://arxiv.org/abs/2505.04678", "authors": ["Shahad Elshehaby", "Alavikunhu Panthakkan", "Hussain Al-Ahmad", "Mina Al-Saad"], "title": "Advanced Deep Learning Approaches for Automated Recognition of Cuneiform Symbols", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents a thoroughly automated method for identifying and\ninterpreting cuneiform characters via advanced deep-learning algorithms. Five\ndistinct deep-learning models were trained on a comprehensive dataset of\ncuneiform characters and evaluated according to critical performance metrics,\nincluding accuracy and precision. Two models demonstrated outstanding\nperformance and were subsequently assessed using cuneiform symbols from the\nHammurabi law acquisition, notably Hammurabi Law 1. Each model effectively\nrecognized the relevant Akkadian meanings of the symbols and delivered precise\nEnglish translations. Future work will investigate ensemble and stacking\napproaches to optimize performance, utilizing hybrid architectures to improve\ndetection accuracy and reliability. This research explores the linguistic\nrelationships between Akkadian, an ancient Mesopotamian language, and Arabic,\nemphasizing their historical and cultural linkages. This study demonstrates the\ncapability of deep learning to decipher ancient scripts by merging\ncomputational linguistics with archaeology, therefore providing significant\ninsights for the comprehension and conservation of human history.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u8bc6\u522b\u548c\u89e3\u91ca\u6954\u5f62\u6587\u5b57\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e94\u79cd\u6a21\u578b\u8bad\u7ec3\u5e76\u8bc4\u4f30\uff0c\u5176\u4e2d\u4e24\u79cd\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u8bc6\u522b\u4e86\u6c49\u8c1f\u62c9\u6bd4\u6cd5\u5178\u4e2d\u7684\u7b26\u53f7\u5e76\u63d0\u4f9b\u4e86\u7cbe\u786e\u7ffb\u8bd1\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u63a2\u7d22\u96c6\u6210\u65b9\u6cd5\u4ee5\u4f18\u5316\u6027\u80fd\uff0c\u7814\u7a76\u963f\u5361\u5fb7\u8bed\u4e0e\u963f\u62c9\u4f2f\u8bed\u7684\u8bed\u8a00\u5b66\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8ba1\u7b97\u8bed\u8a00\u5b66\u548c\u8003\u53e4\u5b66\u7684\u7ed3\u5408\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u89e3\u5bc6\u53e4\u4ee3\u6954\u5f62\u6587\u5b57\uff0c\u4ee5\u589e\u8fdb\u5bf9\u4eba\u7c7b\u5386\u53f2\u548c\u6587\u5316\u9057\u4ea7\u7684\u7406\u89e3\u4e0e\u4fdd\u62a4\u3002", "method": "\u8bad\u7ec3\u4e86\u4e94\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4f7f\u7528\u5168\u9762\u7684\u6954\u5f62\u6587\u5b57\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u51c6\u786e\u7387\u548c\u7cbe\u786e\u5ea6\u7b49\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002\u4f18\u5f02\u6a21\u578b\u8fdb\u4e00\u6b65\u5e94\u7528\u4e8e\u6c49\u8c1f\u62c9\u6bd4\u6cd5\u5178\u7684\u7b26\u53f7\u8bc6\u522b\u4e0e\u7ffb\u8bd1\u3002", "result": "\u4e24\u79cd\u6a21\u578b\u8868\u73b0\u7a81\u51fa\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u963f\u5361\u5fb7\u8bed\u7b26\u53f7\u7684\u8bed\u4e49\u5e76\u63d0\u4f9b\u7cbe\u786e\u7684\u82f1\u6587\u7ffb\u8bd1\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6df1\u5ea6\u5b66\u4e60\u53ef\u7528\u4e8e\u53e4\u4ee3\u6587\u5b57\u7684\u89e3\u5bc6\uff0c\u672a\u6765\u5c06\u901a\u8fc7\u96c6\u6210\u65b9\u6cd5\u4f18\u5316\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u63a2\u7d22\u963f\u5361\u5fb7\u8bed\u4e0e\u963f\u62c9\u4f2f\u8bed\u7684\u8bed\u7cfb\u5173\u8054\u3002"}}
{"id": "2505.05396", "pdf": "https://arxiv.org/pdf/2505.05396", "abs": "https://arxiv.org/abs/2505.05396", "authors": ["Stefanos Gkikas"], "title": "A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "From the original abstract:\n  This thesis initially aims to study the pain assessment process from a\nclinical-theoretical perspective while exploring and examining existing\nautomatic approaches. Building on this foundation, the primary objective of\nthis Ph.D. project is to develop innovative computational methods for automatic\npain assessment that achieve high performance and are applicable in real\nclinical settings. A primary goal is to thoroughly investigate and assess\nsignificant factors, including demographic elements that impact pain\nperception, as recognized in pain research, through a computational standpoint.\nWithin the limits of the available data in this research area, our goal was to\ndesign, develop, propose, and offer automatic pain assessment pipelines for\nunimodal and multimodal configurations that are applicable to the specific\nrequirements of different scenarios. The studies published in this Ph.D. thesis\nshowcased the effectiveness of the proposed methods, achieving state-of-the-art\nresults. Additionally, they paved the way for exploring new approaches in\nartificial intelligence, foundation models, and generative artificial\nintelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u8ba1\u7b97\u89c6\u89d2\u7814\u7a76\u75bc\u75db\u8bc4\u4f30\uff0c\u5f00\u53d1\u9ad8\u6027\u80fd\u7684\u81ea\u52a8\u75bc\u75db\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u5f71\u54cd\u75bc\u75db\u611f\u77e5\u7684\u56e0\u7d20\uff0c\u6700\u7ec8\u63d0\u51fa\u9002\u7528\u4e8e\u4e34\u5e8a\u7684\u591a\u6a21\u6001\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4ece\u4e34\u5e8a\u548c\u7406\u8bba\u89d2\u5ea6\u7814\u7a76\u75bc\u75db\u8bc4\u4f30\u8fc7\u7a0b\uff0c\u63a2\u7d22\u73b0\u6709\u81ea\u52a8\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u76ee\u6807\u662f\u5f00\u53d1\u9002\u7528\u4e8e\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u7684\u9ad8\u6027\u80fd\u81ea\u52a8\u75bc\u75db\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u89c6\u89d2\u5206\u6790\u75bc\u75db\u611f\u77e5\u7684\u5f71\u54cd\u56e0\u7d20\uff0c\u8bbe\u8ba1\u5e76\u5f00\u53d1\u9002\u7528\u4e8e\u4e0d\u540c\u573a\u666f\u7684\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u81ea\u52a8\u75bc\u75db\u8bc4\u4f30\u6d41\u7a0b\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7814\u7a76\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u5e76\u4e3a\u4eba\u5de5\u667a\u80fd\u3001\u57fa\u7840\u6a21\u578b\u548c\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u65b0\u65b9\u6cd5\u63a2\u7d22\u94fa\u5e73\u4e86\u9053\u8def\u3002", "conclusion": "\u8be5\u535a\u58eb\u9879\u76ee\u6210\u529f\u5f00\u53d1\u4e86\u521b\u65b0\u7684\u81ea\u52a8\u75bc\u75db\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63a8\u52a8\u4e86\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.04918", "pdf": "https://arxiv.org/pdf/2505.04918", "abs": "https://arxiv.org/abs/2505.04918", "authors": ["Jiaqi Zheng", "Qing Ling", "Yerong Feng"], "title": "Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "International Joint Conferences on Artificial Intelligence (IJCAI\n  2025)", "summary": "Although deep learning models have demonstrated remarkable potential in\nweather prediction, most of them overlook either the \\textbf{physics} of the\nunderlying weather evolution or the \\textbf{topology} of the Earth's surface.\nIn light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted\nAnd Topology-informed deep learning model for weather prediction. PASSAT\nattributes the weather evolution to two key factors: (i) the advection process\nthat can be characterized by the advection equation and the Navier-Stokes\nequation; (ii) the Earth-atmosphere interaction that is difficult to both model\nand calculate. PASSAT also takes the topology of the Earth's surface into\nconsideration, other than simply treating it as a plane. With these\nconsiderations, PASSAT numerically solves the advection equation and the\nNavier-Stokes equation on the spherical manifold, utilizes a spherical graph\nneural network to capture the Earth-atmosphere interaction, and generates the\ninitial velocity fields that are critical to solving the advection equation\nfrom the same spherical graph neural network. In the $5.625^\\circ$-resolution\nERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based\nweather prediction models and the operational numerical weather prediction\nmodel IFS T42. Code and checkpoint are available at\nhttps://github.com/Yumenomae/PASSAT_5p625.", "AI": {"tldr": "PASSAT\u7ed3\u5408\u7269\u7406\u89c4\u5f8b\u548c\u62d3\u6251\u7ed3\u6784\uff0c\u6539\u8fdb\u5929\u6c14\u9884\u6d4b\u6a21\u578b\uff0c\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u548c\u6570\u503c\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u5929\u6c14\u9884\u6d4b\u6a21\u578b\u5e38\u5ffd\u7565\u7269\u7406\u89c4\u5f8b\u6216\u5730\u7403\u8868\u9762\u62d3\u6251\u7ed3\u6784\uff0cPASSAT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "PASSAT\u901a\u8fc7\u6570\u503c\u6c42\u89e3\u5e73\u6d41\u65b9\u7a0b\u548cNavier-Stokes\u65b9\u7a0b\uff0c\u5229\u7528\u7403\u5f62\u56fe\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u5730\u7403-\u5927\u6c14\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u57285.625\u00b0\u5206\u8fa8\u7387\u7684ERA5\u6570\u636e\u96c6\u4e0a\uff0cPASSAT\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u548c\u6570\u503c\u5929\u6c14\u9884\u6d4b\u6a21\u578b\u3002", "conclusion": "PASSAT\u6210\u529f\u6574\u5408\u7269\u7406\u548c\u62d3\u6251\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u5929\u6c14\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2505.04723", "pdf": "https://arxiv.org/pdf/2505.04723", "abs": "https://arxiv.org/abs/2505.04723", "authors": ["Jingyang Deng", "Ran Chen", "Jo-Ku Cheng", "Jinwen Ma"], "title": "SOAEsV2-7B/72B: Full-Pipeline Optimization for State-Owned Enterprise LLMs via Continual Pre-Training, Domain-Progressive SFT and Distillation-Enhanced Speculative Decoding", "categories": ["cs.CL"], "comment": null, "summary": "This study addresses key challenges in developing domain-specific large\nlanguage models (LLMs) for Chinese state-owned assets and enterprises (SOAEs),\nwhere current approaches face three limitations: 1) constrained model capacity\nthat limits knowledge integration and cross-task adaptability; 2) excessive\nreliance on domain-specific supervised fine-tuning (SFT) data, which neglects\nthe broader applicability of general language patterns; and 3) inefficient\ninference acceleration for large models processing long contexts. In this work,\nwe propose SOAEsV2-7B/72B, a specialized LLM series developed via a three-phase\nframework: 1) continual pre-training integrates domain knowledge while\nretaining base capabilities; 2) domain-progressive SFT employs curriculum-based\nlearning strategy, transitioning from weakly relevant conversational data to\nexpert-annotated SOAEs datasets to optimize domain-specific tasks; 3)\ndistillation-enhanced speculative decoding accelerates inference via logit\ndistillation between 72B target and 7B draft models, achieving\n1.39-1.52$\\times$ speedup without quality loss. Experimental results\ndemonstrate that our domain-specific pre-training phase maintains 99.8% of\noriginal general language capabilities while significantly improving domain\nperformance, resulting in a 1.08$\\times$ improvement in Rouge-1 score and a\n1.17$\\times$ enhancement in BLEU-4 score. Ablation studies further show that\ndomain-progressive SFT outperforms single-stage training, achieving\n1.02$\\times$ improvement in Rouge-1 and 1.06$\\times$ in BLEU-4. Our work\nintroduces a comprehensive, full-pipeline approach for optimizing SOAEs LLMs,\nbridging the gap between general language capabilities and domain-specific\nexpertise.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86SOAEsV2-7B/72B\u7cfb\u5217\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6846\u67b6\u89e3\u51b3\u4e2d\u6587\u56fd\u6709\u8d44\u4ea7\u4e1a\u52a1\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e09\u5927\u6311\u6218\uff1a\u6a21\u578b\u5bb9\u91cf\u53d7\u9650\u3001\u8fc7\u5ea6\u4f9d\u8d56\u9886\u57df\u76d1\u7763\u6570\u636e\u4ee5\u53ca\u63a8\u7406\u6548\u7387\u4f4e\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5728\u4fdd\u6301\u901a\u7528\u8bed\u8a00\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u9886\u57df\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u4e2d\u6587\u56fd\u6709\u8d44\u4ea7\u4e1a\u52a1\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u6574\u5408\u3001\u8de8\u4efb\u52a1\u9002\u5e94\u6027\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6846\u67b6\uff1a\u6301\u7eed\u9884\u8bad\u7ec3\u6574\u5408\u9886\u57df\u77e5\u8bc6\uff0c\u9886\u57df\u6e10\u8fdb\u5f0f\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4f18\u5316\u4efb\u52a1\u8868\u73b0\uff0c\u4ee5\u53ca\u84b8\u998f\u589e\u5f3a\u7684\u63a8\u6d4b\u89e3\u7801\u52a0\u901f\u63a8\u7406\u3002", "result": "\u6a21\u578b\u5728\u4fdd\u630199.8%\u901a\u7528\u8bed\u8a00\u80fd\u529b\u7684\u540c\u65f6\uff0c\u9886\u57df\u4efb\u52a1\u8868\u73b0\u663e\u8457\u63d0\u5347\uff08Rouge-1\u548cBLEU-4\u5206\u522b\u63d0\u53471.08\u500d\u548c1.17\u500d\uff09\uff0c\u4e14\u63a8\u7406\u901f\u5ea6\u63d0\u53471.39-1.52\u500d\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f18\u5316\u56fd\u6709\u8d44\u4ea7\u4e1a\u52a1\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u9762\u7684\u5168\u6d41\u7a0b\u65b9\u6cd5\uff0c\u5e73\u8861\u4e86\u901a\u7528\u8bed\u8a00\u80fd\u529b\u4e0e\u9886\u57df\u4e13\u4e1a\u6027\u3002"}}
{"id": "2505.05440", "pdf": "https://arxiv.org/pdf/2505.05440", "abs": "https://arxiv.org/abs/2505.05440", "authors": ["Biao Yi", "Xavier Hu", "Yurun Chen", "Shengyu Zhang", "Hongxia Yang", "Fan Wu", "Fei Wu"], "title": "EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework for Mobile Automation", "categories": ["cs.AI"], "comment": null, "summary": "Cloud-based mobile agents powered by (multimodal) large language models\n((M)LLMs) offer strong reasoning abilities but suffer from high latency and\ncost. While fine-tuned (M)SLMs enable edge deployment, they often lose general\ncapabilities and struggle with complex tasks. To address this, we propose\nEcoAgent, an Edge-Cloud cOllaborative multi-agent framework for mobile\nautomation. EcoAgent features a closed-loop collaboration among a cloud-based\nPlanning Agent and two edge-based agents: the Execution Agent for action\nexecution and the Observation Agent for verifying outcomes. The Observation\nAgent uses a Pre-Understanding Module to compress screen images into concise\ntext, reducing token usage. In case of failure, the Planning Agent retrieves\nscreen history and replans via a Reflection Module. Experiments on AndroidWorld\nshow that EcoAgent maintains high task success rates while significantly\nreducing MLLM token consumption, enabling efficient and practical mobile\nautomation.", "AI": {"tldr": "EcoAgent\u7ed3\u5408\u8fb9\u7f18\u4e0e\u4e91\u8ba1\u7b97\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u4f5c\u6846\u67b6\u964d\u4f4e\u5ef6\u8fdf\u548c\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u4e91\u7684\u79fb\u52a8\u4ee3\u7406\u5728\u5ef6\u8fdf\u548c\u6210\u672c\u4e0a\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u4ec5\u8fb9\u7f18\u90e8\u7f72\u65f6\u901a\u7528\u80fd\u529b\u548c\u590d\u6742\u4efb\u52a1\u5904\u7406\u80fd\u529b\u7684\u4e0b\u964d\u3002", "method": "\u63d0\u51faEcoAgent\u6846\u67b6\uff0c\u5305\u542b\u4e91\u7aef\u89c4\u5212\u4ee3\u7406\u548c\u4e24\u4e2a\u8fb9\u7f18\u4ee3\u7406\uff08\u6267\u884c\u4ee3\u7406\u4e0e\u89c2\u5bdf\u4ee3\u7406\uff09\uff0c\u901a\u8fc7\u56fe\u50cf\u538b\u7f29\u548c\u53cd\u601d\u6a21\u5757\u4f18\u5316\u534f\u4f5c\u3002", "result": "\u5728AndroidWorld\u5b9e\u9a8c\u4e2d\uff0cEcoAgent\u663e\u8457\u51cf\u5c11\u4e86MLLM\u7684token\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "EcoAgent\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u79fb\u52a8\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2505.04931", "pdf": "https://arxiv.org/pdf/2505.04931", "abs": "https://arxiv.org/abs/2505.04931", "authors": ["Yonghong Li", "Xiuzhuang Zhou"], "title": "Fair Uncertainty Quantification for Depression Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Trustworthy depression prediction based on deep learning, incorporating both\npredictive reliability and algorithmic fairness across diverse demographic\ngroups, is crucial for clinical application. Recently, achieving reliable\ndepression predictions through uncertainty quantification has attracted\nincreasing attention. However, few studies have focused on the fairness of\nuncertainty quantification (UQ) in depression prediction. In this work, we\ninvestigate the algorithmic fairness of UQ, namely Equal Opportunity Coverage\n(EOC) fairness, and propose Fair Uncertainty Quantification (FUQ) for\ndepression prediction. FUQ pursues reliable and fair depression predictions\nthrough group-based analysis. Specifically, we first group all the participants\nby different sensitive attributes and leverage conformal prediction to quantify\nuncertainty within each demographic group, which provides a theoretically\nguaranteed and valid way to quantify uncertainty for depression prediction and\nfacilitates the investigation of fairness across different demographic groups.\nFurthermore, we propose a fairness-aware optimization strategy that formulates\nfairness as a constrained optimization problem under EOC constraints. This\nenables the model to preserve predictive reliability while adapting to the\nheterogeneous uncertainty levels across demographic groups, thereby achieving\noptimal fairness. Through extensive evaluations on several visual and audio\ndepression datasets, our approach demonstrates its effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86Fair Uncertainty Quantification\uff08FUQ\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6291\u90c1\u9884\u6d4b\u4e2d\u5b9e\u73b0\u53ef\u9760\u4e14\u516c\u5e73\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u901a\u8fc7\u5206\u7ec4\u5206\u6790\u548c\u516c\u5e73\u611f\u77e5\u4f18\u5316\u7b56\u7565\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u4e2d\u540c\u65f6\u4fdd\u8bc1\u9884\u6d4b\u7684\u53ef\u9760\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6291\u90c1\u9884\u6d4b\u7814\u7a76\u867d\u5173\u6ce8\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u516c\u5e73\u6027\u7684\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3UQ\u5728\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u786e\u4fdd\u5176\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u654f\u611f\u5c5e\u6027\u7684\u5206\u7ec4\u5206\u6790\uff0c\u5229\u7528\u5171\u5f62\u9884\u6d4b\u91cf\u5316\u6bcf\u7ec4\u5185\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7EOC\u7ea6\u675f\u4e0b\u7684\u4f18\u5316\u95ee\u9898\u5b9e\u73b0\u516c\u5e73\u6027\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u89c9\u548c\u97f3\u9891\u6291\u90c1\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86FUQ\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5e73\u8861\u9884\u6d4b\u53ef\u9760\u6027\u4e0e\u516c\u5e73\u6027\u3002", "conclusion": "FUQ\u4e3a\u6291\u90c1\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u6709\u4fdd\u969c\u7684\u516c\u5e73UQ\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u7684\u53ef\u9760\u6027\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.04785", "pdf": "https://arxiv.org/pdf/2505.04785", "abs": "https://arxiv.org/abs/2505.04785", "authors": ["Shuai Gong", "Tiange Zhou"], "title": "Flower Across Time and Media: Sentiment Analysis of Tang Song Poetry and Visual Correspondence", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 9 figures", "summary": "The Tang (618 to 907) and Song (960 to 1279) dynasties witnessed an\nextraordinary flourishing of Chinese cultural expression, where floral motifs\nserved as a dynamic medium for both poetic sentiment and artistic design. While\nprevious scholarship has examined these domains independently, the systematic\ncorrelation between evolving literary emotions and visual culture remains\nunderexplored. This study addresses that gap by employing BERT-based sentiment\nanalysis to quantify emotional patterns in floral imagery across Tang Song\npoetry, then validating these patterns against contemporaneous developments in\ndecorative arts.Our approach builds upon recent advances in computational\nhumanities while remaining grounded in traditional sinological methods. By\napplying a fine tuned BERT model to analyze peony and plum blossom imagery in\nclassical poetry, we detect measurable shifts in emotional connotations between\nthe Tang and Song periods. These textual patterns are then cross berenced with\nvisual evidence from textiles, ceramics, and other material culture, revealing\npreviously unrecognized synergies between literary expression and artistic\nrepresentation.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7BERT\u60c5\u611f\u5206\u6790\u91cf\u5316\u5510\u5b8b\u8bd7\u8bcd\u4e2d\u82b1\u5349\u610f\u8c61\u7684\u60c5\u611f\u53d8\u5316\uff0c\u5e76\u4e0e\u540c\u671f\u88c5\u9970\u827a\u672f\u8fdb\u884c\u5bf9\u6bd4\uff0c\u63ed\u793a\u4e86\u6587\u5b66\u60c5\u611f\u4e0e\u89c6\u89c9\u6587\u5316\u4e4b\u95f4\u7684\u7cfb\u7edf\u6027\u5173\u8054\u3002", "motivation": "\u586b\u8865\u6587\u5b66\u60c5\u611f\u4e0e\u89c6\u89c9\u6587\u5316\u5173\u8054\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u7ed3\u5408\u4f20\u7edf\u6c49\u5b66\u65b9\u6cd5\u4e0e\u8ba1\u7b97\u4eba\u6587\u6280\u672f\u3002", "method": "\u4f7f\u7528\u5fae\u8c03BERT\u6a21\u578b\u5206\u6790\u5510\u5b8b\u8bd7\u8bcd\u4e2d\u7684\u7261\u4e39\u4e0e\u6885\u82b1\u610f\u8c61\uff0c\u5e76\u5c06\u5176\u60c5\u611f\u6a21\u5f0f\u4e0e\u540c\u671f\u7269\u8d28\u6587\u5316\uff08\u5982\u7eba\u7ec7\u54c1\u3001\u9676\u74f7\uff09\u4e2d\u7684\u89c6\u89c9\u8bc1\u636e\u8fdb\u884c\u4ea4\u53c9\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0\u5510\u5b8b\u65f6\u671f\u82b1\u5349\u610f\u8c61\u7684\u60c5\u611f\u5185\u6db5\u5b58\u5728\u53ef\u6d4b\u91cf\u7684\u53d8\u5316\uff0c\u5e76\u63ed\u793a\u4e86\u6587\u5b66\u8868\u8fbe\u4e0e\u827a\u672f\u8868\u73b0\u4e4b\u95f4\u672a\u88ab\u8ba4\u8bc6\u5230\u7684\u534f\u540c\u5173\u7cfb\u3002", "conclusion": "\u901a\u8fc7\u8ba1\u7b97\u6280\u672f\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7684\u7ed3\u5408\uff0c\u672c\u7814\u7a76\u4e3a\u5510\u5b8b\u6587\u5316\u8868\u8fbe\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u8bc1\u660e\u4e86\u60c5\u611f\u6a21\u5f0f\u5728\u6587\u5b66\u4e0e\u827a\u672f\u4e2d\u7684\u540c\u6b65\u6f14\u53d8\u3002"}}
{"id": "2505.05453", "pdf": "https://arxiv.org/pdf/2505.05453", "abs": "https://arxiv.org/abs/2505.05453", "authors": ["Nataliia Klievtsova", "Timotheus Kampik", "Juergen Mangler", "Stefanie Rinderle-Ma"], "title": "Conversational Process Model Redesign", "categories": ["cs.AI"], "comment": null, "summary": "With the recent success of large language models (LLMs), the idea of\nAI-augmented Business Process Management systems is becoming more feasible. One\nof their essential characteristics is the ability to be conversationally\nactionable, allowing humans to interact with the LLM effectively to perform\ncrucial process life cycle tasks such as process model design and redesign.\nHowever, most current research focuses on single-prompt execution and\nevaluation of results, rather than on continuous interaction between the user\nand the LLM. In this work, we aim to explore the feasibility of using LLMs to\nempower domain experts in the creation and redesign of process models in an\niterative and effective way. The proposed conversational process model redesign\n(CPD) approach receives as input a process model and a redesign request by the\nuser in natural language. Instead of just letting the LLM make changes, the LLM\nis employed to (a) identify process change patterns from literature, (b)\nre-phrase the change request to be aligned with an expected wording for the\nidentified pattern (i.e., the meaning), and then to (c) apply the meaning of\nthe change to the process model. This multi-step approach allows for\nexplainable and reproducible changes. In order to ensure the feasibility of the\nCPD approach, and to find out how well the patterns from literature can be\nhandled by the LLM, we performed an extensive evaluation. The results show that\nsome patterns are hard to understand by LLMs and by users. Within the scope of\nthe study, we demonstrated that users need support to describe the changes\nclearly. Overall the evaluation shows that the LLMs can handle most changes\nwell according to a set of completeness and correctness criteria.", "AI": {"tldr": "\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u4e1a\u52a1\u6d41\u7a0b\u7ba1\u7406\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6b65\u9aa4\u5bf9\u8bdd\u5f0f\u6d41\u7a0b\u6a21\u578b\u91cd\u8bbe\u8ba1\uff08CPD\uff09\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u591a\u5173\u6ce8\u5355\u6b21\u63d0\u793a\u6267\u884c\uff0c\u800c\u672c\u5de5\u4f5c\u65e8\u5728\u63a2\u7d22LLMs\u5728\u6301\u7eed\u4ea4\u4e92\u4e2d\u652f\u6301\u9886\u57df\u4e13\u5bb6\u8fed\u4ee3\u8bbe\u8ba1\u548c\u91cd\u8bbe\u8ba1\u6d41\u7a0b\u6a21\u578b\u7684\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51faCPD\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u53d8\u66f4\u6a21\u5f0f\u3001\u91cd\u8ff0\u53d8\u66f4\u8bf7\u6c42\u5e76\u5e94\u7528\u53d8\u66f4\u542b\u4e49\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u548c\u53ef\u91cd\u73b0\u7684\u6d41\u7a0b\u91cd\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c3d\u7ba1\u67d0\u4e9b\u6a21\u5f0f\u5bf9LLMs\u548c\u7528\u6237\u8f83\u96be\u7406\u89e3\uff0cLLMs\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u80fd\u826f\u597d\u5904\u7406\u53d8\u66f4\u9700\u6c42\u3002", "conclusion": "LLMs\u5728\u4e1a\u52a1\u6d41\u7a0b\u7ba1\u7406\u4e2d\u5177\u5907\u6f5c\u529b\uff0c\u4f46\u7528\u6237\u9700\u6e05\u6670\u63cf\u8ff0\u53d8\u66f4\u8bf7\u6c42\u4ee5\u83b7\u5f97\u6700\u4f73\u652f\u6301\u3002"}}
{"id": "2505.04939", "pdf": "https://arxiv.org/pdf/2505.04939", "abs": "https://arxiv.org/abs/2505.04939", "authors": ["Jeffrey Seathr\u00fan Sardina"], "title": "Structural Alignment in Link Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "Ph.D. thesis submitted to Trinity College Dublin", "summary": "While Knowledge Graphs (KGs) have become increasingly popular across various\nscientific disciplines for their ability to model and interlink huge quantities\nof data, essentially all real-world KGs are known to be incomplete. As such,\nwith the growth of KG use has been a concurrent development of machine learning\ntools designed to predict missing information in KGs, which is referred to as\nthe Link Prediction Task. The majority of state-of-the-art link predictors to\ndate have followed an embedding-based paradigm. In this paradigm, it is assumed\nthat the information content of a KG is best represented by the (individual)\nvector representations of its nodes and edges, and that therefore node and edge\nembeddings are particularly well-suited to performing link prediction.\n  This thesis proposes an alternative perspective on the field's approach to\nlink prediction and KG data modelling. Specifically, this work re-analyses KGs\nand state-of-the-art link predictors from a graph-structure-first perspective\nthat models the information content of a KG in terms of whole triples, rather\nthan individual nodes and edges.\n  Following a literature review and two core sets of experiments, this thesis\nconcludes that a structure-first perspective on KGs and link prediction is both\nviable and useful for understanding KG learning and for enabling cross-KG\ntransfer learning for the link prediction task. This observation is used to\ncreate and propose the Structural Alignment Hypothesis, which postulates that\nlink prediction can be understood and modelled as a structural task.\n  All code and data used for this thesis are open-sourced. This thesis was\nwritten bilingually, with the main document in English and an informal extended\nsummary in Irish. An Irish-language translation dictionary of machine learning\nterms (the Focl\\'oir Tr\\'achtais) created for this work is open-sourced as\nwell.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7ed3\u6784\u4f18\u5148\u7684\u89c6\u89d2\u6765\u5904\u7406\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u4e2d\u7684\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\uff0c\u5f3a\u8c03\u4ece\u6574\u4f53\u4e09\u5143\u7ec4\u800c\u975e\u5355\u4e2a\u8282\u70b9\u548c\u8fb9\u7684\u89d2\u5ea6\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u5b9e\u7528\u6027\uff0c\u6700\u7ec8\u63d0\u51fa\u4e86\u7ed3\u6784\u5bf9\u9f50\u5047\u8bbe\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u666e\u904d\u4e0d\u5b8c\u6574\uff0c\u4e14\u4e3b\u6d41\u94fe\u63a5\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8282\u70b9\u548c\u8fb9\u7684\u5d4c\u5165\u8868\u793a\uff0c\u8be5\u7814\u7a76\u8bd5\u56fe\u4ece\u6574\u4f53\u4e09\u5143\u7ec4\u7ed3\u6784\u7684\u89d2\u5ea6\u91cd\u65b0\u5206\u6790\u77e5\u8bc6\u56fe\u8c31\u548c\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\uff0c\u4ee5\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u7406\u89e3\u548c\u66f4\u6709\u6548\u7684\u8de8\u56fe\u8c31\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u4e24\u7ec4\u6838\u5fc3\u5b9e\u9a8c\uff0c\u7814\u7a76\u4ece\u56fe\u7ed3\u6784\u4f18\u5148\u7684\u89c6\u89d2\u91cd\u65b0\u5efa\u6a21\u77e5\u8bc6\u56fe\u8c31\u7684\u4fe1\u606f\u5185\u5bb9\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5bf9\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8e\u7ed3\u6784\u4f18\u5148\u7684\u89c6\u89d2\u4e0d\u4ec5\u53ef\u884c\uff0c\u8fd8\u80fd\u66f4\u597d\u5730\u652f\u6301\u8de8\u77e5\u8bc6\u56fe\u8c31\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u5e76\u636e\u6b64\u63d0\u51fa\u4e86\u7ed3\u6784\u5bf9\u9f50\u5047\u8bbe\u3002", "conclusion": "\u7ed3\u6784\u4f18\u5148\u7684\u89c6\u89d2\u4e3a\u77e5\u8bc6\u56fe\u8c31\u5b66\u4e60\u548c\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u89e3\u548c\u5efa\u6a21\u65b9\u5f0f\uff0c\u540c\u65f6\u7814\u7a76\u6210\u679c\u548c\u5de5\u5177\uff08\u5982\u7231\u5c14\u5170\u8bed\u673a\u5668\u5b66\u4e60\u672f\u8bed\u8bcd\u5178\uff09\u5747\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.04844", "pdf": "https://arxiv.org/pdf/2505.04844", "abs": "https://arxiv.org/abs/2505.04844", "authors": ["Alex Shan", "John Bauer", "Christopher D. Manning"], "title": "Osiris: A Lightweight Open-Source Hallucination Detection System", "categories": ["cs.CL"], "comment": "Stanford 191W", "summary": "Retrieval-Augmented Generation (RAG) systems have gained widespread adoption\nby application builders because they leverage sources of truth to enable Large\nLanguage Models (LLMs) to generate more factually sound responses. However,\nhallucinations, instances of LLM responses that are unfaithful to the provided\ncontext, often prevent these systems from being deployed in production\nenvironments. Current hallucination detection methods typically involve human\nevaluation or the use of closed-source models to review RAG system outputs for\nhallucinations. Both human evaluators and closed-source models suffer from\nscaling issues due to their high costs and slow inference speeds. In this work,\nwe introduce a perturbed multi-hop QA dataset with induced hallucinations. Via\nsupervised fine-tuning on our dataset, we achieve better recall with a 7B model\nthan GPT-4o on the RAGTruth hallucination detection benchmark and offer\ncompetitive performance on precision and accuracy, all while using a fraction\nof the parameters. Code is released at our repository.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6270\u52a8\u591a\u8df3QA\u6570\u636e\u96c6\u8bf1\u5bfc\u5e7b\u89c9\u7684\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f7f\u5f977B\u6a21\u578b\u5728RAGTruth\u57fa\u51c6\u4e0a\u7684\u53ec\u56de\u7387\u4f18\u4e8eGPT-4o\uff0c\u540c\u65f6\u53c2\u6570\u91cf\u66f4\u5c11\u3002", "motivation": "\u5f53\u524d\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bc4\u4f30\u6216\u95ed\u6e90\u6a21\u578b\uff0c\u6210\u672c\u9ad8\u4e14\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u96be\u4ee5\u6269\u5c55\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u6270\u52a8\u591a\u8df3QA\u6570\u636e\u96c6\uff08\u5305\u542b\u8bf1\u5bfc\u5e7b\u89c9\uff09\uff0c\u5e76\u5229\u7528\u76d1\u7763\u5fae\u8c03\u8bad\u7ec37B\u6a21\u578b\u8fdb\u884c\u5e7b\u89c9\u68c0\u6d4b\u3002", "result": "\u5728RAGTruth\u57fa\u51c6\u4e0a\uff0c7B\u6a21\u578b\u7684\u53ec\u56de\u7387\u4f18\u4e8eGPT-4o\uff0c\u4e14\u5728\u7cbe\u5ea6\u548c\u51c6\u786e\u7387\u4e0a\u8868\u73b0\u7ade\u4e89\u6027\uff0c\u53c2\u6570\u91cf\u66f4\u5c11\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u6210\u672c\u66f4\u4f4e\uff0c\u4e3a\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.06667", "pdf": "https://arxiv.org/pdf/2504.06667", "abs": "https://arxiv.org/abs/2504.06667", "authors": ["Yashar Deldjoo", "Nikhil Mehta", "Maheswaran Sathiamoorthy", "Shuai Zhang", "Pablo Castells", "Julian McAuley"], "title": "Toward Holistic Evaluation of Recommender Systems Powered by Generative Models", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Recommender systems powered by generative models (Gen-RecSys) extend beyond\nclassical item ranking by producing open-ended content, which simultaneously\nunlocks richer user experiences and introduces new risks. On one hand, these\nsystems can enhance personalization and appeal through dynamic explanations and\nmulti-turn dialogues. On the other hand, they might venture into unknown\nterritory-hallucinating nonexistent items, amplifying bias, or leaking private\ninformation. Traditional accuracy metrics cannot fully capture these\nchallenges, as they fail to measure factual correctness, content safety, or\nalignment with user intent.\n  This paper makes two main contributions. First, we categorize the evaluation\nchallenges of Gen-RecSys into two groups: (i) existing concerns that are\nexacerbated by generative outputs (e.g., bias, privacy) and (ii) entirely new\nrisks (e.g., item hallucinations, contradictory explanations). Second, we\npropose a holistic evaluation approach that includes scenario-based assessments\nand multi-metric checks-incorporating relevance, factual grounding, bias\ndetection, and policy compliance. Our goal is to provide a guiding framework so\nresearchers and practitioners can thoroughly assess Gen-RecSys, ensuring\neffective personalization and responsible deployment.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\uff08Gen-RecSys\uff09\u7684\u8bc4\u4f30\u6311\u6218\uff0c\u5c06\u5176\u5206\u4e3a\u73b0\u6709\u95ee\u9898\u52a0\u5267\u548c\u65b0\u98ce\u9669\u4e24\u7c7b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u573a\u666f\u8bc4\u4f30\u548c\u591a\u6307\u6807\u68c0\u67e5\u7684\u6574\u4f53\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u5728\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7528\u6237\u4f53\u9a8c\u7684\u540c\u65f6\uff0c\u4e5f\u5e26\u6765\u4e86\u65b0\u7684\u98ce\u9669\uff0c\u5982\u5e7b\u89c9\u9879\u76ee\u3001\u504f\u89c1\u653e\u5927\u6216\u9690\u79c1\u6cc4\u9732\u3002\u4f20\u7edf\u51c6\u786e\u6027\u6307\u6807\u65e0\u6cd5\u5168\u9762\u8861\u91cf\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u4f53\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5305\u62ec\u573a\u666f\u5316\u8bc4\u4f30\u548c\u591a\u6307\u6807\u68c0\u67e5\uff0c\u6db5\u76d6\u76f8\u5173\u6027\u3001\u4e8b\u5b9e\u57fa\u7840\u3001\u504f\u89c1\u68c0\u6d4b\u548c\u653f\u7b56\u5408\u89c4\u6027\u3002", "result": "\u901a\u8fc7\u5206\u7c7b\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u8bc4\u4f30Gen-RecSys\u7684\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u6709\u6548\u4e2a\u6027\u5316\u548c\u8d1f\u8d23\u4efb\u90e8\u7f72\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u6307\u5bfc\uff0c\u5e2e\u52a9\u5728\u6280\u672f\u53d1\u5c55\u7684\u540c\u65f6\u786e\u4fdd\u5185\u5bb9\u5b89\u5168\u548c\u7528\u6237\u610f\u56fe\u5bf9\u9f50\u3002"}}
{"id": "2505.04956", "pdf": "https://arxiv.org/pdf/2505.04956", "abs": "https://arxiv.org/abs/2505.04956", "authors": ["Dingshuo Chen", "Shuchen Xue", "Liuji Chen", "Yingheng Wang", "Qiang Liu", "Shu Wu", "Zhi-Ming Ma", "Liang Wang"], "title": "Graffe: Graph Representation Learning via Diffusion Probabilistic Models", "categories": ["cs.LG", "cs.AI"], "comment": "16 pages, 4 figures, under review", "summary": "Diffusion probabilistic models (DPMs), widely recognized for their potential\nto generate high-quality samples, tend to go unnoticed in representation\nlearning. While recent progress has highlighted their potential for capturing\nvisual semantics, adapting DPMs to graph representation learning remains in its\ninfancy. In this paper, we introduce Graffe, a self-supervised diffusion model\nproposed for graph representation learning. It features a graph encoder that\ndistills a source graph into a compact representation, which, in turn, serves\nas the condition to guide the denoising process of the diffusion decoder. To\nevaluate the effectiveness of our model, we first explore the theoretical\nfoundations of applying diffusion models to representation learning, proving\nthat the denoising objective implicitly maximizes the conditional mutual\ninformation between data and its representation. Specifically, we prove that\nthe negative logarithm of the denoising score matching loss is a tractable\nlower bound for the conditional mutual information. Empirically, we conduct a\nseries of case studies to validate our theoretical insights. In addition,\nGraffe delivers competitive results under the linear probing setting on node\nand graph classification tasks, achieving state-of-the-art performance on 9 of\nthe 11 real-world datasets. These findings indicate that powerful generative\nmodels, especially diffusion models, serve as an effective tool for graph\nrepresentation learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGraffe\uff0c\u4e00\u79cd\u81ea\u76d1\u7763\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u56fe\u8868\u793a\u5b66\u4e60\uff0c\u7406\u8bba\u8bc1\u660e\u53bb\u566a\u76ee\u6807\u9690\u542b\u5730\u6700\u5927\u5316\u6570\u636e\u4e0e\u5176\u8868\u793a\u7684\u6761\u4ef6\u4e92\u4fe1\u606f\uff0c\u5e76\u572811\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e2d\u76849\u4e2a\u4e0a\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DPMs\uff09\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\u65b9\u9762\u6f5c\u529b\u663e\u8457\uff0c\u4f46\u5728\u8868\u793a\u5b66\u4e60\u9886\u57df\u672a\u88ab\u5145\u5206\u5173\u6ce8\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7Graffe\u6a21\u578b\uff0c\u63a2\u7d22DPMs\u5728\u56fe\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002", "method": "Graffe\u91c7\u7528\u81ea\u76d1\u7763\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u56fe\u7f16\u7801\u5668\u63d0\u53d6\u56fe\u7684\u7d27\u51d1\u8868\u793a\uff0c\u5e76\u4ee5\u6b64\u4f5c\u4e3a\u6761\u4ef6\u6307\u5bfc\u6269\u6563\u89e3\u7801\u5668\u7684\u53bb\u566a\u8fc7\u7a0b\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u53bb\u566a\u76ee\u6807\u9690\u542b\u6700\u5927\u5316\u6761\u4ef6\u4e92\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\uff0cGraffe\u5728\u8282\u70b9\u548c\u56fe\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c11\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e2d\u76849\u4e2a\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u5f3a\u5927\u751f\u6210\u6a21\u578b\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\u662f\u56fe\u8868\u793a\u5b66\u4e60\u7684\u6709\u6548\u5de5\u5177\uff0cGraffe\u7684\u6210\u529f\u4e3aDPMs\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.04847", "pdf": "https://arxiv.org/pdf/2505.04847", "abs": "https://arxiv.org/abs/2505.04847", "authors": ["Manveer Singh Tamber", "Forrest Sheng Bao", "Chenyu Xu", "Ge Luo", "Suleman Kazi", "Minseok Bae", "Miaoran Li", "Ofer Mendelevitch", "Renyi Qu", "Jimmy Lin"], "title": "Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucinations remain a persistent challenge for LLMs. RAG aims to reduce\nhallucinations by grounding responses in contexts. However, even when provided\ncontext, LLMs still frequently introduce unsupported information or\ncontradictions. This paper presents our efforts to measure LLM hallucinations\nwith a focus on summarization tasks, assessing how often various LLMs introduce\nhallucinations when summarizing documents. We discuss Vectara's existing LLM\nhallucination leaderboard, based on the Hughes Hallucination Evaluation Model\n(HHEM). While HHEM and Vectara's Hallucination Leaderboard have garnered great\nresearch interest, we examine challenges faced by HHEM and current\nhallucination detection methods by analyzing the effectiveness of these methods\non existing hallucination datasets. To address these limitations, we propose\nFaithJudge, an LLM-as-a-judge approach guided by few-shot human hallucination\nannotations, which substantially improves automated LLM hallucination\nevaluation over current methods. We introduce an enhanced hallucination\nleaderboard centered on FaithJudge, alongside our current hallucination\nleaderboard, enabling more reliable benchmarking of LLMs for hallucinations in\nRAG.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86LLM\u5728RAG\u4e2d\u4ecd\u5b58\u5728\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eFaithJudge\u7684\u65b0\u65b9\u6cd5\u4ee5\u6539\u8fdb\u5e7b\u89c9\u8bc4\u4f30\uff0c\u5e76\u53d1\u5e03\u4e86\u589e\u5f3a\u7248\u5e7b\u89c9\u6392\u884c\u699c\u3002", "motivation": "\u5c3d\u7ba1RAG\u5c1d\u8bd5\u901a\u8fc7\u4e0a\u4e0b\u6587\u51cf\u5c11LLM\u5e7b\u89c9\uff0c\u4f46\u95ee\u9898\u4ecd\u7136\u5b58\u5728\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982HHEM\uff09\u5b58\u5728\u5c40\u9650\uff0c\u4e9f\u9700\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faFaithJudge\u65b9\u6cd5\uff0c\u7ed3\u5408\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u7684\u793a\u4f8b\uff0c\u4ee5LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\uff0c\u6539\u8fdb\u5e7b\u89c9\u68c0\u6d4b\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u3002", "result": "FaithJudge\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u63a8\u51fa\u4e86\u57fa\u4e8e\u8be5\u65b9\u6cd5\u7684\u589e\u5f3a\u7248\u5e7b\u89c9\u6392\u884c\u699c\uff0c\u4e3aRAG\u4efb\u52a1\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u57fa\u51c6\u3002", "conclusion": "\u7814\u7a76\u901a\u8fc7FaithJudge\u63d0\u5347\u4e86LLM\u5e7b\u89c9\u8bc4\u4f30\u7684\u53ef\u9760\u6027\uff0c\u5e76\u516c\u5f00\u4e86\u65b0\u6392\u884c\u699c\uff0c\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2505.04969", "pdf": "https://arxiv.org/pdf/2505.04969", "abs": "https://arxiv.org/abs/2505.04969", "authors": ["Gekko Budiutama", "Shunsuke Daimon", "Hirofumi Nishi", "Yu-ichiro Matsushita"], "title": "General Transform: A Unified Framework for Adaptive Transform to Enhance Representations", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Discrete transforms, such as the discrete Fourier transform, are widely used\nin machine learning to improve model performance by extracting meaningful\nfeatures. However, with numerous transforms available, selecting an appropriate\none often depends on understanding the dataset's properties, making the\napproach less effective when such knowledge is unavailable. In this work, we\npropose General Transform (GT), an adaptive transform-based representation\ndesigned for machine learning applications. Unlike conventional transforms, GT\nlearns data-driven mapping tailored to the dataset and task of interest. Here,\nwe demonstrate that models incorporating GT outperform conventional\ntransform-based approaches across computer vision and natural language\nprocessing tasks, highlighting its effectiveness in diverse learning scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u53d8\u6362\u8868\u793a\u65b9\u6cd5GT\uff0c\u4f18\u4e8e\u4f20\u7edf\u53d8\u6362\u65b9\u6cd5\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u79bb\u6563\u53d8\u6362\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5bf9\u6570\u636e\u96c6\u7279\u6027\u7684\u4e86\u89e3\uff0c\u5f53\u8fd9\u4e9b\u77e5\u8bc6\u4e0d\u53ef\u5f97\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86GT\uff0c\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u53d8\u6362\u8868\u793a\u65b9\u6cd5\u3002", "result": "\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528GT\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u53d8\u6362\u65b9\u6cd5\u3002", "conclusion": "GT\u5728\u591a\u6837\u5316\u5b66\u4e60\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u7684\u7279\u5f81\u63d0\u53d6\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.04916", "pdf": "https://arxiv.org/pdf/2505.04916", "abs": "https://arxiv.org/abs/2505.04916", "authors": ["Ramteja Sajja", "Yusuf Sermet", "Ibrahim Demir"], "title": "An Open-Source Dual-Loss Embedding Model for Semantic Retrieval in Higher Education", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "17 pages, 3 Tables", "summary": "Recent advances in AI have catalyzed the adoption of intelligent educational\ntools, yet many semantic retrieval systems remain ill-suited to the unique\nlinguistic and structural characteristics of academic content. This study\npresents two open-source embedding models fine-tuned for educational question\nanswering, particularly in the context of course syllabi. A synthetic dataset\nof 3,197 sentence pairs, spanning synonymous terminology, paraphrased\nquestions, and implicit-explicit mappings, was constructed through a\ncombination of manual curation and large language model (LLM)-assisted\ngeneration. Two training strategies were evaluated: (1) a baseline model\nfine-tuned using MultipleNegativesRankingLoss (MNRL), and (2) a dual-loss model\nthat combines MNRL with CosineSimilarityLoss to improve both semantic ranking\nand similarity calibration. Evaluations were conducted on 28 university course\nsyllabi using a fixed set of natural language questions categorized into\ncourse, faculty, and teaching assistant information. Results demonstrate that\nboth fine-tuned models outperform strong open-source baselines, including\nall-MiniLM-L6-v2 and multi-qa-MiniLM-L6-cos-v1, and that the dual-loss model\nnarrows the performance gap with high-performing proprietary embeddings such as\nOpenAI's text-embedding-3 series. This work contributes reusable,\ndomain-aligned embedding models and provides a replicable framework for\neducational semantic retrieval, supporting downstream applications such as\nacademic chatbots, retrieval-augmented generation (RAG) systems, and learning\nmanagement system (LMS) integrations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u9488\u5bf9\u6559\u80b2\u95ee\u7b54\u4f18\u5316\u7684\u5f00\u6e90\u5d4c\u5165\u6a21\u578b\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u548c\u53cc\u91cd\u635f\u5931\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bfe\u7a0b\u5927\u7eb2\u8bed\u4e49\u68c0\u7d22\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u68c0\u7d22\u7cfb\u7edf\u96be\u4ee5\u9002\u5e94\u5b66\u672f\u5185\u5bb9\u7684\u72ec\u7279\u8bed\u8a00\u548c\u7ed3\u6784\u7279\u70b9\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u4f18\u5316\u7684\u5d4c\u5165\u6a21\u578b\u4ee5\u63d0\u5347\u6559\u80b2\u9886\u57df\u95ee\u7b54\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b3,197\u53e5\u5bf9\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e24\u79cd\u5fae\u8c03\u7b56\u7565\uff1a\u57fa\u4e8eMNRL\u7684\u57fa\u7ebf\u6a21\u578b\u548c\u7ed3\u5408MNRL\u4e0eCosineSimilarityLoss\u7684\u53cc\u91cd\u635f\u5931\u6a21\u578b\u3002", "result": "\u4e24\u79cd\u5fae\u8c03\u6a21\u578b\u5747\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u57fa\u7ebf\uff08\u5982all-MiniLM-L6-v2\uff09\uff0c\u53cc\u91cd\u635f\u5931\u6a21\u578b\u8fdb\u4e00\u6b65\u7f29\u5c0f\u4e86\u4e0e\u9ad8\u6027\u80fd\u4e13\u6709\u5d4c\u5165\uff08\u5982OpenAI\u7684text-embedding-3\u7cfb\u5217\uff09\u7684\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6559\u80b2\u8bed\u4e49\u68c0\u7d22\u63d0\u4f9b\u4e86\u53ef\u590d\u7528\u7684\u5d4c\u5165\u6a21\u578b\u548c\u6846\u67b6\uff0c\u652f\u6301\u5b66\u672f\u804a\u5929\u673a\u5668\u4eba\u3001RAG\u7cfb\u7edf\u548cLMS\u96c6\u6210\u7b49\u4e0b\u6e38\u5e94\u7528\u3002"}}
{"id": "2505.04629", "pdf": "https://arxiv.org/pdf/2505.04629", "abs": "https://arxiv.org/abs/2505.04629", "authors": ["Abdulhady Abas Abdullah", "Soran Badawi", "Dana A. Abdullah", "Dana Rasul Hamad", "Hanan Abdulrahman Taher", "Sabat Salih Muhamad", "Aram Mahmood Ahmed", "Bryar A. Hassan", "Sirwan Abdolwahed Aula", "Tarik A. Rashid"], "title": "From Dialect Gaps to Identity Maps: Tackling Variability in Speaker Verification", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "The complexity and difficulties of Kurdish speaker detection among its\nseveral dialects are investigated in this work. Because of its great phonetic\nand lexical differences, Kurdish with several dialects including Kurmanji,\nSorani, and Hawrami offers special challenges for speaker recognition systems.\nThe main difficulties in building a strong speaker identification system\ncapable of precisely identifying speakers across several dialects are\ninvestigated in this work. To raise the accuracy and dependability of these\nsystems, it also suggests solutions like sophisticated machine learning\napproaches, data augmentation tactics, and the building of thorough\ndialect-specific corpus. The results show that customized strategies for every\ndialect together with cross-dialect training greatly enhance recognition\nperformance.", "AI": {"tldr": "\u9488\u5bf9\u5e93\u5c14\u5fb7\u8bed\u591a\u65b9\u8a00\u7684\u8bf4\u8bdd\u4eba\u8bc6\u522b\u6311\u6218\uff0c\u672c\u6587\u7814\u7a76\u4e86\u5176\u590d\u6742\u6027\uff0c\u63d0\u51fa\u4e86\u5305\u62ec\u673a\u5668\u5b66\u4e60\u3001\u6570\u636e\u589e\u5f3a\u548c\u65b9\u8a00\u8bed\u6599\u5e93\u6784\u5efa\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u679c\u8868\u660e\u8de8\u65b9\u8a00\u8bad\u7ec3\u53ef\u663e\u8457\u63d0\u5347\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u5e93\u5c14\u5fb7\u8bed\u56e0\u5176\u591a\u65b9\u8a00\uff08\u5982Kurmanji\u3001Sorani\u3001Hawrami\uff09\u5728\u8bed\u97f3\u548c\u8bcd\u6c47\u4e0a\u7684\u5de8\u5927\u5dee\u5f02\uff0c\u7ed9\u8bf4\u8bdd\u4eba\u8bc6\u522b\u7cfb\u7edf\u5e26\u6765\u72ec\u7279\u6311\u6218\uff0c\u9700\u63a2\u7d22\u63d0\u5347\u7cfb\u7edf\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u3001\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u5e76\u6784\u5efa\u65b9\u8a00\u7279\u5b9a\u7684\u8bed\u6599\u5e93\uff0c\u7ed3\u5408\u8de8\u65b9\u8a00\u8bad\u7ec3\u4f18\u5316\u8bc6\u522b\u7cfb\u7edf\u3002", "result": "\u7814\u7a76\u663e\u793a\uff0c\u9488\u5bf9\u5404\u65b9\u8a00\u7684\u5b9a\u5236\u5316\u7b56\u7565\u53ca\u8de8\u65b9\u8a00\u8bad\u7ec3\u80fd\u663e\u8457\u63d0\u9ad8\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u6280\u672f\u4f18\u5316\u548c\u8d44\u6e90\u5efa\u8bbe\uff0c\u53ef\u6709\u6548\u89e3\u51b3\u5e93\u5c14\u5fb7\u8bed\u591a\u65b9\u8a00\u7684\u8bf4\u8bdd\u4eba\u8bc6\u522b\u96be\u9898\u3002"}}
{"id": "2505.04981", "pdf": "https://arxiv.org/pdf/2505.04981", "abs": "https://arxiv.org/abs/2505.04981", "authors": ["Zhifeng Hu", "Chong Han"], "title": "Graph Neural Network Aided Deep Reinforcement Learning for Resource Allocation in Dynamic Terahertz UAV Networks", "categories": ["cs.LG"], "comment": null, "summary": "Terahertz (THz) unmanned aerial vehicle (UAV) networks with flexible\ntopologies and ultra-high data rates are expected to empower numerous\napplications in security surveillance, disaster response, and environmental\nmonitoring, among others. However, the dynamic topologies hinder the efficient\nlong-term joint power and antenna array resource allocation for THz links among\nUAVs. Furthermore, the continuous nature of power and the discrete nature of\nantennas cause this joint resource allocation problem to be a mixed-integer\nnonlinear programming (MINLP) problem with non-convexity and NP-hardness.\nInspired by recent rapid advancements in deep reinforcement learning (DRL), a\ngraph neural network (GNN) aided DRL algorithm for resource allocation in the\ndynamic THz UAV network with an emphasis on self-node features (GLOVE) is\nproposed in this paper, with the aim of resource efficiency (RE) maximization.\nWhen training the allocation policy for each UAV, GLOVE learns the relationship\nbetween this UAV and its neighboring UAVs via GNN, while also emphasizing the\nimportant self-node features of this UAV. In addition, a multi-task structure\nis leveraged by GLOVE to cooperatively train resource allocation decisions for\nthe power and sub-arrays of all UAVs. Experimental results illustrate that\nGLOVE outperforms benchmark schemes in terms of the highest RE and the lowest\nlatency. Moreover, unlike the benchmark methods with severe packet loss, GLOVE\nmaintains zero packet loss during the entire training process, demonstrating\nits better robustness under the highly dynamic THz UAV network.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u7b97\u6cd5\uff08GLOVE\uff09\uff0c\u7528\u4e8e\u52a8\u6001\u592a\u8d6b\u5179\uff08THz\uff09\u65e0\u4eba\u673a\uff08UAV\uff09\u7f51\u7edc\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u4ee5\u6700\u5927\u5316\u8d44\u6e90\u6548\u7387\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u52a8\u6001\u62d3\u6251\u7ed3\u6784\u4f7f\u5f97\u592a\u8d6b\u5179\u65e0\u4eba\u673a\u7f51\u7edc\u4e2d\u957f\u671f\u7684\u8054\u5408\u529f\u7387\u548c\u5929\u7ebf\u9635\u5217\u8d44\u6e90\u5206\u914d\u95ee\u9898\u53d8\u5f97\u590d\u6742\uff0c\u8fd9\u662f\u4e00\u4e2a\u6df7\u5408\u6574\u6570\u975e\u7ebf\u6027\u89c4\u5212\uff08MINLP\uff09\u95ee\u9898\uff0c\u5177\u6709\u975e\u51f8\u6027\u548cNP\u96be\u5ea6\u3002", "method": "GLOVE\u7b97\u6cd5\u5229\u7528GNN\u6355\u6349\u65e0\u4eba\u673a\u4e0e\u5176\u90bb\u5c45\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u5f3a\u8c03\u81ea\u8eab\u8282\u70b9\u7279\u5f81\uff0c\u540c\u65f6\u91c7\u7528\u591a\u4efb\u52a1\u7ed3\u6784\u534f\u540c\u8bad\u7ec3\u529f\u7387\u548c\u5b50\u9635\u5217\u8d44\u6e90\u5206\u914d\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGLOVE\u5728\u8d44\u6e90\u6548\u7387\u548c\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\uff0c\u5e76\u4e14\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u96f6\u4e22\u5305\uff0c\u663e\u793a\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "GLOVE\u7b97\u6cd5\u5728\u52a8\u6001\u7684\u592a\u8d6b\u5179\u65e0\u4eba\u673a\u7f51\u7edc\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u8d44\u6e90\u5206\u914d\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.04955", "pdf": "https://arxiv.org/pdf/2505.04955", "abs": "https://arxiv.org/abs/2505.04955", "authors": ["Fangwei Zhu", "Peiyi Wang", "Zhifang Sui"], "title": "Chain-of-Thought Tokens are Computer Program Variables", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-thoughts (CoT) requires large language models (LLMs) to generate\nintermediate steps before reaching the final answer, and has been proven\neffective to help LLMs solve complex reasoning tasks. However, the inner\nmechanism of CoT still remains largely unclear. In this paper, we empirically\nstudy the role of CoT tokens in LLMs on two compositional tasks: multi-digit\nmultiplication and dynamic programming. While CoT is essential for solving\nthese problems, we find that preserving only tokens that store intermediate\nresults would achieve comparable performance. Furthermore, we observe that\nstoring intermediate results in an alternative latent form will not affect\nmodel performance. We also randomly intervene some values in CoT, and notice\nthat subsequent CoT tokens and the final answer would change correspondingly.\nThese findings suggest that CoT tokens may function like variables in computer\nprograms but with potential drawbacks like unintended shortcuts and\ncomputational complexity limits between tokens. The code and data are available\nat https://github.com/solitaryzero/CoTs_are_Variables.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u4ec5\u4fdd\u7559\u5b58\u50a8\u4e2d\u95f4\u7ed3\u679c\u7684CoT\u6807\u8bb0\u4e5f\u80fd\u8fbe\u5230\u7c7b\u4f3c\u6548\u679c\uff0c\u4e14\u4e2d\u95f4\u7ed3\u679c\u4ee5\u6f5c\u5728\u5f62\u5f0f\u5b58\u50a8\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "motivation": "\u65e8\u5728\u63ed\u793aCoT\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5185\u5728\u673a\u5236\uff0c\u7279\u522b\u662f\u5728\u89e3\u51b3\u590d\u5408\u4efb\u52a1\u65f6\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u6570\u5b57\u4e58\u6cd5\u548c\u52a8\u6001\u89c4\u5212\u4efb\u52a1\uff0c\u5206\u6790\u4e86CoT\u6807\u8bb0\u7684\u4f5c\u7528\uff0c\u6d4b\u8bd5\u4e86\u4e0d\u540c\u5f62\u5f0f\u7684CoT\u5f71\u54cd\u3002", "result": "\u53d1\u73b0CoT\u6807\u8bb0\u7c7b\u4f3c\u53d8\u91cf\uff0c\u4ec5\u4fdd\u7559\u4e2d\u95f4\u7ed3\u679c\u6807\u8bb0\u5373\u53ef\u7ef4\u6301\u6027\u80fd\uff0c\u4f46\u53ef\u80fd\u5f15\u5165\u7f3a\u9677\u3002", "conclusion": "CoT\u6807\u8bb0\u53ef\u80fd\u7c7b\u4f3c\u7f16\u7a0b\u53d8\u91cf\uff0c\u4f46\u4ecd\u9700\u5173\u6ce8\u5176\u6f5c\u5728\u95ee\u9898\u3002"}}
{"id": "2505.05015", "pdf": "https://arxiv.org/pdf/2505.05015", "abs": "https://arxiv.org/abs/2505.05015", "authors": ["Roberto Dillon", "Arushi"], "title": "An Agent-Based Modeling Approach to Free-Text Keyboard Dynamics for Continuous Authentication", "categories": ["cs.LG", "cs.AI", "cs.CR", "68T10, 62H30", "I.2.6; I.5.4; I.6.3"], "comment": "16 pages, 5 figures, 12 tables", "summary": "Continuous authentication systems leveraging free-text keyboard dynamics\noffer a promising additional layer of security in a multifactor authentication\nsetup that can be used in a transparent way with no impact on user experience.\nThis study investigates the efficacy of behavioral biometrics by employing an\nAgent-Based Model (ABM) to simulate diverse typing profiles across mechanical\nand membrane keyboards. Specifically, we generated synthetic keystroke data\nfrom five unique agents, capturing features related to dwell time, flight time,\nand error rates within sliding 5-second windows updated every second. Two\nmachine learning approaches, One-Class Support Vector Machine (OC-SVM) and\nRandom Forest (RF), were evaluated for user verification. Results revealed a\nstark contrast in performance: while One-Class SVM failed to differentiate\nindividual users within each group, Random Forest achieved robust\nintra-keyboard user recognition (Accuracy > 0.7) but struggled to generalize\nacross keyboards for the same user, highlighting the significant impact of\nkeyboard hardware on typing behavior. These findings suggest that: (1)\nkeyboard-specific user profiles may be necessary for reliable authentication,\nand (2) ensemble methods like RF outperform One-Class SVM in capturing\nfine-grained user-specific patterns.", "AI": {"tldr": "\u7814\u7a76\u4e86\u57fa\u4e8e\u952e\u76d8\u52a8\u6001\u884c\u4e3a\u7684\u8fde\u7eed\u8ba4\u8bc1\u7cfb\u7edf\uff0c\u53d1\u73b0\u952e\u76d8\u786c\u4ef6\u5bf9\u8ba4\u8bc1\u6548\u679c\u5f71\u54cd\u663e\u8457\uff0c\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u540c\u7c7b\u952e\u76d8\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u8de8\u952e\u76d8\u8ba4\u8bc1\u6548\u679c\u4e0d\u4f73\u3002", "motivation": "\u63a2\u7d22\u952e\u76d8\u52a8\u6001\u884c\u4e3a\u5728\u591a\u56e0\u7d20\u8ba4\u8bc1\u4e2d\u7684\u6f5c\u529b\uff0c\u8bc4\u4f30\u5176\u5728\u4e0d\u540c\u952e\u76d8\u786c\u4ef6\u4e0a\u7684\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u578b\u751f\u6210\u5408\u6210\u51fb\u952e\u6570\u636e\uff0c\u5e76\u901a\u8fc7OC-SVM\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\u8fdb\u884c\u7528\u6237\u9a8c\u8bc1\u3002", "result": "\u968f\u673a\u68ee\u6797\u5728\u5355\u4e00\u952e\u76d8\u5185\u8ba4\u8bc1\u51c6\u786e\u7387\u8d85\u8fc70.7\uff0c\u4f46\u8de8\u952e\u76d8\u8868\u73b0\u4e0d\u4f73\uff1bOC-SVM\u5219\u65e0\u6cd5\u6709\u6548\u533a\u5206\u7528\u6237\u3002", "conclusion": "\u5efa\u8bae\u9488\u5bf9\u7279\u5b9a\u952e\u76d8\u521b\u5efa\u7528\u6237\u8ba4\u8bc1\u6a21\u677f\uff0c\u5e76\u4f18\u5148\u9009\u62e9\u96c6\u6210\u65b9\u6cd5\u5982\u968f\u673a\u68ee\u6797\u4ee5\u63d0\u9ad8\u8ba4\u8bc1\u53ef\u9760\u6027\u3002"}}
{"id": "2505.04984", "pdf": "https://arxiv.org/pdf/2505.04984", "abs": "https://arxiv.org/abs/2505.04984", "authors": ["Kai Nakaishi", "Ryo Yoshida", "Kohei Kajikawa", "Koji Hukushima", "Yohei Oseki"], "title": "Rethinking the Relationship between the Power Law and Hierarchical Structures", "categories": ["cs.CL"], "comment": "13 pages, 11 figures", "summary": "Statistical analysis of corpora provides an approach to quantitatively\ninvestigate natural languages. This approach has revealed that several power\nlaws consistently emerge across different corpora and languages, suggesting the\nuniversal principles underlying languages. Particularly, the power-law decay of\ncorrelation has been interpreted as evidence for underlying hierarchical\nstructures in syntax, semantics, and discourse. This perspective has also been\nextended to child languages and animal signals. However, the argument\nsupporting this interpretation has not been empirically tested. To address this\nproblem, this study examines the validity of the argument for syntactic\nstructures. Specifically, we test whether the statistical properties of parse\ntrees align with the implicit assumptions in the argument. Using English\ncorpora, we analyze the mutual information, deviations from probabilistic\ncontext-free grammars (PCFGs), and other properties in parse trees, as well as\nin the PCFG that approximates these trees. Our results indicate that the\nassumptions do not hold for syntactic structures and that it is difficult to\napply the proposed argument to child languages and animal signals, highlighting\nthe need to reconsider the relationship between the power law and hierarchical\nstructures.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5206\u6790\u82f1\u8bed\u8bed\u6599\u5e93\u4e2d\u89e3\u6790\u6811\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u68c0\u9a8c\u4e86\u5e42\u5f8b\u8870\u51cf\u4e0e\u5c42\u6b21\u7ed3\u6784\u5173\u7cfb\u7684\u5047\u8bbe\uff0c\u53d1\u73b0\u8be5\u5047\u8bbe\u4e0d\u6210\u7acb\uff0c\u9700\u91cd\u65b0\u601d\u8003\u5e42\u5f8b\u4e0e\u8bed\u8a00\u5c42\u6b21\u7ed3\u6784\u7684\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8ba4\u4e3a\u8bed\u8a00\u4e2d\u7684\u5e42\u5f8b\u8870\u51cf\u73b0\u8c61\u53cd\u6620\u4e86\u8bed\u6cd5\u3001\u8bed\u4e49\u548c\u8bed\u7bc7\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u4f46\u8fd9\u4e00\u89c2\u70b9\u7f3a\u4e4f\u5b9e\u8bc1\u652f\u6301\u3002\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\u5728\u53e5\u6cd5\u7ed3\u6784\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u82f1\u8bed\u8bed\u6599\u5e93\uff0c\u5206\u6790\u89e3\u6790\u6811\u7684\u4e92\u4fe1\u606f\u3001\u504f\u79bb\u6982\u7387\u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5\uff08PCFG\uff09\u7684\u7a0b\u5ea6\u7b49\u7edf\u8ba1\u7279\u6027\uff0c\u4ee5\u53ca\u8fd1\u4f3c\u8fd9\u4e9b\u89e3\u6790\u6811\u7684PCFG\u6a21\u578b\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5173\u4e8e\u5e42\u5f8b\u4e0e\u5c42\u6b21\u7ed3\u6784\u5173\u7cfb\u7684\u5047\u8bbe\u5728\u53e5\u6cd5\u7ed3\u6784\u4e2d\u4e0d\u6210\u7acb\uff0c\u4e14\u96be\u4ee5\u63a8\u5e7f\u5230\u513f\u7ae5\u8bed\u8a00\u548c\u52a8\u7269\u4fe1\u53f7\uff0c\u9700\u91cd\u65b0\u5ba1\u89c6\u4e8c\u8005\u7684\u5173\u8054\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u73b0\u6709\u7406\u8bba\u5728\u89e3\u91ca\u5e42\u5f8b\u4e0e\u8bed\u8a00\u5c42\u6b21\u7ed3\u6784\u5173\u7cfb\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u4ed6\u53ef\u80fd\u7684\u89e3\u91ca\u6216\u6a21\u578b\u3002"}}
{"id": "2505.05019", "pdf": "https://arxiv.org/pdf/2505.05019", "abs": "https://arxiv.org/abs/2505.05019", "authors": ["Waldemar Hahn", "Jan-Niklas Eckardt", "Christoph R\u00f6llig", "Martin Sedlmayr", "Jan Moritz Middeke", "Markus Wolfien"], "title": "Generating Reliable Synthetic Clinical Trial Data: The Role of Hyperparameter Optimization and Domain Constraints", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The generation of synthetic clinical trial data offers a promising approach\nto mitigating privacy concerns and data accessibility limitations in medical\nresearch. However, ensuring that synthetic datasets maintain high fidelity,\nutility, and adherence to domain-specific constraints remains a key challenge.\nWhile hyperparameter optimization (HPO) has been shown to improve generative\nmodel performance, the effectiveness of different optimization strategies for\nsynthetic clinical data remains unclear. This study systematically evaluates\nfour HPO strategies across eight generative models, comparing single-metric\noptimization against compound metric optimization approaches. Our results\ndemonstrate that HPO consistently improves synthetic data quality, with TVAE,\nCTGAN, and CTAB-GAN+ achieving improvements of up to 60%, 39%, and 38%,\nrespectively. Compound metric optimization outperformed single-metric\nstrategies, producing more balanced and generalizable synthetic datasets.\nInterestingly, HPO alone is insufficient to ensure clinically valid synthetic\ndata, as all models exhibited violations of fundamental survival constraints.\nPreprocessing and postprocessing played a crucial role in reducing these\nviolations, as models lacking robust processing steps produced invalid data in\nup to 61% of cases. These findings underscore the necessity of integrating\nexplicit domain knowledge alongside HPO to create high quality synthetic\ndatasets. Our study provides actionable recommendations for improving synthetic\ndata generation, with future research needed to refine metric selection and\nvalidate these findings on larger datasets to enhance clinical applicability.", "AI": {"tldr": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u56db\u79cd\u8d85\u53c2\u6570\u4f18\u5316\u7b56\u7565\u5728\u516b\u79cd\u751f\u6210\u6a21\u578b\u4e0a\u7684\u6548\u679c\uff0c\u53d1\u73b0\u590d\u5408\u6307\u6807\u4f18\u5316\u4f18\u4e8e\u5355\u4e00\u6307\u6807\uff0c\u4e14\u540e\u5904\u7406\u548c\u9884\u5904\u7406\u5bf9\u4fdd\u8bc1\u4e34\u5e8a\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u89e3\u51b3\u5408\u6210\u4e34\u5e8a\u6570\u636e\u5728\u9690\u79c1\u4fdd\u62a4\u4e0e\u6570\u636e\u53ef\u7528\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u5408\u6210\u6570\u636e\u7684\u4fdd\u771f\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "method": "\u6bd4\u8f83\u4e86\u5355\u4e00\u6307\u6807\u4e0e\u590d\u5408\u6307\u6807\u4f18\u5316\u7b56\u7565\uff0c\u6d4b\u8bd5\u4e86\u516b\u79cd\u751f\u6210\u6a21\u578b\uff0c\u5e76\u8bc4\u4f30\u4e86\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u7684\u5f71\u54cd\u3002", "result": "HPO\u663e\u8457\u63d0\u5347\u6570\u636e\u8d28\u91cf\uff0cTVAE\u7b49\u6a21\u578b\u63d0\u5347\u8fbe60%\uff1b\u590d\u5408\u6307\u6807\u4f18\u5316\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u9700\u7ed3\u5408\u57df\u77e5\u8bc6\u786e\u4fdd\u4e34\u5e8a\u6709\u6548\u6027\u3002", "conclusion": "HPO\u7ed3\u5408\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u662f\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u4e34\u5e8a\u6570\u636e\u7684\u5173\u952e\uff0c\u672a\u6765\u9700\u4f18\u5316\u6307\u6807\u9009\u62e9\u5e76\u6269\u5927\u9a8c\u8bc1\u89c4\u6a21\u3002"}}
{"id": "2505.04993", "pdf": "https://arxiv.org/pdf/2505.04993", "abs": "https://arxiv.org/abs/2505.04993", "authors": ["Zhuocheng Gong", "Jian Guan", "Wei Wu", "Huishuai Zhang", "Dongyan Zhao"], "title": "Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success, yet aligning\ntheir generations with human preferences remains a critical challenge. Existing\napproaches to preference modeling often rely on an explicit or implicit reward\nfunction, overlooking the intricate and multifaceted nature of human\npreferences that may encompass conflicting factors across diverse tasks and\npopulations. To address this limitation, we introduce Latent Preference Coding\n(LPC), a novel framework that models the implicit factors as well as their\ncombinations behind holistic preferences using discrete latent codes. LPC\nseamlessly integrates with various offline alignment algorithms, automatically\ninferring the underlying factors and their importance from data without relying\non pre-defined reward functions and hand-crafted combination weights. Extensive\nexperiments on multiple benchmarks demonstrate that LPC consistently improves\nupon three alignment algorithms (DPO, SimPO, and IPO) using three base models\n(Mistral-7B, Llama3-8B, and Llama3-8B-Instruct). Furthermore, deeper analysis\nreveals that the learned latent codes effectively capture the differences in\nthe distribution of human preferences and significantly enhance the robustness\nof alignment against noise in data. By providing a unified representation for\nthe multifarious preference factors, LPC paves the way towards developing more\nrobust and versatile alignment techniques for the responsible deployment of\npowerful LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLPC\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u4eba\u7c7b\u504f\u597d\u7684\u9690\u542b\u56e0\u7d20\u53ca\u5176\u7ec4\u5408\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u5956\u52b1\u51fd\u6570\u7684\u5c40\u9650\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cLPC\u80fd\u663e\u8457\u63d0\u5347\u591a\u79cd\u5bf9\u9f50\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u5bf9\u6570\u636e\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u5efa\u6a21\u65b9\u6cd5\u591a\u4f9d\u8d56\u663e\u5f0f\u6216\u9690\u5f0f\u5956\u52b1\u51fd\u6570\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u504f\u597d\u7684\u590d\u6742\u6027\u4e0e\u591a\u9762\u6027\u3002\u9762\u5bf9\u8fd9\u4e00\u5c40\u9650\uff0c\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u80fd\u81ea\u52a8\u63a8\u65ad\u9690\u542b\u56e0\u7d20\u53ca\u5176\u7ec4\u5408\u7684\u6846\u67b6\uff0c\u4ece\u800c\u66f4\u5168\u9762\u5730\u6355\u6349\u4eba\u7c7b\u504f\u597d\u3002", "method": "\u901a\u8fc7\u79bb\u6563\u6f5c\u5728\u7f16\u7801\uff08LPC\uff09\u5efa\u6a21\u504f\u597d\u9690\u542b\u56e0\u7d20\u53ca\u5176\u7ec4\u5408\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u5b9a\u4e49\u5956\u52b1\u51fd\u6570\u6216\u624b\u52a8\u6743\u91cd\uff0c\u652f\u6301\u591a\u79cd\u79bb\u7ebf\u5bf9\u9f50\u7b97\u6cd5\uff08\u5982DPO\u3001SimPO\u3001IPO\uff09\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLPC\u663e\u8457\u63d0\u5347\u4e86\u4e09\u79cd\u5bf9\u9f50\u7b97\u6cd5\uff08DPO\u3001SimPO\u3001IPO\uff09\u7684\u6027\u80fd\uff0c\u4e14\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u7f16\u7801\u80fd\u6709\u6548\u533a\u5206\u504f\u597d\u5206\u5e03\u5dee\u5f02\uff0c\u589e\u5f3a\u5bf9\u9f50\u9c81\u68d2\u6027\u3002", "conclusion": "LPC\u4e3a\u591a\u5143\u504f\u597d\u56e0\u7d20\u63d0\u4f9b\u4e86\u7edf\u4e00\u8868\u5f81\uff0c\u4e3a\u5f00\u53d1\u66f4\u7a33\u5065\u3001\u901a\u7528\u7684LLM\u5bf9\u9f50\u6280\u672f\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u8d1f\u8d23\u4efb\u5730\u90e8\u7f72\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2505.05020", "pdf": "https://arxiv.org/pdf/2505.05020", "abs": "https://arxiv.org/abs/2505.05020", "authors": ["Ruwen Fulek", "Markus Lange-Hegermann"], "title": "Generative Models for Long Time Series: Approximately Equivariant Recurrent Network Structures for an Adjusted Training Scheme", "categories": ["cs.LG"], "comment": null, "summary": "We present a simple yet effective generative model for time series data based\non a Variational Autoencoder (VAE) with recurrent layers, referred to as the\nRecurrent Variational Autoencoder with Subsequent Training (RVAE-ST). Our\nmethod introduces an adapted training scheme that progressively increases the\nsequence length, addressing the challenge recurrent layers typically face when\nmodeling long sequences. By leveraging the recurrent architecture, the model\nmaintains a constant number of parameters regardless of sequence length. This\ndesign encourages approximate time-shift equivariance and enables efficient\nmodeling of long-range temporal dependencies. Rather than introducing a\nfundamentally new architecture, we show that a carefully composed combination\nof known components can match or outperform state-of-the-art generative models\non several benchmark datasets. Our model performs particularly well on time\nseries that exhibit quasi-periodic structure,while remaining competitive on\ndatasets with more irregular or partially non-stationary behavior. We evaluate\nits performance using ELBO, Fr\\'echet Distance, discriminative scores, and\nvisualizations of the learned embeddings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u548c\u6539\u8fdb\u8bad\u7ec3\u65b9\u6848\u7684\u65f6\u5e8f\u6570\u636e\u751f\u6210\u6a21\u578bRVAE-ST\uff0c\u901a\u8fc7\u9010\u6b65\u589e\u52a0\u5e8f\u5217\u957f\u5ea6\u7684\u8bad\u7ec3\u65b9\u6cd5\u63d0\u5347\u4e86\u957f\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\uff0c\u6027\u80fd\u5728\u591a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5ab2\u7f8e\u6216\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u9012\u5f52\u5c42\u5728\u5efa\u6a21\u957f\u5e8f\u5217\u65f6\u7684\u6311\u6218\uff0c\u5e76\u4fdd\u6301\u53c2\u6570\u91cf\u6052\u5b9a\uff0c\u540c\u65f6\u63d0\u5347\u5bf9\u957f\u7a0b\u65f6\u5e8f\u4f9d\u8d56\u7684\u5efa\u6a21\u6548\u7387\u3002", "method": "\u91c7\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u548c\u9012\u5f52\u5c42\uff0c\u5f15\u5165\u9010\u6b65\u589e\u52a0\u5e8f\u5217\u957f\u5ea6\u7684\u8bad\u7ec3\u65b9\u6848\uff08RVAE-ST\uff09\u3002", "result": "\u6a21\u578b\u5728\u51c6\u5468\u671f\u7ed3\u6784\u7684\u65f6\u5e8f\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u975e\u5e73\u7a33\u6216\u4e0d\u89c4\u5219\u6570\u636e\u96c6\u4e0a\u4ecd\u5177\u7ade\u4e89\u529b\uff0c\u8bc4\u4f30\u6307\u6807\u5305\u62ecELBO\u3001Fr\u00e9chet\u8ddd\u79bb\u7b49\u3002", "conclusion": "\u901a\u8fc7\u7ec4\u5408\u73b0\u6709\u7ec4\u4ef6\u5e76\u4f18\u5316\u8bad\u7ec3\u65b9\u6848\uff0cRVAE-ST\u5728\u65f6\u5e8f\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u9ad8\u6548\u6027\u548c\u7ade\u4e89\u529b\u3002"}}
{"id": "2505.04994", "pdf": "https://arxiv.org/pdf/2505.04994", "abs": "https://arxiv.org/abs/2505.04994", "authors": ["Lizhe Fang", "Yifei Wang", "Khashayar Gatmiry", "Lei Fang", "Yisen Wang"], "title": "Rethinking Invariance in In-context Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In-Context Learning (ICL) has emerged as a pivotal capability of\nauto-regressive large language models, yet it is hindered by a notable\nsensitivity to the ordering of context examples regardless of their mutual\nindependence. To address this issue, recent studies have introduced several\nvariant algorithms of ICL that achieve permutation invariance. However, many of\nthese do not exhibit comparable performance with the standard auto-regressive\nICL algorithm. In this work, we identify two crucial elements in the design of\nan invariant ICL algorithm: information non-leakage and context\ninterdependence, which are not simultaneously achieved by any of the existing\nmethods. These investigations lead us to the proposed Invariant ICL (InvICL), a\nmethodology designed to achieve invariance in ICL while ensuring the two\nproperties. Empirically, our findings reveal that InvICL surpasses previous\nmodels, both invariant and non-invariant, in most benchmark datasets,\nshowcasing superior generalization capabilities across varying input lengths.\nCode is available at https://github.com/PKU-ML/InvICL.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aInvICL\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e2d\u5bf9\u793a\u4f8b\u987a\u5e8f\u654f\u611f\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u786e\u4fdd\u4fe1\u606f\u4e0d\u6cc4\u6f0f\u548c\u4e0a\u4e0b\u6587\u76f8\u4e92\u4f9d\u8d56\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u81ea\u56de\u5f52\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u5b58\u5728\u5bf9\u793a\u4f8b\u987a\u5e8f\u7684\u654f\u611f\u6027\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u4fe1\u606f\u4e0d\u6cc4\u6f0f\u548c\u4e0a\u4e0b\u6587\u76f8\u4e92\u4f9d\u8d56\u6027\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86InvICL\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u4fe1\u606f\u4e0d\u6cc4\u6f0f\u548c\u4e0a\u4e0b\u6587\u76f8\u4e92\u4f9d\u8d56\u6027\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u6392\u5217\u4e0d\u53d8\u7684ICL\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cInvICL\u5728\u591a\u6570\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u4e0d\u53d8\u548c\u975e\u4e0d\u53d8\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u8f93\u5165\u957f\u5ea6\u4e0b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "InvICL\u901a\u8fc7\u540c\u65f6\u6ee1\u8db3\u4fe1\u606f\u4e0d\u6cc4\u6f0f\u548c\u4e0a\u4e0b\u6587\u76f8\u4e92\u4f9d\u8d56\u6027\uff0c\u6210\u529f\u89e3\u51b3\u4e86ICL\u4e2d\u7684\u987a\u5e8f\u654f\u611f\u6027\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.04650", "pdf": "https://arxiv.org/pdf/2505.04650", "abs": "https://arxiv.org/abs/2505.04650", "authors": ["Kapil Wanaskar", "Gaytri Jena", "Magdalini Eirinaki"], "title": "Multimodal Benchmarking and Recommendation of Text-to-Image Generation Models", "categories": ["cs.GR", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "This work presents an open-source unified benchmarking and evaluation\nframework for text-to-image generation models, with a particular focus on the\nimpact of metadata augmented prompts. Leveraging the DeepFashion-MultiModal\ndataset, we assess generated outputs through a comprehensive set of\nquantitative metrics, including Weighted Score, CLIP (Contrastive Language\nImage Pre-training)-based similarity, LPIPS (Learned Perceptual Image Patch\nSimilarity), FID (Frechet Inception Distance), and retrieval-based measures, as\nwell as qualitative analysis. Our results demonstrate that structured metadata\nenrichments greatly enhance visual realism, semantic fidelity, and model\nrobustness across diverse text-to-image architectures. While not a traditional\nrecommender system, our framework enables task-specific recommendations for\nmodel selection and prompt design based on evaluation metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5f00\u6e90\u7edf\u4e00\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u5143\u6570\u636e\u589e\u5f3a\u63d0\u793a\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u591a\u79cd\u5b9a\u91cf\u548c\u5b9a\u6027\u6307\u6807\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e0d\u540c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5143\u6570\u636e\u589e\u5f3a\u63d0\u793a\u5bf9\u751f\u6210\u6548\u679c\u7684\u5f71\u54cd\uff0c\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u6d4b\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u4e86DeepFashion-MultiModal\u6570\u636e\u96c6\uff0c\u5e76\u7ed3\u5408\u4e86\u591a\u79cd\u8bc4\u4ef7\u6307\u6807\uff0c\u5305\u62ecWeighted Score\u3001CLIP\u76f8\u4f3c\u5ea6\u3001LPIPS\u3001FID\u548c\u68c0\u7d22\u76f8\u5173\u6307\u6807\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u6784\u5316\u5143\u6570\u636e\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u771f\u5b9e\u6027\u3001\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u6a21\u578b\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u57fa\u4e8e\u8bc4\u6d4b\u6307\u6807\u7684\u4efb\u52a1\u7279\u5f02\u6027\u6a21\u578b\u9009\u62e9\u548c\u63d0\u793a\u8bbe\u8ba1\uff0c\u867d\u975e\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\uff0c\u4f46\u5177\u5907\u7c7b\u4f3c\u529f\u80fd\u3002"}}
{"id": "2505.05034", "pdf": "https://arxiv.org/pdf/2505.05034", "abs": "https://arxiv.org/abs/2505.05034", "authors": ["Wei Chen", "Shigui Li", "Jiacheng Li", "Junmei Yang", "John Paisley", "Delu Zeng"], "title": "Dequantified Diffusion Schr\u00f6dinger Bridge for Density Ratio Estimation", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Density ratio estimation is fundamental to tasks involving $f$-divergences,\nyet existing methods often fail under significantly different distributions or\ninadequately overlap supports, suffering from the \\textit{density-chasm} and\nthe \\textit{support-chasm} problems. Additionally, prior approaches yield\ndivergent time scores near boundaries, leading to instability. We propose\n$\\text{D}^3\\text{RE}$, a unified framework for robust and efficient density\nratio estimation. It introduces the Dequantified Diffusion-Bridge Interpolant\n(DDBI), which expands support coverage and stabilizes time scores via diffusion\nbridges and Gaussian dequantization. Building on DDBI, the Dequantified\nSchr\\\"odinger-Bridge Interpolant (DSBI) incorporates optimal transport to solve\nthe Schr\\\"odinger bridge problem, enhancing accuracy and efficiency. Our method\noffers uniform approximation and bounded time scores in theory, and outperforms\nbaselines empirically in mutual information and density estimation tasks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6 $\\text{D}^3\\text{RE}$\uff0c\u7528\u4e8e\u7a33\u5065\u4e14\u9ad8\u6548\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u5e03\u5dee\u5f02\u5927\u6216\u652f\u6491\u91cd\u53e0\u4e0d\u8db3\u65f6\u7684", "motivation": "\u73b0\u6709\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u65b9\u6cd5\u5728\u5206\u5e03\u5dee\u5f02\u8f83\u5927\u6216\u652f\u6491\u96c6\u91cd\u53e0\u4e0d\u8db3\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u53d7\u5230", "method": "\u63d0\u51fa\u4e86$\\text{D}^3\\text{RE}$\u6846\u67b6\uff0c\u5f15\u5165\u4e86Dequantified Diffusion-Bridge Interpolant (DDBI)\u548cDequantified Schr\\\"odinger-Bridge Interpolant (DSBI)\uff0c\u901a\u8fc7\u6269\u6563\u6865\u548c\u9ad8\u65af\u53bb\u91cf\u5316\u6269\u5c55\u652f\u6491\u96c6\u8986\u76d6\u5e76\u7a33\u5b9a\u65f6\u95f4\u5206\u6570\uff0c\u5e76\u7ed3\u5408\u6700\u4f18\u8f93\u8fd0\u89e3\u51b3Schr\\\"odinger\u6865\u95ee\u9898\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u5747\u5300\u903c\u8fd1\u548c\u6709\u754c\u65f6\u95f4\u5206\u6570\uff0c\u5b9e\u9a8c\u5728\u4e92\u4fe1\u606f\u548c\u5bc6\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "$\\text{D}^3\\text{RE}$\u6846\u67b6\u901a\u8fc7\u65b0\u7684\u63d2\u503c\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u7684\u7a33\u5065\u6027\u548c\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.05016", "pdf": "https://arxiv.org/pdf/2505.05016", "abs": "https://arxiv.org/abs/2505.05016", "authors": ["Cedric Waterschoot", "Nava Tintarev", "Francesco Barile"], "title": "The Pitfalls of Growing Group Complexity: LLMs and Social Choice-Based Aggregation for Group Recommendations", "categories": ["cs.CL", "cs.IR"], "comment": "To be published in: Adjunct Proceedings of the 33rd ACM Conference on\n  User Modeling, Adaptation and Personalization (UMAP Adjunct '25), June\n  16--19, 2025, New York City, NY, USA Accepted at the 4th Workshop on Group\n  Modeling, Adaptation and Personalization (GMAP), co-located at UMAP 2025", "summary": "Large Language Models (LLMs) are increasingly applied in recommender systems\naimed at both individuals and groups. Previously, Group Recommender Systems\n(GRS) often used social choice-based aggregation strategies to derive a single\nrecommendation based on the preferences of multiple people. In this paper, we\ninvestigate under which conditions language models can perform these strategies\ncorrectly based on zero-shot learning and analyse whether the formatting of the\ngroup scenario in the prompt affects accuracy. We specifically focused on the\nimpact of group complexity (number of users and items), different LLMs,\ndifferent prompting conditions, including In-Context learning or generating\nexplanations, and the formatting of group preferences. Our results show that\nperformance starts to deteriorate when considering more than 100 ratings.\nHowever, not all language models were equally sensitive to growing group\ncomplexity. Additionally, we showed that In-Context Learning (ICL) can\nsignificantly increase the performance at higher degrees of group complexity,\nwhile adding other prompt modifications, specifying domain cues or prompting\nfor explanations, did not impact accuracy. We conclude that future research\nshould include group complexity as a factor in GRS evaluation due to its effect\non LLM performance. Furthermore, we showed that formatting the group scenarios\ndifferently, such as rating lists per user or per item, affected accuracy. All\nin all, our study implies that smaller LLMs are capable of generating group\nrecommendations under the right conditions, making the case for using smaller\nmodels that require less computing power and costs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u57fa\u4e8e\u96f6\u6837\u672c\u5b66\u4e60\u7684\u7fa4\u4f53\u63a8\u8350\u7cfb\u7edf\uff08GRS\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u63a2\u7a76\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\uff08\u5982\u7fa4\u4f53\u590d\u6742\u6027\u3001\u63d0\u793a\u683c\u5f0f\u7b49\uff09\u6b63\u786e\u6267\u884c\u7fa4\u4f53\u63a8\u8350\u7b56\u7565\u7684\u80fd\u529b\uff0c\u4ee5\u4f18\u5316\u63a8\u8350\u7cfb\u7edf\u7684\u6548\u7387\u548c\u6210\u672c\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e0d\u540cLLMs\u5728\u7fa4\u4f53\u590d\u6742\u6027\uff08\u7528\u6237\u4e0e\u7269\u54c1\u6570\u91cf\uff09\u3001\u63d0\u793a\u6761\u4ef6\uff08\u5982\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u751f\u6210\u89e3\u91ca\u7b49\uff09\u548c\u504f\u597d\u683c\u5f0f\u4e0b\u7684\u51c6\u786e\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6027\u80fd\u5728\u8d85\u8fc7100\u4e2a\u8bc4\u5206\u65f6\u5f00\u59cb\u4e0b\u964d\uff0c\u4f46\u4e0d\u540c\u6a21\u578b\u5bf9\u590d\u6742\u6027\u589e\u957f\u7684\u654f\u611f\u6027\u4e0d\u540c\u3002\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u663e\u8457\u63d0\u9ad8\u9ad8\u590d\u6742\u6027\u4e0b\u7684\u6027\u80fd\uff0c\u800c\u5176\u4ed6\u63d0\u793a\u4fee\u6539\u5bf9\u51c6\u786e\u6027\u65e0\u5f71\u54cd\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u8003\u8651\u7fa4\u4f53\u590d\u6742\u6027\u5bf9LLM\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u8868\u660e\u5728\u9002\u5f53\u6761\u4ef6\u4e0b\uff0c\u5c0f\u578bLLMs\u53ef\u9ad8\u6548\u751f\u6210\u7fa4\u4f53\u63a8\u8350\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2505.05047", "pdf": "https://arxiv.org/pdf/2505.05047", "abs": "https://arxiv.org/abs/2505.05047", "authors": ["Azgar Ali Noor Ahamed"], "title": "Neural Pathways to Program Success: Hopfield Networks for PERT Analysis", "categories": ["cs.LG"], "comment": null, "summary": "Project and task scheduling under uncertainty remains a fundamental challenge\nin program and project management, where accurate estimation of task durations\nand dependencies is critical for delivering complex, multi project systems. The\nProgram Evaluation and Review Technique provides a probabilistic framework to\nmodel task variability and critical paths. In this paper, the author presents a\nnovel formulation of PERT scheduling as an energy minimization problem within a\nHopfield neural network architecture. By mapping task start times and\nprecedence constraints into a neural computation framework, the networks\ninherent optimization dynamics is exploited to approximate globally consistent\nschedules. The author addresses key theoretical issues related to energy\nfunction differentiability, constraint encoding, and convergence, and extends\nthe Hopfield model for structured precedence graphs. Numerical simulations on\nsynthetic project networks comprising up to 1000 tasks demonstrate the\nviability of this approach, achieving near optimal makespans with minimal\nconstraint violations. The findings suggest that neural optimization models\noffer a promising direction for scalable and adaptive project tasks scheduling\nunder uncertainty in areas such as the agentic AI workflows, microservice based\napplications that the modern AI systems are being built upon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06PERT\u8c03\u5ea6\u95ee\u9898\u8f6c\u5316\u4e3aHopfield\u795e\u7ecf\u7f51\u7edc\u80fd\u91cf\u6700\u5c0f\u5316\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u7684\u4f18\u5316\u52a8\u6001\u6027\u8fd1\u4f3c\u5168\u5c40\u4e00\u81f4\u7684\u8c03\u5ea6\u65b9\u6848\u3002", "motivation": "\u9879\u76ee\u7ba1\u7406\u4e2d\u7684\u4efb\u52a1\u8c03\u5ea6\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u4ecd\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u9700\u8981\u7cbe\u786e\u4f30\u8ba1\u4efb\u52a1\u6301\u7eed\u65f6\u95f4\u548c\u4f9d\u8d56\u5173\u7cfb\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u591a\u9879\u76ee\u7cfb\u7edf\u4e2d\u3002", "method": "\u4f5c\u8005\u5c06PERT\u8c03\u5ea6\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3aHopfield\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u80fd\u91cf\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u4efb\u52a1\u5f00\u59cb\u65f6\u95f4\u548c\u4f18\u5148\u7ea6\u675f\u6620\u5c04\u5230\u795e\u7ecf\u8ba1\u7b97\u6846\u67b6\u4e2d\uff0c\u5229\u7528\u7f51\u7edc\u7684\u4f18\u5316\u52a8\u6001\u6027\u8fd1\u4f3c\u5168\u5c40\u4e00\u81f4\u7684\u8c03\u5ea6\u65b9\u6848\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5305\u542b\u591a\u8fbe1000\u4e2a\u4efb\u52a1\u7684\u5408\u6210\u9879\u76ee\u7f51\u7edc\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u5b8c\u5de5\u65f6\u95f4\u4e14\u7ea6\u675f\u8fdd\u53cd\u6700\u5c0f\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u795e\u7ecf\u4f18\u5316\u6a21\u578b\u4e3a\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u53ef\u6269\u5c55\u548c\u81ea\u9002\u5e94\u9879\u76ee\u4efb\u52a1\u8c03\u5ea6\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u73b0\u4ee3AI\u7cfb\u7edf\u6784\u5efa\u4e2d\u7684\u4ee3\u7406AI\u5de5\u4f5c\u6d41\u548c\u5fae\u670d\u52a1\u5e94\u7528\u3002"}}
{"id": "2505.05017", "pdf": "https://arxiv.org/pdf/2505.05017", "abs": "https://arxiv.org/abs/2505.05017", "authors": ["Yuntai Bao", "Xuhong Zhang", "Tianyu Du", "Xinkui Zhao", "Jiang Zong", "Hao Peng", "Jianwei Yin"], "title": "Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization", "categories": ["cs.CL"], "comment": "9 pages, accepted by IJCAI 2025", "summary": "Pre-trained large language models (LLMs) are commonly fine-tuned to adapt to\ndownstream tasks. Since the majority of knowledge is acquired during\npre-training, attributing the predictions of fine-tuned LLMs to their\npre-training data may provide valuable insights. Influence functions have been\nproposed as a means to explain model predictions based on training data.\nHowever, existing approaches fail to compute ``multi-stage'' influence and lack\nscalability to billion-scale LLMs.\n  In this paper, we propose the multi-stage influence function to attribute the\ndownstream predictions of fine-tuned LLMs to pre-training data under the\nfull-parameter fine-tuning paradigm. To enhance the efficiency and practicality\nof our multi-stage influence function, we leverage Eigenvalue-corrected\nKronecker-Factored (EK-FAC) parameterization for efficient approximation.\nEmpirical results validate the superior scalability of EK-FAC approximation and\nthe effectiveness of our multi-stage influence function. Additionally, case\nstudies on a real-world LLM, dolly-v2-3b, demonstrate its interpretive power,\nwith exemplars illustrating insights provided by multi-stage influence\nestimates. Our code is public at\nhttps://github.com/colored-dye/multi_stage_influence_function.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u5f71\u54cd\u51fd\u6570\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u9884\u8bad\u7ec3\u6570\u636e\u5bf9\u5fae\u8c03\u540e\u5927\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u7684\u5f71\u54cd\u8fdb\u884c\u91cf\u5316\uff0c\u5e76\u901a\u8fc7EK-FAC\u53c2\u6570\u5316\u63d0\u5347\u6548\u7387\u3002\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u53ef\u6269\u5c55\u6027\u548c\u89e3\u91ca\u529b\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u56e0\u8ba1\u7b97\u80fd\u529b\u548c\u6548\u7387\u4e0d\u8db3\u800c\u65e0\u6cd5\u5728\u591a\u9636\u6bb5\u8bad\u7ec3\uff08\u5982\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\uff09\u7684LLMs\u4e2d\u8fdb\u884c\u6709\u6548\u5f71\u54cd\u5206\u6790\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u591a\u9636\u6bb5\u5f71\u54cd\u51fd\u6570\u5e76\u7ed3\u5408EK-FAC\u53c2\u6570\u5316\u9ad8\u6548\u8fd1\u4f3c\u8ba1\u7b97\uff0c\u4ee5\u9002\u7528\u4e8e\u5168\u53c2\u6570\u5fae\u8c03\u7684LLMs\u3002\u5bf9dolly-v2-3b\u7b49\u6a21\u578b\u7684\u6848\u4f8b\u5206\u6790\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "result": "EK-FAC\u8fd1\u4f3c\u663e\u8457\u63d0\u5347\u4e86\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\uff0c\u591a\u9636\u6bb5\u5f71\u54cd\u51fd\u6570\u80fd\u6709\u6548\u8ffd\u8e2a\u9884\u8bad\u7ec3\u6570\u636e\u5bf9\u4e0b\u6e38\u4efb\u52a1\u9884\u6d4b\u7684\u5f71\u54cd\u3002\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u5728\u89e3\u91ca\u6a21\u578b\u884c\u4e3a\u4e0a\u7684\u5b9e\u9645\u4ef7\u503c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u9884\u8bad\u7ec3\u6570\u636e\u5bf9\u5fae\u8c03\u540e\u7684LLMs\u9884\u6d4b\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u5206\u6790\u5de5\u5177\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u900f\u660e\u5ea6\u548c\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.04664", "pdf": "https://arxiv.org/pdf/2505.04664", "abs": "https://arxiv.org/abs/2505.04664", "authors": ["Ziyuan Huang", "Kevin Huggins", "Srikar Bellur"], "title": "Advancing 3D Medical Image Segmentation: Unleashing the Potential of Planarian Neural Networks in Artificial Intelligence", "categories": ["eess.IV", "cs.AI", "cs.CV", "68T07"], "comment": "36 pages, 8 figures, 21 tables", "summary": "Our study presents PNN-UNet as a method for constructing deep neural networks\nthat replicate the planarian neural network (PNN) structure in the context of\n3D medical image data. Planarians typically have a cerebral structure\ncomprising two neural cords, where the cerebrum acts as a coordinator, and the\nneural cords serve slightly different purposes within the organism's\nneurological system. Accordingly, PNN-UNet comprises a Deep-UNet and a\nWide-UNet as the nerve cords, with a densely connected autoencoder performing\nthe role of the brain. This distinct architecture offers advantages over both\nmonolithic (UNet) and modular networks (Ensemble-UNet). Our outcomes on a 3D\nMRI hippocampus dataset, with and without data augmentation, demonstrate that\nPNN-UNet outperforms the baseline UNet and several other UNet variants in image\nsegmentation.", "AI": {"tldr": "PNN-UNet\u662f\u4e00\u79cd\u6a21\u4eff\u6da1\u866b\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e3D\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfUNet\u53ca\u5176\u53d8\u4f53\u3002", "motivation": "\u53d7\u6da1\u866b\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u7684\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u66f4\u9ad8\u6548\u76843D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7f51\u7edc\u3002", "method": "\u7ed3\u5408Deep-UNet\u548cWide-UNet\u6a21\u62df\u795e\u7ecf\u7d22\uff0c\u5e76\u7528\u5bc6\u96c6\u8fde\u63a5\u81ea\u7f16\u7801\u5668\u5145\u5f53\u5927\u8111\u89d2\u8272\u3002", "result": "\u57283D MRI\u6d77\u9a6c\u4f53\u6570\u636e\u96c6\u4e0a\uff0cPNN-UNet\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebfUNet\u548c\u5176\u4ed6\u53d8\u4f53\u3002", "conclusion": "PNN-UNet\u5c55\u793a\u4e86\u751f\u7269\u542f\u53d1\u67b6\u6784\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.05063", "pdf": "https://arxiv.org/pdf/2505.05063", "abs": "https://arxiv.org/abs/2505.05063", "authors": ["Manik Sheokand", "Parth Sawant"], "title": "CodeMixBench: Evaluating Large Language Models on Code Generation with Code-Mixed Prompts", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in code\ngeneration tasks, powering various applications like code completion,\ndebugging, and programming assistance. However, existing benchmarks such as\nHumanEval, MBPP, and BigCodeBench primarily evaluate LLMs on English-only\nprompts, overlooking the real-world scenario where multilingual developers\noften use code-mixed language while interacting with LLMs. To address this gap,\nwe introduce CodeMixBench, a novel benchmark designed to evaluate the\nrobustness of LLMs on code generation from code-mixed prompts. Built upon\nBigCodeBench, CodeMixBench introduces controlled code-mixing (CMD) into the\nnatural language parts of prompts across three language pairs: Hinglish\n(Hindi-English), Spanish-English, and Chinese Pinyin-English. We\ncomprehensively evaluate a diverse set of open-source code generation models\nranging from 1.5B to 15B parameters. Our results show that code-mixed prompts\nconsistently degrade Pass@1 performance compared to their English-only\ncounterparts, with performance drops increasing under higher CMD levels for\nsmaller models. CodeMixBench provides a realistic evaluation framework for\nstudying multilingual code generation and highlights new challenges and\ndirections for building robust code generation models that generalize well\nacross diverse linguistic settings.", "AI": {"tldr": "CodeMixBench\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5728\u591a\u8bed\u8a00\u6df7\u5408\u63d0\u793a\u4e0b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u7ed3\u679c\u8868\u660e\u6df7\u5408\u63d0\u793a\u4f1a\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u5728\u82f1\u8bed\u63d0\u793a\u4e0b\u8bc4\u4f30LLMs\uff0c\u5ffd\u7565\u4e86\u591a\u8bed\u8a00\u5f00\u53d1\u8005\u5b9e\u9645\u4f7f\u7528\u6df7\u5408\u8bed\u8a00\u4e0eLLMs\u4ea4\u4e92\u7684\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8eBigCodeBench\uff0cCodeMixBench\u5728\u4e09\u4e2a\u8bed\u8a00\u5bf9\uff08Hinglish\u3001\u897f\u73ed\u7259\u8bed-\u82f1\u8bed\u3001\u4e2d\u6587\u62fc\u97f3-\u82f1\u8bed\uff09\u7684\u81ea\u7136\u8bed\u8a00\u90e8\u5206\u5f15\u5165\u53d7\u63a7\u4ee3\u7801\u6df7\u5408\uff08CMD\uff09\uff0c\u8bc4\u4f30\u591a\u79cd\u5f00\u6e90\u4ee3\u7801\u751f\u6210\u6a21\u578b\uff081.5B\u81f315B\u53c2\u6570\uff09\u3002", "result": "\u6df7\u5408\u63d0\u793a\u4e0bPass@1\u6027\u80fd\u666e\u904d\u4e0b\u964d\uff0c\u4e14\u5c0f\u578b\u6a21\u578b\u5728\u9ad8CMD\u6c34\u5e73\u4e0b\u6027\u80fd\u4e0b\u964d\u66f4\u660e\u663e\u3002", "conclusion": "CodeMixBench\u4e3a\u591a\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u63ed\u793a\u4e86\u6784\u5efa\u8de8\u8bed\u8a00\u9c81\u68d2\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u65b0\u6311\u6218\u548c\u65b9\u5411\u3002"}}
{"id": "2505.05026", "pdf": "https://arxiv.org/pdf/2505.05026", "abs": "https://arxiv.org/abs/2505.05026", "authors": ["Jaehyun Jeon", "Janghan Yoon", "Minsoo Kim", "Sumin Shim", "Yejin Choi", "Hanbin Kim", "Youngjae Yu"], "title": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness", "categories": ["cs.CL", "cs.LG"], "comment": "31 pages, 17 figures", "summary": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics\nto influencing user behavior, a principle central to Design Persuasiveness. A/B\ntesting is the predominant method for determining which UI variations drive\nhigher user engagement, but it is costly and time-consuming. While recent\nVision-Language Models (VLMs) can process automated UI analysis, current\napproaches focus on isolated design attributes rather than comparative\npersuasiveness-the key factor in optimizing user interactions. To address this,\nwe introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design\nPersuasiveness Assessment task, featuring 300 real-world UI image pairs labeled\nwith A/B test results and expert rationales. Additionally, we propose G-FOCUS,\na novel inference-time reasoning strategy that enhances VLM-based\npersuasiveness assessment by reducing position bias and improving evaluation\naccuracy. Experimental results show that G-FOCUS surpasses existing inference\nstrategies in consistency and accuracy for pairwise UI evaluation. Through\npromoting VLM-driven evaluation of UI persuasiveness, our work offers an\napproach to complement A/B testing, propelling progress in scalable UI\npreference modeling and design optimization. Code and data will be released\npublicly.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWiserUI-Bench\u7684\u57fa\u51c6\u6d4b\u8bd5\u548cG-FOCUS\u63a8\u7406\u7b56\u7565\uff0c\u65e8\u5728\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u7528\u6237\u754c\u9762\uff08UI\uff09\u8bbe\u8ba1\u8bf4\u670d\u529b\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u5f25\u8865\u4f20\u7edfA/B\u6d4b\u8bd5\u7684\u9ad8\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684A/B\u6d4b\u8bd5\u8bc4\u4f30UI\u8bbe\u8ba1\u8bf4\u670d\u529b\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u800c\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u6bd4\u8f83\u8bbe\u8ba1\u7684\u8bf4\u670d\u529b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faWiserUI-Bench\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u542b300\u7ec4\u771f\u5b9eUI\u56fe\u7247\u5bf9\u548c\u6807\u6ce8\u6570\u636e\uff09\uff0c\u5e76\u8bbe\u8ba1G-FOCUS\u63a8\u7406\u7b56\u7565\u4ee5\u51cf\u5c11\u4f4d\u7f6e\u504f\u5dee\u3001\u63d0\u5347\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cG-FOCUS\u5728\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u63a8\u7406\u7b56\u7565\uff0c\u4e3aUI\u8bf4\u670d\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5de5\u5177\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u51cf\u5c11\u5bf9A/B\u6d4b\u8bd5\u7684\u4f9d\u8d56\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u53ef\u6269\u5c55\u7684UI\u504f\u597d\u5efa\u6a21\u548c\u8bbe\u8ba1\u4f18\u5316\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.05064", "pdf": "https://arxiv.org/pdf/2505.05064", "abs": "https://arxiv.org/abs/2505.05064", "authors": ["Xinyang Lu", "Xinyuan Niu", "Gregory Kang Ruey Lau", "Bui Thi Cam Nhung", "Rachael Hwee Ling Sim", "Fanyu Wen", "Chuan-Sheng Foo", "See-Kiong Ng", "Bryan Kian Hsiang Low"], "title": "WaterDrum: Watermarking for Data-centric Unlearning Metric", "categories": ["cs.LG"], "comment": null, "summary": "Large language model (LLM) unlearning is critical in real-world applications\nwhere it is necessary to efficiently remove the influence of private,\ncopyrighted, or harmful data from some users. However, existing utility-centric\nunlearning metrics (based on model utility) may fail to accurately evaluate the\nextent of unlearning in realistic settings such as when (a) the forget and\nretain set have semantically similar content, (b) retraining the model from\nscratch on the retain set is impractical, and/or (c) the model owner can\nimprove the unlearning metric without directly performing unlearning on the\nLLM. This paper presents the first data-centric unlearning metric for LLMs\ncalled WaterDrum that exploits robust text watermarking for overcoming these\nlimitations. We also introduce new benchmark datasets for LLM unlearning that\ncontain varying levels of similar data points and can be used to rigorously\nevaluate unlearning algorithms using WaterDrum. Our code is available at\nhttps://github.com/lululu008/WaterDrum and our new benchmark datasets are\nreleased at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u6570\u636e\u4e2d\u5fc3\u7684LLM\u9057\u5fd8\u5ea6\u91cfWaterDrum\uff0c\u5229\u7528\u9c81\u68d2\u6587\u672c\u6c34\u5370\u514b\u670d\u73b0\u6709\u5b9e\u7528\u6307\u6807\u7f3a\u9677\uff0c\u5e76\u53d1\u5e03\u65b0\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u5ea6\u91cf\u5728\u5b9e\u7528\u573a\u666f\uff08\u5982\u9057\u5fd8\u4e0e\u4fdd\u7559\u6570\u636e\u8bed\u4e49\u76f8\u4f3c\u3001\u65e0\u6cd5\u91cd\u65b0\u8bad\u7ec3\u6216\u6307\u6807\u88ab\u64cd\u7eb5\uff09\u4e0b\u53ef\u80fd\u4e0d\u51c6\u786e\uff0c\u9700\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faWaterDrum\uff0c\u57fa\u4e8e\u6587\u672c\u6c34\u5370\u6280\u672f\u8bbe\u8ba1\u6570\u636e\u4e2d\u5fc3\u7684\u9057\u5fd8\u5ea6\u91cf\uff0c\u5e76\u521b\u5efa\u5305\u542b\u4e0d\u540c\u76f8\u4f3c\u5ea6\u6570\u636e\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "WaterDrum\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u57fa\u51c6\u6570\u636e\u96c6\u652f\u6301\u7b97\u6cd5\u4e25\u683c\u8bc4\u4f30\u3002", "conclusion": "WaterDrum\u4e3aLLM\u9057\u5fd8\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u6570\u636e\u96c6\u7684\u53d1\u5e03\u6709\u52a9\u4e8e\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.05040", "pdf": "https://arxiv.org/pdf/2505.05040", "abs": "https://arxiv.org/abs/2505.05040", "authors": ["Mat\u012bss Rikters", "Edison Marrese-Taylor"], "title": "Image-Text Relation Prediction for Multilingual Tweets", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Various social networks have been allowing media uploads for over a decade\nnow. Still, it has not always been clear what is their relation with the posted\ntext or even if there is any at all. In this work, we explore how multilingual\nvision-language models tackle the task of image-text relation prediction in\ndifferent languages, and construct a dedicated balanced benchmark data set from\nTwitter posts in Latvian along with their manual translations into English. We\ncompare our results to previous work and show that the more recently released\nvision-language model checkpoints are becoming increasingly capable at this\ntask, but there is still much room for further improvement.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u5904\u7406\u56fe\u50cf-\u6587\u672c\u5173\u7cfb\u9884\u6d4b\u7684\u80fd\u529b\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u62c9\u8131\u7ef4\u4e9a\u8bed\u548c\u82f1\u8bed\u7684\u53cc\u8bed\u57fa\u51c6\u6570\u636e\u96c6\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8fd1\u671f\u53d1\u5e03\u7684\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u63a2\u7d22\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf-\u6587\u672c\u5173\u7cfb\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5c24\u5176\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5e73\u8861\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u62c9\u8131\u7ef4\u4e9a\u8bed\u7684Twitter\u5e16\u5b50\u53ca\u5176\u82f1\u8bed\u4eba\u5de5\u7ffb\u8bd1\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u8fd1\u671f\u53d1\u5e03\u7684\u6a21\u578b\u5728\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u4ecd\u672a\u5b8c\u5168\u89e3\u51b3\u95ee\u9898\u3002", "conclusion": "\u867d\u7136\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf-\u6587\u672c\u5173\u7cfb\u9884\u6d4b\u4efb\u52a1\u4e0a\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2505.05082", "pdf": "https://arxiv.org/pdf/2505.05082", "abs": "https://arxiv.org/abs/2505.05082", "authors": ["Sagnik Bhattacharya", "Abhiram R. Gorle", "Ahmed Mohsin", "Ahsan Bilal", "Connor Ding", "Amit Kumar Singh Yadav", "Tsachy Weissman"], "title": "ItDPDM: Information-Theoretic Discrete Poisson Diffusion Model", "categories": ["cs.LG", "cs.IT", "math.IT", "math.PR"], "comment": "Pre-print", "summary": "Existing methods for generative modeling of discrete data, such as symbolic\nmusic tokens, face two primary challenges: (1) they either embed discrete\ninputs into continuous state-spaces or (2) rely on variational losses that only\napproximate the true negative log-likelihood. Previous efforts have\nindividually targeted these limitations. While information-theoretic Gaussian\ndiffusion models alleviate the suboptimality of variational losses, they still\nperform modeling in continuous domains. In this work, we introduce the\nInformation-Theoretic Discrete Poisson Diffusion Model (ItDPDM), which\nsimultaneously addresses both limitations by directly operating in a discrete\nstate-space via a Poisson diffusion process inspired by photon arrival\nprocesses in camera sensors. We introduce a novel Poisson Reconstruction Loss\n(PRL) and derive an exact relationship between PRL and the true negative\nlog-likelihood, thereby eliminating the need for approximate evidence lower\nbounds. Experiments conducted on the Lakh MIDI symbolic music dataset and the\nCIFAR-10 image benchmark demonstrate that ItDPDM delivers significant\nimprovements, reducing test NLL by up to 80% compared to prior baselines, while\nalso achieving faster convergence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6cca\u677e\u6269\u6563\u8fc7\u7a0b\u7684\u4fe1\u606f\u7406\u8bba\u79bb\u6563\u5316\u65b9\u6cd5ItDPDM\uff0c\u76f4\u63a5\u5904\u7406\u79bb\u6563\u6570\u636e\uff0c\u65e0\u9700\u8fde\u7eed\u7a7a\u95f4\u8f6c\u6362\u6216\u8fd1\u4f3c\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u8d1f\u5bf9\u6570\u4f3c\u7136\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u79bb\u6563\u6570\u636e\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u9700\u5c06\u79bb\u6563\u6570\u636e\u6620\u5c04\u5230\u8fde\u7eed\u7a7a\u95f4\uff0c\u8981\u4e48\u4f9d\u8d56\u8fd1\u4f3c\u8d1f\u5bf9\u6570\u4f3c\u7136\u7684\u53d8\u5206\u635f\u5931\uff0cItDPDM\u65e8\u5728\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6cca\u677e\u6269\u6563\u8fc7\u7a0b\u76f4\u63a5\u5728\u79bb\u6563\u7a7a\u95f4\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u65b0\u63d0\u51fa\u7684\u6cca\u677e\u91cd\u5efa\u635f\u5931\uff08PRL\uff09\u7cbe\u786e\u8ba1\u7b97\u8d1f\u5bf9\u6570\u4f3c\u7136\uff0c\u907f\u514d\u8fd1\u4f3c\u635f\u5931\u3002", "result": "\u5728Lakh MIDI\u97f3\u4e50\u6570\u636e\u96c6\u548cCIFAR-10\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0cItDPDM\u5c06\u6d4b\u8bd5NLL\u964d\u4f4e\u9ad8\u8fbe80%\uff0c\u4e14\u6536\u655b\u66f4\u5feb\u3002", "conclusion": "ItDPDM\u901a\u8fc7\u79bb\u6563\u7a7a\u95f4\u7684\u76f4\u63a5\u5efa\u6a21\u548c\u7cbe\u786e\u635f\u5931\u8ba1\u7b97\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u4e3a\u79bb\u6563\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05056", "pdf": "https://arxiv.org/pdf/2505.05056", "abs": "https://arxiv.org/abs/2505.05056", "authors": ["Linrong Pan", "Chenglong Jiang", "Gaoze Hou", "Ying Gao"], "title": "Teochew-Wild: The First In-the-wild Teochew Dataset with Orthographic Annotations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper reports the construction of the Teochew-Wild, a speech corpus of\nthe Teochew dialect. The corpus includes 18.9 hours of in-the-wild Teochew\nspeech data from multiple speakers, covering both formal and colloquial\nexpressions, with precise orthographic and pinyin annotations. Additionally, we\nprovide supplementary text processing tools and resources to propel research\nand applications in speech tasks for this low-resource language, such as\nautomatic speech recognition (ASR) and text-to-speech (TTS). To the best of our\nknowledge, this is the first publicly available Teochew dataset with accurate\northographic annotations. We conduct experiments on the corpus, and the results\nvalidate its effectiveness in ASR and TTS tasks.", "AI": {"tldr": "\u6784\u5efa\u4e86\u9996\u4e2a\u5e26\u7cbe\u786e\u6807\u6ce8\u7684\u6f6e\u6c55\u8bdd\u8bed\u97f3\u6570\u636e\u96c6Teochew-Wild\uff0c\u5305\u542b18.9\u5c0f\u65f6\u771f\u5b9e\u73af\u5883\u8bed\u97f3\uff0c\u652f\u6301ASR\u548cTTS\u7814\u7a76\u3002", "motivation": "\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u6f6e\u6c55\u8bdd\u63d0\u4f9b\u8bed\u97f3\u6570\u636e\u548c\u5de5\u5177\uff0c\u63a8\u52a8\u5176\u8bed\u97f3\u4efb\u52a1\u7814\u7a76\u4e0e\u5e94\u7528\u3002", "method": "\u6536\u96c6\u591a\u8bf4\u8bdd\u8005\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u6f6e\u6c55\u8bdd\u8bed\u6599\uff0c\u63d0\u4f9b\u6b63\u5b57\u548c\u62fc\u97f3\u6807\u6ce8\uff0c\u5e76\u5f00\u53d1\u914d\u5957\u6587\u672c\u5904\u7406\u5de5\u5177\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8be5\u6570\u636e\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u548c\u6587\u672c\u8f6c\u8bed\u97f3\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "Teochew-Wild\u586b\u8865\u4e86\u6f6e\u6c55\u8bdd\u6570\u636e\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2505.04677", "pdf": "https://arxiv.org/pdf/2505.04677", "abs": "https://arxiv.org/abs/2505.04677", "authors": ["Julien Narboux", "Walther Neuper", "Pedro Quaresma"], "title": "Proceedings The 13th International Workshop on Theorem proving components for Educational software", "categories": ["cs.LO", "cs.AI", "cs.LG"], "comment": null, "summary": "The ThEdu series pursues the smooth transition from an intuitive way of doing\nmathematics at secondary school to a more formal approach to the subject in\nSTEM education while favoring software support for this transition by\nexploiting the power of theorem-proving technologies. What follows is a brief\ndescription of how the present volume contributes to this enterprise. The 13th\nInternational Workshop on Theorem Proving Components for Educational Software\n(ThEdu'24), was a satellite event of the CADE29, part of IJCAR 2024, Nancy,\nFrance. ThEdu'24 was a vibrant workshop, with one invited talk by Jeremy Avigad\n(Carnegie Mellon University) and 14 submitted talks. An open call for papers\nwas then issued and attracted 9 submissions. Eight of those submissions have\nbeen accepted by our reviewers. The resulting revised papers are collected in\nthe present volume. The contributions in this volume are a faithful\nrepresentation of the wide spectrum of ThEdu, ranging from those more focused\non the automated deduction research, not losing track of the possible\napplications in an educational setting, to those focused on the applications,\nin educational settings, of automated deduction tools and methods. We, the\nvolume editors, hope that this collection of papers will further promote the\ndevelopment of theorem-proving-based software and that it will allow to improve\nthe mutual understanding between computer scientists, mathematicians, and\nstakeholders in education. While this volume goes to press, the next edition of\nthe ThEdu workshop is being prepared: ThEdu'25 will be a satellite event of the\n30th international Conference on Automated DEduction (CADE-30), July 28th -\nAugust 2nd, 2025, Stuttgart, Germany.", "AI": {"tldr": "ThEdu\u7cfb\u5217\u65e8\u5728\u901a\u8fc7\u5b9a\u7406\u8bc1\u660e\u6280\u672f\u4fc3\u8fdb\u4e2d\u5b66\u6570\u5b66\u76f4\u89c9\u5f0f\u5b66\u4e60\u5411STEM\u6559\u80b2\u4e2d\u66f4\u4e25\u8c28\u65b9\u5f0f\u7684\u8fc7\u6e21\u3002\u7b2c13\u5c4aThEdu\u7814\u8ba8\u4f1a\u6c47\u96c6\u4e86\u76f8\u5173\u7814\u7a76\uff0c\u5e76\u8ba1\u5212\u7ee7\u7eed\u63a8\u52a8\u8fd9\u4e00\u9886\u57df\u7684\u5408\u4f5c\u4e0e\u53d1\u5c55\u3002", "motivation": "\u4fc3\u8fdb\u4e2d\u5b66\u6570\u5b66\u76f4\u89c9\u5b66\u4e60\u5411STEM\u6559\u80b2\u4e2d\u6b63\u5f0f\u65b9\u6cd5\u7684\u8fc7\u6e21\uff0c\u5e76\u5229\u7528\u5b9a\u7406\u8bc1\u660e\u6280\u672f\u4e3a\u8fd9\u4e00\u8fc7\u7a0b\u63d0\u4f9b\u8f6f\u4ef6\u652f\u6301\u3002", "method": "\u901a\u8fc7\u56fd\u9645\u7814\u8ba8\u4f1a\uff08ThEdu'24\uff09\u6536\u96c6\u5e76\u7b5b\u9009\u76f8\u5173\u7814\u7a76\u8bba\u6587\uff0c\u6db5\u76d6\u4ece\u81ea\u52a8\u5316\u63a8\u5bfc\u7814\u7a76\u5230\u6559\u80b2\u5e94\u7528\u7684\u591a\u65b9\u9762\u5185\u5bb9\u3002", "result": "\u4f1a\u8bae\u6536\u52309\u7bc7\u6295\u7a3f\uff0c\u5176\u4e2d8\u7bc7\u88ab\u63a5\u53d7\u5e76\u6c47\u7f16\u6210\u518c\u3002\u8fd9\u4e9b\u8bba\u6587\u53cd\u6620\u4e86ThEdu\u9886\u57df\u7684\u5e7f\u6cdb\u7814\u7a76\u65b9\u5411\u548c\u5e94\u7528\u5b9e\u8df5\u3002", "conclusion": "\u8be5\u8bba\u6587\u96c6\u6709\u671b\u63a8\u52a8\u57fa\u4e8e\u5b9a\u7406\u8bc1\u660e\u7684\u8f6f\u4ef6\u5f00\u53d1\uff0c\u5e76\u4fc3\u8fdb\u8ba1\u7b97\u673a\u79d1\u5b66\u5bb6\u3001\u6570\u5b66\u5bb6\u548c\u6559\u80b2\u5de5\u4f5c\u8005\u7684\u8fdb\u4e00\u6b65\u5408\u4f5c\u3002ThEdu'25\u5c06\u7ee7\u7eed\u8fd9\u4e00\u4f7f\u547d\u3002"}}
{"id": "2505.05086", "pdf": "https://arxiv.org/pdf/2505.05086", "abs": "https://arxiv.org/abs/2505.05086", "authors": ["Le-Trung Nguyen", "Ael Quelennec", "Van-Tam Nguyen", "Enzo Tartaglione"], "title": "Beyond Low-rank Decomposition: A Shortcut Approach for Efficient On-Device Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "On-device learning has emerged as a promising direction for AI development,\nparticularly because of its potential to reduce latency issues and mitigate\nprivacy risks associated with device-server communication, while improving\nenergy efficiency. Despite these advantages, significant memory and\ncomputational constraints still represent major challenges for its deployment.\nDrawing on previous studies on low-rank decomposition methods that address\nactivation memory bottlenecks in backpropagation, we propose a novel shortcut\napproach as an alternative. Our analysis and experiments demonstrate that our\nmethod can reduce activation memory usage, even up to $120.09\\times$ compared\nto vanilla training, while also reducing overall training FLOPs up to\n$1.86\\times$ when evaluated on traditional benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5728\u8bbe\u5907\u4e0a\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u6fc0\u6d3b\u5185\u5b58\u4f7f\u7528\u548c\u8ba1\u7b97\u91cf\u6765\u514b\u670d\u5185\u5b58\u548c\u8ba1\u7b97\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u8bbe\u5907\u4e0a\u5b66\u4e60\u56e0\u5176\u4f4e\u5ef6\u8fdf\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u80fd\u6e90\u6548\u7387\u7684\u4f18\u52bf\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u5185\u5b58\u548c\u8ba1\u7b97\u9650\u5236\u4ecd\u662f\u4e3b\u8981\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u65b0\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5feb\u6377\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4f4e\u79e9\u5206\u89e3\u6280\u672f\uff0c\u4e13\u95e8\u9488\u5bf9\u53cd\u5411\u4f20\u64ad\u4e2d\u7684\u6fc0\u6d3b\u5185\u5b58\u74f6\u9888\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u5c06\u6fc0\u6d3b\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u9ad8\u8fbe120.09\u500d\uff0c\u8bad\u7ec3FLOPs\u51cf\u5c111.86\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8bbe\u5907\u4e0a\u5b66\u4e60\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u9650\u5236\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.05070", "pdf": "https://arxiv.org/pdf/2505.05070", "abs": "https://arxiv.org/abs/2505.05070", "authors": ["Ajwad Abrar", "Farzana Tabassum", "Sabbir Ahmed"], "title": "Performance Evaluation of Large Language Models in Bangla Consumer Health Query Summarization", "categories": ["cs.CL"], "comment": null, "summary": "Consumer Health Queries (CHQs) in Bengali (Bangla), a low-resource language,\noften contain extraneous details, complicating efficient medical responses.\nThis study investigates the zero-shot performance of nine advanced large\nlanguage models (LLMs): GPT-3.5-Turbo, GPT-4, Claude-3.5-Sonnet,\nLlama3-70b-Instruct, Mixtral-8x22b-Instruct, Gemini-1.5-Pro,\nQwen2-72b-Instruct, Gemma-2-27b, and Athene-70B, in summarizing Bangla CHQs.\nUsing the BanglaCHQ-Summ dataset comprising 2,350 annotated query-summary\npairs, we benchmarked these LLMs using ROUGE metrics against Bangla T5, a\nfine-tuned state-of-the-art model. Mixtral-8x22b-Instruct emerged as the top\nperforming model in ROUGE-1 and ROUGE-L, while Bangla T5 excelled in ROUGE-2.\nThe results demonstrate that zero-shot LLMs can rival fine-tuned models,\nachieving high-quality summaries even without task-specific training. This work\nunderscores the potential of LLMs in addressing challenges in low-resource\nlanguages, providing scalable solutions for healthcare query summarization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u4e5d\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u60c5\u51b5\u4e0b\u5bf9\u5b5f\u52a0\u62c9\u8bed\u6d88\u8d39\u8005\u5065\u5eb7\u67e5\u8be2\u7684\u6458\u8981\u6548\u679c\uff0c\u53d1\u73b0Mixtral-8x22b-Instruct\u5728ROUGE-1\u548cROUGE-L\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4e0e\u5fae\u8c03\u7684Bangla T5\u5ab2\u7f8e\uff0c\u8868\u660e\u96f6\u6837\u672c\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u7684\u4f4e\u8d44\u6e90\u7279\u6027\u4f7f\u5f97\u5176\u5065\u5eb7\u67e5\u8be2\u6458\u8981\u4efb\u52a1\u590d\u6742\u5316\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u96f6\u6837\u672c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u63d0\u4f9b\u9ad8\u6548\u4e14\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528BanglaCHQ-Summ\u6570\u636e\u96c6\uff082,350\u5bf9\u6807\u6ce8\u6570\u636e\uff09\uff0c\u8bc4\u6d4b\u4e5d\u79cdLLMs\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5e76\u4ee5ROUGE\u6307\u6807\u5bf9\u6bd4\u5fae\u8c03\u7684Bangla T5\u6a21\u578b\u3002", "result": "Mixtral-8x22b-Instruct\u5728ROUGE-1\u548cROUGE-L\u4e2d\u9886\u5148\uff0cBangla T5\u5728ROUGE-2\u4e0a\u6700\u4f18\uff0c\u96f6\u6837\u672cLLMs\u6574\u4f53\u8868\u73b0\u63a5\u8fd1\u5fae\u8c03\u6a21\u578b\u3002", "conclusion": "\u96f6\u6837\u672cLLMs\u80fd\u6709\u6548\u5904\u7406\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5065\u5eb7\u67e5\u8be2\u6458\u8981\uff0c\u4e3a\u533b\u7597\u9886\u57df\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05094", "pdf": "https://arxiv.org/pdf/2505.05094", "abs": "https://arxiv.org/abs/2505.05094", "authors": ["Leming Zhou", "Zuo Wang", "Zhixuan Duan"], "title": "A Conjoint Graph Representation Learning Framework for Hypertension Comorbidity Risk Prediction", "categories": ["cs.LG"], "comment": null, "summary": "The comorbidities of hypertension impose a heavy burden on patients and\nsociety. Early identification is necessary to prompt intervention, but it\nremains a challenging task. This study aims to address this challenge by\ncombining joint graph learning with network analysis. Motivated by this\ndiscovery, we develop a Conjoint Graph Representation Learning (CGRL) framework\nthat: a) constructs two networks based on disease coding, including the patient\nnetwork and the disease difference network. Three comorbidity network features\nwere generated based on the basic difference network to capture the potential\nrelationship between comorbidities and risk diseases; b) incorporates\ncomputational structure intervention and learning feature representation, CGRL\nwas developed to predict the risks of diabetes and coronary heart disease in\npatients; and c) analysis the comorbidity patterns and exploring the pathways\nof disease progression, the pathological pathogenesis of diabetes and coronary\nheart disease may be revealed. The results show that the network features\nextracted based on the difference network are important, and the framework we\nproposed provides more accurate predictions than other strong models in terms\nof accuracy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8054\u5408\u56fe\u5b66\u4e60\u548c\u7f51\u7edc\u5206\u6790\u7684\u6846\u67b6\uff08CGRL\uff09\uff0c\u7528\u4e8e\u65e9\u671f\u8bc6\u522b\u9ad8\u8840\u538b\u5e76\u53d1\u75c7\u98ce\u9669\uff0c\u901a\u8fc7\u6784\u5efa\u60a3\u8005\u7f51\u7edc\u548c\u75be\u75c5\u5dee\u5f02\u7f51\u7edc\uff0c\u63d0\u53d6\u7f51\u7edc\u7279\u5f81\u5e76\u9884\u6d4b\u7cd6\u5c3f\u75c5\u548c\u51a0\u5fc3\u75c5\u7684\u98ce\u9669\uff0c\u7ed3\u679c\u663e\u793a\u5176\u9884\u6d4b\u51c6\u786e\u6027\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u9ad8\u8840\u538b\u5e76\u53d1\u75c7\u5bf9\u60a3\u8005\u548c\u793e\u4f1a\u9020\u6210\u6c89\u91cd\u8d1f\u62c5\uff0c\u65e9\u671f\u8bc6\u522b\u5e76\u5e72\u9884\u662f\u8feb\u5207\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86CGRL\u6846\u67b6\uff0c\u5305\u62ec\u6784\u5efa\u60a3\u8005\u7f51\u7edc\u548c\u75be\u75c5\u5dee\u5f02\u7f51\u7edc\u3001\u751f\u6210\u7f51\u7edc\u7279\u5f81\u3001\u7ed3\u5408\u7ed3\u6784\u5e72\u9884\u548c\u5b66\u4e60\u7279\u5f81\u8868\u793a\u4ee5\u9884\u6d4b\u7cd6\u5c3f\u75c5\u548c\u51a0\u5fc3\u75c5\u98ce\u9669\u3002", "result": "\u57fa\u4e8e\u5dee\u5f02\u7f51\u7edc\u63d0\u53d6\u7684\u7279\u5f81\u81f3\u5173\u91cd\u8981\uff0cCGRL\u6846\u67b6\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u5176\u4ed6\u5f3a\u6a21\u578b\u3002", "conclusion": "CGRL\u6846\u67b6\u4e0d\u4ec5\u80fd\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8fd8\u80fd\u63ed\u793a\u7cd6\u5c3f\u75c5\u548c\u51a0\u5fc3\u75c5\u7684\u75be\u75c5\u8fdb\u5c55\u8def\u5f84\u548c\u75c5\u7406\u673a\u5236\u3002"}}
{"id": "2505.05084", "pdf": "https://arxiv.org/pdf/2505.05084", "abs": "https://arxiv.org/abs/2505.05084", "authors": ["Xiaowei Zhu", "Yubing Ren", "Yanan Cao", "Xixun Lin", "Fang Fang", "Yangxi Li"], "title": "Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models has raised significant\nconcerns regarding their potential misuse by malicious actors. As a result,\ndeveloping effective detectors to mitigate these risks has become a critical\npriority. However, most existing detection methods focus excessively on\ndetection accuracy, often neglecting the societal risks posed by high false\npositive rates (FPRs). This paper addresses this issue by leveraging Conformal\nPrediction (CP), which effectively constrains the upper bound of FPRs. While\ndirectly applying CP constrains FPRs, it also leads to a significant reduction\nin detection performance. To overcome this trade-off, this paper proposes a\nZero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal\nPrediction (MCP), which both enforces the FPR constraint and improves detection\nperformance. This paper also introduces RealDet, a high-quality dataset that\nspans a wide range of domains, ensuring realistic calibration and enabling\nsuperior detection performance when combined with MCP. Empirical evaluations\ndemonstrate that MCP effectively constrains FPRs, significantly enhances\ndetection performance, and increases robustness against adversarial attacks\nacross multiple detectors and datasets.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u591a\u5c3a\u5ea6\u5171\u5f62\u9884\u6d4b\uff08MCP\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u56e0\u8fc7\u5ea6\u5173\u6ce8\u51c6\u786e\u6027\u800c\u5ffd\u89c6\u9ad8\u8bef\u62a5\u7387\uff08FPR\uff09\u793e\u4f1a\u98ce\u9669\u7684\u95ee\u9898\u3002MCP\u5728\u7ea6\u675fFPR\u7684\u540c\u65f6\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u7ed3\u5408\u9ad8\u8d28\u91cf\u6570\u636e\u96c6RealDet\u5b9e\u73b0\u4f18\u8d8a\u8868\u73b0\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u4e86\u5bf9\u6076\u610f\u6ee5\u7528\u7684\u62c5\u5fe7\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u8fc7\u4e8e\u5173\u6ce8\u51c6\u786e\u6027\u800c\u5ffd\u89c6\u4e86\u9ad8\u8bef\u62a5\u7387\u7684\u793e\u4f1a\u98ce\u9669\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u5e73\u8861FPR\u7ea6\u675f\u4e0e\u68c0\u6d4b\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u591a\u5c3a\u5ea6\u5171\u5f62\u9884\u6d4b\uff08MCP\uff09\u7684\u96f6\u6837\u672c\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u8d28\u91cf\u8de8\u9886\u57df\u6570\u636e\u96c6RealDet\uff0c\u5b9e\u73b0FPR\u7ea6\u675f\u4e0e\u68c0\u6d4b\u6027\u80fd\u7684\u53cc\u91cd\u4f18\u5316\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cMCP\u80fd\u6709\u6548\u7ea6\u675fFPR\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5728\u591a\u79cd\u68c0\u6d4b\u5668\u548c\u6570\u636e\u96c6\u4e2d\u589e\u5f3a\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "MCP\u6846\u67b6\u4e3a\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u517c\u987eFPR\u7ea6\u675f\u4e0e\u6027\u80fd\u63d0\u5347\u7684\u53ef\u884c\u65b9\u6848\uff0c\u7ed3\u5408RealDet\u6570\u636e\u96c6\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u5b9e\u7528\u6027\u4e0e\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.04725", "pdf": "https://arxiv.org/pdf/2505.04725", "abs": "https://arxiv.org/abs/2505.04725", "authors": ["Robin Chhabra", "Farzaneh Abdollahi"], "title": "Geometric Fault-Tolerant Neural Network Tracking Control of Unknown Systems on Matrix Lie Groups", "categories": ["eess.SY", "cs.AI", "cs.RO", "cs.SY", "math.DS"], "comment": null, "summary": "We present a geometric neural network-based tracking controller for systems\nevolving on matrix Lie groups under unknown dynamics, actuator faults, and\nbounded disturbances. Leveraging the left-invariance of the tangent bundle of\nmatrix Lie groups, viewed as an embedded submanifold of the vector space\n$\\R^{N\\times N}$, we propose a set of learning rules for neural network weights\nthat are intrinsically compatible with the Lie group structure and do not\nrequire explicit parameterization. Exploiting the geometric properties of Lie\ngroups, this approach circumvents parameterization singularities and enables a\nglobal search for optimal weights. The ultimate boundedness of all error\nsignals -- including the neural network weights, the coordinate-free\nconfiguration error function, and the tracking velocity error -- is established\nusing Lyapunov's direct method. To validate the effectiveness of the proposed\nmethod, we provide illustrative simulation results for decentralized formation\ncontrol of multi-agent systems on the Special Euclidean group.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u795e\u7ecf\u7f51\u7edc\u7684\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u5904\u7406\u77e9\u9635\u674e\u7fa4\u4e0a\u672a\u77e5\u52a8\u6001\u3001\u6267\u884c\u5668\u6545\u969c\u548c\u6709\u754c\u6270\u52a8\u7684\u7cfb\u7edf\u8ddf\u8e2a\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u674e\u7fa4\u7684\u51e0\u4f55\u7279\u6027\u5b9e\u73b0\u5168\u5c40\u6743\u91cd\u4f18\u5316\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u77e9\u9635\u674e\u7fa4\u4e0a\u7cfb\u7edf\u5728\u672a\u77e5\u52a8\u6001\u3001\u6267\u884c\u5668\u6545\u969c\u548c\u6270\u52a8\u60c5\u51b5\u4e0b\u7684\u8ddf\u8e2a\u63a7\u5236\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u4f20\u7edf\u53c2\u6570\u5316\u65b9\u6cd5\u7684\u5947\u5f02\u6027\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5229\u7528\u674e\u7fa4\u5207\u4e1b\u7684\u5de6\u4e0d\u53d8\u6027\u8bbe\u8ba1\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u5b66\u4e60\u89c4\u5219\uff0c\u65e0\u9700\u663e\u5f0f\u53c2\u6570\u5316\uff0c\u5e76\u901a\u8fc7\u674e\u7fa4\u7684\u51e0\u4f55\u7279\u6027\u5b9e\u73b0\u5168\u5c40\u6743\u91cd\u641c\u7d22\u3002\u4f7f\u7528Lyapunov\u76f4\u63a5\u6cd5\u8bc1\u660e\u8bef\u5dee\u4fe1\u53f7\u7684\u6709\u754c\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u4fdd\u8bc1\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u3001\u65e0\u5750\u6807\u6784\u578b\u8bef\u5dee\u51fd\u6570\u548c\u8ddf\u8e2a\u901f\u5ea6\u8bef\u5dee\u7684\u6700\u7ec8\u6709\u754c\u6027\uff0c\u5e76\u5728\u7279\u6b8a\u6b27\u51e0\u91cc\u5f97\u7fa4\u4e0a\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5206\u6563\u7f16\u961f\u63a7\u5236\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0c\u8be5\u51e0\u4f55\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u5728\u77e9\u9635\u674e\u7fa4\u7cfb\u7edf\u4e2d\u5177\u6709\u5168\u5c40\u4f18\u5316\u548c\u9c81\u68d2\u6027\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u590d\u6742\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8ddf\u8e2a\u63a7\u5236\u3002"}}
{"id": "2505.05099", "pdf": "https://arxiv.org/pdf/2505.05099", "abs": "https://arxiv.org/abs/2505.05099", "authors": ["Alireza Javani", "Zhiying Wang"], "title": "Balancing Client Participation in Federated Learning Using AoI", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": null, "summary": "Federated Learning (FL) offers a decentralized framework that preserves data\nprivacy while enabling collaborative model training across distributed clients.\nHowever, FL faces significant challenges due to limited communication\nresources, statistical heterogeneity, and the need for balanced client\nparticipation. This paper proposes an Age of Information (AoI)-based client\nselection policy that addresses these challenges by minimizing load imbalance\nthrough controlled selection intervals. Our method employs a decentralized\nMarkov scheduling policy, allowing clients to independently manage\nparticipation based on age-dependent selection probabilities, which balances\nclient updates across training rounds with minimal central oversight. We\nprovide a convergence proof for our method, demonstrating that it ensures\nstable and efficient model convergence. Specifically, we derive optimal\nparameters for the Markov selection model to achieve balanced and consistent\nclient participation, highlighting the benefits of AoI in enhancing convergence\nstability. Through extensive simulations, we demonstrate that our AoI-based\nmethod, particularly the optimal Markov variant, improves convergence over the\nFedAvg selection approach across both IID and non-IID data settings by $7.5\\%$\nand up to $20\\%$. Our findings underscore the effectiveness of AoI-based\nscheduling for scalable, fair, and efficient FL systems across diverse learning\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u5e74\u9f84\uff08AoI\uff09\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u7b56\u7565\uff0c\u901a\u8fc7\u4f18\u5316\u9009\u62e9\u95f4\u9694\u6765\u51cf\u5c11\u8d1f\u8f7d\u4e0d\u5747\uff0c\u63d0\u5347\u8054\u90a6\u5b66\u4e60\u7684\u6548\u7387\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u9762\u4e34\u901a\u4fe1\u8d44\u6e90\u6709\u9650\u3001\u7edf\u8ba1\u5f02\u8d28\u6027\u548c\u5ba2\u6237\u7aef\u53c2\u4e0e\u4e0d\u5747\u8861\u7b49\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u7684Markov\u8c03\u5ea6\u7b56\u7565\uff0c\u5ba2\u6237\u7aef\u6839\u636e\u5e74\u9f84\u76f8\u5173\u7684\u9009\u62e9\u6982\u7387\u81ea\u4e3b\u51b3\u5b9a\u53c2\u4e0e\uff0c\u5e73\u8861\u8bad\u7ec3\u8f6e\u6b21\u4e2d\u7684\u66f4\u65b0\uff0c\u51cf\u5c11\u4e2d\u5fc3\u5316\u5e72\u9884\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\uff0cAoI\u65b9\u6cd5\u5728IID\u548c\u975eIID\u6570\u636e\u573a\u666f\u4e0b\u5206\u522b\u6bd4FedAvg\u63d0\u53477.5%\u548c\u6700\u9ad820%\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u57fa\u4e8eAoI\u7684\u8c03\u5ea6\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347FL\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u3001\u516c\u5e73\u6027\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u5b66\u4e60\u73af\u5883\u3002"}}
{"id": "2505.05111", "pdf": "https://arxiv.org/pdf/2505.05111", "abs": "https://arxiv.org/abs/2505.05111", "authors": ["Boyi Deng", "Yu Wan", "Yidan Zhang", "Baosong Yang", "Fuli Feng"], "title": "Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders", "categories": ["cs.CL"], "comment": null, "summary": "The mechanisms behind multilingual capabilities in Large Language Models\n(LLMs) have been examined using neuron-based or internal-activation-based\nmethods. However, these methods often face challenges such as superposition and\nlayer-wise activation variance, which limit their reliability. Sparse\nAutoencoders (SAEs) offer a more nuanced analysis by decomposing the\nactivations of LLMs into sparse linear combination of SAE features. We\nintroduce a novel metric to assess the monolinguality of features obtained from\nSAEs, discovering that some features are strongly related to specific\nlanguages. Additionally, we show that ablating these SAE features only\nsignificantly reduces abilities in one language of LLMs, leaving others almost\nunaffected. Interestingly, we find some languages have multiple synergistic SAE\nfeatures, and ablating them together yields greater improvement than ablating\nindividually. Moreover, we leverage these SAE-derived language-specific\nfeatures to enhance steering vectors, achieving control over the language\ngenerated by LLMs.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u673a\u5236\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u5206\u89e3\u6fc0\u6d3b\u5e76\u8bc4\u4f30\u7279\u5f81\u7684\u8bed\u8a00\u4e13\u4e00\u6027\uff0c\u53d1\u73b0\u67d0\u4e9b\u7279\u5f81\u4e0e\u7279\u5b9a\u8bed\u8a00\u5bc6\u5207\u76f8\u5173\uff0c\u4e14\u53bb\u9664\u8fd9\u4e9b\u7279\u5f81\u4ec5\u663e\u8457\u5f71\u54cd\u4e00\u79cd\u8bed\u8a00\u7684\u80fd\u529b\uff0c\u90e8\u5206\u8bed\u8a00\u5b58\u5728\u534f\u540c\u7279\u5f81\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u7279\u5f81\u63d0\u5347\u8bed\u8a00\u63a7\u5236\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u795e\u7ecf\u5143\u6216\u5185\u90e8\u6fc0\u6d3b\u7684\u7814\u7a76\u65b9\u6cd5\u5728\u591a\u8bed\u8a00LLMs\u5206\u6790\u4e2d\u5b58\u5728\u5c40\u9650\uff08\u5982\u53e0\u52a0\u548c\u5c42\u95f4\u6fc0\u6d3b\u5dee\u5f02\uff09\u3002\u4e3a\u4e86\u66f4\u7cbe\u786e\u5730\u7406\u89e3\u6a21\u578b\u7684\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u7814\u7a76\u5e0c\u671b\u901a\u8fc7SAEs\u63ed\u793a\u8bed\u8a00\u76f8\u5173\u7684\u7279\u5f81\u53ca\u5176\u4f5c\u7528\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u5206\u89e3LLMs\u7684\u6fc0\u6d3b\uff0c\u63d0\u51fa\u65b0\u6307\u6807\u8bc4\u4f30\u7279\u5f81\u7684\u201c\u5355\u8bed\u8a00\u6027\u201d\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u53bb\u9664\uff08ablation\uff09\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u8bed\u8a00\u4e13\u4e00\u6027\u5f71\u54cd\u3002\u8fdb\u4e00\u6b65\u5229\u7528\u8fd9\u4e9b\u7279\u5f81\u4f18\u5316\u8bed\u8a00\u63a7\u5236\u5411\u91cf\u3002", "result": "\u53d1\u73b0\u67d0\u4e9bSAE\u7279\u5f81\u4e0e\u7279\u5b9a\u8bed\u8a00\u5f3a\u76f8\u5173\uff0c\u53bb\u9664\u540e\u4ec5\u663e\u8457\u5f71\u54cd\u8be5\u8bed\u8a00\u80fd\u529b\uff1b\u90e8\u5206\u8bed\u8a00\u5b58\u5728\u534f\u540c\u7279\u5f81\uff0c\u8054\u5408\u53bb\u9664\u6548\u679c\u66f4\u5f3a\u3002\u57fa\u4e8e\u7279\u5f81\u4f18\u5316\u7684\u63a7\u5236\u5411\u91cf\u80fd\u66f4\u7cbe\u51c6\u8c03\u63a7LLMs\u7684\u8f93\u51fa\u8bed\u8a00\u3002", "conclusion": "SAEs\u4e3a\u591a\u8bed\u8a00LLMs\u7684\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u7ec6\u7c92\u5ea6\u7684\u5de5\u5177\uff0c\u8bed\u8a00\u76f8\u5173\u7279\u5f81\u7684\u53d1\u73b0\u53ca\u534f\u540c\u6548\u5e94\u7684\u63ed\u793a\u4e3a\u6a21\u578b\u89e3\u91ca\u548c\u53ef\u63a7\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2505.04732", "pdf": "https://arxiv.org/pdf/2505.04732", "abs": "https://arxiv.org/abs/2505.04732", "authors": ["Sriram Gopalakrishnan", "Sunandita Patra"], "title": "QBD-RankedDataGen: Generating Custom Ranked Datasets for Improving Query-By-Document Search Using LLM-Reranking with Reduced Human Effort", "categories": ["cs.IR", "cs.AI"], "comment": "13 pages", "summary": "The Query-By-Document (QBD) problem is an information retrieval problem where\nthe query is a document, and the retrieved candidates are documents that match\nthe query document, often in a domain or query specific manner. This can be\ncrucial for tasks such as patent matching, legal or compliance case retrieval,\nand academic literature review. Existing retrieval methods, including keyword\nsearch and document embeddings, can be optimized with domain-specific datasets\nto improve QBD search performance. However, creating these domain-specific\ndatasets is often costly and time-consuming. Our work introduces a process to\ngenerate custom QBD-search datasets and compares a set of methods to use in\nthis problem, which we refer to as QBD-RankedDatagen. We provide a comparative\nanalysis of our proposed methods in terms of cost, speed, and the human\ninterface with the domain experts. The methods we compare leverage Large\nLanguage Models (LLMs) which can incorporate domain expert input to produce\ndocument scores and rankings, as well as explanations for human review. The\nprocess and methods for it that we present can significantly reduce human\neffort in dataset creation for custom domains while still obtaining sufficient\nexpert knowledge for tuning retrieval models. We evaluate our methods on QBD\ndatasets from the Text Retrieval Conference (TREC) and finetune the parameters\nof the BM25 model -- which is used in many industrial-strength search engines\nlike OpenSearch -- using the generated data.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86QBD-RankedDatagen\uff0c\u4e00\u79cd\u751f\u6210\u5b9a\u5236\u5316\u67e5\u8be2\u6587\u6863\u5339\u914d\uff08QBD\uff09\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u964d\u4f4e\u9886\u57df\u4e13\u5bb6\u5728\u6570\u636e\u96c6\u521b\u5efa\u4e2d\u7684\u5de5\u4f5c\u91cf\u3002", "motivation": "\u73b0\u6709\u7684QBD\u68c0\u7d22\u65b9\u6cd5\u4f9d\u8d56\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\uff0c\u4f46\u521b\u5efa\u8fd9\u4e9b\u6570\u636e\u96c6\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u65b9\u6cd5\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\uff0c\u540c\u65f6\u4fdd\u6301\u8db3\u591f\u7684\u4e13\u5bb6\u77e5\u8bc6\u4ee5\u4f18\u5316\u68c0\u7d22\u6a21\u578b\u3002", "method": "\u63d0\u51faQBD-RankedDatagen\u6d41\u7a0b\uff0c\u5229\u7528LLM\u751f\u6210\u6587\u6863\u8bc4\u5206\u548c\u6392\u540d\uff0c\u5e76\u7ed3\u5408\u4e13\u5bb6\u8f93\u5165\u3002\u5728TREC\u7684QBD\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5e76\u4f18\u5316BM25\u6a21\u578b\u53c2\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u6570\u636e\u96c6\u521b\u5efa\u7684\u4eba\u529b\u6210\u672c\uff0c\u540c\u65f6\u751f\u6210\u7684\u6392\u540d\u548c\u8bc4\u5206\u8db3\u591f\u7528\u4e8e\u68c0\u7d22\u6a21\u578b\u8c03\u4f18\u3002BM25\u6a21\u578b\u5728\u751f\u6210\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "QBD-RankedDatagen\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5b9a\u5236\u5316QBD\u641c\u7d22\u7684\u9886\u57df\uff0c\u5982\u4e13\u5229\u3001\u6cd5\u5f8b\u6216\u5b66\u672f\u6587\u732e\u3002"}}
{"id": "2505.05119", "pdf": "https://arxiv.org/pdf/2505.05119", "abs": "https://arxiv.org/abs/2505.05119", "authors": ["Chuanbo Hua", "Federico Berto", "Zhikai Zhao", "Jiwoo Son", "Changhyun Kwon", "Jinkyoo Park"], "title": "USPR: Learning a Unified Solver for Profiled Routing", "categories": ["cs.LG", "cs.MA"], "comment": null, "summary": "The Profiled Vehicle Routing Problem (PVRP) extends the classical VRP by\nincorporating vehicle-client-specific preferences and constraints, reflecting\nreal-world requirements such as zone restrictions and service-level\npreferences. While recent reinforcement learning (RL) solvers have shown\npromise, they require retraining for each new profile distribution, suffer from\npoor representation ability, and struggle to generalize to out-of-distribution\ninstances. In this paper, we address these limitations by introducing USPR\n(Unified Solver for Profiled Routing), a novel framework that natively handles\narbitrary profile types. USPR introduces three key innovations: (i) Profile\nEmbeddings (PE) to encode any combination of profile types; (ii) Multi-Head\nProfiled Attention (MHPA), an attention mechanism that models rich interactions\nbetween vehicles and clients; (iii) Profile-aware Score Reshaping (PSR), which\ndynamically adjusts decoder logits using profile scores to improve\ngeneralization. Empirical results on diverse PVRP benchmarks demonstrate that\nUSPR achieves state-of-the-art results among learning-based methods while\noffering significant gains in flexibility and computational efficiency. We make\nour source code publicly available to foster future research at\nhttps://github.com/ai4co/uspr.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86USPR\uff0c\u4e00\u79cd\u80fd\u591f\u7edf\u4e00\u5904\u7406\u4efb\u610f\u8f66\u8f86-\u5ba2\u6237\u914d\u7f6e\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u548c\u7075\u6d3b\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u5d4c\u5165\u3001\u6ce8\u610f\u529b\u673a\u5236\u548c\u52a8\u6001\u5206\u6570\u8c03\u6574\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u8f66\u8f86-\u5ba2\u6237\u7279\u5b9a\u914d\u7f6e\u65f6\u9700\u91cd\u590d\u8bad\u7ec3\uff0c\u6cdb\u5316\u6027\u5dee\u4e14\u7075\u6d3b\u6027\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5e94\u5bf9\u73b0\u5b9e\u4e2d\u7684\u591a\u6837\u5316\u9700\u6c42\uff08\u5982\u533a\u57df\u9650\u5236\u3001\u670d\u52a1\u504f\u597d\uff09\u3002", "method": "USPR\u6846\u67b6\u5305\u542b\u4e09\u9879\u521b\u65b0\uff1a(1) \u914d\u7f6e\u5d4c\u5165\uff08PE\uff09\u7f16\u7801\u4efb\u610f\u914d\u7f6e\u7ec4\u5408\uff1b(2) \u591a\u5934\u90e8\u914d\u7f6e\u6ce8\u610f\u529b\uff08MHPA\uff09\u5efa\u6a21\u8f66\u8f86-\u5ba2\u6237\u4ea4\u4e92\uff1b(3) \u914d\u7f6e\u611f\u77e5\u5206\u6570\u91cd\u5851\uff08PSR\uff09\u52a8\u6001\u8c03\u6574\u89e3\u7801\u903b\u8f91\u4ee5\u63d0\u9ad8\u6cdb\u5316\u6027\u3002", "result": "\u5728\u591a\u79cdPVRP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUSPR\u53d6\u5f97\u4e86\u57fa\u4e8e\u5b66\u4e60\u65b9\u6cd5\u7684\u6700\u4f18\u7ed3\u679c\uff0c\u540c\u65f6\u5728\u7075\u6d3b\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "USPR\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u914d\u7f6e\u591a\u6837\u6027\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2505.05148", "pdf": "https://arxiv.org/pdf/2505.05148", "abs": "https://arxiv.org/abs/2505.05148", "authors": ["Hussain Ahmad", "Qingyang Zeng", "Jing Wan"], "title": "A Benchmark Dataset and a Framework for Urdu Multimodal Named Entity Recognition", "categories": ["cs.CL"], "comment": "16 pages, 5 figures. Preprint", "summary": "The emergence of multimodal content, particularly text and images on social\nmedia, has positioned Multimodal Named Entity Recognition (MNER) as an\nincreasingly important area of research within Natural Language Processing.\nDespite progress in high-resource languages such as English, MNER remains\nunderexplored for low-resource languages like Urdu. The primary challenges\ninclude the scarcity of annotated multimodal datasets and the lack of\nstandardized baselines. To address these challenges, we introduce the U-MNER\nframework and release the Twitter2015-Urdu dataset, a pioneering resource for\nUrdu MNER. Adapted from the widely used Twitter2015 dataset, it is annotated\nwith Urdu-specific grammar rules. We establish benchmark baselines by\nevaluating both text-based and multimodal models on this dataset, providing\ncomparative analyses to support future research on Urdu MNER. The U-MNER\nframework integrates textual and visual context using Urdu-BERT for text\nembeddings and ResNet for visual feature extraction, with a Cross-Modal Fusion\nModule to align and fuse information. Our model achieves state-of-the-art\nperformance on the Twitter2015-Urdu dataset, laying the groundwork for further\nMNER research in low-resource languages.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e4c\u5c14\u90fd\u8bed\u7684\u591a\u6a21\u6001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08MNER\uff09\u6846\u67b6U-MNER\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u4e4c\u5c14\u90fd\u8bed\u591a\u6a21\u6001\u6570\u636e\u96c6Twitter2015-Urdu\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u4fe1\u606f\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5185\u5bb9\uff08\u5982\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u6587\u672c\u548c\u56fe\u50cf\uff09\u7684\u5174\u8d77\u4f7f\u5f97MNER\u6210\u4e3a\u91cd\u8981\u7814\u7a76\u65b9\u5411\uff0c\u4f46\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u4e4c\u5c14\u90fd\u8bed\uff09\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6a21\u578b\uff0c\u4e9f\u9700\u89e3\u51b3\u3002", "method": "\u63d0\u51faU-MNER\u6846\u67b6\uff0c\u6574\u5408\u4e4c\u5c14\u90fd\u8bedBERT\uff08\u6587\u672c\u5d4c\u5165\uff09\u548cResNet\uff08\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\uff09\uff0c\u5e76\u901a\u8fc7\u8de8\u6a21\u6001\u878d\u5408\u6a21\u5757\u5bf9\u9f50\u548c\u878d\u5408\u4fe1\u606f\u3002\u57fa\u4e8eTwitter2015\u6570\u636e\u96c6\u6784\u5efa\u4e86\u4e4c\u5c14\u90fd\u8bed\u591a\u6a21\u6001\u6570\u636e\u96c6Twitter2015-Urdu\u3002", "result": "U-MNER\u5728Twitter2015-Urdu\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684MNER\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u4e4c\u5c14\u90fd\u8bedMNER\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6a21\u578b\u7684\u53d1\u5e03\u63a8\u52a8\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u591a\u6a21\u6001\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.05126", "pdf": "https://arxiv.org/pdf/2505.05126", "abs": "https://arxiv.org/abs/2505.05126", "authors": ["Xuyang Chen", "Keyu Yan", "Lin Zhao"], "title": "Taming OOD Actions for Offline Reinforcement Learning: An Advantage-Based Approach", "categories": ["cs.LG"], "comment": null, "summary": "Offline reinforcement learning (RL) aims to learn decision-making policies\nfrom fixed datasets without online interactions, providing a practical solution\nwhere online data collection is expensive or risky. However, offline RL often\nsuffers from distribution shift, resulting in inaccurate evaluation and\nsubstantial overestimation on out-of-distribution (OOD) actions. To address\nthis, existing approaches incorporate conservatism by indiscriminately\ndiscouraging all OOD actions, thereby hindering the agent's ability to\ngeneralize and exploit beneficial ones. In this paper, we propose\nAdvantage-based Diffusion Actor-Critic (ADAC), a novel method that\nsystematically evaluates OOD actions using the batch-optimal value function.\nBased on this evaluation, ADAC defines an advantage function to modulate the\nQ-function update, enabling more precise assessment of OOD action quality. We\ndesign a custom PointMaze environment and collect datasets to visually reveal\nthat advantage modulation can effectively identify and select superior OOD\nactions. Extensive experiments show that ADAC achieves state-of-the-art\nperformance on almost all tasks in the D4RL benchmark, with particularly clear\nmargins on the more challenging tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u4f18\u52bf\u6269\u6563\u7684Actor-Critic\uff08ADAC\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u548c\u8c03\u5236\u4f18\u52bf\u51fd\u6570\u6765\u5e94\u5bf9\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7531\u4e8e\u65e0\u6cd5\u5728\u7ebf\u4ea4\u4e92\uff0c\u5e38\u56e0\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u5bf9\u5206\u5e03\u5916\u52a8\u4f5c\u7684\u8bc4\u4f30\u4e0d\u51c6\u786e\u3002\u73b0\u6709\u65b9\u6cd5\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u963b\u788d\u4e86\u5bf9\u6709\u76ca\u52a8\u4f5c\u7684\u5229\u7528\u3002ADAC\u65e8\u5728\u901a\u8fc7\u4f18\u52bf\u8c03\u5236\u66f4\u7cbe\u786e\u8bc4\u4f30\u52a8\u4f5c\u8d28\u91cf\u3002", "method": "ADAC\u4f7f\u7528\u6279\u91cf\u6700\u4f18\u503c\u51fd\u6570\u7cfb\u7edf\u8bc4\u4f30\u5206\u5e03\u5916\u52a8\u4f5c\uff0c\u5b9a\u4e49\u4f18\u52bf\u51fd\u6570\u8c03\u5236Q\u51fd\u6570\u66f4\u65b0\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u52a8\u4f5c\u8d28\u91cf\u7684\u66f4\u7cbe\u51c6\u8bc4\u4f30\u3002", "result": "\u5728\u81ea\u5b9a\u4e49PointMaze\u73af\u5883\u548cD4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cADAC\u5728\u51e0\u4e4e\u6240\u6709\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4e14\u5728\u66f4\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e0a\u4f18\u52bf\u660e\u663e\u3002", "conclusion": "ADAC\u901a\u8fc7\u4f18\u52bf\u8c03\u5236\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4efb\u52a1\u8868\u73b0\u3002"}}
{"id": "2505.05225", "pdf": "https://arxiv.org/pdf/2505.05225", "abs": "https://arxiv.org/abs/2505.05225", "authors": ["Mengze Hong", "Wailing Ng", "Di Jiang", "Chen Jason Zhang"], "title": "QualBench: Benchmarking Chinese LLMs with Localized Professional Qualifications for Vertical Domain Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of Chinese large language models (LLMs) underscores the\nneed for domain-specific evaluations to ensure reliable applications. However,\nexisting benchmarks often lack coverage in vertical domains and offer limited\ninsights into the Chinese working context. Leveraging qualification exams as a\nunified framework for human expertise evaluation, we introduce QualBench, the\nfirst multi-domain Chinese QA benchmark dedicated to localized assessment of\nChinese LLMs. The dataset includes over 17,000 questions across six vertical\ndomains, with data selections grounded in 24 Chinese qualifications to closely\nalign with national policies and working standards. Through comprehensive\nevaluation, the Qwen2.5 model outperformed the more advanced GPT-4o, with\nChinese LLMs consistently surpassing non-Chinese models, highlighting the\nimportance of localized domain knowledge in meeting qualification requirements.\nThe best performance of 75.26% reveals the current gaps in domain coverage\nwithin model capabilities. Furthermore, we present the failure of LLM\ncollaboration with crowdsourcing mechanisms and suggest the opportunities for\nmulti-domain RAG knowledge enhancement and vertical domain LLM training with\nFederated Learning.", "AI": {"tldr": "QualBench\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u4e2d\u6587\u5927\u6a21\u578b\uff08LLM\uff09\u672c\u5730\u5316\u8bc4\u4f30\u7684\u591a\u9886\u57df\u95ee\u7b54\u57fa\u51c6\uff0c\u8986\u76d66\u4e2a\u5782\u76f4\u9886\u57df\u768417,000\u591a\u9053\u9898\u76ee\uff0c\u57fa\u4e8e24\u79cd\u4e2d\u56fd\u8d44\u683c\u8003\u8bd5\u8bbe\u8ba1\u3002Qwen2.5\u6a21\u578b\u8868\u73b0\u4f18\u4e8eGPT-4o\uff0c\u51f8\u663e\u672c\u5730\u5316\u9886\u57df\u77e5\u8bc6\u7684\u91cd\u8981\u6027\u3002\u6a21\u578b\u80fd\u529b\u5728\u9886\u57df\u8986\u76d6\u4e0a\u4ecd\u6709\u5dee\u8ddd\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u591a\u9886\u57dfRAG\u589e\u5f3a\u548c\u8054\u90a6\u5b66\u4e60\u8bad\u7ec3\u5782\u76f4\u9886\u57dfLLM\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5728\u5782\u76f4\u9886\u57df\u8986\u76d6\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u4e2d\u56fd\u5de5\u4f5c\u573a\u666f\u7684\u9488\u5bf9\u6027\u8bc4\u4f30\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u57fa\u4e8e\u8d44\u683c\u8003\u8bd5\u7684\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u66f4\u8d34\u5408\u4e2d\u56fd\u653f\u7b56\u548c\u884c\u4e1a\u6807\u51c6\u3002", "method": "\u5229\u752824\u79cd\u4e2d\u56fd\u8d44\u683c\u8003\u8bd5\u6784\u5efaQualBench\u6570\u636e\u96c6\uff0817,000+\u9898\u76ee\uff09\uff0c\u8986\u76d66\u4e2a\u5782\u76f4\u9886\u57df\u3002\u901a\u8fc7\u5168\u9762\u8bc4\u4f30\u5bf9\u6bd4\u4e2d/\u975e\u4e2d\u6587LLM\u8868\u73b0\uff0c\u5e76\u63a2\u7d22\u4f17\u5305\u673a\u5236\u4e0eLLM\u534f\u4f5c\u7684\u5931\u8d25\u6848\u4f8b\u3002", "result": "Qwen2.5\uff08\u4e2d\u6587LLM\uff09\u4f18\u4e8eGPT-4o\uff0c\u4e2d\u6587\u6a21\u578b\u6574\u4f53\u8868\u73b0\u66f4\u597d\uff1b\u6700\u4f73\u51c6\u786e\u738775.26%\u66b4\u9732\u6a21\u578b\u9886\u57df\u8986\u76d6\u7684\u4e0d\u8db3\u3002\u4f17\u5305\u534f\u4f5c\u5931\u8d25\uff0c\u4f46\u591a\u9886\u57dfRAG\u548c\u8054\u90a6\u5b66\u4e60\u5c55\u73b0\u51fa\u6f5c\u529b\u3002", "conclusion": "\u672c\u5730\u5316\u9886\u57df\u77e5\u8bc6\u5bf9LLM\u81f3\u5173\u91cd\u8981\uff0c\u5f53\u524d\u6a21\u578b\u80fd\u529b\u4ecd\u6709\u7f3a\u53e3\u3002\u672a\u6765\u53ef\u901a\u8fc7RAG\u77e5\u8bc6\u589e\u5f3a\u548c\u8054\u90a6\u5b66\u4e60\u4f18\u5316\u5782\u76f4\u9886\u57dfLLM\uff0c\u63a8\u52a8\u53ef\u9760\u843d\u5730\u5e94\u7528\u3002"}}
{"id": "2505.04759", "pdf": "https://arxiv.org/pdf/2505.04759", "abs": "https://arxiv.org/abs/2505.04759", "authors": ["Mohit Chaudhary", "Chirag Jain", "Preethu Rose Anish"], "title": "Exploring Zero-Shot App Review Classification with ChatGPT: Challenges and Potential", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "App reviews are a critical source of user feedback, offering valuable\ninsights into an app's performance, features, usability, and overall user\nexperience. Effectively analyzing these reviews is essential for guiding app\ndevelopment, prioritizing feature updates, and enhancing user satisfaction.\nClassifying reviews into functional and non-functional requirements play a\npivotal role in distinguishing feedback related to specific app features\n(functional requirements) from feedback concerning broader quality attributes,\nsuch as performance, usability, and reliability (non-functional requirements).\nBoth categories are integral to informed development decisions. Traditional\napproaches to classifying app reviews are hindered by the need for large,\ndomain-specific datasets, which are often costly and time-consuming to curate.\nThis study explores the potential of zero-shot learning with ChatGPT for\nclassifying app reviews into four categories: functional requirement,\nnon-functional requirement, both, or neither. We evaluate ChatGPT's performance\non a benchmark dataset of 1,880 manually annotated reviews from ten diverse\napps spanning multiple domains. Our findings demonstrate that ChatGPT achieves\na robust F1 score of 0.842 in review classification, despite certain challenges\nand limitations. Additionally, we examine how factors such as review\nreadability and length impact classification accuracy and conduct a manual\nanalysis to identify review categories more prone to misclassification.", "AI": {"tldr": "\u5229\u7528ChatGPT\u7684\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u529b\u5206\u7c7b\u5e94\u7528\u8bc4\u8bba\uff0c\u5206\u4e3a\u529f\u80fd\u9700\u6c42\u3001\u975e\u529f\u80fd\u9700\u6c42\u3001\u4e24\u8005\u517c\u5177\u6216\u65e0\u5173\u56db\u7c7b\uff0c\u8868\u73b0\u826f\u597d\u4f46\u5b58\u5728\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u5206\u7c7b\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\uff0c\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u7814\u7a76\u63a2\u7d22ChatGPT\u5728\u96f6\u6837\u672c\u5b66\u4e60\u4e0b\u7684\u5206\u7c7b\u6f5c\u529b\u3002", "method": "\u572810\u4e2a\u5e94\u7528\u51711,880\u6761\u4eba\u5de5\u6807\u6ce8\u8bc4\u8bba\u4e0a\u8bc4\u4f30ChatGPT\uff0c\u5206\u6790\u8bc4\u8bba\u53ef\u8bfb\u6027\u3001\u957f\u5ea6\u5bf9\u5206\u7c7b\u7684\u5f71\u54cd\uff0c\u5e76\u8fdb\u884c\u8bef\u5206\u7c7b\u7c7b\u522b\u624b\u52a8\u5206\u6790\u3002", "result": "ChatGPT\u5206\u7c7bF1\u5206\u6570\u8fbe0.842\uff0c\u6548\u679c\u7a33\u5065\uff0c\u4f46\u53ef\u8bfb\u6027\u548c\u957f\u5ea6\u7b49\u56e0\u7d20\u5f71\u54cd\u51c6\u786e\u6027\uff0c\u90e8\u5206\u7c7b\u522b\u6613\u88ab\u8bef\u5206\u7c7b\u3002", "conclusion": "ChatGPT\u5728\u65e0\u9886\u57df\u6570\u636e\u65f6\u4ecd\u80fd\u6709\u6548\u5206\u7c7b\u8bc4\u8bba\uff0c\u4f46\u9700\u6ce8\u610f\u5176\u5c40\u9650\u6027\u548c\u7279\u5b9a\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002"}}
{"id": "2505.05137", "pdf": "https://arxiv.org/pdf/2505.05137", "abs": "https://arxiv.org/abs/2505.05137", "authors": ["Yi Chen"], "title": "Research on Anomaly Detection Methods Based on Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": "6 pages, 3 table", "summary": "Anomaly detection is a fundamental task in machine learning and data mining,\nwith significant applications in cybersecurity, industrial fault diagnosis, and\nclinical disease monitoring. Traditional methods, such as statistical modeling\nand machine learning-based approaches, often face challenges in handling\ncomplex, high-dimensional data distributions. In this study, we explore the\npotential of diffusion models for anomaly detection, proposing a novel\nframework that leverages the strengths of diffusion probabilistic models (DPMs)\nto effectively identify anomalies in both image and audio data. The proposed\nmethod models the distribution of normal data through a diffusion process and\nreconstructs input data via reverse diffusion, using a combination of\nreconstruction errors and semantic discrepancies as anomaly indicators. To\nenhance the framework's performance, we introduce multi-scale feature\nextraction, attention mechanisms, and wavelet-domain representations, enabling\nthe model to capture fine-grained structures and global dependencies in the\ndata. Extensive experiments on benchmark datasets, including MVTec AD and\nUrbanSound8K, demonstrate that our method outperforms state-of-the-art anomaly\ndetection techniques, achieving superior accuracy and robustness across diverse\ndata modalities. This research highlights the effectiveness of diffusion models\nin anomaly detection and provides a robust and efficient solution for\nreal-world applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DPMs\uff09\u7684\u65b0\u578b\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u56fe\u50cf\u548c\u97f3\u9891\u6570\u636e\u4e2d\u9ad8\u6548\u8bc6\u522b\u5f02\u5e38\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u9ad8\u7ef4\u6570\u636e\u65f6\u9762\u4e34\u6311\u6218\uff0c\u800c\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u5206\u5e03\u5efa\u6a21\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u56e0\u6b64\u63a2\u7d22\u5176\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528\u6269\u6563\u8fc7\u7a0b\u5efa\u6a21\u6b63\u5e38\u6570\u636e\u5206\u5e03\uff0c\u901a\u8fc7\u53cd\u5411\u6269\u6563\u91cd\u6784\u8f93\u5165\u6570\u636e\uff0c\u7ed3\u5408\u91cd\u6784\u8bef\u5dee\u548c\u8bed\u4e49\u5dee\u5f02\u4f5c\u4e3a\u5f02\u5e38\u6307\u6807\uff0c\u5e76\u5f15\u5165\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u3001\u6ce8\u610f\u529b\u673a\u5236\u548c\u5c0f\u6ce2\u57df\u8868\u793a\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728MVTec AD\u548cUrbanSound8K\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05271", "pdf": "https://arxiv.org/pdf/2505.05271", "abs": "https://arxiv.org/abs/2505.05271", "authors": ["Kun Peng", "Chaodong Tong", "Cong Cao", "Hao Peng", "Qian Li", "Guanlin Wu", "Lei Jiang", "Yanbing Liu", "Philip S. Yu"], "title": "T-T: Table Transformer for Tagging-based Aspect Sentiment Triplet Extraction", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IJCAI2025", "summary": "Aspect sentiment triplet extraction (ASTE) aims to extract triplets composed\nof aspect terms, opinion terms, and sentiment polarities from given sentences.\nThe table tagging method is a popular approach to addressing this task, which\nencodes a sentence into a 2-dimensional table, allowing for the tagging of\nrelations between any two words. Previous efforts have focused on designing\nvarious downstream relation learning modules to better capture interactions\nbetween tokens in the table, revealing that a stronger capability to capture\nrelations can lead to greater improvements in the model. Motivated by this, we\nattempt to directly utilize transformer layers as downstream relation learning\nmodules. Due to the powerful semantic modeling capability of transformers, it\nis foreseeable that this will lead to excellent improvement. However, owing to\nthe quadratic relation between the length of the table and the length of the\ninput sentence sequence, using transformers directly faces two challenges:\noverly long table sequences and unfair local attention interaction. To address\nthese challenges, we propose a novel Table-Transformer (T-T) for the\ntagging-based ASTE method. Specifically, we introduce a stripe attention\nmechanism with a loop-shift strategy to tackle these challenges. The former\nmodifies the global attention mechanism to only attend to a 2-dimensional local\nattention window, while the latter facilitates interaction between different\nattention windows. Extensive and comprehensive experiments demonstrate that the\nT-T, as a downstream relation learning module, achieves state-of-the-art\nperformance with lower computational costs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684Table-Transformer (T-T)\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u6761\u7eb9\u6ce8\u610f\u529b\u548c\u5faa\u73af\u79fb\u4f4d\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfTransformer\u5728\u8868\u683c\u6807\u8bb0\u4efb\u52a1\u4e2d\u8fc7\u957f\u5e8f\u5217\u548c\u4e0d\u516c\u5e73\u5c40\u90e8\u6ce8\u610f\u529b\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86ASTE\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u8868\u683c\u6807\u8bb0\u65b9\u6cd5\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u8ba1\u7b97\u6548\u7387\u4f4e\u4e14\u5c40\u90e8\u6ce8\u610f\u529b\u4e0d\u5747\u8861\uff0c\u672c\u6587\u5c1d\u8bd5\u76f4\u63a5\u4f7f\u7528Transformer\u5c42\u4f5c\u4e3a\u5173\u7cfb\u5b66\u4e60\u6a21\u5757\uff0c\u5229\u7528\u5176\u5f3a\u5927\u7684\u8bed\u4e49\u5efa\u6a21\u80fd\u529b\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51faT-T\u6a21\u578b\uff0c\u7ed3\u5408\u6761\u7eb9\u6ce8\u610f\u529b\u673a\u5236\uff08\u9650\u5236\u4e3a\u4e8c\u7ef4\u5c40\u90e8\u7a97\u53e3\uff09\u548c\u5faa\u73af\u79fb\u4f4d\u7b56\u7565\uff08\u4fc3\u8fdb\u4e0d\u540c\u7a97\u53e3\u95f4\u7684\u4ea4\u4e92\uff09\uff0c\u4f18\u5316\u4e86\u4f20\u7edfTransformer\u5728ASTE\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cT-T\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u4f5c\u4e3a\u5173\u7cfb\u5b66\u4e60\u6a21\u5757\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "T-T\u901a\u8fc7\u521b\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86Transformer\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u74f6\u9888\uff0c\u4e3aASTE\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.04784", "pdf": "https://arxiv.org/pdf/2505.04784", "abs": "https://arxiv.org/abs/2505.04784", "authors": ["Pedro Pinacho-Davidson", "Fernando Gutierrez", "Pablo Zapata", "Rodolfo Vergara", "Pablo Aqueveque"], "title": "A Proposal for Evaluating the Operational Risk for ChatBots based on Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.CY"], "comment": "21 pages", "summary": "The emergence of Generative AI (Gen AI) and Large Language Models (LLMs) has\nenabled more advanced chatbots capable of human-like interactions. However,\nthese conversational agents introduce a broader set of operational risks that\nextend beyond traditional cybersecurity considerations. In this work, we\npropose a novel, instrumented risk-assessment metric that simultaneously\nevaluates potential threats to three key stakeholders: the service-providing\norganization, end users, and third parties. Our approach incorporates the\ntechnical complexity required to induce erroneous behaviors in the\nchatbot--ranging from non-induced failures to advanced prompt-injection\nattacks--as well as contextual factors such as the target industry, user age\nrange, and vulnerability severity. To validate our metric, we leverage Garak,\nan open-source framework for LLM vulnerability testing. We further enhance\nGarak to capture a variety of threat vectors (e.g., misinformation, code\nhallucinations, social engineering, and malicious code generation). Our\nmethodology is demonstrated in a scenario involving chatbots that employ\nretrieval-augmented generation (RAG), showing how the aggregated risk scores\nguide both short-term mitigation and longer-term improvements in model design\nand deployment. The results underscore the importance of multi-dimensional risk\nassessments in operationalizing secure, reliable AI-driven conversational\nsystems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u578b\u7684\u98ce\u9669\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u5f0fAI\u804a\u5929\u673a\u5668\u4eba\u53ef\u80fd\u5bf9\u670d\u52a1\u63d0\u4f9b\u5546\u3001\u7ec8\u7aef\u7528\u6237\u548c\u7b2c\u4e09\u65b9\u5e26\u6765\u7684\u5a01\u80c1\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684\u5f00\u6e90\u6846\u67b6Garak\u9a8c\u8bc1\u6307\u6807\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u804a\u5929\u673a\u5668\u4eba\u5e26\u6765\u7684\u4eba\u673a\u4ea4\u4e92\u80fd\u529b\u63d0\u5347\u4e5f\u4f34\u968f\u7740\u4f20\u7edf\u7f51\u7edc\u5b89\u5168\u4e4b\u5916\u7684\u66f4\u5e7f\u6cdb\u64cd\u4f5c\u98ce\u9669\uff0c\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e9b\u591a\u7ef4\u5ea6\u5a01\u80c1\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6280\u672f\u590d\u6742\u6027\u548c\u4e0a\u4e0b\u6587\u56e0\u7d20\uff08\u5982\u884c\u4e1a\u3001\u7528\u6237\u5e74\u9f84\u548c\u6f0f\u6d1e\u4e25\u91cd\u6027\uff09\u7684\u98ce\u9669\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5229\u7528\u6539\u8fdb\u7684Garak\u6846\u67b6\u6d4b\u8bd5\u5a01\u80c1\u5411\u91cf\uff08\u5982\u9519\u8bef\u4fe1\u606f\u3001\u4ee3\u7801\u5e7b\u89c9\u7b49\uff09\u3002", "result": "\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u804a\u5929\u673a\u5668\u4eba\u573a\u666f\u9a8c\u8bc1\uff0c\u8be5\u6307\u6807\u80fd\u6709\u6548\u6307\u5bfc\u77ed\u671f\u98ce\u9669\u7f13\u89e3\u548c\u957f\u671f\u6a21\u578b\u8bbe\u8ba1\u4e0e\u90e8\u7f72\u4f18\u5316\u3002", "conclusion": "\u591a\u7ef4\u5ea6\u98ce\u9669\u8bc4\u4f30\u5bf9\u4e8e\u786e\u4fddAI\u9a71\u52a8\u804a\u5929\u673a\u5668\u4eba\u7684\u5b89\u5168\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u8be5\u7814\u7a76\u4e3a\u76f8\u5173\u5b9e\u8df5\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u548c\u65b9\u6cd5\u3002"}}
{"id": "2505.05143", "pdf": "https://arxiv.org/pdf/2505.05143", "abs": "https://arxiv.org/abs/2505.05143", "authors": ["Mohammed Adnan", "Rohan Jain", "Ekansh Sharma", "Rahul Krishnan", "Yani Ioannou"], "title": "Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry", "categories": ["cs.LG"], "comment": "Accepted at ICML 2025", "summary": "The Lottery Ticket Hypothesis (LTH) suggests there exists a sparse LTH mask\nand weights that achieve the same generalization performance as the dense model\nwhile using significantly fewer parameters. However, finding a LTH solution is\ncomputationally expensive, and a LTH sparsity mask does not generalize to other\nrandom weight initializations. Recent work has suggested that neural networks\ntrained from random initialization find solutions within the same basin modulo\npermutation, and proposes a method to align trained models within the same loss\nbasin. We hypothesize that misalignment of basins is the reason why LTH masks\ndo not generalize to new random initializations and propose permuting the LTH\nmask to align with the new optimization basin when performing sparse training\nfrom a different random init. We empirically show a significant increase in\ngeneralization when sparse training from random initialization with the\npermuted mask as compared to using the non-permuted LTH mask, on multiple\ndatasets (CIFAR-10, CIFAR-100 and ImageNet) and models (VGG11, ResNet20 and\nResNet50).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6392\u5217\u7a00\u758f\u63a9\u7801\u5bf9\u9f50\u4f18\u5316\u76c6\u5730\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ece\u968f\u673a\u521d\u59cb\u5316\u5f00\u59cb\u7684\u7a00\u758f\u8bad\u7ec3\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u5f69\u7968\u5047\u8bbe\uff08LTH\uff09\u7684\u7a00\u758f\u63a9\u7801\u65e0\u6cd5\u63a8\u5e7f\u5230\u5176\u4ed6\u968f\u673a\u6743\u91cd\u521d\u59cb\u5316\uff0c\u63a8\u6d4b\u539f\u56e0\u662f\u4f18\u5316\u76c6\u5730\u672a\u5bf9\u9f50\u3002\u56e0\u6b64\uff0c\u672c\u6587\u8bd5\u56fe\u901a\u8fc7\u6392\u5217\u63a9\u7801\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5728\u7a00\u758f\u8bad\u7ec3\u65f6\u5bf9LTH\u63a9\u7801\u8fdb\u884c\u6392\u5217\uff0c\u4ee5\u5bf9\u9f50\u65b0\u7684\u4f18\u5316\u76c6\u5730\u3002\u5b9e\u9a8c\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08CIFAR-10\u3001CIFAR-100\u548cImageNet\uff09\u548c\u6a21\u578b\uff08VGG11\u3001ResNet20\u548cResNet50\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u6392\u5217\u540e\u7684LTH\u63a9\u7801\u8fdb\u884c\u7a00\u758f\u8bad\u7ec3\u65f6\uff0c\u6cdb\u5316\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u672a\u6392\u5217\u7684\u63a9\u7801\u3002", "conclusion": "\u901a\u8fc7\u6392\u5217\u7a00\u758f\u63a9\u7801\u5bf9\u9f50\u4f18\u5316\u76c6\u5730\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u4ece\u968f\u673a\u521d\u59cb\u5316\u5f00\u59cb\u7684\u7a00\u758f\u8bad\u7ec3\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2505.05298", "pdf": "https://arxiv.org/pdf/2505.05298", "abs": "https://arxiv.org/abs/2505.05298", "authors": ["Elena Musi", "Nadin Kokciyan", "Khalid Al-Khatib", "Davide Ceolin", "Emmanuelle Dietz", "Klara Gutekunst", "Annette Hautli-Janisz", "Cristian Manuel Santiba\u00f1ez Ya\u00f1ez", "Jodi Schneider", "Jonas Scholz", "Cor Steging", "Jacky Visser", "Henning Wachsmuth"], "title": "Toward Reasonable Parrots: Why Large Language Models Should Argue with Us by Design", "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "In this position paper, we advocate for the development of conversational\ntechnology that is inherently designed to support and facilitate argumentative\nprocesses. We argue that, at present, large language models (LLMs) are\ninadequate for this purpose, and we propose an ideal technology design aimed at\nenhancing argumentative skills. This involves re-framing LLMs as tools to\nexercise our critical thinking rather than replacing them. We introduce the\nconcept of 'reasonable parrots' that embody the fundamental principles of\nrelevance, responsibility, and freedom, and that interact through argumentative\ndialogical moves. These principles and moves arise out of millennia of work in\nargumentation theory and should serve as the starting point for LLM-based\ntechnology that incorporates basic principles of argumentation.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u8bbe\u8ba1\u80fd\u591f\u652f\u6301\u8bba\u8bc1\u8fc7\u7a0b\u7684\u5bf9\u8bdd\u6280\u672f\uff0c\u8ba4\u4e3a\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0d\u8db3\uff0c\u63d0\u51fa\u6280\u672f\u6539\u8fdb\u65b9\u6848\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u652f\u6301\u8bba\u8bc1\u8fc7\u7a0b\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u91cd\u65b0\u8bbe\u8ba1\u6280\u672f\u4ee5\u4fc3\u8fdb\u6279\u5224\u6027\u601d\u7ef4\u3002", "method": "\u63d0\u51fa\u201c\u5408\u7406\u9e66\u9e49\u201d\u6982\u5ff5\uff0c\u57fa\u4e8e\u8bba\u8bc1\u7406\u8bba\u7684\u57fa\u672c\u539f\u5219\uff08\u76f8\u5173\u6027\u3001\u8d23\u4efb\u3001\u81ea\u7531\uff09\u8fdb\u884c\u5bf9\u8bdd\u8bbe\u8ba1\u3002", "result": "\u6280\u672f\u6539\u8fdb\u65b9\u6848\u53ef\u80fd\u63d0\u5347LLMs\u5728\u8bba\u8bc1\u5bf9\u8bdd\u4e2d\u7684\u6548\u7528\u3002", "conclusion": "\u8bba\u8bc1\u7406\u8bba\u5e94\u6210\u4e3aLLM\u6280\u672f\u8bbe\u8ba1\u7684\u57fa\u7840\uff0c\u4ee5\u652f\u6301\u66f4\u597d\u7684\u8bba\u8bc1\u5bf9\u8bdd\u3002"}}
{"id": "2505.05145", "pdf": "https://arxiv.org/pdf/2505.05145", "abs": "https://arxiv.org/abs/2505.05145", "authors": ["Xinyan Hu", "Kayo Yin", "Michael I. Jordan", "Jacob Steinhardt", "Lijie Chen"], "title": "Understanding In-context Learning of Addition via Activation Subspaces", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "16 pages", "summary": "To perform in-context learning, language models must extract signals from\nindividual few-shot examples, aggregate these into a learned prediction rule,\nand then apply this rule to new examples. How is this implemented in the\nforward pass of modern transformer models? To study this, we consider a\nstructured family of few-shot learning tasks for which the true prediction rule\nis to add an integer $k$ to the input. We find that Llama-3-8B attains high\naccuracy on this task for a range of $k$, and localize its few-shot ability to\njust three attention heads via a novel optimization approach. We further show\nthe extracted signals lie in a six-dimensional subspace, where four of the\ndimensions track the unit digit and the other two dimensions track overall\nmagnitude. We finally examine how these heads extract information from\nindividual few-shot examples, identifying a self-correction mechanism in which\nmistakes from earlier examples are suppressed by later examples. Our results\ndemonstrate how tracking low-dimensional subspaces across a forward pass can\nprovide insight into fine-grained computational structures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u5b66\u4e60\uff0c\u5e76\u5728\u73b0\u4ee3Transformer\u6a21\u578b\u7684\u524d\u5411\u4f20\u64ad\u4e2d\u5b9e\u73b0\u9884\u6d4b\u89c4\u5219\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u4efb\u52a1\u53d1\u73b0\uff0cLlama-3-8B\u5728\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4fe1\u53f7\u96c6\u4e2d\u5728\u516d\u7ef4\u5b50\u7a7a\u95f4\u4e2d\uff0c\u5e76\u63ed\u793a\u4e86\u81ea\u6821\u6b63\u673a\u5236\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u5b66\u4e60\uff0c\u5e76\u7406\u89e3\u5176\u5728Transformer\u524d\u5411\u4f20\u64ad\u4e2d\u7684\u5177\u4f53\u5b9e\u73b0\u673a\u5236\u3002", "method": "\u4f7f\u7528\u7ed3\u6784\u5316\u4efb\u52a1\uff08\u8f93\u5165\u52a0\u6574\u6570k\uff09\u6d4b\u8bd5Llama-3-8B\u7684\u5c11\u91cf\u5b66\u4e60\u80fd\u529b\uff0c\u901a\u8fc7\u4f18\u5316\u65b9\u6cd5\u5b9a\u4f4d\u5173\u952e\u6ce8\u610f\u529b\u5934\uff0c\u5206\u6790\u4fe1\u53f7\u5b50\u7a7a\u95f4\u3002", "result": "\u6a21\u578b\u5728\u7279\u5b9a\u4e09\u4e2a\u6ce8\u610f\u529b\u5934\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4fe1\u53f7\u96c6\u4e2d\u5728\u516d\u7ef4\u5b50\u7a7a\u95f4\uff08\u5355\u4f4d\u6570\u5b57\u548c\u6574\u4f53\u5927\u5c0f\uff09\uff0c\u5e76\u53d1\u73b0\u81ea\u6821\u6b63\u673a\u5236\u3002", "conclusion": "\u901a\u8fc7\u8ddf\u8e2a\u4f4e\u7ef4\u5b50\u7a7a\u95f4\uff0c\u53ef\u4ee5\u63ed\u793a\u6a21\u578b\u524d\u5411\u4f20\u64ad\u4e2d\u7ec6\u7c92\u5ea6\u8ba1\u7b97\u7ed3\u6784\u7684\u6d1e\u5bdf\u3002"}}
{"id": "2505.05327", "pdf": "https://arxiv.org/pdf/2505.05327", "abs": "https://arxiv.org/abs/2505.05327", "authors": ["Yixin Yang", "Qingxiu Dong", "Linli Yao", "Fangwei Zhu", "Zhifang Sui"], "title": "ICon: In-Context Contribution for Automatic Data Selection", "categories": ["cs.CL"], "comment": null, "summary": "Data selection for instruction tuning is essential for improving the\nperformance of Large Language Models (LLMs) and reducing training cost.\nHowever, existing automated selection methods either depend on computationally\nexpensive gradient-based measures or manually designed heuristics, which may\nfail to fully exploit the intrinsic attributes of data. In this paper, we\npropose In-context Learning for Contribution Measurement (ICon), a novel\ngradient-free method that takes advantage of the implicit fine-tuning nature of\nin-context learning (ICL) to measure sample contribution without gradient\ncomputation or manual indicators engineering. ICon offers a computationally\nefficient alternative to gradient-based methods and reduces human inductive\nbias inherent in heuristic-based approaches. ICon comprises three components\nand identifies high-contribution data by assessing performance shifts under\nimplicit learning through ICL. Extensive experiments on three LLMs across 12\nbenchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of\nICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data\noutperform full datasets by 5.42% points and exceed the best performance of\nwidely used selection methods by 2.06% points. We further analyze\nhigh-contribution samples selected by ICon, which show both diverse tasks and\nappropriate difficulty levels, rather than just the hardest ones.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aICon\u7684\u65e0\u68af\u5ea6\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u9009\u62e9\u6307\u4ee4\u5fae\u8c03\u6570\u636e\uff0c\u901a\u8fc7\u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u9690\u5f0f\u5fae\u8c03\u7279\u6027\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u68af\u5ea6\u8ba1\u7b97\u6216\u4eba\u5de5\u542f\u53d1\u5f0f\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u8981\u4e48\u4f9d\u8d56\u4eba\u5de5\u542f\u53d1\u5f0f\uff0c\u65e0\u6cd5\u5145\u5206\u6316\u6398\u6570\u636e\u5185\u5728\u5c5e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u65e0\u504f\u7684\u65b9\u6cd5\u3002", "method": "ICon\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u9690\u5f0f\u5b66\u4e60\u6027\u80fd\u53d8\u5316\u6765\u8bc4\u4f30\u6837\u672c\u8d21\u732e\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u65e0\u9700\u68af\u5ea6\u8ba1\u7b97\u6216\u4eba\u5de5\u6307\u6807\u8bbe\u8ba1\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cICon\u9009\u62e915%\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u5168\u6570\u636e\u96c65.42%\uff0c\u4e14\u6bd4\u5176\u4ed6\u65b9\u6cd5\u9ad82.06%\u3002", "conclusion": "ICon\u4e0d\u4ec5\u80fd\u9ad8\u6548\u9009\u62e9\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u8fd8\u80fd\u907f\u514d\u8fc7\u5ea6\u4f9d\u8d56\u6700\u56f0\u96be\u6837\u672c\uff0c\u63d0\u5347\u4e86\u6307\u4ee4\u5fae\u8c03\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2505.04787", "pdf": "https://arxiv.org/pdf/2505.04787", "abs": "https://arxiv.org/abs/2505.04787", "authors": ["Sriram Mandalika", "Harsha Vardhan", "Athira Nambiar"], "title": "Replay to Remember (R2R): An Efficient Uncertainty-driven Unsupervised Continual Learning Framework Using Generative Replay", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Submitted to the 28th European Conference on Artificial Intelligence\n  (ECAI-2025)", "summary": "Continual Learning entails progressively acquiring knowledge from new data\nwhile retaining previously acquired knowledge, thereby mitigating\n``Catastrophic Forgetting'' in neural networks. Our work presents a novel\nuncertainty-driven Unsupervised Continual Learning framework using Generative\nReplay, namely ``Replay to Remember (R2R)''. The proposed R2R architecture\nefficiently uses unlabelled and synthetic labelled data in a balanced\nproportion using a cluster-level uncertainty-driven feedback mechanism and a\nVLM-powered generative replay module. Unlike traditional memory-buffer methods\nthat depend on pretrained models and pseudo-labels, our R2R framework operates\nwithout any prior training. It leverages visual features from unlabeled data\nand adapts continuously using clustering-based uncertainty estimation coupled\nwith dynamic thresholding. Concurrently, a generative replay mechanism along\nwith DeepSeek-R1 powered CLIP VLM produces labelled synthetic data\nrepresentative of past experiences, resembling biological visual thinking that\nreplays memory to remember and act in new, unseen tasks. Extensive experimental\nanalyses are carried out in CIFAR-10, CIFAR-100, CINIC-10, SVHN and\nTinyImageNet datasets. Our proposed R2R approach improves knowledge retention,\nachieving a state-of-the-art performance of 98.13%, 73.06%, 93.41%, 95.18%,\n59.74%, respectively, surpassing state-of-the-art performance by over 4.36%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u56de\u653e\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5R2R\uff0c\u5229\u7528\u805a\u7c7b\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u53cd\u9988\u548cVLM\u6a21\u5757\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u65b9\u5f0f\u548c\u751f\u6210\u56de\u653e\u63d0\u5347\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\u3002", "method": "\u91c7\u7528\u65e0\u76d1\u7763\u6301\u7eed\u5b66\u4e60\u6846\u67b6R2R\uff0c\u7ed3\u5408\u805a\u7c7b\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u53cd\u9988\u673a\u5236\u548cVLM\u751f\u6210\u56de\u653e\u6a21\u5757\uff0c\u52a8\u6001\u5229\u7528\u65e0\u6807\u6ce8\u548c\u5408\u6210\u6807\u6ce8\u6570\u636e\u3002", "result": "R2R\u5728CIFAR-10\u3001CIFAR-100\u7b49\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523098.13%\u300173.06%\u7b49\u6027\u80fd\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd54.36%\u3002", "conclusion": "R2R\u6846\u67b6\u901a\u8fc7\u751f\u6210\u56de\u653e\u548c\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u673a\u5236\uff0c\u6709\u6548\u5b9e\u73b0\u4e86\u6301\u7eed\u5b66\u4e60\u7684\u77e5\u8bc6\u4fdd\u7559\u548c\u8fc1\u79fb\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.05155", "pdf": "https://arxiv.org/pdf/2505.05155", "abs": "https://arxiv.org/abs/2505.05155", "authors": ["Zhihao Zeng", "Ziquan Fang", "Wei Shao", "Lu Chen", "Yunjun Gao"], "title": "FedTDP: A Privacy-Preserving and Unified Framework for Trajectory Data Preparation via Federated Learning", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Trajectory data, which capture the movement patterns of people and vehicles\nover time and space, are crucial for applications like traffic optimization and\nurban planning. However, issues such as noise and incompleteness often\ncompromise data quality, leading to inaccurate trajectory analyses and limiting\nthe potential of these applications. While Trajectory Data Preparation (TDP)\ncan enhance data quality, existing methods suffer from two key limitations: (i)\nthey do not address data privacy concerns, particularly in federated settings\nwhere trajectory data sharing is prohibited, and (ii) they typically design\ntask-specific models that lack generalizability across diverse TDP scenarios.\nTo overcome these challenges, we propose FedTDP, a privacy-preserving and\nunified framework that leverages the capabilities of Large Language Models\n(LLMs) for TDP in federated environments. Specifically, we: (i) design a\ntrajectory privacy autoencoder to secure data transmission and protect privacy,\n(ii) introduce a trajectory knowledge enhancer to improve model learning of\nTDP-related knowledge, enabling the development of TDP-oriented LLMs, and (iii)\npropose federated parallel optimization to enhance training efficiency by\nreducing data transmission and enabling parallel model training. Experiments on\n6 real datasets and 10 mainstream TDP tasks demonstrate that FedTDP\nconsistently outperforms 13 state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa FedTDP\uff0c\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8054\u90a6\u73af\u5883\u4e2d\u8fdb\u884c\u8f68\u8ff9\u6570\u636e\u51c6\u5907\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7f16\u7801\u5668\u548c\u77e5\u8bc6\u589e\u5f3a\u5668\u63d0\u5347\u6570\u636e\u8d28\u91cf\u4e0e\u9690\u79c1\u4fdd\u62a4\uff0c\u5e76\u5728\u591a\u9879\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u6570\u636e\u51c6\u5907\uff08TDP\uff09\u65b9\u6cd5\u5b58\u5728\u9690\u79c1\u4fdd\u62a4\u4e0d\u8db3\u548c\u7f3a\u4e4f\u901a\u7528\u6027\u7684\u95ee\u9898\uff0c\u5c24\u5176\u5728\u8054\u90a6\u73af\u5883\u4e2d\u6570\u636e\u5171\u4eab\u53d7\u9650\uff0c\u4e9f\u9700\u4e00\u79cd\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u5e7f\u6cdb\u9002\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "FedTDP \u6846\u67b6\u7ed3\u5408\u8f68\u8ff9\u9690\u79c1\u81ea\u7f16\u7801\u5668\u4fdd\u62a4\u6570\u636e\u9690\u79c1\uff0c\u8f68\u8ff9\u77e5\u8bc6\u589e\u5f3a\u5668\u63d0\u5347\u6a21\u578b\u5b66\u4e60 TDP \u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u8054\u90a6\u5e76\u884c\u4f18\u5316\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5728 6 \u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u548c 10 \u9879\u4e3b\u6d41 TDP \u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedTDP \u5728\u6027\u80fd\u4e0a\u4f18\u4e8e 13 \u79cd\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "FedTDP \u6210\u529f\u89e3\u51b3\u4e86 TDP \u4e2d\u7684\u9690\u79c1\u548c\u901a\u7528\u6027\u95ee\u9898\uff0c\u4e3a\u8054\u90a6\u73af\u5883\u4e0b\u7684\u8f68\u8ff9\u6570\u636e\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05406", "pdf": "https://arxiv.org/pdf/2505.05406", "abs": "https://arxiv.org/abs/2505.05406", "authors": ["Valeria Pastorino", "Nafise Sadat Moosavi"], "title": "Frame In, Frame Out: Do LLMs Generate More Biased News Headlines than Humans?", "categories": ["cs.CL"], "comment": null, "summary": "Framing in media critically shapes public perception by selectively\nemphasizing some details while downplaying others. With the rise of large\nlanguage models in automated news and content creation, there is growing\nconcern that these systems may introduce or even amplify framing biases\ncompared to human authors. In this paper, we explore how framing manifests in\nboth out-of-the-box and fine-tuned LLM-generated news content. Our analysis\nreveals that, particularly in politically and socially sensitive contexts, LLMs\ntend to exhibit more pronounced framing than their human counterparts. In\naddition, we observe significant variation in framing tendencies across\ndifferent model architectures, with some models displaying notably higher\nbiases. These findings point to the need for effective post-training mitigation\nstrategies and tighter evaluation frameworks to ensure that automated news\ncontent upholds the standards of balanced reporting.", "AI": {"tldr": "LLM\u751f\u6210\u7684\u65b0\u95fb\u5185\u5bb9\u5728\u653f\u6cbb\u548c\u793e\u4f1a\u654f\u611f\u8bdd\u9898\u4e2d\u8868\u73b0\u51fa\u6bd4\u4eba\u7c7b\u4f5c\u8005\u66f4\u660e\u663e\u7684\u6846\u67b6\u504f\u89c1\uff0c\u4e14\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u7684\u6846\u67b6\u503e\u5411\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u81ea\u52a8\u5316\u65b0\u95fb\u548c\u5185\u5bb9\u521b\u4f5c\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u5f15\u53d1\u4e86\u5bf9\u6846\u67b6\u504f\u89c1\u6f5c\u5728\u5f15\u5165\u6216\u653e\u5927\u7684\u62c5\u5fe7\uff0c\u56e0\u6b64\u7814\u7a76LLM\u751f\u6210\u5185\u5bb9\u7684\u6846\u67b6\u8868\u73b0\u3002", "method": "\u5206\u6790\u4e86\u672a\u7ecf\u4fee\u6539\u548c\u7ecf\u8fc7\u5fae\u8c03\u7684LLM\u751f\u6210\u7684\u65b0\u95fb\u5185\u5bb9\uff0c\u7279\u522b\u5173\u6ce8\u653f\u6cbb\u548c\u793e\u4f1a\u654f\u611f\u80cc\u666f\u4e0b\u7684\u6846\u67b6\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLM\u5728\u654f\u611f\u8bdd\u9898\u4e2d\u8868\u73b0\u51fa\u6bd4\u4eba\u7c7b\u66f4\u660e\u663e\u7684\u6846\u67b6\u504f\u89c1\uff0c\u4e14\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u7684\u6846\u67b6\u503e\u5411\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u9700\u8981\u6709\u6548\u7684\u8bad\u7ec3\u540e\u7f13\u89e3\u7b56\u7565\u548c\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u786e\u4fdd\u81ea\u52a8\u5316\u65b0\u95fb\u5185\u5bb9\u4fdd\u6301\u5e73\u8861\u62a5\u9053\u7684\u6807\u51c6\u3002"}}
{"id": "2505.04792", "pdf": "https://arxiv.org/pdf/2505.04792", "abs": "https://arxiv.org/abs/2505.04792", "authors": ["Jack O'Hagan", "Andrew Keane", "Andrew Flynn"], "title": "Confabulation dynamics in a reservoir computer: Filling in the gaps with untrained attractors", "categories": ["math.DS", "cs.AI", "cs.LG"], "comment": null, "summary": "Artificial Intelligence has advanced significantly in recent years thanks to\ninnovations in the design and training of artificial neural networks (ANNs).\nDespite these advancements, we still understand relatively little about how\nelementary forms of ANNs learn, fail to learn, and generate false information\nwithout the intent to deceive, a phenomenon known as `confabulation'. To\nprovide some foundational insight, in this paper we analyse how confabulation\noccurs in reservoir computers (RCs): a dynamical system in the form of an ANN.\nRCs are particularly useful to study as they are known to confabulate in a\nwell-defined way: when RCs are trained to reconstruct the dynamics of a given\nattractor, they sometimes construct an attractor that they were not trained to\nconstruct, a so-called `untrained attractor' (UA). This paper sheds light on\nthe role played by UAs when reconstruction fails and their influence when\nmodelling transitions between reconstructed attractors. Based on our results,\nwe conclude that UAs are an intrinsic feature of learning systems whose state\nspaces are bounded, and that this means of confabulation may be present in\nsystems beyond RCs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86AI\u4e2d\u7684\u2018\u865a\u6784\u2019\u73b0\u8c61\uff08\u5373\u795e\u7ecf\u7f51\u7edc\u65e0\u610f\u4e2d\u751f\u6210\u9519\u8bef\u4fe1\u606f\uff09\uff0c\u4ee5\u50a8\u5907\u6c60\u8ba1\u7b97\u673a\uff08RCs\uff09\u4e3a\u7814\u7a76\u5bf9\u8c61\uff0c\u63ed\u793a\u4e86\u672a\u8bad\u7ec3\u5438\u5f15\u5b50\uff08UAs\uff09\u5728\u91cd\u5efa\u52a8\u529b\u5b66\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u5bf9\u5efa\u6a21\u7684\u5f71\u54cd\uff0c\u4e3a\u7406\u89e3\u6b64\u7c7b\u73b0\u8c61\u63d0\u4f9b\u4e86\u57fa\u7840\u89c1\u89e3\u3002", "motivation": "\u5c3d\u7ba1\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANNs\uff09\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u57fa\u7840\u5b66\u4e60\u673a\u5236\u4e2d\u7684\u2018\u865a\u6784\u2019\u73b0\u8c61\u4ecd\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u7814\u7a76\u50a8\u5907\u6c60\u8ba1\u7b97\u673a\uff08RCs\uff09\u4e2d\u660e\u786e\u5b58\u5728\u7684\u865a\u6784\u884c\u4e3a\uff08\u8868\u73b0\u4e3a\u672a\u8bad\u7ec3\u5438\u5f15\u5b50UAs\uff09\uff0c\u4e3a\u8fd9\u4e00\u73b0\u8c61\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u4ee5\u50a8\u5907\u6c60\u8ba1\u7b97\u673a\uff08RCs\uff09\u4e3a\u6a21\u578b\uff0c\u5206\u6790\u5176\u5728\u52a8\u529b\u5b66\u5438\u5f15\u5b50\u91cd\u5efa\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u672a\u8bad\u7ec3\u5438\u5f15\u5b50\uff08UAs\uff09\u7684\u884c\u4e3a\uff0c\u5e76\u63a2\u8ba8UAs\u5982\u4f55\u5bfc\u81f4\u91cd\u5efa\u5931\u8d25\u53ca\u5f71\u54cd\u6a21\u578b\u4e2d\u7684\u5438\u5f15\u5b50\u8f6c\u6362\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cUAs\u662f\u72b6\u6001\u7a7a\u95f4\u53d7\u9650\u7684\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u56fa\u6709\u7279\u5f81\uff1b\u5b83\u4e0d\u4ec5\u5b58\u5728\u4e8eRCs\u4e2d\uff0c\u4e5f\u53ef\u80fd\u5e7f\u6cdb\u5b58\u5728\u4e8e\u5176\u4ed6\u7cfb\u7edf\u4e2d\uff0c\u4ece\u800c\u89e3\u91ca\u4e86\u865a\u6784\u73b0\u8c61\u7684\u666e\u904d\u6027\u3002", "conclusion": "\u8bba\u6587\u8bc1\u5b9e\u4e86UAs\u4f5c\u4e3a\u865a\u6784\u73b0\u8c61\u7684\u6838\u5fc3\u673a\u5236\uff0c\u5e76\u6307\u51fa\u5176\u5728\u66f4\u5e7f\u6cdb\u7cfb\u7edf\u4e2d\u7684\u6f5c\u5728\u5b58\u5728\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u7c7b\u4f3c\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u65b9\u6cd5\u542f\u793a\u3002"}}
{"id": "2505.05169", "pdf": "https://arxiv.org/pdf/2505.05169", "abs": "https://arxiv.org/abs/2505.05169", "authors": ["Tsubasa Harada", "Shinji Ito", "Hanna Sumita"], "title": "Bandit Max-Min Fair Allocation", "categories": ["cs.LG"], "comment": "23 pages", "summary": "In this paper, we study a new decision-making problem called the bandit\nmax-min fair allocation (BMMFA) problem. The goal of this problem is to\nmaximize the minimum utility among agents with additive valuations by\nrepeatedly assigning indivisible goods to them. One key feature of this problem\nis that each agent's valuation for each item can only be observed through the\nsemi-bandit feedback, while existing work supposes that the item values are\nprovided at the beginning of each round. Another key feature is that the\nalgorithm's reward function is not additive with respect to rounds, unlike most\nbandit-setting problems.\n  Our first contribution is to propose an algorithm that has an asymptotic\nregret bound of $O(m\\sqrt{T}\\ln T/n + m\\sqrt{T \\ln(mnT)})$, where $n$ is the\nnumber of agents, $m$ is the number of items, and $T$ is the time horizon. This\nis based on a novel combination of bandit techniques and a resource allocation\nalgorithm studied in the literature on competitive analysis. Our second\ncontribution is to provide the regret lower bound of $\\Omega(m\\sqrt{T}/n)$.\nWhen $T$ is sufficiently larger than $n$, the gap between the upper and lower\nbounds is a logarithmic factor of $T$.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u4e2a\u65b0\u7684\u51b3\u7b56\u95ee\u9898\u2014\u2014Bandit Max-Min\u516c\u5e73\u5206\u914d\u95ee\u9898\uff08BMMFA\uff09\uff0c\u76ee\u6807\u662f\u901a\u8fc7\u91cd\u590d\u5206\u914d\u4e0d\u53ef\u5206\u5272\u5546\u54c1\u6765\u6700\u5927\u5316\u4ee3\u7406\u7684\u6700\u5c0f\u6548\u7528\u3002\u4e0e\u73b0\u6709\u5de5\u4f5c\u4e0d\u540c\uff0c\u4ee3\u7406\u5bf9\u5546\u54c1\u7684\u4f30\u503c\u53ea\u80fd\u901a\u8fc7\u534a\u8d4c\u535a\u53cd\u9988\u89c2\u5bdf\uff0c\u4e14\u5956\u52b1\u51fd\u6570\u5728\u5404\u8f6e\u6b21\u95f4\u4e0d\u5177\u53ef\u52a0\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e2d\uff0c\u5546\u54c1\u7684\u4f30\u503c\u901a\u5e38\u5728\u6bcf\u8f6e\u5f00\u59cb\u65f6\u63d0\u4f9b\uff0c\u800c\u672c\u6587\u5173\u6ce8\u7684\u662f\u901a\u8fc7\u534a\u8d4c\u535a\u53cd\u9988\u89c2\u5bdf\u4f30\u503c\u7684\u60c5\u51b5\uff0c\u540c\u65f6\u5904\u7406\u975e\u52a0\u6027\u5956\u52b1\u51fd\u6570\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86\u8d4c\u535a\u6280\u672f\u548c\u7ade\u4e89\u5206\u6790\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86$O(m\\sqrt{T}\\ln T/n + m\\sqrt{T \\ln(mnT)})$\u7684\u6e10\u8fdb\u540e\u6094\u4e0a\u9650\u3002", "result": "\u7b97\u6cd5\u5728\u65f6\u95f4\u8303\u56f4$T$\u8db3\u591f\u5927\u65f6\uff0c\u5176\u4e0a\u754c\u4e0e\u4e0b\u754c$\\Omega(m\\sqrt{T}/n)$\u4e4b\u95f4\u7684\u5dee\u8ddd\u4e3a$T$\u7684\u5bf9\u6570\u56e0\u5b50\u3002", "conclusion": "\u672c\u6587\u4e0d\u4ec5\u63d0\u51fa\u4e86\u89e3\u51b3BMMFA\u95ee\u9898\u7684\u65b0\u7b97\u6cd5\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5176\u540e\u6094\u4e0b\u754c\uff0c\u586b\u8865\u4e86\u7406\u8bba\u4e0e\u5b9e\u9645\u6027\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2505.05408", "pdf": "https://arxiv.org/pdf/2505.05408", "abs": "https://arxiv.org/abs/2505.05408", "authors": ["Zheng-Xin Yong", "M. Farid Adilazuarda", "Jonibek Mansurov", "Ruochen Zhang", "Niklas Muennighoff", "Carsten Eickhoff", "Genta Indra Winata", "Julia Kreutzer", "Stephen H. Bach", "Alham Fikri Aji"], "title": "Crosslingual Reasoning through Test-Time Scaling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u7684\u957f\u94fe\u601d\u7ef4\uff08CoTs\uff09\u5fae\u8c03\u6a21\u578b\u5728\u591a\u8bed\u8a00\u63a8\u7406\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u6269\u5c55\u63a8\u7406\u8ba1\u7b97\u80fd\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8de8\u9886\u57df\u63a8\u7406\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u7814\u7a76\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63a2\u7d22\u82f1\u8bed\u4e2d\u5fc3\u5316\u5fae\u8c03\u5bf9\u591a\u8bed\u8a00\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u63a8\u7406\u8ba1\u7b97\u3001\u5206\u6790CoTs\u6a21\u5f0f\u3001\u63a7\u5236\u63a8\u7406\u8bed\u8a00\u7b56\u7565\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u591a\u8bed\u8a00\u6570\u5b66\u63a8\u7406\u548c\u6587\u5316\u5e38\u8bc6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e2d\u63a8\u7406\u66f4\u9ad8\u6548\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u8868\u73b0\u6709\u6240\u63d0\u5347\uff0c\u4f46\u8de8\u9886\u57df\uff08\u5982STEM\u5230\u6587\u5316\u5e38\u8bc6\uff09\u63a8\u7406\u80fd\u529b\u8f83\u5dee\u3002", "conclusion": "\u5efa\u8bae\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e2d\u4f7f\u7528\u82f1\u8bed\u4e2d\u5fc3\u5316\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff0c\u672a\u6765\u9700\u6539\u8fdb\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u8de8\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2505.05180", "pdf": "https://arxiv.org/pdf/2505.05180", "abs": "https://arxiv.org/abs/2505.05180", "authors": ["Cong Hua", "Qianqian Xu", "Zhiyong Yang", "Zitai Wang", "Shilong Bao", "Qingming Huang"], "title": "OpenworldAUC: Towards Unified Evaluation and Optimization for Open-world Prompt Tuning", "categories": ["cs.LG"], "comment": "This paper has been accepted by ICML2025", "summary": "Prompt tuning adapts Vision-Language Models like CLIP to open-world tasks\nwith minimal training costs. In this direction, one typical paradigm evaluates\nmodel performance separately on known classes (i.e., base domain) and unseen\nclasses (i.e., new domain). However, real-world scenarios require models to\nhandle inputs without prior domain knowledge. This practical challenge has\nspurred the development of open-world prompt tuning, which demands a unified\nevaluation of two stages: 1) detecting whether an input belongs to the base or\nnew domain (P1), and 2) classifying the sample into its correct class (P2).\nWhat's more, as domain distributions are generally unknown, a proper metric\nshould be insensitive to varying base/new sample ratios (P3). However, we find\nthat current metrics, including HM, overall accuracy, and AUROC, fail to\nsatisfy these three properties simultaneously. To bridge this gap, we propose\nOpenworldAUC, a unified metric that jointly assesses detection and\nclassification through pairwise instance comparisons. To optimize OpenworldAUC\neffectively, we introduce Gated Mixture-of-Prompts (GMoP), which employs\ndomain-specific prompts and a gating mechanism to dynamically balance detection\nand classification. Theoretical guarantees ensure generalization of GMoP under\npractical conditions. Experiments on 15 benchmarks in open-world scenarios show\nGMoP achieves SOTA performance on OpenworldAUC and other metrics. We release\nthe code at https://github.com/huacong/OpenworldAUC", "AI": {"tldr": "Prompt tuning adapts CLIP for open-world tasks, but current metrics fail to assess both domain detection and classification simultaneously. The paper proposes OpenworldAUC for unified evaluation and GMoP for dynamic prompt tuning, achieving SOTA results.", "motivation": "Real-world scenarios require models to handle inputs without prior domain knowledge, which current metrics and methods fail to address comprehensively.", "method": "Proposes OpenworldAUC for unified evaluation and GMoP, a gated mixture-of-prompts method, to dynamically balance domain detection and classification.", "result": "GMoP achieves state-of-the-art performance on 15 benchmarks in open-world scenarios.", "conclusion": "OpenworldAUC and GMoP provide a robust solution for open-world prompt tuning, addressing limitations of current metrics and methods."}}
{"id": "2505.05410", "pdf": "https://arxiv.org/pdf/2505.05410", "abs": "https://arxiv.org/abs/2505.05410", "authors": ["Yanda Chen", "Joe Benton", "Ansh Radhakrishnan", "Jonathan Uesato", "Carson Denison", "John Schulman", "Arushi Somani", "Peter Hase", "Misha Wagner", "Fabien Roger", "Vlad Mikulik", "Samuel R. Bowman", "Jan Leike", "Jared Kaplan", "Ethan Perez"], "title": "Reasoning Models Don't Always Say What They Think", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Chain-of-thought (CoT) offers a potential boon for AI safety as it allows\nmonitoring a model's CoT to try to understand its intentions and reasoning\nprocesses. However, the effectiveness of such monitoring hinges on CoTs\nfaithfully representing models' actual reasoning processes. We evaluate CoT\nfaithfulness of state-of-the-art reasoning models across 6 reasoning hints\npresented in the prompts and find: (1) for most settings and models tested,\nCoTs reveal their usage of hints in at least 1% of examples where they use the\nhint, but the reveal rate is often below 20%, (2) outcome-based reinforcement\nlearning initially improves faithfulness but plateaus without saturating, and\n(3) when reinforcement learning increases how frequently hints are used (reward\nhacking), the propensity to verbalize them does not increase, even without\ntraining against a CoT monitor. These results suggest that CoT monitoring is a\npromising way of noticing undesired behaviors during training and evaluations,\nbut that it is not sufficient to rule them out. They also suggest that in\nsettings like ours where CoT reasoning is not necessary, test-time monitoring\nof CoTs is unlikely to reliably catch rare and catastrophic unexpected\nbehaviors.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u601d\u7ef4\u94fe\uff08CoT\uff09\u867d\u7136\u5728\u76d1\u6d4bAI\u610f\u56fe\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5177\u6709\u4e00\u5b9a\u6f5c\u529b\uff0c\u4f46\u5176\u5fe0\u5b9e\u6027\u6709\u9650\uff0c\u5c24\u5176\u5728\u63d0\u793a\u4f7f\u7528\u548c\u5f3a\u5316\u5b66\u4e60\u573a\u666f\u4e0b\u6548\u679c\u4e0d\u4f73\uff0c\u65e0\u6cd5\u5b8c\u5168\u6392\u9664\u610f\u5916\u884c\u4e3a\u3002", "motivation": "\u63a2\u8ba8\u601d\u7ef4\u94fe\uff08CoT\uff09\u662f\u5426\u80fd\u5fe0\u5b9e\u53cd\u6620\u6a21\u578b\u7684\u771f\u5b9e\u63a8\u7406\u8fc7\u7a0b\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728AI\u5b89\u5168\u76d1\u63a7\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u5bf96\u79cd\u63a8\u7406\u63d0\u793a\u4e0b\u7684\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884cCoT\u5fe0\u5b9e\u6027\u8bc4\u4f30\uff0c\u5206\u6790\u63d0\u793a\u4f7f\u7528\u7387\u3001\u5f3a\u5316\u5b66\u4e60\u5bf9\u5fe0\u5b9e\u6027\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5956\u52b1\u673a\u5236\u4e0b\u7684\u884c\u4e3a\u53d8\u5316\u3002", "result": "CoT\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u80fd\u90e8\u5206\u63ed\u793a\u63d0\u793a\u4f7f\u7528\uff081%-20%\uff09\uff0c\u5f3a\u5316\u5b66\u4e60\u521d\u671f\u63d0\u5347\u5fe0\u5b9e\u6027\u4f46\u4f1a\u505c\u6ede\uff0c\u5956\u52b1\u673a\u5236\u4e0b\u63d0\u793a\u4f7f\u7528\u9891\u7387\u589e\u52a0\u4f46\u672a\u4f34\u968fCoT\u8868\u8fbe\u63d0\u5347\u3002", "conclusion": "CoT\u76d1\u63a7\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e2d\u6709\u6f5c\u529b\u4f46\u4e0d\u8db3\u591f\uff0c\u5c24\u5176\u5728\u975e\u5fc5\u8981\u63a8\u7406\u573a\u666f\u4e0b\uff0c\u96be\u4ee5\u53ef\u9760\u6355\u6349\u7f55\u89c1\u6216\u707e\u96be\u6027\u610f\u5916\u884c\u4e3a\u3002"}}
{"id": "2505.05181", "pdf": "https://arxiv.org/pdf/2505.05181", "abs": "https://arxiv.org/abs/2505.05181", "authors": ["Bojian Yin", "Federico Corradi"], "title": "Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 5 figures", "summary": "Backpropagation (BP) is the cornerstone of deep learning, but its reliance on\nglobal gradient synchronization limits scalability and imposes significant\nmemory overhead. We propose Stochastic Variational Propagation (SVP), a\nscalable alternative that reframes training as hierarchical variational\ninference. SVP treats layer activations as latent variables and optimizes local\nEvidence Lower Bounds (ELBOs), enabling independent, local updates while\npreserving global coherence. However, directly applying KL divergence in\nlayer-wise ELBOs risks inter-layer's representation collapse due to excessive\ncompression. To prevent this, SVP projects activations into low-dimensional\nspaces via fixed random matrices, ensuring information preservation and\nrepresentational diversity. Combined with a feature alignment loss for\ninter-layer consistency, SVP achieves competitive accuracy with BP across\ndiverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to\nImageNet), reduces memory usage by up to 4x, and significantly improves\nscalability. More broadly, SVP introduces a probabilistic perspective to deep\nrepresentation learning, opening pathways toward more modular and interpretable\nneural network design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u968f\u673a\u53d8\u5206\u4f20\u64ad\uff08SVP\uff09\u7684\u53ef\u6269\u5c55\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u5206\u5c42\u53d8\u5206\u63a8\u65ad\u91cd\u65b0\u5b9a\u4e49\u8bad\u7ec3\uff0c\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u5e76\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u4f20\u7edf\u53cd\u5411\u4f20\u64ad\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u867d\u7136\u5bf9\u6df1\u5ea6\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u4f9d\u8d56\u5168\u5c40\u68af\u5ea6\u540c\u6b65\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u5e76\u5e26\u6765\u663e\u8457\u7684\u5185\u5b58\u5f00\u9500\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86SVP\u3002", "method": "SVP\u5c06\u5c42\u6fc0\u6d3b\u89c6\u4e3a\u6f5c\u53d8\u91cf\uff0c\u901a\u8fc7\u4f18\u5316\u5c40\u90e8\u8bc1\u636e\u4e0b\u9650\uff08ELBO\uff09\u5b9e\u73b0\u72ec\u7acb\u3001\u5c40\u90e8\u66f4\u65b0\u3002\u4e3a\u907f\u514d\u5c42\u95f4\u8868\u793a\u5d29\u6e83\uff0c\u4f7f\u7528\u56fa\u5b9a\u7684\u968f\u673a\u77e9\u9635\u5c06\u6fc0\u6d3b\u6295\u5f71\u5230\u4f4e\u7ef4\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408\u7279\u5f81\u5bf9\u9f50\u635f\u5931\u4fdd\u8bc1\u5c42\u95f4\u4e00\u81f4\u6027\u3002", "result": "SVP\u5728\u591a\u79cd\u67b6\u6784\uff08MLP\u3001CNN\u3001Transformer\uff09\u548c\u6570\u636e\u96c6\uff08MNIST\u5230ImageNet\uff09\u4e0a\u8fbe\u5230\u4e86\u4e0eBP\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u4e864\u500d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "SVP\u4e3a\u6df1\u5ea6\u8868\u793a\u5b66\u4e60\u5f15\u5165\u4e86\u4e00\u79cd\u6982\u7387\u89c6\u89d2\uff0c\u4e3a\u66f4\u5177\u6a21\u5757\u5316\u548c\u53ef\u89e3\u91ca\u6027\u7684\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2505.05423", "pdf": "https://arxiv.org/pdf/2505.05423", "abs": "https://arxiv.org/abs/2505.05423", "authors": ["Ran Zhang", "Wei Zhao", "Lieve Macken", "Steffen Eger"], "title": "TransProQA: an LLM-based literary Translation evaluation metric with Professional Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "WIP", "summary": "The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation (MT) as being\nsuperior to experienced professional human translation. In the long run, this\nbias could result in a permanent decline in translation quality and cultural\nauthenticity. In response to the urgent need for a specialized literary\nevaluation metric, we introduce TransProQA, a novel, reference-free, LLM-based\nquestion-answering (QA) framework designed specifically for literary\ntranslation evaluation. TransProQA uniquely integrates insights from\nprofessional literary translators and researchers, focusing on critical\nelements in literary quality assessment such as literary devices, cultural\nunderstanding, and authorial voice. Our extensive evaluation shows that while\nliterary-finetuned XCOMET-XL yields marginal gains, TransProQA substantially\noutperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ\nand Kendall's tau) and surpassing the best state-of-the-art (SOTA) metrics by\nover 15 points in adequacy assessments. Incorporating professional translator\ninsights as weights further improves performance, highlighting the value of\ntranslator inputs. Notably, TransProQA approaches human-level evaluation\nperformance comparable to trained linguistic annotators. It demonstrates broad\napplicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,\nindicating its potential as an accessible and training-free literary evaluation\nmetric and a valuable tool for evaluating texts that require local processing\ndue to copyright or ethical considerations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86TransProQA\uff0c\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u6587\u5b66\u7ffb\u8bd1\u8bc4\u4f30\u7684\u65e0\u53c2\u8003LLM\u95ee\u7b54\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6307\u6807\u8fc7\u5ea6\u5f3a\u8c03\u673a\u68b0\u51c6\u786e\u6027\u800c\u5ffd\u89c6\u827a\u672f\u8868\u8fbe\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u6027\u80fd\u5e76\u63a5\u8fd1\u4eba\u5de5\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u7ffb\u8bd1\u8bc4\u4f30\u6307\u6807\u8fc7\u4e8e\u5173\u6ce8\u673a\u68b0\u51c6\u786e\u6027\uff0c\u5ffd\u89c6\u4e86\u827a\u672f\u8868\u8fbe\u548c\u6587\u5316\u771f\u5b9e\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u7ffb\u8bd1\u8d28\u91cf\u7684\u957f\u671f\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u6025\u9700\u4e00\u79cd\u4e13\u95e8\u9488\u5bf9\u6587\u5b66\u7ffb\u8bd1\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86TransProQA\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u65e0\u53c2\u8003\u95ee\u7b54\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4e13\u4e1a\u6587\u5b66\u7ffb\u8bd1\u8005\u548c\u7814\u7a76\u8005\u7684\u89c1\u89e3\uff0c\u91cd\u70b9\u8bc4\u4f30\u6587\u5b66\u8d28\u91cf\u7684\u5173\u952e\u8981\u7d20\uff0c\u5982\u6587\u5b66\u624b\u6cd5\u3001\u6587\u5316\u7406\u89e3\u548c\u4f5c\u8005\u58f0\u97f3\u3002", "result": "TransProQA\u5728\u76f8\u5173\u6027\uff08ACC-EQ\u548cKendall's tau\uff09\u4e0a\u63d0\u5347\u4e860.07\uff0c\u5728\u9002\u5f53\u6027\u8bc4\u4f30\u4e0a\u8d85\u8fc7\u73b0\u6709\u6700\u4f73\u6307\u680715\u5206\u4ee5\u4e0a\uff0c\u5e76\u63a5\u8fd1\u4eba\u7c7b\u8bc4\u4f30\u6c34\u5e73\u3002", "conclusion": "TransProQA\u4f5c\u4e3a\u4e00\u79cd\u65e0\u8bad\u7ec3\u9700\u6c42\u7684\u6587\u5b66\u8bc4\u4f30\u6307\u6807\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u80fd\u6709\u6548\u8bc4\u4f30\u9700\u672c\u5730\u5904\u7406\u7684\u6587\u672c\uff0c\u662f\u6587\u5b66\u7ffb\u8bd1\u8bc4\u4f30\u7684\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2505.04841", "pdf": "https://arxiv.org/pdf/2505.04841", "abs": "https://arxiv.org/abs/2505.04841", "authors": ["Nishikanta Mohanty", "Bikash K. Behera", "Badsah Mukherjee", "Christopher Ferrie"], "title": "Quantum-Inspired Optimization Process for Data Imputation", "categories": ["quant-ph", "cs.AI"], "comment": null, "summary": "Data imputation is a critical step in data pre-processing, particularly for\ndatasets with missing or unreliable values. This study introduces a novel\nquantum-inspired imputation framework evaluated on the UCI Diabetes dataset,\nwhich contains biologically implausible missing values across several clinical\nfeatures. The method integrates Principal Component Analysis (PCA) with\nquantum-assisted rotations, optimized through gradient-free classical\noptimizers -COBYLA, Simulated Annealing, and Differential Evolution to\nreconstruct missing values while preserving statistical fidelity. Reconstructed\nvalues are constrained within +/-2 standard deviations of original feature\ndistributions, avoiding unrealistic clustering around central tendencies. This\napproach achieves a substantial and statistically significant improvement,\nincluding an average reduction of over 85% in Wasserstein distance and\nKolmogorov-Smirnov test p-values between 0.18 and 0.22, compared to p-values >\n0.99 in classical methods such as Mean, KNN, and MICE. The method also\neliminates zero-value artifacts and enhances the realism and variability of\nimputed data. By combining quantum-inspired transformations with a scalable\nclassical framework, this methodology provides a robust solution for imputation\ntasks in domains such as healthcare and AI pipelines, where data quality and\nintegrity are crucial.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u542f\u53d1\u5f0f\u6570\u636e\u586b\u8865\u6846\u67b6\uff0c\u7ed3\u5408PCA\u548c\u91cf\u5b50\u8f85\u52a9\u65cb\u8f6c\uff0c\u5728UCI\u7cd6\u5c3f\u75c5\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u586b\u8865\u6548\u679c\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u9884\u5904\u7406\u4e2d\u7f3a\u5931\u503c\u6216\u4e0d\u53ef\u9760\u503c\u7684\u95ee\u9898\uff0c\u5c24\u5176\u5728\u533b\u7597\u7b49\u5173\u952e\u9886\u57df\uff0c\u9700\u8981\u9ad8\u4fdd\u771f\u5ea6\u7684\u586b\u8865\u65b9\u6cd5\u3002", "method": "\u96c6\u6210PCA\u4e0e\u91cf\u5b50\u8f85\u52a9\u65cb\u8f6c\uff0c\u901a\u8fc7\u65e0\u68af\u5ea6\u4f18\u5316\u5668\uff08COBYLA\u3001\u6a21\u62df\u9000\u706b\u3001\u5dee\u5206\u8fdb\u5316\uff09\u4f18\u5316\u586b\u8865\u7f3a\u5931\u503c\uff0c\u5e76\u7ea6\u675f\u586b\u8865\u503c\u5728\u00b12\u6807\u51c6\u5dee\u5185\u3002", "result": "\u586b\u8865\u6548\u679c\u663e\u8457\u63d0\u5347\uff0cWasserstein\u8ddd\u79bb\u5e73\u5747\u51cf\u5c1185%\u4ee5\u4e0a\uff0cKolmogorov-Smirnov\u68c0\u9a8cp\u503c\u66f4\u4f18\uff080.18-0.22 vs \u4f20\u7edf\u65b9\u6cd5>0.99\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u91cf\u5b50\u542f\u53d1\u5f0f\u53d8\u6362\u4e0e\u53ef\u6269\u5c55\u7684\u7ecf\u5178\u6846\u67b6\uff0c\u4e3a\u533b\u7597\u548cAI\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u586b\u8865\u65b9\u6848\u3002"}}
{"id": "2505.05190", "pdf": "https://arxiv.org/pdf/2505.05190", "abs": "https://arxiv.org/abs/2505.05190", "authors": ["Yixin Cheng", "Hongcheng Guo", "Yangming Li", "Leonid Sigal"], "title": "Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "ICML 2025 Accpeted", "summary": "Text watermarking aims to subtly embed statistical signals into text by\ncontrolling the Large Language Model (LLM)'s sampling process, enabling\nwatermark detectors to verify that the output was generated by the specified\nmodel. The robustness of these watermarking algorithms has become a key factor\nin evaluating their effectiveness. Current text watermarking algorithms embed\nwatermarks in high-entropy tokens to ensure text quality. In this paper, we\nreveal that this seemingly benign design can be exploited by attackers, posing\na significant risk to the robustness of the watermark. We introduce a generic\nefficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA),\nwhich leverages the vulnerability by calculating the self-information of each\ntoken to identify potential pattern tokens and perform targeted attack. Our\nwork exposes a widely prevalent vulnerability in current watermarking\nalgorithms. The experimental results show SIRA achieves nearly 100% attack\nsuccess rates on seven recent watermarking methods with only 0.88 USD per\nmillion tokens cost. Our approach does not require any access to the watermark\nalgorithms or the watermarked LLM and can seamlessly transfer to any LLM as the\nattack model, even mobile-level models. Our findings highlight the urgent need\nfor more robust watermarking.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u5f53\u524d\u6587\u672c\u6c34\u5370\u6280\u672f\u5728\u9ad8\u71b5\u4ee4\u724c\u5d4c\u5165\u6c34\u5370\u7684\u8bbe\u8ba1\u5b58\u5728\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u4fe1\u606f\u91cd\u5199\u653b\u51fb\uff08SIRA\uff09\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cSIRA\u80fd\u4ee5\u4f4e\u6210\u672c\u5bf9\u4e03\u79cd\u6700\u65b0\u6c34\u5370\u65b9\u6cd5\u5b9e\u73b0\u63a5\u8fd1100%\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u7a81\u663e\u4e86\u63d0\u5347\u6c34\u5370\u9c81\u68d2\u6027\u7684\u7d27\u8feb\u6027\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u6c34\u5370\u7b97\u6cd5\u901a\u8fc7\u5728\u9ad8\u71b5\u4ee4\u724c\u5d4c\u5165\u6c34\u5370\u4ee5\u4fdd\u8bc1\u6587\u672c\u8d28\u91cf\uff0c\u4f46\u8be5\u8bbe\u8ba1\u53ef\u80fd\u88ab\u653b\u51fb\u8005\u5229\u7528\u3002\u672c\u6587\u65e8\u5728\u63ed\u9732\u8fd9\u4e00\u6f0f\u6d1e\u5e76\u63d0\u51fa\u653b\u51fb\u65b9\u6cd5\uff0c\u4ee5\u63a8\u52a8\u66f4\u5065\u58ee\u7684\u6c34\u5370\u6280\u672f\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u81ea\u4fe1\u606f\u91cd\u5199\u653b\u51fb\uff08SIRA\uff09\uff0c\u901a\u8fc7\u8ba1\u7b97\u4ee4\u724c\u7684\u81ea\u4fe1\u606f\u8bc6\u522b\u6f5c\u5728\u6a21\u5f0f\u4ee4\u724c\u5e76\u8fdb\u884c\u9488\u5bf9\u6027\u653b\u51fb\u3002\u65e0\u9700\u8bbf\u95ee\u6c34\u5370\u7b97\u6cd5\u6216\u6c34\u5370\u6a21\u578b\uff0c\u53ef\u8de8\u6a21\u578b\u8fc1\u79fb\u3002", "result": "SIRA\u5bf9\u4e03\u79cd\u6700\u65b0\u6c34\u5370\u65b9\u6cd5\u7684\u653b\u51fb\u6210\u529f\u7387\u63a5\u8fd1100%\uff0c\u6210\u672c\u4ec5\u4e3a\u6bcf\u767e\u4e07\u4ee4\u724c0.88\u7f8e\u5143\uff0c\u4e14\u9002\u7528\u4e8e\u79fb\u52a8\u7aef\u6a21\u578b\u3002", "conclusion": "\u5f53\u524d\u6c34\u5370\u6280\u672f\u5b58\u5728\u666e\u904d\u8106\u5f31\u6027\uff0c\u4e9f\u9700\u8bbe\u8ba1\u66f4\u9c81\u68d2\u7684\u7b97\u6cd5\u4ee5\u62b5\u5fa1\u7c7b\u4f3c\u653b\u51fb\u3002"}}
{"id": "2505.05427", "pdf": "https://arxiv.org/pdf/2505.05427", "abs": "https://arxiv.org/abs/2505.05427", "authors": ["Yudong Wang", "Zixuan Fu", "Jie Cai", "Peijun Tang", "Hongya Lyu", "Yewei Fang", "Zhi Zheng", "Jie Zhou", "Guoyang Zeng", "Chaojun Xiao", "Xu Han", "Zhiyuan Liu"], "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data", "categories": ["cs.CL"], "comment": "The datasets are available on\n  https://huggingface.co/datasets/openbmb/UltraFineWeb", "summary": "Data quality has become a key factor in enhancing model performance with the\nrapid development of large language models (LLMs). Model-driven data filtering\nhas increasingly become a primary approach for acquiring high-quality data.\nHowever, it still faces two main challenges: (1) the lack of an efficient data\nverification strategy makes it difficult to provide timely feedback on data\nquality; and (2) the selection of seed data for training classifiers lacks\nclear criteria and relies heavily on human expertise, introducing a degree of\nsubjectivity. To address the first challenge, we introduce an efficient\nverification strategy that enables rapid evaluation of the impact of data on\nLLM training with minimal computational cost. To tackle the second challenge,\nwe build upon the assumption that high-quality seed data is beneficial for LLM\ntraining, and by integrating the proposed verification strategy, we optimize\nthe selection of positive and negative samples and propose an efficient data\nfiltering pipeline. This pipeline not only improves filtering efficiency,\nclassifier quality, and robustness, but also significantly reduces experimental\nand inference costs. In addition, to efficiently filter high-quality data, we\nemploy a lightweight classifier based on fastText, and successfully apply the\nfiltering pipeline to two widely-used pre-training corpora, FineWeb and Chinese\nFineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb\ndataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120\nbillion Chinese tokens. Empirical results demonstrate that the LLMs trained on\nUltra-FineWeb exhibit significant performance improvements across multiple\nbenchmark tasks, validating the effectiveness of our pipeline in enhancing both\ndata quality and training efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u6570\u636e\u8fc7\u6ee4\u7ba1\u9053\uff0c\u89e3\u51b3\u6a21\u578b\u9a71\u52a8\u6570\u636e\u8fc7\u6ee4\u4e2d\u7684\u9a8c\u8bc1\u7b56\u7565\u548c\u79cd\u5b50\u6570\u636e\u9009\u62e9\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u6570\u636e\u8d28\u91cf\u6210\u4e3a\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002\u7136\u800c\uff0c\u5f53\u524d\u65b9\u6cd5\u7f3a\u4e4f\u9ad8\u6548\u7684\u6570\u636e\u9a8c\u8bc1\u7b56\u7565\uff0c\u4e14\u79cd\u5b50\u6570\u636e\u9009\u62e9\u4f9d\u8d56\u4e3b\u89c2\u6027\u8f83\u5f3a\u7684\u4eba\u5de5\u7ecf\u9a8c\u3002", "method": "\u5f15\u5165\u9ad8\u6548\u9a8c\u8bc1\u7b56\u7565\u548c\u4f18\u5316\u7684\u79cd\u5b50\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u6784\u5efa\u57fa\u4e8efastText\u7684\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\uff0c\u5f62\u6210\u6570\u636e\u8fc7\u6ee4\u7ba1\u9053\uff0c\u5e94\u7528\u4e8eFineWeb\u548cChinese FineWeb\u6570\u636e\u96c6\u3002", "result": "\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684Ultra-FineWeb\u6570\u636e\u96c6\uff08\u5305\u542b\u7ea61\u4e07\u4ebf\u82f1\u6587\u548c1200\u4ebf\u4e2d\u6587token\uff09\uff0c\u8bad\u7ec3\u51fa\u7684\u6a21\u578b\u5728\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u8fc7\u6ee4\u7ba1\u9053\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8fc7\u6ee4\u6548\u7387\u548c\u5206\u7c7b\u5668\u8d28\u91cf\uff0c\u8fd8\u663e\u8457\u964d\u4f4e\u4e86\u5b9e\u9a8c\u548c\u63a8\u7406\u6210\u672c\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u63d0\u5347\u6570\u636e\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.05192", "pdf": "https://arxiv.org/pdf/2505.05192", "abs": "https://arxiv.org/abs/2505.05192", "authors": ["Ruichu Cai", "Junjie Wan", "Weilin Chen", "Zeqin Yang", "Zijian Li", "Peng Zhen", "Jiecheng Guo"], "title": "Long-Term Individual Causal Effect Estimation via Identifiable Latent Representation Learning", "categories": ["cs.LG"], "comment": null, "summary": "Estimating long-term causal effects by combining long-term observational and\nshort-term experimental data is a crucial but challenging problem in many\nreal-world scenarios. In existing methods, several ideal assumptions, e.g.\nlatent unconfoundedness assumption or additive equi-confounding bias\nassumption, are proposed to address the latent confounder problem raised by the\nobservational data. However, in real-world applications, these assumptions are\ntypically violated which limits their practical effectiveness. In this paper,\nwe tackle the problem of estimating the long-term individual causal effects\nwithout the aforementioned assumptions. Specifically, we propose to utilize the\nnatural heterogeneity of data, such as data from multiple sources, to identify\nlatent confounders, thereby significantly avoiding reliance on idealized\nassumptions. Practically, we devise a latent representation learning-based\nestimator of long-term causal effects. Theoretically, we establish the\nidentifiability of latent confounders, with which we further achieve long-term\neffect identification. Extensive experimental studies, conducted on multiple\nsynthetic and semi-synthetic datasets, demonstrate the effectiveness of our\nproposed method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6570\u636e\u5929\u7136\u5f02\u8d28\u6027\uff08\u5982\u591a\u6e90\u6570\u636e\uff09\u6765\u8bc6\u522b\u6f5c\u5728\u6df7\u6742\u56e0\u7d20\u7684\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u4f9d\u8d56\u7406\u60f3\u5316\u5047\u8bbe\uff0c\u5e76\u901a\u8fc7\u6f5c\u5728\u8868\u793a\u5b66\u4e60\u4f30\u8ba1\u957f\u671f\u56e0\u679c\u6548\u5e94\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7406\u60f3\u5316\u5047\u8bbe\uff08\u5982\u6f5c\u5728\u65e0\u6df7\u6742\u5047\u8bbe\u6216\u52a0\u6027\u7b49\u6df7\u6742\u504f\u7f6e\u5047\u8bbe\uff09\u6765\u5e94\u5bf9\u89c2\u6d4b\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u6df7\u6742\u95ee\u9898\uff0c\u4f46\u8fd9\u4e9b\u5047\u8bbe\u5728\u73b0\u5b9e\u4e2d\u5e38\u88ab\u8fdd\u53cd\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "method": "\u5229\u7528\u6570\u636e\u7684\u5929\u7136\u5f02\u8d28\u6027\uff08\u5982\u591a\u6e90\u6570\u636e\uff09\u8bc6\u522b\u6f5c\u5728\u6df7\u6742\u56e0\u7d20\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u6f5c\u5728\u8868\u793a\u5b66\u4e60\u7684\u957f\u671f\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u5668\u3002", "result": "\u5728\u591a\u4e2a\u5408\u6210\u548c\u534a\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65e0\u9700\u4f9d\u8d56\u7406\u60f3\u5316\u5047\u8bbe\uff0c\u901a\u8fc7\u6570\u636e\u5f02\u8d28\u6027\u8bc6\u522b\u6f5c\u5728\u6df7\u6742\u56e0\u7d20\uff0c\u80fd\u6709\u6548\u4f30\u8ba1\u957f\u671f\u56e0\u679c\u6548\u5e94\u3002"}}
{"id": "2505.05445", "pdf": "https://arxiv.org/pdf/2505.05445", "abs": "https://arxiv.org/abs/2505.05445", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "title": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations", "categories": ["cs.CL"], "comment": "30 pages", "summary": "The emergence of instruction-tuned large language models (LLMs) has advanced\nthe field of dialogue systems, enabling both realistic user simulations and\nrobust multi-turn conversational agents. However, existing research often\nevaluates these components in isolation-either focusing on a single user\nsimulator or a specific system design-limiting the generalisability of insights\nacross architectures and configurations. In this work, we propose clem todd\n(chat-optimized LLMs for task-oriented dialogue systems development), a\nflexible framework for systematically evaluating dialogue systems under\nconsistent conditions. clem todd enables detailed benchmarking across\ncombinations of user simulators and dialogue systems, whether existing models\nfrom literature or newly developed ones. It supports plug-and-play integration\nand ensures uniform datasets, evaluation metrics, and computational\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\ntask-oriented dialogue systems within this unified setup and integrating three\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\nprovide actionable insights into how architecture, scale, and prompting\nstrategies affect dialogue performance, offering practical guidance for\nbuilding efficient and effective conversational AI systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86clem todd\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e00\u81f4\u6761\u4ef6\u4e0b\u7cfb\u7edf\u8bc4\u4f30\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u652f\u6301\u4e0d\u540c\u7528\u6237\u6a21\u62df\u5668\u548c\u5bf9\u8bdd\u7cfb\u7edf\u7684\u7ec4\u5408\uff0c\u63d0\u4f9b\u4e86\u5bf9\u67b6\u6784\u3001\u89c4\u6a21\u548c\u63d0\u793a\u7b56\u7565\u5f71\u54cd\u7684\u5b9e\u7528\u89c1\u89e3\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5b64\u7acb\u8bc4\u4f30\u5bf9\u8bdd\u7cfb\u7edf\u7684\u7ec4\u4ef6\uff0c\u7f3a\u4e4f\u5bf9\u8de8\u67b6\u6784\u548c\u914d\u7f6e\u7684\u901a\u7528\u6027\u6d1e\u5bdf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7075\u6d3b\u7684\u6846\u67b6\u6765\u7edf\u4e00\u8bc4\u4f30\u3002", "method": "\u63d0\u51faclem todd\u6846\u67b6\uff0c\u652f\u6301\u63d2\u62d4\u5f0f\u96c6\u6210\uff0c\u786e\u4fdd\u7edf\u4e00\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u91cd\u65b0\u8bc4\u4f30\u73b0\u6709\u7cfb\u7edf\u5e76\u6574\u5408\u65b0\u7cfb\u7edf\u3002", "result": "\u5c55\u793a\u4e86clem todd\u7684\u7075\u6d3b\u6027\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u67b6\u6784\u3001\u89c4\u6a21\u548c\u63d0\u793a\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u5bf9\u8bdd\u6027\u80fd\u7684\u5b9e\u7528\u89c1\u89e3\u3002", "conclusion": "clem todd\u4e3a\u6784\u5efa\u9ad8\u6548\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\uff0c\u63a8\u52a8\u4e86\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2505.05195", "pdf": "https://arxiv.org/pdf/2505.05195", "abs": "https://arxiv.org/abs/2505.05195", "authors": ["Xinyue Xu", "Yueying Hu", "Hui Tang", "Yi Qin", "Lu Mi", "Hao Wang", "Xiaomeng Li"], "title": "Concept-Based Unsupervised Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Concept Bottleneck Models (CBMs) enhance interpretability by explaining\npredictions through human-understandable concepts but typically assume that\ntraining and test data share the same distribution. This assumption often fails\nunder domain shifts, leading to degraded performance and poor generalization.\nTo address these limitations and improve the robustness of CBMs, we propose the\nConcept-based Unsupervised Domain Adaptation (CUDA) framework. CUDA is designed\nto: (1) align concept representations across domains using adversarial\ntraining, (2) introduce a relaxation threshold to allow minor domain-specific\ndifferences in concept distributions, thereby preventing performance drop due\nto over-constraints of these distributions, (3) infer concepts directly in the\ntarget domain without requiring labeled concept data, enabling CBMs to adapt to\ndiverse domains, and (4) integrate concept learning into conventional domain\nadaptation (DA) with theoretical guarantees, improving interpretability and\nestablishing new benchmarks for DA. Experiments demonstrate that our approach\nsignificantly outperforms the state-of-the-art CBM and DA methods on real-world\ndatasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCUDA\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u5bf9\u9f50\u6982\u5ff5\u8868\u5f81\u3001\u5f15\u5165\u677e\u5f1b\u9608\u503c\u5141\u8bb8\u6982\u5ff5\u5206\u5e03\u5dee\u5f02\u3001\u65e0\u9700\u6807\u6ce8\u6982\u5ff5\u6570\u636e\u76f4\u63a5\u63a8\u65ad\uff0c\u5e76\u878d\u5165\u4f20\u7edf\u57df\u9002\u5e94\u7406\u8bba\uff0c\u663e\u8457\u63d0\u5347CBMs\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "CBMs\u5728\u57df\u504f\u79fb\u4e0b\u6027\u80fd\u4e0b\u964d\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u63d0\u5347\u5176\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u5bf9\u6297\u8bad\u7ec3\u5bf9\u9f50\u6982\u5ff5\u8868\u5f81\uff0c\u5f15\u5165\u677e\u5f1b\u9608\u503c\uff0c\u65e0\u9700\u6807\u6ce8\u6982\u5ff5\u6570\u636e\u76f4\u63a5\u63a8\u65ad\uff0c\u5e76\u7406\u8bba\u878d\u5165\u4f20\u7edf\u57df\u9002\u5e94\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCUDA\u663e\u8457\u4f18\u4e8e\u73b0\u6709CBM\u548c\u57df\u9002\u5e94\u65b9\u6cd5\u3002", "conclusion": "CUDA\u6846\u67b6\u6709\u6548\u63d0\u5347CBMs\u5728\u57df\u9002\u5e94\u4e2d\u7684\u6027\u80fd\uff0c\u517c\u5177\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.05459", "pdf": "https://arxiv.org/pdf/2505.05459", "abs": "https://arxiv.org/abs/2505.05459", "authors": ["Fatima Haouari", "Carolina Scarton", "Nicol\u00f2 Faggiani", "Nikolaos Nikolaidis", "Bonka Kotseva", "Ibrahim Abu Farha", "Jens Linge", "Kalina Bontcheva"], "title": "UKElectionNarratives: A Dataset of Misleading Narratives Surrounding Recent UK General Elections", "categories": ["cs.CL", "cs.SI"], "comment": "This work was accepted at the International AAAI Conference on Web\n  and Social Media (ICWSM 2025)", "summary": "Misleading narratives play a crucial role in shaping public opinion during\nelections, as they can influence how voters perceive candidates and political\nparties. This entails the need to detect these narratives accurately. To\naddress this, we introduce the first taxonomy of common misleading narratives\nthat circulated during recent elections in Europe. Based on this taxonomy, we\nconstruct and analyse UKElectionNarratives: the first dataset of\nhuman-annotated misleading narratives which circulated during the UK General\nElections in 2019 and 2024. We also benchmark Pre-trained and Large Language\nModels (focusing on GPT-4o), studying their effectiveness in detecting\nelection-related misleading narratives. Finally, we discuss potential use cases\nand make recommendations for future research directions using the proposed\ncodebook and dataset.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u6b27\u6d32\u9009\u4e3e\u4e2d\u5e38\u89c1\u7684\u8bef\u5bfc\u6027\u53d9\u4e8b\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86UK\u9009\u4e3e\u53d9\u4e8b\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u9884\u8bad\u7ec3\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\uff09\u5728\u68c0\u6d4b\u8fd9\u7c7b\u53d9\u4e8b\u4e2d\u7684\u6548\u679c\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u5efa\u8bae\u3002", "motivation": "\u8bef\u5bfc\u6027\u53d9\u4e8b\u5728\u9009\u4e3e\u4e2d\u5f71\u54cd\u516c\u4f17\u8206\u8bba\uff0c\u56e0\u6b64\u9700\u8981\u51c6\u786e\u68c0\u6d4b\u8fd9\u4e9b\u53d9\u4e8b\u3002\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u7684\u5206\u7c7b\u548c\u6570\u636e\u96c6\u6765\u652f\u6301\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u7b2c\u4e00\u4e2a\u8bef\u5bfc\u6027\u53d9\u4e8b\u5206\u7c7b\u6cd5\uff0c\u6784\u5efa\u4eba\u7c7b\u6807\u6ce8\u7684UK\u9009\u4e3e\u53d9\u4e8b\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u6bd4\u9884\u8bad\u7ec3\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\uff09\u7684\u68c0\u6d4b\u6548\u679c\u3002", "result": "\u5efa\u7acb\u4e86\u5206\u7c7b\u6cd5\u548c\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86GPT-4o\u7b49\u6a21\u578b\u5728\u68c0\u6d4b\u8bef\u5bfc\u6027\u53d9\u4e8b\u4e0a\u7684\u6f5c\u529b\u3002", "conclusion": "\u5206\u7c7b\u6cd5\u548c\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u6d4b\u8bef\u5bfc\u6027\u53d9\u4e8b\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2505.04852", "pdf": "https://arxiv.org/pdf/2505.04852", "abs": "https://arxiv.org/abs/2505.04852", "authors": ["Yifei Gao", "Chengpeng Wang", "Pengxiang Huang", "Xuwei Liu", "Mingwei Zheng", "Xiangyu Zhang"], "title": "PR2: Peephole Raw Pointer Rewriting with LLMs for Translating C to Safer Rust", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "There has been a growing interest in translating C code to Rust due to Rust's\nrobust memory and thread safety guarantees. Tools such as C2RUST enable\nsyntax-guided transpilation from C to semantically equivalent Rust code.\nHowever, the resulting Rust programs often rely heavily on unsafe\nconstructs--particularly raw pointers--which undermines Rust's safety\nguarantees. This paper aims to improve the memory safety of Rust programs\ngenerated by C2RUST by eliminating raw pointers. Specifically, we propose a\npeephole raw pointer rewriting technique that lifts raw pointers in individual\nfunctions to appropriate Rust data structures. Technically, PR2 employs\ndecision-tree-based prompting to guide the pointer lifting process.\nAdditionally, it leverages code change analysis to guide the repair of errors\nintroduced during rewriting, effectively addressing errors encountered during\ncompilation and test case execution. We implement PR2 as a prototype and\nevaluate it using gpt-4o-mini on 28 real-world C projects. The results show\nthat PR2 successfully eliminates 13.22% of local raw pointers across these\nprojects, significantly enhancing the safety of the translated Rust code. On\naverage, PR2 completes the transformation of a project in 5.44 hours, at an\naverage cost of $1.46.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPR2\u6280\u672f\uff0c\u901a\u8fc7\u4f18\u5316C2RUST\u751f\u6210\u7684Rust\u4ee3\u7801\uff0c\u51cf\u5c11\u4e0d\u5b89\u5168\u6307\u9488\u7684\u4f7f\u7528\uff0c\u63d0\u5347\u5185\u5b58\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709C2RUST\u5de5\u5177\u751f\u6210\u7684Rust\u4ee3\u7801\u4f9d\u8d56\u4e0d\u5b89\u5168\u6307\u9488\uff0c\u524a\u5f31\u4e86Rust\u7684\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u5c40\u90e8\u6307\u9488\u91cd\u5199\u6280\u672f\uff08PR2\uff09\uff0c\u7ed3\u5408\u51b3\u7b56\u6811\u63d0\u793a\u548c\u4ee3\u7801\u53d8\u66f4\u5206\u6790\uff0c\u63d0\u5347\u6307\u9488\u8f6c\u6362\u7684\u51c6\u786e\u6027\u3002", "result": "\u572828\u4e2aC\u9879\u76ee\u4e2d\uff0cPR2\u6210\u529f\u6d88\u9664\u4e8613.22%\u7684\u672c\u5730\u539f\u59cb\u6307\u9488\uff0c\u5e73\u5747\u6bcf\u4e2a\u9879\u76ee\u8017\u65f65.44\u5c0f\u65f6\uff0c\u6210\u672c1.46\u7f8e\u5143\u3002", "conclusion": "PR2\u663e\u8457\u63d0\u5347\u4e86Rust\u4ee3\u7801\u7684\u5b89\u5168\u6027\uff0c\u4e3aC\u5230Rust\u7684\u9ad8\u8d28\u91cf\u8f6c\u6362\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.05224", "pdf": "https://arxiv.org/pdf/2505.05224", "abs": "https://arxiv.org/abs/2505.05224", "authors": ["Charbel Bou Chaaya", "Mehdi Bennis"], "title": "GFlowNets for Active Learning Based Resource Allocation in Next Generation Wireless Networks", "categories": ["cs.LG"], "comment": null, "summary": "In this work, we consider the radio resource allocation problem in a wireless\nsystem with various integrated functionalities, such as communication, sensing\nand computing. We design suitable resource management techniques that can\nsimultaneously cater to those heterogeneous requirements, and scale\nappropriately with the high-dimensional and discrete nature of the problem. We\npropose a novel active learning framework where resource allocation patterns\nare drawn sequentially, evaluated in the environment, and then used to\niteratively update a surrogate model of the environment. Our method leverages a\ngenerative flow network (GFlowNet) to sample favorable solutions, as such\nmodels are trained to generate compositional objects proportionally to their\ntraining reward, hence providing an appropriate coverage of its modes. As such,\nGFlowNet generates diverse and high return resource management designs that\nupdate the surrogate model and swiftly discover suitable solutions. We provide\nsimulation results showing that our method can allocate radio resources\nachieving 20% performance gains against benchmarks, while requiring less than\nhalf of the number of acquisition rounds.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u548c\u751f\u6210\u6d41\u7f51\u7edc\uff08GFlowNet\uff09\u7684\u65e0\u7ebf\u7cfb\u7edf\u8d44\u6e90\u5206\u914d\u65b9\u6cd5\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u9ad8\u7ef4\u79bb\u6563\u95ee\u9898\uff0c\u6027\u80fd\u63d0\u534720%\uff0c\u4e14\u51cf\u5c11\u4e00\u534a\u7684\u83b7\u53d6\u8f6e\u6b21\u3002", "motivation": "\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u901a\u4fe1\u3001\u611f\u77e5\u548c\u8ba1\u7b97\u7b49\u529f\u80fd\u7684\u9700\u6c42\u591a\u6837\u5316\uff0c\u4f20\u7edf\u8d44\u6e90\u5206\u914d\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u9ad8\u7ef4\u548c\u79bb\u6563\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408GFlowNet\u751f\u6210\u591a\u6837\u5316\u8d44\u6e90\u5206\u914d\u65b9\u6848\uff0c\u901a\u8fc7\u8fed\u4ee3\u66f4\u65b0\u73af\u5883\u4ee3\u7406\u6a21\u578b\u6765\u4f18\u5316\u5206\u914d\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u63d0\u534720%\uff0c\u4e14\u6240\u9700\u83b7\u53d6\u8f6e\u6b21\u51cf\u534a\u3002", "conclusion": "\u63d0\u51fa\u7684GFlowNet\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u9ad8\u7ef4\u79bb\u6563\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u517c\u5177\u591a\u6837\u6027\u548c\u9ad8\u6548\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u65e0\u7ebf\u7cfb\u7edf\u3002"}}
{"id": "2505.05464", "pdf": "https://arxiv.org/pdf/2505.05464", "abs": "https://arxiv.org/abs/2505.05464", "authors": ["Shiqi Chen", "Jinghan Zhang", "Tongyao Zhu", "Wei Liu", "Siyang Gao", "Miao Xiong", "Manling Li", "Junxian He"], "title": "Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging", "categories": ["cs.CL"], "comment": "ICML 2025. Our code is publicly available at\n  https://github.com/shiqichen17/VLM_Merging", "summary": "Vision-Language Models (VLMs) combine visual perception with the general\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\nmechanisms by which these two abilities can be combined and contribute remain\npoorly understood. In this work, we explore to compose perception and reasoning\nthrough model merging that connects parameters of different models. Unlike\nprevious works that often focus on merging models of the same kind, we propose\nmerging models across modalities, enabling the incorporation of the reasoning\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\nthat model merging offers a successful pathway to transfer reasoning abilities\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\nmodels to understand the internal mechanism of perception and reasoning and how\nmerging affects it. We find that perception capabilities are predominantly\nencoded in the early layers of the model, whereas reasoning is largely\nfacilitated by the middle-to-late layers. After merging, we observe that all\nlayers begin to contribute to reasoning, whereas the distribution of perception\nabilities across layers remains largely unchanged. These observations shed\nlight on the potential of model merging as a tool for multimodal integration\nand interpretation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u6a21\u578b\u878d\u5408\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\u878d\u5165\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002\u5b9e\u9a8c\u8868\u660e\u611f\u77e5\u80fd\u529b\u4e3b\u8981\u5b58\u5728\u4e8e\u6a21\u578b\u7684\u65e9\u671f\u5c42\uff0c\u800c\u63a8\u7406\u80fd\u529b\u5219\u7531\u4e2d\u540e\u5c42\u4e3b\u5bfc\u3002\u878d\u5408\u540e\u5404\u5c42\u5747\u53c2\u4e0e\u63a8\u7406\uff0c\u800c\u611f\u77e5\u80fd\u529b\u5206\u5e03\u4e0d\u53d8\u3002", "motivation": "\u7814\u7a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e2d\u89c6\u89c9\u611f\u77e5\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u7684\u7ed3\u5408\u673a\u5236\uff0c\u63a2\u7d22\u8de8\u6a21\u6001\u6a21\u578b\u878d\u5408\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u8de8\u6a21\u6001\u6a21\u578b\u878d\u5408\u65b9\u6cd5\uff0c\u5c06LLMs\u7684\u53c2\u6570\u4e0eVLMs\u5408\u5e76\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u4f20\u9012\u63a8\u7406\u80fd\u529b\u7684\u6548\u679c\u3002", "result": "\u878d\u5408\u6a21\u578b\u6210\u529f\u5c06LLMs\u7684\u63a8\u7406\u80fd\u529b\u8f6c\u79fb\u5230VLMs\uff0c\u5e76\u63ed\u793a\u4e86\u611f\u77e5\u80fd\u529b\u96c6\u4e2d\u4e8e\u65e9\u671f\u5c42\u3001\u63a8\u7406\u80fd\u529b\u7531\u4e2d\u540e\u5c42\u4e3b\u5bfc\u7684\u673a\u5236\u3002\u878d\u5408\u540e\u5404\u5c42\u5747\u53c2\u4e0e\u63a8\u7406\uff0c\u611f\u77e5\u80fd\u529b\u5206\u5e03\u4e0d\u53d8\u3002", "conclusion": "\u6a21\u578b\u878d\u5408\u662f\u8de8\u6a21\u6001\u80fd\u529b\u6574\u5408\u4e0e\u673a\u5236\u7406\u89e3\u7684\u6709\u6548\u5de5\u5177\uff0c\u4e3a\u591a\u6a21\u6001\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2505.04860", "pdf": "https://arxiv.org/pdf/2505.04860", "abs": "https://arxiv.org/abs/2505.04860", "authors": ["I-Chun Arthur Liu", "Jason Chen", "Gaurav Sukhatme", "Daniel Seita"], "title": "D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Learning bimanual manipulation is challenging due to its high dimensionality\nand tight coordination required between two arms. Eye-in-hand imitation\nlearning, which uses wrist-mounted cameras, simplifies perception by focusing\non task-relevant views. However, collecting diverse demonstrations remains\ncostly, motivating the need for scalable data augmentation. While prior work\nhas explored visual augmentation in single-arm settings, extending these\napproaches to bimanual manipulation requires generating viewpoint-consistent\nobservations across both arms and producing corresponding action labels that\nare both valid and feasible. In this work, we propose Diffusion for COordinated\nDual-arm Data Augmentation (D-CODA), a method for offline data augmentation\ntailored to eye-in-hand bimanual imitation learning that trains a diffusion\nmodel to synthesize novel, viewpoint-consistent wrist-camera images for both\narms while simultaneously generating joint-space action labels. It employs\nconstrained optimization to ensure that augmented states involving\ngripper-to-object contacts adhere to constraints suitable for bimanual\ncoordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our\nresults across 2250 simulation trials and 300 real-world trials demonstrate\nthat it outperforms baselines and ablations, showing its potential for scalable\ndata augmentation in eye-in-hand bimanual manipulation. Our project website is\nat: https://dcodaaug.github.io/D-CODA/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aD-CODA\u7684\u53cc\u81c2\u534f\u8c03\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u89c6\u89d2\u4e00\u81f4\u7684\u8155\u90e8\u76f8\u673a\u56fe\u50cf\u548c\u5173\u8282\u52a8\u4f5c\u6807\u7b7e\uff0c\u89e3\u51b3\u4e86\u53cc\u81c2\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u5e76\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u53cc\u81c2\u64cd\u4f5c\u5177\u6709\u9ad8\u7ef4\u5ea6\u548c\u7d27\u5bc6\u534f\u8c03\u6027\u7684\u6311\u6218\uff0c\u4f20\u7edf\u7684\u901a\u8fc7\u89c2\u5bdf\u5b66\u4e60\uff08eye-in-hand imitation learning\uff09\u867d\u7136\u7b80\u5316\u4e86\u611f\u77e5\uff0c\u4f46\u6536\u96c6\u591a\u6837\u5316\u7684\u6f14\u793a\u6570\u636e\u6210\u672c\u9ad8\u6602\uff0c\u4e9f\u9700\u53ef\u6269\u5c55\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86D-CODA\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u79bb\u7ebf\u751f\u6210\u53cc\u81c2\u7684\u89c6\u89d2\u4e00\u81f4\u56fe\u50cf\u548c\u5173\u8282\u52a8\u4f5c\u6807\u7b7e\uff0c\u5e76\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u786e\u4fdd\u751f\u6210\u7684\u63a5\u89e6\u72b6\u6001\u9002\u5408\u53cc\u81c2\u534f\u8c03\u3002", "result": "\u57285\u4e2a\u6a21\u62df\u4efb\u52a1\u548c3\u4e2a\u73b0\u5b9e\u4efb\u52a1\u4e2d\uff0cD-CODA\u57282250\u6b21\u6a21\u62df\u8bd5\u9a8c\u548c300\u6b21\u73b0\u5b9e\u8bd5\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6570\u636e\u589e\u5f3a\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "D-CODA\u4e3a\u53cc\u81c2\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u80fd\u5728\u66f4\u590d\u6742\u7684\u53cc\u81c2\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5176\u6f5c\u529b\u3002"}}
{"id": "2505.05226", "pdf": "https://arxiv.org/pdf/2505.05226", "abs": "https://arxiv.org/abs/2505.05226", "authors": ["Amir Rezaei Balef", "Claire Vernade", "Katharina Eggensperger"], "title": "Put CASH on Bandits: A Max K-Armed Problem for Automated Machine Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The Combined Algorithm Selection and Hyperparameter optimization (CASH) is a\nchallenging resource allocation problem in the field of AutoML. We propose\nMaxUCB, a max $k$-armed bandit method to trade off exploring different model\nclasses and conducting hyperparameter optimization. MaxUCB is specifically\ndesigned for the light-tailed and bounded reward distributions arising in this\nsetting and, thus, provides an efficient alternative compared to classic max\n$k$-armed bandit methods assuming heavy-tailed reward distributions. We\ntheoretically and empirically evaluate our method on four standard AutoML\nbenchmarks, demonstrating superior performance over prior approaches.", "AI": {"tldr": "MaxUCB\u662f\u4e00\u79cd\u9488\u5bf9AutoML\u4e2dCASH\u95ee\u9898\u7684max $k$-armed bandit\u65b9\u6cd5\uff0c\u4e13\u4e3a\u8f7b\u5c3e\u548c\u6709\u754c\u5956\u52b1\u5206\u5e03\u8bbe\u8ba1\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3AutoML\u4e2d\u7ec4\u5408\u7b97\u6cd5\u9009\u62e9\u548c\u8d85\u53c2\u6570\u4f18\u5316\uff08CASH\uff09\u7684\u9ad8\u8d44\u6e90\u6d88\u8017\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMaxUCB\uff0c\u4e00\u79cdmax $k$-armed bandit\u65b9\u6cd5\uff0c\u4e13\u4e3a\u8f7b\u5c3e\u548c\u6709\u754c\u5956\u52b1\u5206\u5e03\u8bbe\u8ba1\uff0c\u5e73\u8861\u63a2\u7d22\u4e0d\u540c\u6a21\u578b\u7c7b\u548c\u8d85\u53c2\u6570\u4f18\u5316\u3002", "result": "\u5728\u56db\u4e2a\u6807\u51c6AutoML\u57fa\u51c6\u4e0a\uff0cMaxUCB\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MaxUCB\u4e3aCASH\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8f7b\u5c3e\u5956\u52b1\u5206\u5e03\u573a\u666f\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2505.05465", "pdf": "https://arxiv.org/pdf/2505.05465", "abs": "https://arxiv.org/abs/2505.05465", "authors": ["Peter Chen", "Xi Chen", "Wotao Yin", "Tianyi Lin"], "title": "ComPO: Preference Alignment via Comparison Oracles", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages", "summary": "Direct alignment methods are increasingly used for aligning large language\nmodels (LLMs) with human preferences. However, these methods suffer from the\nissues of verbosity and likelihood displacement, which can be driven by the\nnoisy preference pairs that induce similar likelihood for preferred and\ndispreferred responses. The contributions of this paper are two-fold. First, we\npropose a new preference alignment method based on comparison oracles and\nprovide the convergence guarantee for its basic scheme. Second, we improve our\nmethod using some heuristics and conduct the experiments to demonstrate the\nflexibility and compatibility of practical scheme in improving the performance\nof LLMs using noisy preference pairs. Evaluations are conducted across multiple\nbase and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with\nbenchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show\nthe effectiveness of our method as an alternative to addressing the limitations\nof existing direct alignment methods. A highlight of our work is that we\nevidence the importance of designing specialized methods for preference pairs\nwith distinct likelihood margin, which complements the recent findings in\n\\citet{Razin-2025-Unintentional}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6bd4\u8f83\u9884\u8a00\u673a\u7684\u65b0\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u76f4\u63a5\u5bf9\u9f50\u65b9\u6cd5\u4e2d\u7684\u5197\u4f59\u95ee\u9898\u548c\u4f3c\u7136\u504f\u79fb\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u76f4\u63a5\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u5197\u4f59\u548c\u4f3c\u7136\u504f\u79fb\u95ee\u9898\uff0c\u5c24\u5176\u5728\u566a\u58f0\u504f\u597d\u5bf9\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u517c\u5bb9\u6027\u5f3a\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6bd4\u8f83\u9884\u8a00\u673a\u7684\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\uff0c\u63d0\u4f9b\u57fa\u672c\u65b9\u6848\u7684\u6536\u655b\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7\u542f\u53d1\u5f0f\u65b9\u6cd5\u6539\u8fdb\u65b9\u6848\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6a21\u578b\uff08\u5982Mistral-7B\u3001Llama-3-8B\u7b49\uff09\u548c\u57fa\u51c6\u6d4b\u8bd5\uff08AlpacaEval 2\u7b49\uff09\u4e2d\uff0c\u65b0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u9488\u5bf9\u4f3c\u7136\u8fb9\u9645\u5dee\u5f02\u7684\u504f\u597d\u5bf9\u8bbe\u8ba1\u4e13\u7528\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\uff0c\u8865\u5145\u4e86\u8fd1\u671f\u7814\u7a76\u4e2d\u7684\u53d1\u73b0\u3002"}}
{"id": "2505.04864", "pdf": "https://arxiv.org/pdf/2505.04864", "abs": "https://arxiv.org/abs/2505.04864", "authors": ["Kanggeon Lee", "Soochahn Lee", "Kyoung Mu Lee"], "title": "Auto-regressive transformation for image alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing methods for image alignment struggle in cases involving\nfeature-sparse regions, extreme scale and field-of-view differences, and large\ndeformations, often resulting in suboptimal accuracy. Robustness to these\nchallenges improves through iterative refinement of the transformation field\nwhile focusing on critical regions in multi-scale image representations. We\nthus propose Auto-Regressive Transformation (ART), a novel method that\niteratively estimates the coarse-to-fine transformations within an\nauto-regressive framework. Leveraging hierarchical multi-scale features, our\nnetwork refines the transformations using randomly sampled points at each\nscale. By incorporating guidance from the cross-attention layer, the model\nfocuses on critical regions, ensuring accurate alignment even in challenging,\nfeature-limited conditions. Extensive experiments across diverse datasets\ndemonstrate that ART significantly outperforms state-of-the-art methods,\nestablishing it as a powerful new method for precise image alignment with broad\napplicability.", "AI": {"tldr": "ART\u662f\u4e00\u79cd\u65b0\u578b\u56fe\u50cf\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u6846\u67b6\u8fed\u4ee3\u4f18\u5316\u591a\u5c3a\u5ea6\u53d8\u6362\uff0c\u5229\u7528\u8de8\u6ce8\u610f\u529b\u5c42\u5f15\u5bfc\u5173\u952e\u533a\u57df\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u7279\u5f81\u7a00\u758f\u6216\u5927\u53d8\u5f62\u573a\u666f\u4e2d\u7684\u5bf9\u9f50\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u5bf9\u9f50\u65b9\u6cd5\u5728\u7279\u5f81\u7a00\u758f\u3001\u5c3a\u5ea6\u5dee\u5f02\u5927\u6216\u53d8\u5f62\u4e25\u91cd\u7684\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faART\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u56de\u5f52\u6846\u67b6\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u901a\u8fc7\u8fed\u4ee3\u91c7\u6837\u5173\u952e\u70b9\u5e76\u5229\u7528\u8de8\u6ce8\u610f\u529b\u5c42\u4f18\u5316\u53d8\u6362\u573a\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\uff0cART\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u5bf9\u9f50\u7cbe\u5ea6\u3002", "conclusion": "ART\u4e3a\u89e3\u51b3\u590d\u6742\u56fe\u50cf\u5bf9\u9f50\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2505.05237", "pdf": "https://arxiv.org/pdf/2505.05237", "abs": "https://arxiv.org/abs/2505.05237", "authors": ["Ruxue Shi", "Hengrui Gu", "Hangting Ye", "Yiwei Dai", "Xu Shen", "Xin Wang"], "title": "Latte: Transfering LLMs` Latent-level Knowledge for Few-shot Tabular Learning", "categories": ["cs.LG"], "comment": null, "summary": "Few-shot tabular learning, in which machine learning models are trained with\na limited amount of labeled data, provides a cost-effective approach to\naddressing real-world challenges. The advent of Large Language Models (LLMs)\nhas sparked interest in leveraging their pre-trained knowledge for few-shot\ntabular learning. Despite promising results, existing approaches either rely on\ntest-time knowledge extraction, which introduces undesirable latency, or\ntext-level knowledge, which leads to unreliable feature engineering. To\novercome these limitations, we propose Latte, a training-time knowledge\nextraction framework that transfers the latent prior knowledge within LLMs to\noptimize a more generalized downstream model. Latte enables general\nknowledge-guided downstream tabular learning, facilitating the weighted fusion\nof information across different feature values while reducing the risk of\noverfitting to limited labeled data. Furthermore, Latte is compatible with\nexisting unsupervised pre-training paradigms and effectively utilizes available\nunlabeled samples to overcome the performance limitations imposed by an\nextremely small labeled dataset. Extensive experiments on various few-shot\ntabular learning benchmarks demonstrate the superior performance of Latte,\nestablishing it as a state-of-the-art approach in this domain", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLatte\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5c11\u91cf\u6807\u8bb0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u8868\u683c\u5b66\u4e60\uff0c\u901a\u8fc7\u8bad\u7ec3\u65f6\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u63d0\u53d6\u6f5c\u77e5\u8bc6\u6765\u4f18\u5316\u4e0b\u6e38\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6d4b\u8bd5\u65f6\u77e5\u8bc6\u63d0\u53d6\u6216\u6587\u672c\u7ea7\u77e5\u8bc6\u5229\u7528\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u5c11\u91cf\u8868\u683c\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6d4b\u8bd5\u65f6\u7684\u77e5\u8bc6\u63d0\u53d6\uff08\u5bfc\u81f4\u5ef6\u8fdf\uff09\uff0c\u8981\u4e48\u4ec5\u5229\u7528\u6587\u672c\u7ea7\u77e5\u8bc6\uff08\u5bfc\u81f4\u7279\u5f81\u5de5\u7a0b\u4e0d\u53ef\u9760\uff09\uff0cLatte\u65e8\u5728\u901a\u8fc7\u8bad\u7ec3\u65f6\u63d0\u53d6LLMs\u7684\u6f5c\u77e5\u8bc6\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "Latte\u901a\u8fc7\u5c06LLMs\u4e2d\u7684\u6f5c\u77e5\u8bc6\u8f6c\u79fb\u5230\u4e0b\u6e38\u6a21\u578b\uff0c\u5b9e\u73b0\u5e7f\u4e49\u7684\u77e5\u8bc6\u5f15\u5bfc\u5b66\u4e60\uff0c\u652f\u6301\u4e0d\u540c\u7279\u5f81\u503c\u4fe1\u606f\u7684\u52a0\u6743\u878d\u5408\uff0c\u5e76\u51cf\u5c11\u5bf9\u6709\u9650\u6807\u8bb0\u6570\u636e\u7684\u8fc7\u62df\u5408\u3002\u6b64\u5916\uff0cLatte\u517c\u5bb9\u73b0\u6709\u7684\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u80fd\u6709\u6548\u5229\u7528\u672a\u6807\u8bb0\u6837\u672c\u3002", "result": "\u5728\u591a\u7ec4\u5c11\u91cf\u8868\u683c\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLatte\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u6210\u4e3a\u8be5\u9886\u57df\u7684\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "Latte\u901a\u8fc7\u8bad\u7ec3\u65f6\u63d0\u53d6LLMs\u6f5c\u77e5\u8bc6\uff0c\u6709\u6548\u4f18\u5316\u4e86\u5c11\u91cf\u8868\u683c\u5b66\u4e60\u4efb\u52a1\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u5353\u8d8a\u6027\u80fd\uff0c\u6210\u4e3a\u8be5\u9886\u57df\u7684\u6807\u6746\u65b9\u6cd5\u3002"}}
{"id": "2505.05242", "pdf": "https://arxiv.org/pdf/2505.05242", "abs": "https://arxiv.org/abs/2505.05242", "authors": ["Hechuan Wen", "Tong Chen", "Mingming Gong", "Li Kheng Chai", "Shazia Sadiq", "Hongzhi Yin"], "title": "Enhancing Treatment Effect Estimation via Active Learning: A Counterfactual Covering Perspective", "categories": ["cs.LG"], "comment": "Accepted by ICML'25", "summary": "Although numerous complex algorithms for treatment effect estimation have\nbeen developed in recent years, their effectiveness remains limited when\nhandling insufficiently labeled training sets due to the high cost of labeling\nthe effect after treatment, e.g., expensive tumor imaging or biopsy procedures\nneeded to evaluate treatment effects. Therefore, it becomes essential to\nactively incorporate more high-quality labeled data, all while adhering to a\nconstrained labeling budget. To enable data-efficient treatment effect\nestimation, we formalize the problem through rigorous theoretical analysis\nwithin the active learning context, where the derived key measures --\n\\textit{factual} and \\textit{counterfactual covering radius} determine the risk\nupper bound. To reduce the bound, we propose a greedy radius reduction\nalgorithm, which excels under an idealized, balanced data distribution. To\ngeneralize to more realistic data distributions, we further propose FCCM, which\ntransforms the optimization objective into the \\textit{Factual} and\n\\textit{Counterfactual Coverage Maximization} to ensure effective radius\nreduction during data acquisition. Furthermore, benchmarking FCCM against other\nbaselines demonstrates its superiority across both fully synthetic and\nsemi-synthetic datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u65b9\u6cd5FCCM\uff0c\u7528\u4e8e\u5728\u6709\u9650\u6807\u6ce8\u9884\u7b97\u4e0b\u9ad8\u6548\u4f30\u8ba1\u6cbb\u7597\u6548\u679c\uff0c\u901a\u8fc7\u4f18\u5316\u8986\u76d6\u534a\u5f84\u964d\u4f4e\u98ce\u9669\u4e0a\u754c\uff0c\u5728\u5408\u6210\u548c\u534a\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u6cbb\u7597\u540e\u6548\u679c\u6807\u6ce8\u6210\u672c\u9ad8\uff08\u5982\u80bf\u7624\u5f71\u50cf\u6216\u6d3b\u68c0\uff09\uff0c\u73b0\u6709\u7b97\u6cd5\u5728\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u65f6\u6548\u679c\u53d7\u9650\uff0c\u56e0\u6b64\u9700\u5728\u6709\u9650\u9884\u7b97\u4e0b\u4e3b\u52a8\u83b7\u53d6\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u3002", "method": "\u7406\u8bba\u5206\u6790\u4e86\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\u4e0b\u7684\u98ce\u9669\u4e0a\u754c\uff0c\u63d0\u51fa\u8d2a\u5fc3\u534a\u5f84\u7f29\u51cf\u7b97\u6cd5\u53caFCCM\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u4e8b\u5b9e\u4e0e\u53cd\u4e8b\u5b9e\u8986\u76d6\u4f18\u5316\u6570\u636e\u91c7\u96c6\u3002", "result": "FCCM\u5728\u5408\u6210\u548c\u534a\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FCCM\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6709\u9650\u6807\u6ce8\u9884\u7b97\u4e0b\u7684\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u6548\u7387\u3002"}}
{"id": "2505.04877", "pdf": "https://arxiv.org/pdf/2505.04877", "abs": "https://arxiv.org/abs/2505.04877", "authors": ["Lianbo Ma", "Jianlun Ma", "Yuee Zhou", "Guoyang Xie", "Qiang He", "Zhichao Lu"], "title": "Learning from Loss Landscape: Generalizable Mixed-Precision Quantization via Adaptive Sharpness-Aware Gradient Aligning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mixed Precision Quantization (MPQ) has become an essential technique for\noptimizing neural network by determining the optimal bitwidth per layer.\nExisting MPQ methods, however, face a major hurdle: they require a\ncomputationally expensive search for quantization policies on large-scale\ndatasets. To resolve this issue, we introduce a novel approach that first\nsearches for quantization policies on small datasets and then generalizes them\nto large-scale datasets. This approach simplifies the process, eliminating the\nneed for large-scale quantization fine-tuning and only necessitating model\nweight adjustment. Our method is characterized by three key techniques:\nsharpness-aware minimization for enhanced quantization generalization, implicit\ngradient direction alignment to handle gradient conflicts among different\noptimization objectives, and an adaptive perturbation radius to accelerate\noptimization. Both theoretical analysis and experimental results validate our\napproach. Using the CIFAR10 dataset (just 0.5\\% the size of ImageNet training\ndata) for MPQ policy search, we achieved equivalent accuracy on ImageNet with a\nsignificantly lower computational cost, while improving efficiency by up to\n150% over the baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u641c\u7d22\u91cf\u5316\u7b56\u7565\u5e76\u63a8\u5e7f\u81f3\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u641c\u7d22\u91cf\u5316\u7b56\u7565\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u5173\u952e\u6280\u672f\uff1a\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\u589e\u5f3a\u91cf\u5316\u6cdb\u5316\u80fd\u529b\u3001\u9690\u5f0f\u68af\u5ea6\u65b9\u5411\u5bf9\u9f50\u5904\u7406\u4e0d\u540c\u4f18\u5316\u76ee\u6807\u7684\u68af\u5ea6\u51b2\u7a81\u3001\u81ea\u9002\u5e94\u6270\u52a8\u534a\u5f84\u52a0\u901f\u4f18\u5316\u3002", "result": "\u5728CIFAR10\u6570\u636e\u96c6\uff08\u4ec5ImageNet\u76840.5%\u5927\u5c0f\uff09\u4e0a\u641c\u7d22\u91cf\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u5728ImageNet\u4e0a\u540c\u7b49\u7cbe\u5ea6\u4e14\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\uff0c\u6548\u7387\u63d0\u5347\u9ad8\u8fbe150%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u641c\u7d22\u91cf\u5316\u7b56\u7565\u5e76\u63a8\u5e7f\u81f3\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05262", "pdf": "https://arxiv.org/pdf/2505.05262", "abs": "https://arxiv.org/abs/2505.05262", "authors": ["Andreas Kontogiannis", "Konstantinos Papathanasiou", "Yi Shen", "Giorgos Stamou", "Michael M. Zavlanos", "George Vouros"], "title": "Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": "Accepted (Poster) at ICML 2025", "summary": "Learning to cooperate in distributed partially observable environments with\nno communication abilities poses significant challenges for multi-agent deep\nreinforcement learning (MARL). This paper addresses key concerns in this\ndomain, focusing on inferring state representations from individual agent\nobservations and leveraging these representations to enhance agents'\nexploration and collaborative task execution policies. To this end, we propose\na novel state modelling framework for cooperative MARL, where agents infer\nmeaningful belief representations of the non-observable state, with respect to\noptimizing their own policies, while filtering redundant and less informative\njoint state information. Building upon this framework, we propose the MARL SMPE\nalgorithm. In SMPE, agents enhance their own policy's discriminative abilities\nunder partial observability, explicitly by incorporating their beliefs into the\npolicy network, and implicitly by adopting an adversarial type of exploration\npolicies which encourages agents to discover novel, high-value states while\nimproving the discriminative abilities of others. Experimentally, we show that\nSMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative\ntasks from the MPE, LBF, and RWARE benchmarks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u72b6\u6001\u5efa\u6a21\u6846\u67b6MARL SMPE\u7b97\u6cd5\uff0c\u7528\u4e8e\u5206\u5e03\u5f0f\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u65e0\u901a\u4fe1\u80fd\u529b\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60(MARL)\uff0c\u901a\u8fc7\u63a8\u65ad\u72b6\u6001\u8868\u5f81\u548c\u5bf9\u6297\u63a2\u7d22\u7b56\u7565\u63d0\u5347\u534f\u4f5c\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u5728\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60(MARL)\u4e2d\uff0c\u65e0\u901a\u4fe1\u80fd\u529b\u7684\u5206\u5e03\u5f0f\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u534f\u4f5c\u7684\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u4ece\u4e2a\u4f53\u89c2\u5bdf\u63a8\u65ad\u72b6\u6001\u8868\u5f81\u5e76\u4f18\u5316\u534f\u4f5c\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u72b6\u6001\u5efa\u6a21\u6846\u67b6\uff0c\u667a\u80fd\u4f53\u63a8\u65ad\u975e\u53ef\u89c2\u6d4b\u72b6\u6001\u7684\u4fe1\u5ff5\u8868\u5f81\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1MARL SMPE\u7b97\u6cd5\uff0c\u7ed3\u5408\u663e\u5f0f\u4fe1\u5ff5\u7f51\u7edc\u548c\u9690\u5f0f\u5bf9\u6297\u63a2\u7d22\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cSMPE\u5728MPE\u3001LBF\u548cRWARE\u57fa\u51c6\u6d4b\u8bd5\u7684\u590d\u6742\u5168\u534f\u4f5c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u4f73MARL\u7b97\u6cd5\u3002", "conclusion": "SMPE\u901a\u8fc7\u65b0\u9896\u7684\u72b6\u6001\u5efa\u6a21\u548c\u63a2\u7d22\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u5728\u65e0\u901a\u4fe1\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u80fd\u529b\u3002"}}
{"id": "2505.04880", "pdf": "https://arxiv.org/pdf/2505.04880", "abs": "https://arxiv.org/abs/2505.04880", "authors": ["Min Chen", "Jinglei Cheng", "Pingzhi Li", "Haoran Wang", "Tianlong Chen", "Junyu Liu"], "title": "GroverGPT-2: Simulating Grover's Algorithm via Chain-of-Thought Reasoning and Quantum-Native Tokenization", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": "26 pages, 12 figures", "summary": "Quantum computing offers theoretical advantages over classical computing for\nspecific tasks, yet the boundary of practical quantum advantage remains an open\nquestion. To investigate this boundary, it is crucial to understand whether,\nand how, classical machines can learn and simulate quantum algorithms. Recent\nprogress in large language models (LLMs) has demonstrated strong reasoning\nabilities, prompting exploration into their potential for this challenge. In\nthis work, we introduce GroverGPT-2, an LLM-based method for simulating\nGrover's algorithm using Chain-of-Thought (CoT) reasoning and quantum-native\ntokenization. Building on its predecessor, GroverGPT-2 performs simulation\ndirectly from quantum circuit representations while producing logically\nstructured and interpretable outputs. Our results show that GroverGPT-2 can\nlearn and internalize quantum circuit logic through efficient processing of\nquantum-native tokens, providing direct evidence that classical models like\nLLMs can capture the structure of quantum algorithms. Furthermore, GroverGPT-2\noutputs interleave circuit data with natural language, embedding explicit\nreasoning into the simulation. This dual capability positions GroverGPT-2 as a\nprototype for advancing machine understanding of quantum algorithms and\nmodeling quantum circuit logic. We also identify an empirical scaling law for\nGroverGPT-2 with increasing qubit numbers, suggesting a path toward scalable\nclassical simulation. These findings open new directions for exploring the\nlimits of classical simulatability, enhancing quantum education and research,\nand laying groundwork for future foundation models in quantum computing.", "AI": {"tldr": "GroverGPT-2\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u91cf\u5b50\u7b97\u6cd5\u4eff\u771f\u65b9\u6cd5\uff0c\u901a\u8fc7Chain-of-Thought\u63a8\u7406\u548c\u91cf\u5b50\u539f\u751f\u6807\u8bb0\u5316\u6765\u6a21\u62dfGrover\u7b97\u6cd5\uff0c\u5c55\u793a\u4e86\u7ecf\u5178\u6a21\u578b\u5b66\u4e60\u91cf\u5b50\u7b97\u6cd5\u7684\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7ecf\u5178\u673a\u5668\uff08\u5982\u5927\u8bed\u8a00\u6a21\u578b\uff09\u662f\u5426\u80fd\u591f\u5b66\u4e60\u548c\u6a21\u62df\u91cf\u5b50\u7b97\u6cd5\uff0c\u4ee5\u754c\u5b9a\u91cf\u5b50\u8ba1\u7b97\u7684\u5b9e\u9645\u4f18\u52bf\u8fb9\u754c\u3002", "method": "\u91c7\u7528Chain-of-Thought\u63a8\u7406\u548c\u91cf\u5b50\u539f\u751f\u6807\u8bb0\u5316\u6280\u672f\uff0c\u76f4\u63a5\u4ece\u91cf\u5b50\u7535\u8def\u8868\u793a\u8fdb\u884c\u4eff\u771f\uff0c\u751f\u6210\u903b\u8f91\u6e05\u6670\u4e14\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\u3002", "result": "GroverGPT-2\u80fd\u591f\u901a\u8fc7\u91cf\u5b50\u539f\u751f\u6807\u8bb0\u9ad8\u6548\u5904\u7406\u91cf\u5b50\u7535\u8def\u903b\u8f91\uff0c\u9a8c\u8bc1\u4e86\u7ecf\u5178\u6a21\u578b\u6355\u6349\u91cf\u5b50\u7b97\u6cd5\u7ed3\u6784\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u7684\u7ecf\u5178\u4eff\u771f\u8def\u5f84\u3002", "conclusion": "GroverGPT-2\u4e3a\u673a\u5668\u7406\u89e3\u91cf\u5b50\u7b97\u6cd5\u548c\u5efa\u6a21\u91cf\u5b50\u7535\u8def\u903b\u8f91\u63d0\u4f9b\u4e86\u539f\u578b\uff0c\u4e3a\u91cf\u5b50\u6559\u80b2\u548c\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.05279", "pdf": "https://arxiv.org/pdf/2505.05279", "abs": "https://arxiv.org/abs/2505.05279", "authors": ["Yi Yu", "Song Xia", "Siyuan Yang", "Chenqi Kong", "Wenhan Yang", "Shijian Lu", "Yap-Peng Tan", "Alex C. Kot"], "title": "MTL-UE: Learning to Learn Nothing for Multi-Task Learning", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Most existing unlearnable strategies focus on preventing unauthorized users\nfrom training single-task learning (STL) models with personal data.\nNevertheless, the paradigm has recently shifted towards multi-task data and\nmulti-task learning (MTL), targeting generalist and foundation models that can\nhandle multiple tasks simultaneously. Despite their growing importance, MTL\ndata and models have been largely neglected while pursuing unlearnable\nstrategies. This paper presents MTL-UE, the first unified framework for\ngenerating unlearnable examples for multi-task data and MTL models. Instead of\noptimizing perturbations for each sample, we design a generator-based structure\nthat introduces label priors and class-wise feature embeddings which leads to\nmuch better attacking performance. In addition, MTL-UE incorporates intra-task\nand inter-task embedding regularization to increase inter-class separation and\nsuppress intra-class variance which enhances the attack robustness greatly.\nFurthermore, MTL-UE is versatile with good supports for dense prediction tasks\nin MTL. It is also plug-and-play allowing integrating existing\nsurrogate-dependent unlearnable methods with little adaptation. Extensive\nexperiments show that MTL-UE achieves superior attacking performance\nconsistently across 4 MTL datasets, 3 base UE methods, 5 model backbones, and 5\nMTL task-weighting strategies.", "AI": {"tldr": "MTL-UE\u662f\u9996\u4e2a\u9488\u5bf9\u591a\u4efb\u52a1\u6570\u636e\u548cMTL\u6a21\u578b\u751f\u6210\u4e0d\u53ef\u5b66\u4e60\u6837\u672c\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5668\u7ed3\u6784\u548c\u5d4c\u5165\u6b63\u5219\u5316\u663e\u8457\u63d0\u5347\u653b\u51fb\u6027\u80fd\uff0c\u652f\u6301\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e14\u517c\u5bb9\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4e0d\u53ef\u5b66\u4e60\u7b56\u7565\u4e3b\u8981\u9488\u5bf9\u5355\u4efb\u52a1\u5b66\u4e60\uff0c\u5ffd\u89c6\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u901a\u7528\u6a21\u578b\u7684\u8d8b\u52bf\uff0cMTL-UE\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u751f\u6210\u5668\u7684\u7ed3\u6784\uff0c\u5f15\u5165\u6807\u7b7e\u5148\u9a8c\u548c\u7c7b\u7279\u5f81\u5d4c\u5165\uff0c\u5e76\u7ed3\u5408\u4efb\u52a1\u5185\u548c\u4efb\u52a1\u95f4\u5d4c\u5165\u6b63\u5219\u5316\u3002", "result": "\u57284\u4e2aMTL\u6570\u636e\u96c6\u30013\u79cd\u57fa\u7840UE\u65b9\u6cd5\u30015\u79cd\u6a21\u578b\u67b6\u6784\u548c5\u79cdMTL\u4efb\u52a1\u52a0\u6743\u7b56\u7565\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "MTL-UE\u4e3a\u591a\u4efb\u52a1\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u901a\u7528\u7684\u4e0d\u53ef\u5b66\u4e60\u6837\u672c\u751f\u6210\u65b9\u6848\u3002"}}
{"id": "2505.04806", "pdf": "https://arxiv.org/pdf/2505.04806", "abs": "https://arxiv.org/abs/2505.04806", "authors": ["Chetan Pathade"], "title": "Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs", "categories": ["cs.CR", "cs.CL"], "comment": "7 Pages, 6 Figures", "summary": "Large Language Models (LLMs) are increasingly integrated into consumer and\nenterprise applications. Despite their capabilities, they remain susceptible to\nadversarial attacks such as prompt injection and jailbreaks that override\nalignment safeguards. This paper provides a systematic investigation of\njailbreak strategies against various state-of-the-art LLMs. We categorize over\n1,400 adversarial prompts, analyze their success against GPT-4, Claude 2,\nMistral 7B, and Vicuna, and examine their generalizability and construction\nlogic. We further propose layered mitigation strategies and recommend a hybrid\nred-teaming and sandboxing approach for robust LLM security.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9762\u4e34\u7684\u5bf9\u6297\u653b\u51fb\uff08\u5982\u63d0\u793a\u6ce8\u5165\u548c\u8d8a\u72f1\uff09\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u5206\u6790\u4e861400\u591a\u79cd\u5bf9\u6297\u63d0\u793a\u5728\u4e0d\u540c\u6a21\u578b\u4e0a\u7684\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u5206\u5c42\u7684\u7f13\u89e3\u7b56\u7565\u548c\u6df7\u5408\u7ea2\u961f\u4e0e\u6c99\u7bb1\u7684\u5b89\u5168\u5efa\u8bae\u3002", "motivation": "\u5c3d\u7ba1LLMs\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u5176\u4ecd\u6613\u53d7\u5bf9\u6297\u653b\u51fb\u4fb5\u5bb3\uff0c\u7834\u574f\u9884\u7f6e\u7684\u5b89\u5168\u673a\u5236\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7814\u7a76\u8fd9\u4e9b\u653b\u51fb\u6a21\u5f0f\u4ee5\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u548c\u5206\u67901400\u591a\u79cd\u5bf9\u6297\u63d0\u793a\uff0c\u6d4b\u8bd5\u5176\u5728GPT-4\u3001Claude 2\u3001Mistral 7B\u548cVicuna\u7b49\u6a21\u578b\u4e0a\u7684\u6548\u679c\uff0c\u5e76\u7814\u7a76\u5176\u901a\u7528\u6027\u548c\u6784\u5efa\u903b\u8f91\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u653b\u51fb\u7b56\u7565\u5177\u6709\u9ad8\u901a\u7528\u6027\uff0c\u4e14\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u8868\u73b0\u663e\u8457\uff0c\u9a8c\u8bc1\u4e86\u73b0\u6709\u5b89\u5168\u63aa\u65bd\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u5efa\u8bae\u91c7\u7528\u5206\u5c42\u7f13\u89e3\u7b56\u7565\u548c\u6df7\u5408\u7ea2\u961f\u4e0e\u6c99\u7bb1\u7684\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3aLLMs\u7684\u5bf9\u6297\u653b\u51fb\u9632\u5fa1\u80fd\u529b\u3002"}}
{"id": "2505.05295", "pdf": "https://arxiv.org/pdf/2505.05295", "abs": "https://arxiv.org/abs/2505.05295", "authors": ["Juhani Kivim\u00e4ki", "Jakub Bia\u0142ek", "Wojtek Kuberski", "Jukka K. Nurminen"], "title": "Performance Estimation in Binary Classification Using Calibrated Confidence", "categories": ["cs.LG", "I.2.6"], "comment": null, "summary": "Model monitoring is a critical component of the machine learning lifecycle,\nsafeguarding against undetected drops in the model's performance after\ndeployment. Traditionally, performance monitoring has required access to ground\ntruth labels, which are not always readily available. This can result in\nunacceptable latency or render performance monitoring altogether impossible.\nRecently, methods designed to estimate the accuracy of classifier models\nwithout access to labels have shown promising results. However, there are\nvarious other metrics that might be more suitable for assessing model\nperformance in many cases. Until now, none of these important metrics has\nreceived similar interest from the scientific community. In this work, we\naddress this gap by presenting CBPE, a novel method that can estimate any\nbinary classification metric defined using the confusion matrix. In particular,\nwe choose four metrics from this large family: accuracy, precision, recall, and\nF$_1$, to demonstrate our method. CBPE treats the elements of the confusion\nmatrix as random variables and leverages calibrated confidence scores of the\nmodel to estimate their distributions. The desired metric is then also treated\nas a random variable, whose full probability distribution can be derived from\nthe estimated confusion matrix. CBPE is shown to produce estimates that come\nwith strong theoretical guarantees and valid confidence intervals.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCBPE\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u4efb\u4f55\u57fa\u4e8e\u6df7\u6dc6\u77e9\u9635\u7684\u6027\u80fd\u6307\u6807\uff0c\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "\u4f20\u7edf\u7684\u6a21\u578b\u6027\u80fd\u76d1\u63a7\u9700\u8981\u771f\u5b9e\u6807\u7b7e\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5f80\u5f80\u65e0\u6cd5\u53ca\u65f6\u83b7\u53d6\uff0c\u5bfc\u81f4\u6027\u80fd\u76d1\u63a7\u5ef6\u8fdf\u6216\u65e0\u6cd5\u8fdb\u884c\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u80fd\u4f30\u8ba1\u51c6\u786e\u7387\uff0c\u4f46\u5ffd\u7565\u4e86\u5176\u4ed6\u91cd\u8981\u6307\u6807\uff0c\u5982\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u3002", "method": "CBPE\u5c06\u6df7\u6dc6\u77e9\u9635\u5143\u7d20\u89c6\u4e3a\u968f\u673a\u53d8\u91cf\uff0c\u5229\u7528\u6a21\u578b\u7684\u6821\u51c6\u7f6e\u4fe1\u5ea6\u5206\u6570\u4f30\u8ba1\u5176\u5206\u5e03\uff0c\u8fdb\u800c\u63a8\u5bfc\u76ee\u6807\u6027\u80fd\u6307\u6807\u7684\u5168\u6982\u7387\u5206\u5e03\u3002", "result": "CBPE\u80fd\u751f\u6210\u5177\u6709\u5f3a\u7406\u8bba\u4fdd\u8bc1\u548c\u6709\u6548\u7f6e\u4fe1\u533a\u95f4\u7684\u6027\u80fd\u6307\u6807\u4f30\u8ba1\uff0c\u5c24\u4ee5\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e3a\u793a\u4f8b\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "CBPE\u4e3a\u65e0\u6807\u7b7e\u573a\u666f\u4e0b\u7684\u6a21\u578b\u6027\u80fd\u76d1\u63a7\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u7406\u8bba\u4e0a\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2505.04846", "pdf": "https://arxiv.org/pdf/2505.04846", "abs": "https://arxiv.org/abs/2505.04846", "authors": ["Ozan Gokdemir", "Carlo Siebenschuh", "Alexander Brace", "Azton Wells", "Brian Hsu", "Kyle Hippe", "Priyanka V. Setty", "Aswathy Ajith", "J. Gregory Pauloski", "Varuni Sastry", "Sam Foreman", "Huihuo Zheng", "Heng Ma", "Bharat Kale", "Nicholas Chia", "Thomas Gibbs", "Michael E. Papka", "Thomas Brettin", "Francis J. Alexander", "Anima Anandkumar", "Ian Foster", "Rick Stevens", "Venkatram Vishwanath", "Arvind Ramanathan"], "title": "HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights", "categories": ["cs.IR", "cs.CE", "cs.CL", "cs.DC", "cs.LG", "H.3.3; I.2.7"], "comment": "This paper has been accepted at the Platform for Advanced Scientific\n  Computing Conference (PASC 25), June 16-18, 2025, Brugg-Windisch, Switzerland", "summary": "The volume of scientific literature is growing exponentially, leading to\nunderutilized discoveries, duplicated efforts, and limited cross-disciplinary\ncollaboration. Retrieval Augmented Generation (RAG) offers a way to assist\nscientists by improving the factuality of Large Language Models (LLMs) in\nprocessing this influx of information. However, scaling RAG to handle millions\nof articles introduces significant challenges, including the high computational\ncosts associated with parsing documents and embedding scientific knowledge, as\nwell as the algorithmic complexity of aligning these representations with the\nnuanced semantics of scientific content. To address these issues, we introduce\nHiPerRAG, a RAG workflow powered by high performance computing (HPC) to index\nand retrieve knowledge from more than 3.6 million scientific articles. At its\ncore are Oreo, a high-throughput model for multimodal document parsing, and\nColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval\naccuracy by using contrastive learning and late-interaction techniques.\nHiPerRAG delivers robust performance on existing scientific question answering\nbenchmarks and two new benchmarks introduced in this work, achieving 90%\naccuracy on SciQ and 76% on PubMedQA-outperforming both domain-specific models\nlike PubMedGPT and commercial LLMs such as GPT-4. Scaling to thousands of GPUs\non the Polaris, Sunspot, and Frontier supercomputers, HiPerRAG delivers million\ndocument-scale RAG workflows for unifying scientific knowledge and fostering\ninterdisciplinary innovation.", "AI": {"tldr": "HiPerRAG is a high-performance RAG workflow that scales to millions of scientific articles, using Oreo for document parsing and ColTrast for retrieval accuracy, achieving superior performance on benchmarks.", "motivation": "The rapid growth of scientific literature leads to underutilized knowledge and duplicated efforts. HiPerRAG aims to improve LLM factuality and cross-disciplinary collaboration by efficiently processing vast amounts of scientific content.", "method": "HiPerRAG combines Oreo (a high-throughput multimodal document parser) and ColTrast (a query-aware encoder fine-tuned with contrastive learning and late-interaction techniques), leveraging HPC to scale to millions of documents.", "result": "HiPerRAG achieves 90% accuracy on SciQ and 76% on PubMedQA, outperforming domain-specific models like PubMedGPT and commercial LLMs such as GPT-4. It scales effectively on supercomputers like Polaris, Sunspot, and Frontier.", "conclusion": "HiPerRAG demonstrates robust performance and scalability, unifying scientific knowledge and enabling interdisciplinary innovation through efficient RAG workflows."}}
{"id": "2505.04883", "pdf": "https://arxiv.org/pdf/2505.04883", "abs": "https://arxiv.org/abs/2505.04883", "authors": ["Mingruo Yuan", "Ben Kao", "Tien-Hsuan Wu"], "title": "QBR: A Question-Bank-Based Approach to Fine-Grained Legal Knowledge Retrieval for the General Public", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Retrieval of legal knowledge by the general public is a challenging problem\ndue to the technicality of the professional knowledge and the lack of\nfundamental understanding by laypersons on the subject. Traditional information\nretrieval techniques assume that users are capable of formulating succinct and\nprecise queries for effective document retrieval. In practice, however, the\nwide gap between the highly technical contents and untrained users makes legal\nknowledge retrieval very difficult. We propose a methodology, called QBR, which\nemploys a Questions Bank (QB) as an effective medium for bridging the knowledge\ngap. We show how the QB is used to derive training samples to enhance the\nembedding of knowledge units within documents, which leads to effective\nfine-grained knowledge retrieval. We discuss and evaluate through experiments\nvarious advantages of QBR over traditional methods. These include more\naccurate, efficient, and explainable document retrieval, better comprehension\nof retrieval results, and highly effective fine-grained knowledge retrieval. We\nalso present some case studies and show that QBR achieves social impact by\nassisting citizens to resolve everyday legal concerns.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86QBR\u65b9\u6cd5\uff0c\u901a\u8fc7\u95ee\u9898\u5e93\uff08QB\uff09\u5f25\u8865\u6cd5\u5f8b\u77e5\u8bc6\u68c0\u7d22\u4e2d\u4e13\u4e1a\u5185\u5bb9\u4e0e\u666e\u901a\u7528\u6237\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u63d0\u5347\u6cd5\u5f8b\u77e5\u8bc6\u7684\u7ec6\u7c92\u5ea6\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u7531\u4e8e\u6cd5\u5f8b\u77e5\u8bc6\u7684\u4e13\u4e1a\u6027\u548c\u666e\u901a\u7528\u6237\u7f3a\u4e4f\u76f8\u5173\u77e5\u8bc6\u80cc\u666f\uff0c\u4f20\u7edf\u7684\u6cd5\u5f8b\u77e5\u8bc6\u68c0\u7d22\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86QBR\u65b9\u6cd5\uff0c\u5229\u7528\u95ee\u9898\u5e93\uff08QB\uff09\u4f5c\u4e3a\u4e2d\u4ecb\uff0c\u901a\u8fc7\u751f\u6210\u8bad\u7ec3\u6837\u672c\u6765\u589e\u5f3a\u6587\u6863\u4e2d\u77e5\u8bc6\u5355\u5143\u7684\u5d4c\u5165\uff0c\u4ece\u800c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u77e5\u8bc6\u68c0\u7d22\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cQBR\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u80fd\u5e2e\u52a9\u666e\u901a\u7528\u6237\u66f4\u597d\u5730\u7406\u89e3\u68c0\u7d22\u7ed3\u679c\u5e76\u89e3\u51b3\u65e5\u5e38\u6cd5\u5f8b\u95ee\u9898\u3002", "conclusion": "QBR\u4e0d\u4ec5\u63d0\u5347\u4e86\u6cd5\u5f8b\u77e5\u8bc6\u68c0\u7d22\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u8fd8\u5177\u6709\u663e\u8457\u7684\u793e\u4f1a\u5f71\u54cd\uff0c\u5e2e\u52a9\u666e\u901a\u7528\u6237\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u3002"}}
{"id": "2505.05315", "pdf": "https://arxiv.org/pdf/2505.05315", "abs": "https://arxiv.org/abs/2505.05315", "authors": ["Yuhui Xu", "Hanze Dong", "Lei Wang", "Doyen Sahoo", "Junnan Li", "Caiming Xiong"], "title": "Scalable Chain of Thoughts via Elastic Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale.", "AI": {"tldr": "Elastic Reasoning\u6846\u67b6\u901a\u8fc7\u5c06\u63a8\u7406\u5206\u4e3a\u601d\u8003\u548c\u89e3\u4e24\u9636\u6bb5\uff0c\u5e76\u72ec\u7acb\u5206\u914d\u8d44\u6e90\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u4e25\u683c\u8d44\u6e90\u7ea6\u675f\u4e0b\u7684\u63a8\u7406\u53ef\u9760\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u8f93\u51fa\u957f\u5ea6\u4e0d\u53d7\u63a7\u5236\u65f6\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u90e8\u7f72\u4e2d\u4e25\u683c\u7684\u8d44\u6e90\u9650\u5236\u95ee\u9898\u3002", "method": "\u63d0\u51faElastic Reasoning\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u5206\u4e3a\u601d\u8003\u548c\u89e3\u7b54\u4e24\u9636\u6bb5\uff0c\u5e76\u91c7\u7528\u9884\u7b97\u7ea6\u675f\u7684rollout\u7b56\u7565\u8bad\u7ec3\u6a21\u578b\uff0c\u4f7f\u5176\u5728\u8d44\u6e90\u6709\u9650\u65f6\u4ecd\u80fd\u6709\u6548\u63a8\u7406\u3002", "result": "\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cElastic Reasoning\u5728\u4e25\u683c\u8d44\u6e90\u9650\u5236\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u4e14\u8bad\u7ec3\u6210\u672c\u4f4e\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Elastic Reasoning\u4e3a\u5927\u89c4\u6a21\u53ef\u63a7\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.04888", "pdf": "https://arxiv.org/pdf/2505.04888", "abs": "https://arxiv.org/abs/2505.04888", "authors": ["Tharindu Fernando", "Clinton Fookes", "Sridha Sridharan", "Simon Denman"], "title": "Cross-Branch Orthogonality for Improved Generalization in Face Deepfake Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Remarkable advancements in generative AI technology have given rise to a\nspectrum of novel deepfake categories with unprecedented leaps in their\nrealism, and deepfakes are increasingly becoming a nuisance to law enforcement\nauthorities and the general public. In particular, we observe alarming levels\nof confusion, deception, and loss of faith regarding multimedia content within\nsociety caused by face deepfakes, and existing deepfake detectors are\nstruggling to keep up with the pace of improvements in deepfake generation.\nThis is primarily due to their reliance on specific forgery artifacts, which\nlimits their ability to generalise and detect novel deepfake types. To combat\nthe spread of malicious face deepfakes, this paper proposes a new strategy that\nleverages coarse-to-fine spatial information, semantic information, and their\ninteractions while ensuring feature distinctiveness and reducing the redundancy\nof the modelled features. A novel feature orthogonality-based disentanglement\nstrategy is introduced to ensure branch-level and cross-branch feature\ndisentanglement, which allows us to integrate multiple feature vectors without\nadding complexity to the feature space or compromising generalisation.\nComprehensive experiments on three public benchmarks: FaceForensics++,\nCeleb-DF, and the Deepfake Detection Challenge (DFDC) show that these design\nchoices enable the proposed approach to outperform current state-of-the-art\nmethods by 5% on the Celeb-DF dataset and 7% on the DFDC dataset in a\ncross-dataset evaluation setting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u7c97\u5230\u7ec6\u7684\u7a7a\u95f4\u4fe1\u606f\u3001\u8bed\u4e49\u4fe1\u606f\u53ca\u5176\u4ea4\u4e92\uff0c\u5e76\u7ed3\u5408\u7279\u5f81\u6b63\u4ea4\u6027\u89e3\u8026\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u5bf9\u65b0\u578b\u6df1\u5ea6\u4f2a\u9020\u7684\u68c0\u6d4b\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u751f\u6210AI\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u771f\u5b9e\u6027\u4e0d\u65ad\u63d0\u9ad8\uff0c\u5bf9\u793e\u4f1a\u9020\u6210\u4e86\u6df7\u6dc6\u3001\u6b3a\u9a97\u548c\u5bf9\u591a\u5a92\u4f53\u5185\u5bb9\u7684\u4fe1\u4efb\u7f3a\u5931\u3002\u73b0\u6709\u7684\u68c0\u6d4b\u5668\u56e0\u4f9d\u8d56\u7279\u5b9a\u4f2a\u9020\u4f2a\u5f71\u800c\u96be\u4ee5\u8ddf\u4e0a\u6280\u672f\u6539\u8fdb\u7684\u6b65\u4f10\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b56\u7565\uff0c\u7ed3\u5408\u7c97\u5230\u7ec6\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u4fe1\u606f\u53ca\u5176\u4ea4\u4e92\uff0c\u5e76\u5f15\u5165\u7279\u5f81\u6b63\u4ea4\u6027\u89e3\u8026\u7b56\u7565\uff0c\u4ee5\u786e\u4fdd\u7279\u5f81\u72ec\u7279\u6027\u548c\u51cf\u5c11\u5197\u4f59\uff0c\u540c\u65f6\u6574\u5408\u591a\u4e2a\u7279\u5f81\u5411\u91cf\u800c\u4e0d\u589e\u52a0\u590d\u6742\u6027\u3002", "result": "\u5728FaceForensics++\u3001Celeb-DF\u548cDFDC\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u5728Celeb-DF\u548cDFDC\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u9ad8\u4e865%\u548c7%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u521b\u65b0\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\uff0c\u4e3a\u5bf9\u6297\u6076\u610f\u6df1\u5ea6\u4f2a\u9020\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05355", "pdf": "https://arxiv.org/pdf/2505.05355", "abs": "https://arxiv.org/abs/2505.05355", "authors": ["Robert Busa-Fekete", "Travis Dick", "Claudio Gentile", "Haim Kaplan", "Tomer Koren", "Uri Stemmer"], "title": "Nearly Optimal Sample Complexity for Learning with Label Proportions", "categories": ["cs.LG"], "comment": null, "summary": "We investigate Learning from Label Proportions (LLP), a partial information\nsetting where examples in a training set are grouped into bags, and only\naggregate label values in each bag are available. Despite the partial\nobservability, the goal is still to achieve small regret at the level of\nindividual examples. We give results on the sample complexity of LLP under\nsquare loss, showing that our sample complexity is essentially optimal. From an\nalgorithmic viewpoint, we rely on carefully designed variants of Empirical Risk\nMinimization, and Stochastic Gradient Descent algorithms, combined with ad hoc\nvariance reduction techniques. On one hand, our theoretical results improve in\nimportant ways on the existing literature on LLP, specifically in the way the\nsample complexity depends on the bag size. On the other hand, we validate our\nalgorithmic solutions on several datasets, demonstrating improved empirical\nperformance (better accuracy for less samples) against recent baselines.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4ece\u6807\u7b7e\u6bd4\u4f8b\u5b66\u4e60(LLP)\u7684\u6837\u672c\u590d\u6742\u6027\u53ca\u5176\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u8bc1\u660e\u4e86\u5728\u5e73\u65b9\u635f\u5931\u4e0b\u6837\u672c\u590d\u6742\u5ea6\u7684\u6700\u4f18\u6027\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u4ee5\u53ca\u65b9\u5dee\u51cf\u5c11\u6280\u672f\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3LLP\u8bbe\u7f6e\u4e0b\u90e8\u5206\u4fe1\u606f\u7684\u5b66\u4e60\u95ee\u9898\uff0c\u5373\u8bad\u7ec3\u96c6\u4e2d\u7684\u6837\u672c\u88ab\u5206\u7ec4\u4e3a\u591a\u5305\uff08bags\uff09\uff0c\u4e14\u53ea\u80fd\u83b7\u53d6\u6bcf\u4e2a\u5305\u7684\u805a\u5408\u6807\u7b7e\u503c\uff0c\u4f46\u4ecd\u5e0c\u671b\u5728\u5c0f\u6837\u672c\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e2a\u4f53\u7ea7\u522b\u7684\u4f4e\u540e\u6094\uff08regret\uff09\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6539\u8fdb\u7684\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\uff08Empirical Risk Minimization\uff09\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08Stochastic Gradient Descent\uff09\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u7279\u5b9a\u7684\u65b9\u5dee\u51cf\u5c11\u6280\u672f\u3002", "result": "\u7406\u8bba\u7ed3\u679c\u8868\u660e\uff0c\u6837\u672c\u590d\u6742\u5ea6\u5728\u5305\u5927\u5c0f\u4f9d\u8d56\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6587\u732e\uff0c\u540c\u65f6\u591a\u4e2a\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u5728\u6837\u672c\u6548\u7387\uff08\u4ee5\u66f4\u5c11\u6837\u672c\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\uff09\u4e0a\u7684\u63d0\u5347\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9645\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u6837\u672c\u590d\u6742\u5ea6\u7684\u4f18\u5316\u548c\u65b9\u5dee\u63a7\u5236\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002"}}
{"id": "2505.05381", "pdf": "https://arxiv.org/pdf/2505.05381", "abs": "https://arxiv.org/abs/2505.05381", "authors": ["Kazi Ashik Islam", "Zakaria Mehrab", "Mahantesh Halappanavar", "Henning Mortveit", "Sridhar Katragadda", "Jon Derek Loftis", "Madhav Marathe"], "title": "Denoising Diffusion Probabilistic Models for Coastal Inundation Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Coastal flooding poses significant risks to communities, necessitating fast\nand accurate forecasting methods to mitigate potential damage. To approach this\nproblem, we present DIFF-FLOOD, a probabilistic spatiotemporal forecasting\nmethod designed based on denoising diffusion models. DIFF-FLOOD predicts\ninundation level at a location by taking both spatial and temporal context into\naccount. It utilizes inundation levels at neighboring locations and digital\nelevation data as spatial context. Inundation history from a context time\nwindow, together with additional co-variates are used as temporal context.\nConvolutional neural networks and cross-attention mechanism are then employed\nto capture the spatiotemporal dynamics in the data. We trained and tested\nDIFF-FLOOD on coastal inundation data from the Eastern Shore of Virginia, a\nregion highly impacted by coastal flooding. Our results show that, DIFF-FLOOD\noutperforms existing forecasting methods in terms of prediction performance (6%\nto 64% improvement in terms of two performance metrics) and scalability.", "AI": {"tldr": "DIFF-FLOOD\u662f\u4e00\u79cd\u57fa\u4e8e\u53bb\u566a\u6269\u6563\u6a21\u578b\u7684\u6982\u7387\u65f6\u7a7a\u9884\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5feb\u901f\u51c6\u786e\u5730\u9884\u6d4b\u6cbf\u6d77\u6d2a\u6c34\u6df9\u6ca1\u6c34\u5e73\u3002\u5b83\u7ed3\u5408\u4e86\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u4e0b\u6587\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6cbf\u6d77\u6d2a\u6c34\u5bf9\u793e\u533a\u6784\u6210\u91cd\u5927\u98ce\u9669\uff0c\u9700\u8981\u5feb\u901f\u51c6\u786e\u7684\u9884\u6d4b\u65b9\u6cd5\u4ee5\u51cf\u5c11\u6f5c\u5728\u635f\u5bb3\u3002", "method": "DIFF-FLOOD\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u4e0b\u6587\uff08\u90bb\u8fd1\u4f4d\u7f6e\u7684\u6df9\u6ca1\u6c34\u5e73\u3001\u6570\u5b57\u9ad8\u7a0b\u6570\u636e\u3001\u5386\u53f2\u6df9\u6ca1\u6570\u636e\u53ca\u5176\u4ed6\u534f\u53d8\u91cf\uff09\uff0c\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u65f6\u7a7a\u52a8\u6001\u3002", "result": "\u5728\u5f17\u5409\u5c3c\u4e9a\u4e1c\u6d77\u5cb8\u7684\u6570\u636e\u96c6\u4e0a\uff0cDIFF-FLOOD\u5728\u9884\u6d4b\u6027\u80fd\uff08\u4e24\u4e2a\u6307\u6807\u63d0\u53476%\u81f364%\uff09\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DIFF-FLOOD\u4e3a\u6cbf\u6d77\u6d2a\u6c34\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2505.04911", "pdf": "https://arxiv.org/pdf/2505.04911", "abs": "https://arxiv.org/abs/2505.04911", "authors": ["Shun Taguchi", "Hideki Deguchi", "Takumi Hamazaki", "Hiroyuki Sakai"], "title": "SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "18 pages, 11 figures", "summary": "This study introduces SpatialPrompting, a novel framework that harnesses the\nemergent reasoning capabilities of off-the-shelf multimodal large language\nmodels to achieve zero-shot spatial reasoning in three-dimensional (3D)\nenvironments. Unlike existing methods that rely on expensive 3D-specific\nfine-tuning with specialized 3D inputs such as point clouds or voxel-based\nfeatures, SpatialPrompting employs a keyframe-driven prompt generation\nstrategy. This framework uses metrics such as vision-language similarity,\nMahalanobis distance, field of view, and image sharpness to select a diverse\nand informative set of keyframes from image sequences and then integrates them\nwith corresponding camera pose data to effectively abstract spatial\nrelationships and infer complex 3D structures. The proposed framework not only\nestablishes a new paradigm for flexible spatial reasoning that utilizes\nintuitive visual and positional cues but also achieves state-of-the-art\nzero-shot performance on benchmark datasets, such as ScanQA and SQA3D, across\nseveral metrics. The proposed method effectively eliminates the need for\nspecialized 3D inputs and fine-tuning, offering a simpler and more scalable\nalternative to conventional approaches.", "AI": {"tldr": "SpatialPrompting\u6846\u67b6\u5229\u7528\u73b0\u6210\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u96f6\u6837\u672c\u4e09\u7ef4\u7a7a\u95f4\u63a8\u7406\uff0c\u65e0\u9700\u6602\u8d35\u76843D\u7279\u5b9a\u5fae\u8c03\u6216\u4e13\u7528\u8f93\u5165\uff0c\u901a\u8fc7\u5173\u952e\u5e27\u9009\u62e9\u548c\u89c6\u89c9-\u8bed\u8a00\u76f8\u4f3c\u6027\u7b49\u65b9\u6cd5\uff0c\u5728ScanQA\u548cSQA3D\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u76843D\u4e13\u7528\u5fae\u8c03\u548c\u8f93\u5165\uff08\u5982\u70b9\u4e91\u6216\u4f53\u7d20\u7279\u5f81\uff09\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002SpatialPrompting\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u6a21\u578b\u548c\u5173\u952e\u5e27\u7b56\u7565\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5173\u952e\u5e27\u9a71\u52a8\u63d0\u793a\u751f\u6210\u7b56\u7565\uff0c\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u76f8\u4f3c\u6027\u3001\u9a6c\u6c0f\u8ddd\u79bb\u7b49\u6307\u6807\u9009\u62e9\u591a\u6837\u4fe1\u606f\u5173\u952e\u5e27\uff0c\u5e76\u4e0e\u76f8\u673a\u4f4d\u59ff\u6570\u636e\u96c6\u6210\u4ee5\u63a8\u74063D\u7ed3\u6784\u3002", "result": "\u5728ScanQA\u548cSQA3D\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u65e0\u9700\u4e13\u75283D\u8f93\u5165\u6216\u5fae\u8c03\u3002", "conclusion": "SpatialPrompting\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u53ef\u6269\u5c55\u7684\u4e09\u7ef4\u7a7a\u95f4\u63a8\u7406\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.05402", "pdf": "https://arxiv.org/pdf/2505.05402", "abs": "https://arxiv.org/abs/2505.05402", "authors": ["Andrew D. Laack"], "title": "CART-ELC: Oblique Decision Tree Induction via Exhaustive Search", "categories": ["cs.LG", "cs.AI", "cs.DS", "I.2.6; I.5.2; F.2.2; G.3; G.2.1"], "comment": "16 pages, 4 figures", "summary": "Oblique decision trees have attracted attention due to their potential for\nimproved classification performance over traditional axis-aligned decision\ntrees. However, methods that rely on exhaustive search to find oblique splits\nface computational challenges. As a result, they have not been widely explored.\nWe introduce a novel algorithm, Classification and Regression Tree - Exhaustive\nLinear Combinations (CART-ELC), for inducing oblique decision trees that\nperforms an exhaustive search on a restricted set of hyperplanes. We then\ninvestigate the algorithm's computational complexity and its predictive\ncapabilities. Our results demonstrate that CART-ELC consistently achieves\ncompetitive performance on small datasets, often yielding statistically\nsignificant improvements in classification accuracy relative to existing\ndecision tree induction algorithms, while frequently producing shallower,\nsimpler, and thus more interpretable trees.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCART-ELC\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u659c\u51b3\u7b56\u6811\uff0c\u901a\u8fc7\u9650\u5236\u8d85\u5e73\u9762\u7684\u7a77\u4e3e\u641c\u7d22\u6765\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u659c\u51b3\u7b56\u6811\u5728\u5206\u7c7b\u6027\u80fd\u4e0a\u6709\u6f5c\u529b\u4f18\u4e8e\u4f20\u7edf\u8f74\u5bf9\u9f50\u51b3\u7b56\u6811\uff0c\u4f46\u7a77\u4e3e\u641c\u7d22\u65b9\u6cd5\u7684\u8ba1\u7b97\u590d\u6742\u6027\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u63d0\u51faCART-ELC\u7b97\u6cd5\uff0c\u901a\u8fc7\u9650\u5236\u8d85\u5e73\u9762\u7684\u7a77\u4e3e\u641c\u7d22\u6765\u751f\u6210\u659c\u51b3\u7b56\u6811\uff0c\u5e76\u5206\u6790\u5176\u8ba1\u7b97\u590d\u6742\u6027\u548c\u9884\u6d4b\u80fd\u529b\u3002", "result": "CART-ELC\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5206\u7c7b\u51c6\u786e\u6027\u663e\u8457\u63d0\u5347\uff0c\u4e14\u751f\u6210\u7684\u6811\u66f4\u6d45\u3001\u66f4\u7b80\u5355\u3001\u66f4\u6613\u89e3\u91ca\u3002", "conclusion": "CART-ELC\u7b97\u6cd5\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u80fd\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u5e76\u751f\u6210\u66f4\u7b80\u5355\u7684\u51b3\u7b56\u6811\u3002"}}
{"id": "2505.05409", "pdf": "https://arxiv.org/pdf/2505.05409", "abs": "https://arxiv.org/abs/2505.05409", "authors": ["Marvin F. da Silva", "Felix Dangel", "Sageev Oore"], "title": "Hide & Seek: Transformer Symmetries Obscure Sharpness & Riemannian Geometry Finds It", "categories": ["cs.LG"], "comment": null, "summary": "The concept of sharpness has been successfully applied to traditional\narchitectures like MLPs and CNNs to predict their generalization. For\ntransformers, however, recent work reported weak correlation between flatness\nand generalization. We argue that existing sharpness measures fail for\ntransformers, because they have much richer symmetries in their attention\nmechanism that induce directions in parameter space along which the network or\nits loss remain identical. We posit that sharpness must account fully for these\nsymmetries, and thus we redefine it on a quotient manifold that results from\nquotienting out the transformer symmetries, thereby removing their ambiguities.\nLeveraging tools from Riemannian geometry, we propose a fully general notion of\nsharpness, in terms of a geodesic ball on the symmetry-corrected quotient\nmanifold. In practice, we need to resort to approximating the geodesics. Doing\nso up to first order yields existing adaptive sharpness measures, and we\ndemonstrate that including higher-order terms is crucial to recover correlation\nwith generalization. We present results on diagonal networks with synthetic\ndata, and show that our geodesic sharpness reveals strong correlation for\nreal-world transformers on both text and image classification tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684sharpness\u8861\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651Transformer\u4e2d\u7684\u5bf9\u79f0\u6027\uff0c\u91cd\u65b0\u5b9a\u4e49\u57fa\u4e8e\u5546\u6d41\u5f62\u7684sharpness\u6982\u5ff5\uff0c\u4ece\u800c\u6062\u590d\u5176\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u5f3a\u76f8\u5173\u6027\u3002", "motivation": "\u73b0\u6709\u7684sharpness\u8861\u91cf\u65b9\u6cd5\u5bf9\u4f20\u7edf\u67b6\u6784\uff08\u5982MLP\u548cCNN\uff09\u6709\u6548\uff0c\u4f46\u5728Transformer\u4e2d\u7531\u4e8e\u53c2\u6570\u7a7a\u95f4\u7684\u5bf9\u79f0\u6027\u4e30\u5bcc\uff0c\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u901a\u8fc7Riemannian\u51e0\u4f55\u5de5\u5177\uff0c\u63d0\u51fa\u5728\u5546\u6d41\u5f62\u4e0a\u5b9a\u4e49sharpness\u7684\u65b9\u6cd5\uff0c\u6d88\u9664\u5bf9\u79f0\u6027\u6b67\u4e49\uff0c\u5e76\u901a\u8fc7\u9ad8\u9636\u8fd1\u4f3c\u8ba1\u7b97geodesic sharpness\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u7684Transformer\u4efb\u52a1\uff08\u5982\u6587\u672c\u548c\u56fe\u50cf\u5206\u7c7b\uff09\u4e2d\uff0c\u65b0\u65b9\u6cd5\u5c55\u73b0\u51fasharpness\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u5f3a\u76f8\u5173\u6027\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\uff0c\u8003\u8651\u5bf9\u79f0\u6027\u5e76\u6539\u8fdbsharpness\u5b9a\u4e49\u662f\u51c6\u786e\u9884\u6d4bTransformer\u6cdb\u5316\u80fd\u529b\u7684\u5173\u952e\u3002"}}
{"id": "2505.04921", "pdf": "https://arxiv.org/pdf/2505.04921", "abs": "https://arxiv.org/abs/2505.04921", "authors": ["Yunxin Li", "Zhenyu Liu", "Zitao Li", "Xuanyu Zhang", "Zhenran Xu", "Xinyu Chen", "Haoyuan Shi", "Shenyuan Jiang", "Xintong Wang", "Jifang Wang", "Shouzheng Huang", "Xinping Zhao", "Borui Jiang", "Lanqing Hong", "Longyue Wang", "Zhuotao Tian", "Baoxing Huai", "Wenhan Luo", "Weihua Luo", "Zheng Zhang", "Baotian Hu", "Min Zhang"], "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models", "categories": ["cs.CV", "cs.CL"], "comment": "75 Pages,10 figures; Project:\n  https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models", "summary": "Reasoning lies at the heart of intelligence, shaping the ability to make\ndecisions, draw conclusions, and generalize across domains. In artificial\nintelligence, as systems increasingly operate in open, uncertain, and\nmultimodal environments, reasoning becomes essential for enabling robust and\nadaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a\npromising paradigm, integrating modalities such as text, images, audio, and\nvideo to support complex reasoning capabilities and aiming to achieve\ncomprehensive perception, precise understanding, and deep reasoning. As\nresearch advances, multimodal reasoning has rapidly evolved from modular,\nperception-driven pipelines to unified, language-centric frameworks that offer\nmore coherent cross-modal understanding. While instruction tuning and\nreinforcement learning have improved model reasoning, significant challenges\nremain in omni-modal generalization, reasoning depth, and agentic behavior. To\naddress these issues, we present a comprehensive and structured survey of\nmultimodal reasoning research, organized around a four-stage developmental\nroadmap that reflects the field's shifting design philosophies and emerging\ncapabilities. First, we review early efforts based on task-specific modules,\nwhere reasoning was implicitly embedded across stages of representation,\nalignment, and fusion. Next, we examine recent approaches that unify reasoning\ninto multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT)\nand multimodal reinforcement learning enabling richer and more structured\nreasoning chains. Finally, drawing on empirical insights from challenging\nbenchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the\nconceptual direction of native large multimodal reasoning models (N-LMRMs),\nwhich aim to support scalable, agentic, and adaptive reasoning and planning in\ncomplex, real-world environments.", "AI": {"tldr": "\u5927\u578b\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\uff08LMRMs\uff09\u5728\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\uff0c\u65e8\u5728\u901a\u8fc7\u6574\u5408\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u7b49\u591a\u6a21\u6001\u6570\u636e\u5b9e\u73b0\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002\u672c\u6587\u7efc\u8ff0\u4e86\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u5e76\u63d0\u51fa\u56db\u9636\u6bb5\u53d1\u5c55\u8def\u7ebf\u56fe\uff0c\u63a2\u8ba8\u4e86\u4ece\u6a21\u5757\u5316\u8bbe\u8ba1\u5230\u7edf\u4e00\u6846\u67b6\u7684\u6f14\u8fdb\uff0c\u6700\u7ec8\u5c55\u671b\u4e86\u539f\u751f\u5927\u89c4\u6a21\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\uff08N-LMRMs\uff09\u7684\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5728\u5f00\u653e\u3001\u4e0d\u786e\u5b9a\u548c\u591a\u6a21\u6001\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u63a8\u7406\u80fd\u529b\u6210\u4e3a\u5b9e\u73b0\u7a33\u5065\u548c\u81ea\u9002\u5e94\u884c\u4e3a\u7684\u5173\u952e\u3002\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u7684\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8de8\u6a21\u6001\u6cdb\u5316\u3001\u63a8\u7406\u6df1\u5ea6\u548c\u667a\u80fd\u4f53\u884c\u4e3a\u7b49\u6311\u6218\u3002", "method": "\u672c\u6587\u901a\u8fc7\u56db\u9636\u6bb5\u53d1\u5c55\u8def\u7ebf\u56fe\u7cfb\u7edf\u68b3\u7406\u4e86\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u7684\u6f14\u53d8\uff1a\u4ece\u65e9\u671f\u57fa\u4e8e\u4efb\u52a1\u7279\u5b9a\u6a21\u5757\u7684\u9690\u5f0f\u63a8\u7406\uff0c\u5230\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u601d\u7ef4\u94fe\uff08MCoT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\u5bf9\u7ed3\u6784\u5316\u63a8\u7406\u7684\u63a8\u52a8\u3002", "result": "\u7814\u7a76\u603b\u7ed3\u4e86\u591a\u6a21\u6001\u63a8\u7406\u9886\u57df\u7684\u8fdb\u5c55\uff0c\u5305\u62ec\u4ece\u6a21\u5757\u5316\u5230\u7edf\u4e00\u6846\u67b6\u7684\u8f6c\u53d8\uff0c\u5e76\u901a\u8fc7OpenAI O3\u548cO4-mini\u7b49\u5b9e\u9a8c\u6848\u4f8b\u5c55\u793a\u4e86\u5f53\u524d\u6280\u672f\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u3002", "conclusion": "\u672a\u6765\u7684\u65b9\u5411\u662f\u5f00\u53d1\u539f\u751f\u5927\u89c4\u6a21\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\uff08N-LMRMs\uff09\uff0c\u4ee5\u652f\u6301\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u3001\u81ea\u4e3b\u548c\u81ea\u9002\u5e94\u63a8\u7406\u4e0e\u89c4\u5212\u3002"}}
{"id": "2505.05413", "pdf": "https://arxiv.org/pdf/2505.05413", "abs": "https://arxiv.org/abs/2505.05413", "authors": ["Nilesh Prasad Pandey", "Shriniwas Kulkarni", "David Wang", "Onat Gungor", "Flavio Ponzina", "Tajana Rosing"], "title": "DPQ-HD: Post-Training Compression for Ultra-Low Power Hyperdimensional Computing", "categories": ["cs.LG"], "comment": null, "summary": "Hyperdimensional Computing (HDC) is emerging as a promising approach for edge\nAI, offering a balance between accuracy and efficiency. However, current\nHDC-based applications often rely on high-precision models and/or encoding\nmatrices to achieve competitive performance, which imposes significant\ncomputational and memory demands, especially for ultra-low power devices. While\nrecent efforts use techniques like precision reduction and pruning to increase\nthe efficiency, most require retraining to maintain performance, making them\nexpensive and impractical. To address this issue, we propose a novel Post\nTraining Compression algorithm, Decomposition-Pruning-Quantization (DPQ-HD),\nwhich aims at compressing the end-to-end HDC system, achieving near floating\npoint performance without the need of retraining. DPQ-HD reduces computational\nand memory overhead by uniquely combining the above three compression\ntechniques and efficiently adapts to hardware constraints. Additionally, we\nintroduce an energy-efficient inference approach that progressively evaluates\nsimilarity scores such as cosine similarity and performs early exit to reduce\nthe computation, accelerating prediction inference while maintaining accuracy.\nWe demonstrate that DPQ-HD achieves up to 20-100x reduction in memory for image\nand graph classification tasks with only a 1-2% drop in accuracy compared to\nuncompressed workloads. Lastly, we show that DPQ-HD outperforms the existing\npost-training compression methods and performs better or at par with\nretraining-based state-of-the-art techniques, requiring significantly less\noverall optimization time (up to 100x) and faster inference (up to 56x) on a\nmicrocontroller", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDPQ-HD\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u3001\u526a\u679d\u548c\u91cf\u5316\u6280\u672f\u538b\u7f29\u8d85\u7ef4\u8ba1\u7b97\u7cfb\u7edf\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u5b9e\u73b0\u63a5\u8fd1\u6d6e\u70b9\u6027\u80fd\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u8d85\u7ef4\u8ba1\u7b97\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9ad8\u7cbe\u5ea6\u6a21\u578b\u6216\u7f16\u7801\u77e9\u9635\uff0c\u5bfc\u81f4\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u9ad8\uff0c\u5c24\u5176\u5728\u8d85\u4f4e\u529f\u8017\u8bbe\u5907\u4e0a\u4e0d\u5207\u5b9e\u9645\u3002\u73b0\u6709\u538b\u7f29\u6280\u672f\u591a\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u6210\u672c\u9ad8\u4e14\u4e0d\u5b9e\u7528\u3002", "method": "\u91c7\u7528\u5206\u89e3-\u526a\u679d-\u91cf\u5316\uff08DPQ-HD\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e09\u79cd\u538b\u7f29\u6280\u672f\uff0c\u5e76\u5f15\u5165\u6e10\u8fdb\u76f8\u4f3c\u6027\u8bc4\u5206\u548c\u65e9\u9000\u673a\u5236\u4ee5\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "result": "\u5728\u56fe\u50cf\u548c\u56fe\u5f62\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5185\u5b58\u51cf\u5c1120-100\u500d\uff0c\u7cbe\u5ea6\u4ec5\u4e0b\u964d1-2%\uff0c\u4e14\u4f18\u5316\u65f6\u95f4\u548c\u63a8\u7406\u901f\u5ea6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u6700\u9ad8100\u500d\u548c56\u500d\uff09\u3002", "conclusion": "DPQ-HD\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9ad8\u6548\u538b\u7f29\u8d85\u7ef4\u8ba1\u7b97\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18AI\u8bbe\u5907\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2505.04946", "pdf": "https://arxiv.org/pdf/2505.04946", "abs": "https://arxiv.org/abs/2505.04946", "authors": ["Xuyang Guo", "Jiayan Huo", "Zhenmei Shi", "Zhao Song", "Jiahao Zhang", "Jiale Zhao"], "title": "T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Thanks to recent advancements in scalable deep architectures and large-scale\npretraining, text-to-video generation has achieved unprecedented capabilities\nin producing high-fidelity, instruction-following content across a wide range\nof styles, enabling applications in advertising, entertainment, and education.\nHowever, these models' ability to render precise on-screen text, such as\ncaptions or mathematical formulas, remains largely untested, posing significant\nchallenges for applications requiring exact textual accuracy. In this work, we\nintroduce T2VTextBench, the first human-evaluation benchmark dedicated to\nevaluating on-screen text fidelity and temporal consistency in text-to-video\nmodels. Our suite of prompts integrates complex text strings with dynamic scene\nchanges, testing each model's ability to maintain detailed instructions across\nframes. We evaluate ten state-of-the-art systems, ranging from open-source\nsolutions to commercial offerings, and find that most struggle to generate\nlegible, consistent text. These results highlight a critical gap in current\nvideo generators and provide a clear direction for future research aimed at\nenhancing textual manipulation in video synthesis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86T2VTextBench\uff0c\u9996\u4e2a\u4e13\u6ce8\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u5728\u5c4f\u5e55\u4e0a\u6587\u672c\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u57fa\u51c6\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u751f\u6210\u6e05\u6670\u3001\u4e00\u81f4\u7684\u6587\u672c\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u5c3d\u7ba1\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6280\u672f\u5728\u5e7f\u544a\u3001\u5a31\u4e50\u548c\u6559\u80b2\u7b49\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u751f\u6210\u7cbe\u786e\u5c4f\u5e55\u6587\u672c\uff08\u5982\u5b57\u5e55\u6216\u6570\u5b66\u516c\u5f0f\uff09\u65b9\u9762\u80fd\u529b\u4e0d\u8db3\uff0c\u4e9f\u9700\u8bc4\u4f30\u548c\u6539\u8fdb\u3002", "method": "\u901a\u8fc7T2VTextBench\u57fa\u51c6\uff0c\u6574\u5408\u590d\u6742\u6587\u672c\u5b57\u7b26\u4e32\u548c\u52a8\u6001\u573a\u666f\u53d8\u5316\uff0c\u5bf9\u5341\u79cd\u6700\u5148\u8fdb\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u8fdb\u884c\u4eba\u7c7b\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u5927\u591a\u6570\u6a21\u578b\u96be\u4ee5\u751f\u6210\u6e05\u6670\u3001\u4e00\u81f4\u7684\u6587\u672c\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u89c6\u9891\u751f\u6210\u6280\u672f\u7684\u91cd\u5927\u7f3a\u9677\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u63d0\u5347\u89c6\u9891\u5408\u6210\u4e2d\u6587\u672c\u64cd\u7eb5\u80fd\u529b\u7684\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.05452", "pdf": "https://arxiv.org/pdf/2505.05452", "abs": "https://arxiv.org/abs/2505.05452", "authors": ["Pouria Behnoudfar", "Nan Chen"], "title": "RL-DAUNCE: Reinforcement Learning-Driven Data Assimilation with Uncertainty-Aware Constrained Ensembles", "categories": ["cs.LG", "math-ph", "math.MP"], "comment": null, "summary": "Machine learning has become a powerful tool for enhancing data assimilation.\nWhile supervised learning remains the standard method, reinforcement learning\n(RL) offers unique advantages through its sequential decision-making framework,\nwhich naturally fits the iterative nature of data assimilation by dynamically\nbalancing model forecasts with observations. We develop RL-DAUNCE, a new\nRL-based method that enhances data assimilation with physical constraints\nthrough three key aspects. First, RL-DAUNCE inherits the computational\nefficiency of machine learning while it uniquely structures its agents to\nmirror ensemble members in conventional data assimilation methods. Second,\nRL-DAUNCE emphasizes uncertainty quantification by advancing multiple ensemble\nmembers, moving beyond simple mean-state optimization. Third, RL-DAUNCE's\nensemble-as-agents design facilitates the enforcement of physical constraints\nduring the assimilation process, which is crucial to improving the state\nestimation and subsequent forecasting. A primal-dual optimization strategy is\ndeveloped to enforce constraints, which dynamically penalizes the reward\nfunction to ensure constraint satisfaction throughout the learning process.\nAlso, state variable bounds are respected by constraining the RL action space.\nTogether, these features ensure physical consistency without sacrificing\nefficiency. RL-DAUNCE is applied to the Madden-Julian Oscillation, an\nintermittent atmospheric phenomenon characterized by strongly non-Gaussian\nfeatures and multiple physical constraints. RL-DAUNCE outperforms the standard\nensemble Kalman filter (EnKF), which fails catastrophically due to the\nviolation of physical constraints. Notably, RL-DAUNCE matches the performance\nof constrained EnKF, particularly in recovering intermittent signals, capturing\nextreme events, and quantifying uncertainties, while requiring substantially\nless computational effort.", "AI": {"tldr": "\u63d0\u51faRL-DAUNCE\uff0c\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u7269\u7406\u7ea6\u675f\u7684\u6570\u636e\u540c\u5316\u65b9\u6cd5\uff0c\u4f18\u4e8e\u4f20\u7edfEnsemble Kalman Filter\uff08EnKF\uff09\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5177\u5907\u987a\u5e8f\u51b3\u7b56\u7279\u70b9\uff0c\u4e0e\u6570\u636e\u540c\u5316\u7684\u8fed\u4ee3\u7279\u6027\u5929\u7136\u9002\u914d\uff0c\u800c\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65e0\u6cd5\u52a8\u6001\u5e73\u8861\u6a21\u578b\u9884\u6d4b\u4e0e\u89c2\u6d4b\u3002", "method": "RL-DAUNCE\u901a\u8fc7\u4e09\u4e2a\u65b9\u9762\u4f18\u5316\uff1a\u91c7\u7528\u7c7b\u4f3c\u4f20\u7edf\u540c\u5316\u7684\u96c6\u5408\u6210\u5458\u4ee3\u7406\u3001\u5f3a\u8c03\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u4ee5\u53ca\u901a\u8fc7\u5bf9\u5076\u4f18\u5316\u7b56\u7565\u786e\u4fdd\u7269\u7406\u7ea6\u675f\u3002", "result": "\u5728Madden-Julian Oscillation\u7684\u5e94\u7528\u4e2d\uff0cRL-DAUNCE\u4f18\u4e8e\u6807\u51c6EnKF\uff0c\u63a5\u8fd1\u7ea6\u675fEnKF\u7684\u6027\u80fd\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "RL-DAUNCE\u4e0d\u4ec5\u63d0\u5347\u4e86\u6570\u636e\u540c\u5316\u7684\u7269\u7406\u4e00\u81f4\u6027\uff0c\u4e14\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2505.04948", "pdf": "https://arxiv.org/pdf/2505.04948", "abs": "https://arxiv.org/abs/2505.04948", "authors": ["Md Aminul Islam", "Ahmed Sayeed Faruk"], "title": "Prompt-Based LLMs for Position Bias-Aware Reranking in Personalized Recommendations", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Recommender systems are essential for delivering personalized content across\ndigital platforms by modeling user preferences and behaviors. Recently, large\nlanguage models (LLMs) have been adopted for prompt-based recommendation due to\ntheir ability to generate personalized outputs without task-specific training.\nHowever, LLM-based methods face limitations such as limited context window\nsize, inefficient pointwise and pairwise prompting, and difficulty handling\nlistwise ranking due to token constraints. LLMs can also be sensitive to\nposition bias, as they may overemphasize earlier items in the prompt regardless\nof their true relevance. To address and investigate these issues, we propose a\nhybrid framework that combines a traditional recommendation model with an LLM\nfor reranking top-k items using structured prompts. We evaluate the effects of\nuser history reordering and instructional prompts for mitigating position bias.\nExperiments on MovieLens-100K show that randomizing user history improves\nranking quality, but LLM-based reranking does not outperform the base model.\nExplicit instructions to reduce position bias are also ineffective. Our\nevaluations reveal limitations in LLMs' ability to model ranking context and\nmitigate bias. Our code is publicly available at\nhttps://github.com/aminul7506/LLMForReRanking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u4f20\u7edf\u63a8\u8350\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u91cd\u65b0\u6392\u5e8f\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u524dk\u9879\uff0c\u65e8\u5728\u89e3\u51b3LLM\u5728\u63a8\u8350\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u548c\u4f4d\u7f6e\u504f\u5dee\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5c3d\u7ba1\u968f\u673a\u5316\u7528\u6237\u5386\u53f2\u63d0\u9ad8\u4e86\u6392\u5e8f\u8d28\u91cf\uff0cLLM\u91cd\u65b0\u6392\u5e8f\u5e76\u672a\u8d85\u8d8a\u57fa\u7840\u6a21\u578b\uff0c\u4e14\u51cf\u5c11\u4f4d\u7f6e\u504f\u5dee\u7684\u663e\u5f0f\u6307\u4ee4\u6548\u679c\u4e0d\u4f73\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u5728\u6570\u5b57\u5e73\u53f0\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u4f46LLM\u5728\u63a8\u8350\u4efb\u52a1\u4e2d\u9762\u4e34\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u3001\u6548\u7387\u4f4e\u7684\u70b9\u5bf9\u548c\u6210\u5bf9\u63d0\u793a\u4ee5\u53ca\u5904\u7406\u5217\u8868\u6392\u5e8f\u56f0\u96be\u7684\u6311\u6218\u3002\u6b64\u5916\uff0cLLM\u5bf9\u4f4d\u7f6e\u504f\u5dee\u654f\u611f\uff0c\u53ef\u80fd\u8fc7\u5ea6\u5f3a\u8c03\u63d0\u793a\u4e2d\u7684\u65e9\u671f\u9879\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u4f20\u7edf\u63a8\u8350\u6a21\u578b\u548cLLM\u7684\u6df7\u5408\u6846\u67b6\uff0c\u5229\u7528\u7ed3\u6784\u5316\u63d0\u793a\u5bf9\u524dk\u9879\u8fdb\u884c\u91cd\u65b0\u6392\u5e8f\uff0c\u540c\u65f6\u8bc4\u4f30\u7528\u6237\u5386\u53f2\u91cd\u65b0\u6392\u5e8f\u548c\u6307\u4ee4\u63d0\u793a\u5bf9\u7f13\u89e3\u4f4d\u7f6e\u504f\u5dee\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5728MovieLens-100K\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u968f\u673a\u5316\u7528\u6237\u5386\u53f2\u63d0\u9ad8\u4e86\u6392\u5e8f\u8d28\u91cf\uff0c\u4f46LLM\u91cd\u65b0\u6392\u5e8f\u672a\u8d85\u8d8a\u57fa\u7840\u6a21\u578b\u3002\u663e\u5f0f\u6307\u4ee4\u51cf\u5c11\u4f4d\u7f6e\u504f\u5dee\u7684\u6548\u679c\u4e0d\u4f73\uff0c\u63ed\u793a\u4e86LLM\u5728\u5efa\u6a21\u6392\u5e8f\u4e0a\u4e0b\u6587\u548c\u51cf\u5c11\u504f\u5dee\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8bba\u6587\u5c55\u793a\u4e86LLM\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u6392\u5e8f\u4e0a\u4e0b\u6587\u548c\u4f4d\u7f6e\u504f\u5dee\u65b9\u9762\u7684\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2504.03387", "pdf": "https://arxiv.org/pdf/2504.03387", "abs": "https://arxiv.org/abs/2504.03387", "authors": ["Claudius Krause", "Daohan Wang", "Ramon Winterhalder"], "title": "BitHEP -- The Limits of Low-Precision ML in HEP", "categories": ["hep-ph", "cs.LG", "hep-ex"], "comment": "15 pages, 5 figures", "summary": "The increasing complexity of modern neural network architectures demands fast\nand memory-efficient implementations to mitigate computational bottlenecks. In\nthis work, we evaluate the recently proposed BitNet architecture in HEP\napplications, assessing its performance in classification, regression, and\ngenerative modeling tasks. Specifically, we investigate its suitability for\nquark-gluon discrimination, SMEFT parameter estimation, and detector\nsimulation, comparing its efficiency and accuracy to state-of-the-art methods.\nOur results show that while BitNet consistently performs competitively in\nclassification tasks, its performance in regression and generation varies with\nthe size and type of the network, highlighting key limitations and potential\nareas for improvement.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86BitNet\u67b6\u6784\u5728HEP\u5e94\u7528\u4e2d\u7684\u5206\u7c7b\u3001\u56de\u5f52\u548c\u751f\u6210\u4efb\u52a1\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u56de\u5f52\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u56e0\u7f51\u7edc\u5927\u5c0f\u548c\u7c7b\u578b\u800c\u5f02\u3002", "motivation": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u590d\u6742\u6027\u589e\u52a0\uff0c\u9700\u8981\u5feb\u901f\u4e14\u5185\u5b58\u9ad8\u6548\u7684\u5b9e\u73b0\u4ee5\u7f13\u89e3\u8ba1\u7b97\u74f6\u9888\u3002", "method": "\u8bc4\u4f30BitNet\u5728HEP\u5e94\u7528\u4e2d\u7684\u6027\u80fd\uff0c\u5305\u62ec\u5206\u7c7b\uff08\u5982\u5938\u514b-\u80f6\u5b50\u9274\u522b\uff09\u3001\u56de\u5f52\uff08\u5982SMEFT\u53c2\u6570\u4f30\u8ba1\uff09\u548c\u751f\u6210\uff08\u5982\u63a2\u6d4b\u5668\u6a21\u62df\uff09\u3002", "result": "BitNet\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u56de\u5f52\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u56e0\u7f51\u7edc\u5927\u5c0f\u548c\u7c7b\u578b\u800c\u6709\u6240\u4e0d\u540c\u3002", "conclusion": "BitNet\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4f46\u5728\u56de\u5f52\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u4ee5\u514b\u670d\u5176\u5c40\u9650\u6027\u3002"}}
{"id": "2505.04631", "pdf": "https://arxiv.org/pdf/2505.04631", "abs": "https://arxiv.org/abs/2505.04631", "authors": ["Joshua W. Betts", "John M. Still", "Thomas A. Lasko"], "title": "Cryptogenic stroke and migraine: using probabilistic independence and machine learning to uncover latent sources of disease from the electronic health record", "categories": ["stat.AP", "cs.LG", "I.2.1; I.2.3; I.2.6; I.5.1; I.6.4; J.3"], "comment": "10 pages, 5 figures, 1 table, LaTeX. Submitted as a student paper to\n  the American Medical Informatics Association 2025 Annual Symposium for\n  presentation", "summary": "Migraine is a common but complex neurological disorder that doubles the\nlifetime risk of cryptogenic stroke (CS). However, this relationship remains\npoorly characterized, and few clinical guidelines exist to reduce this\nassociated risk. We therefore propose a data-driven approach to extract\nprobabilistically-independent sources from electronic health record (EHR) data\nand create a 10-year risk-predictive model for CS in migraine patients. These\nsources represent external latent variables acting on the causal graph\nconstructed from the EHR data and approximate root causes of CS in our\npopulation. A random forest model trained on patient expressions of these\nsources demonstrated good accuracy (ROC 0.771) and identified the top 10 most\npredictive sources of CS in migraine patients. These sources revealed that\npharmacologic interventions were the most important factor in minimizing CS\nrisk in our population and identified a factor related to allergic rhinitis as\na potential causative source of CS in migraine patients.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u901a\u8fc7\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u521b\u5efa10\u5e74\u98ce\u9669\u9884\u6d4b\u6a21\u578b\uff0c\u63ed\u793a\u504f\u5934\u75db\u60a3\u8005\u9690\u6e90\u6027\u5352\u4e2d\uff08CS\uff09\u7684\u6f5c\u5728\u539f\u56e0\u3002\u968f\u673a\u68ee\u6797\u6a21\u578b\u8868\u73b0\u826f\u597d\uff08ROC 0.771\uff09\uff0c\u53d1\u73b0\u836f\u7269\u5e72\u9884\u662f\u964d\u4f4e\u98ce\u9669\u7684\u6700\u91cd\u8981\u56e0\u7d20\uff0c\u5e76\u8bc6\u522b\u8fc7\u654f\u9f3b\u708e\u76f8\u5173\u56e0\u7d20\u4e3a\u6f5c\u5728\u81f4\u75c5\u6e90\u3002", "motivation": "\u504f\u5934\u75db\u4e0e\u9690\u6e90\u6027\u5352\u4e2d\uff08CS\uff09\u98ce\u9669\u589e\u52a0\u76f8\u5173\uff0c\u4f46\u4e24\u8005\u5173\u7cfb\u5c1a\u672a\u660e\u786e\uff0c\u4e14\u7f3a\u4e4f\u4e34\u5e8a\u6307\u5357\u964d\u4f4e\u98ce\u9669\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5206\u6790EHR\u6570\u636e\uff0c\u63ed\u793aCS\u7684\u6f5c\u5728\u539f\u56e0\u5e76\u6784\u5efa\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u63d0\u53d6\u6982\u7387\u72ec\u7acb\u7684\u6f5c\u5728\u53d8\u91cf\uff0c\u6784\u5efa\u56e0\u679c\u56fe\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u68ee\u6797\u6a21\u578b\u8bad\u7ec3\u9884\u6d4bCS\u98ce\u9669\u3002", "result": "\u6a21\u578b\u51c6\u786e\u7387\u826f\u597d\uff08ROC 0.771\uff09\uff0c\u8bc6\u522b\u4e86\u524d10\u4e2a\u6700\u5177\u9884\u6d4b\u6027\u7684CS\u6765\u6e90\uff0c\u53d1\u73b0\u836f\u7269\u5e72\u9884\u662f\u6700\u91cd\u8981\u56e0\u7d20\uff0c\u5e76\u63ed\u793a\u8fc7\u654f\u9f3b\u708e\u76f8\u5173\u56e0\u7d20\u53ef\u80fd\u662f\u81f4\u75c5\u6e90\u4e4b\u4e00\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6210\u529f\u6784\u5efa\u4e86CS\u98ce\u9669\u9884\u6d4b\u6a21\u578b\uff0c\u4e3a\u504f\u5934\u75db\u60a3\u8005\u7684CS\u98ce\u9669\u7ba1\u7406\u548c\u5e72\u9884\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2505.05098", "pdf": "https://arxiv.org/pdf/2505.05098", "abs": "https://arxiv.org/abs/2505.05098", "authors": ["Wei Liu", "Jiyuan Zhang", "Binxiong Zheng", "Yufeng Hu", "Yingzhan Lin", "Zengfeng Zeng"], "title": "X-Driver: Explainable Autonomous Driving with Vision-Language Models", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.ET"], "comment": null, "summary": "End-to-end autonomous driving has advanced significantly, offering benefits\nsuch as system simplicity and stronger driving performance in both open-loop\nand closed-loop settings than conventional pipelines. However, existing\nframeworks still suffer from low success rates in closed-loop evaluations,\nhighlighting their limitations in real-world deployment. In this paper, we\nintroduce X-Driver, a unified multi-modal large language models(MLLMs)\nframework designed for closed-loop autonomous driving, leveraging\nChain-of-Thought(CoT) and autoregressive modeling to enhance perception and\ndecision-making. We validate X-Driver across multiple autonomous driving tasks\nusing public benchmarks in CARLA simulation environment, including\nBench2Drive[6]. Our experimental results demonstrate superior closed-loop\nperformance, surpassing the current state-of-the-art(SOTA) while improving the\ninterpretability of driving decisions. These findings underscore the importance\nof structured reasoning in end-to-end driving and establish X-Driver as a\nstrong baseline for future research in closed-loop autonomous driving.", "AI": {"tldr": "X-Driver\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u548c\u81ea\u56de\u5f52\u5efa\u6a21\u63d0\u5347\u611f\u77e5\u4e0e\u51b3\u7b56\u80fd\u529b\uff0c\u5728CARLA\u4eff\u771f\u73af\u5883\u4e2d\u9a8c\u8bc1\u5176\u95ed\u73af\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\u5728\u95ed\u73af\u8bc4\u4f30\u4e2d\u6210\u529f\u7387\u8f83\u4f4e\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u6548\u679c\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "X-Driver\u7ed3\u5408\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u548c\u81ea\u56de\u5f52\u5efa\u6a21\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u4f18\u5316\u611f\u77e5\u4e0e\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u5728CARLA\u4eff\u771f\u73af\u5883\uff08\u5982Bench2Drive\uff09\u4e2d\uff0cX-Driver\u7684\u95ed\u73af\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\uff08SOTA\uff09\uff0c\u5e76\u63d0\u5347\u4e86\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "X-Driver\u8bc1\u660e\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u5728\u7aef\u5230\u7aef\u9a7e\u9a76\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u95ed\u73af\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u57fa\u7ebf\u3002"}}
{"id": "2505.04647", "pdf": "https://arxiv.org/pdf/2505.04647", "abs": "https://arxiv.org/abs/2505.04647", "authors": ["Md Rahat-uz- Zaman", "Bei Wang", "Paul Rosen"], "title": "ChannelExplorer: Exploring Class Separability Through Activation Channel Visualization", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "Deep neural networks (DNNs) achieve state-of-the-art performance in many\nvision tasks, yet understanding their internal behavior remains challenging,\nparticularly how different layers and activation channels contribute to class\nseparability. We introduce ChannelExplorer, an interactive visual analytics\ntool for analyzing image-based outputs across model layers, emphasizing\ndata-driven insights over architecture analysis for exploring class\nseparability. ChannelExplorer summarizes activations across layers and\nvisualizes them using three primary coordinated views: a Scatterplot View to\nreveal inter- and intra-class confusion, a Jaccard Similarity View to quantify\nactivation overlap, and a Heatmap View to inspect activation channel patterns.\nOur technique supports diverse model architectures, including CNNs, GANs,\nResNet and Stable Diffusion models. We demonstrate the capabilities of\nChannelExplorer through four use-case scenarios: (1) generating class hierarchy\nin ImageNet, (2) finding mislabeled images, (3) identifying activation channel\ncontributions, and(4) locating latent states' position in Stable Diffusion\nmodel. Finally, we evaluate the tool with expert users.", "AI": {"tldr": "ChannelExplorer\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5206\u6790\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5404\u5c42\u7684\u6fc0\u6d3b\u901a\u9053\u5bf9\u7c7b\u522b\u53ef\u5206\u6027\u7684\u8d21\u732e\uff0c\u652f\u6301\u591a\u79cd\u6a21\u578b\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u4f7f\u7528\u573a\u666f\u5c55\u793a\u4e86\u5176\u5b9e\u7528\u6027\u3002", "motivation": "\u7406\u89e3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5185\u90e8\u884c\u4e3a\uff0c\u5c24\u5176\u662f\u4e0d\u540c\u5c42\u548c\u6fc0\u6d3b\u901a\u9053\u5bf9\u7c7b\u522b\u53ef\u5206\u6027\u7684\u8d21\u732e\uff0c\u4ecd\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f00\u53d1\u4e86ChannelExplorer\u5de5\u5177\uff0c\u63d0\u4f9b\u6563\u70b9\u56fe\u3001Jaccard\u76f8\u4f3c\u5ea6\u548c\u70ed\u529b\u56fe\u4e09\u79cd\u534f\u540c\u89c6\u56fe\uff0c\u652f\u6301\u5bf9\u591a\u79cd\u6a21\u578b\uff08\u5982CNN\u3001GAN\u3001ResNet\u7b49\uff09\u7684\u5206\u6790\u3002", "result": "\u901a\u8fc7\u56db\u4e2a\u4f7f\u7528\u573a\u666f\uff08\u5982\u751f\u6210\u7c7b\u522b\u5c42\u6b21\u3001\u53d1\u73b0\u9519\u8bef\u6807\u7b7e\u7b49\uff09\u5c55\u793a\u4e86\u5de5\u5177\u7684\u6709\u6548\u6027\uff0c\u5e76\u8fdb\u884c\u4e86\u4e13\u5bb6\u8bc4\u4f30\u3002", "conclusion": "ChannelExplorer\u4e3a\u5206\u6790\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6fc0\u6d3b\u901a\u9053\u548c\u7c7b\u522b\u53ef\u5206\u6027\u63d0\u4f9b\u4e86\u76f4\u89c2\u4e14\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.04961", "pdf": "https://arxiv.org/pdf/2505.04961", "abs": "https://arxiv.org/abs/2505.04961", "authors": ["Ziyu Zhang", "Sergey Bashkirov", "Dun Yang", "Michael Taylor", "Xue Bin Peng"], "title": "ADD: Physics-Based Motion Imitation with Adversarial Differential Discriminators", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.RO"], "comment": "19 pages, 15 figures", "summary": "Multi-objective optimization problems, which require the simultaneous\noptimization of multiple terms, are prevalent across numerous applications.\nExisting multi-objective optimization methods often rely on manually tuned\naggregation functions to formulate a joint optimization target. The performance\nof such hand-tuned methods is heavily dependent on careful weight selection, a\ntime-consuming and laborious process. These limitations also arise in the\nsetting of reinforcement-learning-based motion tracking for physically\nsimulated characters, where intricately crafted reward functions are typically\nused to achieve high-fidelity results. Such solutions not only require domain\nexpertise and significant manual adjustment, but also limit the applicability\nof the resulting reward function across diverse skills. To bridge this gap, we\npresent a novel adversarial multi-objective optimization technique that is\nbroadly applicable to a range of multi-objective optimization problems,\nincluding motion tracking. The proposed adversarial differential discriminator\nreceives a single positive sample, yet is still effective at guiding the\noptimization process. We demonstrate that our technique can enable characters\nto closely replicate a variety of acrobatic and agile behaviors, achieving\ncomparable quality to state-of-the-art motion-tracking methods, without relying\non manually tuned reward functions. Results are best visualized through\nhttps://youtu.be/rz8BYCE9E2w.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5bf9\u6297\u6027\u591a\u76ee\u6807\u4f18\u5316\u6280\u672f\uff0c\u9002\u7528\u4e8e\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u52a8\u4f5c\u8ddf\u8e2a\u9886\u57df\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u5956\u52b1\u51fd\u6570\u5373\u53ef\u8fbe\u5230\u9ad8\u8d28\u91cf\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u8c03\u6574\u805a\u5408\u51fd\u6570\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u4f5c\u8ddf\u8e2a\u4e2d\uff0c\u624b\u52a8\u8c03\u6574\u65e2\u8d39\u65f6\u53c8\u9650\u5236\u4e86\u9002\u7528\u6027\u3002", "method": "\u91c7\u7528\u5bf9\u6297\u6027\u5dee\u5206\u5224\u522b\u5668\uff0c\u4ec5\u9700\u5355\u4e2a\u6b63\u6837\u672c\u5373\u53ef\u6709\u6548\u6307\u5bfc\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u6280\u672f\u80fd\u590d\u73b0\u591a\u79cd\u654f\u6377\u52a8\u4f5c\uff0c\u6548\u679c\u4e0e\u6700\u5148\u8fdb\u7684\u52a8\u4f5c\u8ddf\u8e2a\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u8be5\u6280\u672f\u5e7f\u6cdb\u9002\u7528\u4e8e\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.04648", "pdf": "https://arxiv.org/pdf/2505.04648", "abs": "https://arxiv.org/abs/2505.04648", "authors": ["Alejandro Giraldo", "Daniel Ruiz", "Mariano Caruso", "Guido Bellomo"], "title": "Quantum QSAR for drug discovery", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "Quantitative Structure-Activity Relationship (QSAR) modeling is key in drug\ndiscovery, but classical methods face limitations when handling\nhigh-dimensional data and capturing complex molecular interactions. This\nresearch proposes enhancing QSAR techniques through Quantum Support Vector\nMachines (QSVMs), which leverage quantum computing principles to process\ninformation Hilbert spaces. By using quantum data encoding and quantum kernel\nfunctions, we aim to develop more accurate and efficient predictive models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u91cf\u5b50\u652f\u6301\u5411\u91cf\u673a\uff08QSVM\uff09\u589e\u5f3aQSAR\u5efa\u6a21\uff0c\u4ee5\u89e3\u51b3\u9ad8\u7ef4\u6570\u636e\u548c\u590d\u6742\u5206\u5b50\u4e92\u4f5c\u7684\u6311\u6218\u3002", "motivation": "\u4f20\u7edfQSAR\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u548c\u590d\u6742\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u91cf\u5b50\u8ba1\u7b97\u4e3a\u6b64\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u901a\u8fc7\u91cf\u5b50\u6570\u636e\u7f16\u7801\u548c\u91cf\u5b50\u6838\u51fd\u6570\uff0c\u5229\u7528\u91cf\u5b50\u652f\u6301\u5411\u91cf\u673a\uff08QSVMs\uff09\u5728\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u5904\u7406\u4fe1\u606f\u3002", "result": "\u76ee\u6807\u662f\u5f00\u53d1\u66f4\u7cbe\u51c6\u3001\u9ad8\u6548\u7684\u9884\u6d4b\u6a21\u578b\u3002", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u6280\u672f\u6709\u671b\u63d0\u5347QSAR\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.04971", "pdf": "https://arxiv.org/pdf/2505.04971", "abs": "https://arxiv.org/abs/2505.04971", "authors": ["Yuta Kawakami", "Jin Tian"], "title": "Moments of Causal Effects", "categories": ["stat.ME", "cs.AI"], "comment": null, "summary": "The moments of random variables are fundamental statistical measures for\ncharacterizing the shape of a probability distribution, encompassing metrics\nsuch as mean, variance, skewness, and kurtosis. Additionally, the product\nmoments, including covariance and correlation, reveal the relationships between\nmultiple random variables. On the other hand, the primary focus of causal\ninference is the evaluation of causal effects, which are defined as the\ndifference between two potential outcomes. While traditional causal effect\nassessment focuses on the average causal effect, this work provides\ndefinitions, identification theorems, and bounds for moments and product\nmoments of causal effects to analyze their distribution and relationships. We\nconduct experiments to illustrate the estimation of the moments of causal\neffects from finite samples and demonstrate their practical application using a\nreal-world medical dataset.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5bf9\u56e0\u679c\u6548\u5e94\u7684\u77e9\u548c\u4e58\u79ef\u77e9\u7684\u5b9a\u4e49\u3001\u8bc6\u522b\u5b9a\u7406\u548c\u754c\u9650\uff0c\u4ee5\u5206\u6790\u5176\u5206\u5e03\u548c\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u5982\u4f55\u4ece\u6709\u9650\u6837\u672c\u4e2d\u4f30\u8ba1\u56e0\u679c\u6548\u5e94\u7684\u77e9\u53ca\u5176\u5728\u5b9e\u9645\u533b\u5b66\u6570\u636e\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u7684\u56e0\u679c\u6548\u5e94\u8bc4\u4f30\u901a\u5e38\u4ec5\u5173\u6ce8\u5e73\u5747\u56e0\u679c\u6548\u5e94\uff0c\u4f46\u5ffd\u7565\u4e86\u5176\u66f4\u9ad8\u9636\u7684\u5206\u5e03\u7279\u5f81\u548c\u53d8\u91cf\u95f4\u5173\u7cfb\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u5206\u6790\u56e0\u679c\u6548\u5e94\u7684\u77e9\u548c\u4e58\u79ef\u77e9\u6765\u66f4\u5168\u9762\u5730\u63cf\u8ff0\u5176\u7edf\u8ba1\u7279\u6027\u3002", "method": "\u8bba\u6587\u63d0\u4f9b\u4e86\u56e0\u679c\u6548\u5e94\u7684\u77e9\u548c\u4e58\u79ef\u77e9\u7684\u5b9a\u4e49\u548c\u8bc6\u522b\u5b9a\u7406\uff0c\u5e76\u63a8\u5bfc\u4e86\u5176\u754c\u9650\u3002\u968f\u540e\u901a\u8fc7\u5b9e\u9a8c\u4ece\u6709\u9650\u6837\u672c\u4e2d\u4f30\u8ba1\u8fd9\u4e9b\u77e9\uff0c\u5e76\u4ee5\u771f\u5b9e\u533b\u5b66\u6570\u636e\u96c6\u4e3a\u4f8b\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f30\u8ba1\u56e0\u679c\u6548\u5e94\u7684\u77e9\u548c\u4e58\u79ef\u77e9\uff0c\u5e76\u5728\u5b9e\u9645\u6570\u636e\u4e2d\u5c55\u793a\u4e86\u5176\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u901a\u8fc7\u6269\u5c55\u56e0\u679c\u6548\u5e94\u7684\u5206\u6790\u6846\u67b6\u81f3\u66f4\u9ad8\u9636\u77e9\uff0c\u672c\u7814\u7a76\u4e3a\u66f4\u5168\u9762\u5730\u7406\u89e3\u548c\u8bc4\u4f30\u56e0\u679c\u6548\u5e94\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2505.05422", "pdf": "https://arxiv.org/pdf/2505.05422", "abs": "https://arxiv.org/abs/2505.05422", "authors": ["Haokun Lin", "Teng Wang", "Yixiao Ge", "Yuying Ge", "Zhichao Lu", "Ying Wei", "Qingfu Zhang", "Zhenan Sun", "Ying Shan"], "title": "TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Technical Report", "summary": "Pioneering token-based works such as Chameleon and Emu3 have established a\nfoundation for multimodal unification but face challenges of high training\ncomputational overhead and limited comprehension performance due to a lack of\nhigh-level semantics. In this paper, we introduce TokLIP, a visual tokenizer\nthat enhances comprehension by semanticizing vector-quantized (VQ) tokens and\nincorporating CLIP-level semantics while enabling end-to-end multimodal\nautoregressive training with standard VQ tokens. TokLIP integrates a low-level\ndiscrete VQ tokenizer with a ViT-based token encoder to capture high-level\ncontinuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize\nhigh-level features, TokLIP disentangles training objectives for comprehension\nand generation, allowing the direct application of advanced VQ tokenizers\nwithout the need for tailored quantization operations. Our empirical results\ndemonstrate that TokLIP achieves exceptional data efficiency, empowering visual\ntokens with high-level semantic understanding while enhancing low-level\ngenerative capacity, making it well-suited for autoregressive Transformers in\nboth comprehension and generation tasks. The code and models are available at\nhttps://github.com/TencentARC/TokLIP.", "AI": {"tldr": "TokLIP\u662f\u4e00\u79cd\u89c6\u89c9\u5206\u8bcd\u5668\uff0c\u901a\u8fc7\u8bed\u4e49\u5316\u5411\u91cf\u91cf\u5316\uff08VQ\uff09token\u5e76\u878d\u5165CLIP\u7ea7\u8bed\u4e49\uff0c\u63d0\u5347\u7406\u89e3\u80fd\u529b\uff0c\u540c\u65f6\u652f\u6301\u7aef\u5230\u7aef\u591a\u6a21\u6001\u81ea\u56de\u5f52\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709token-based\u65b9\u6cd5\uff08\u5982Chameleon\u548cEmu3\uff09\u8ba1\u7b97\u5f00\u9500\u9ad8\u3001\u7406\u89e3\u6027\u80fd\u6709\u9650\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u9ad8\u5c42\u8bed\u4e49\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002", "method": "TokLIP\u7ed3\u5408\u4f4e\u5c42\u79bb\u6563VQ\u5206\u8bcd\u5668\u4e0eViT-based token\u7f16\u7801\u5668\uff0c\u5206\u79bb\u7406\u89e3\u4e0e\u751f\u6210\u8bad\u7ec3\u76ee\u6807\uff0c\u65e0\u9700\u5b9a\u5236\u91cf\u5316\u64cd\u4f5c\u3002", "result": "\u5b9e\u9a8c\u663e\u793aTokLIP\u5177\u6709\u51fa\u8272\u7684\u6570\u636e\u6548\u7387\uff0c\u589e\u5f3atoken\u7684\u9ad8\u5c42\u8bed\u4e49\u7406\u89e3\u548c\u4f4e\u5c42\u751f\u6210\u80fd\u529b\uff0c\u9002\u5408\u81ea\u56de\u5f52Transformer\u3002", "conclusion": "TokLIP\u4e3a\u591a\u6a21\u6001\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.04972", "pdf": "https://arxiv.org/pdf/2505.04972", "abs": "https://arxiv.org/abs/2505.04972", "authors": ["Mattia Sartori", "Chetna Singhal", "Neelabhro Roy", "Davide Brunelli", "James Gross"], "title": "AI and Vision based Autonomous Navigation of Nano-Drones in Partially-Known Environments", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.NI"], "comment": "in DCOSS-IoT 2025, Wi-DroIT 2025", "summary": "The miniaturisation of sensors and processors, the advancements in connected\nedge intelligence, and the exponential interest in Artificial Intelligence are\nboosting the affirmation of autonomous nano-size drones in the Internet of\nRobotic Things ecosystem. However, achieving safe autonomous navigation and\nhigh-level tasks such as exploration and surveillance with these tiny platforms\nis extremely challenging due to their limited resources. This work focuses on\nenabling the safe and autonomous flight of a pocket-size, 30-gram platform\ncalled Crazyflie 2.1 in a partially known environment. We propose a novel\nAI-aided, vision-based reactive planning method for obstacle avoidance under\nthe ambit of Integrated Sensing, Computing and Communication paradigm. We deal\nwith the constraints of the nano-drone by splitting the navigation task into\ntwo parts: a deep learning-based object detector runs on the edge (external\nhardware) while the planning algorithm is executed onboard. The results show\nthe ability to command the drone at $\\sim8$ frames-per-second and a model\nperformance reaching a COCO mean-average-precision of $60.8$. Field experiments\ndemonstrate the feasibility of the solution with the drone flying at a top\nspeed of $1$ m/s while steering away from an obstacle placed in an unknown\nposition and reaching the target destination. The outcome highlights the\ncompatibility of the communication delay and the model performance with the\nrequirements of the real-time navigation task. We provide a feasible\nalternative to a fully onboard implementation that can be extended to\nautonomous exploration with nano-drones.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u89c6\u89c9\u53cd\u5e94\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u534730\u514b\u5c0f\u578b\u65e0\u4eba\u673a\u5728\u90e8\u5206\u5df2\u77e5\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u907f\u969c\u80fd\u529b\uff0c\u901a\u8fc7\u5206\u5272\u4efb\u52a1\uff08\u5916\u90e8\u786c\u4ef6\u5904\u7406\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\uff0c\u673a\u8f7d\u6267\u884c\u89c4\u5212\u7b97\u6cd5\uff09\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u8d44\u6e90\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5bfc\u822a\u3002", "motivation": "\u7531\u4e8e\u7eb3\u7c73\u65e0\u4eba\u673a\u8d44\u6e90\u6709\u9650\uff0c\u5b9e\u73b0\u5176\u5b89\u5168\u81ea\u4e3b\u5bfc\u822a\u53ca\u9ad8\u7ea7\u4efb\u52a1\uff08\u5982\u63a2\u7d22\u3001\u76d1\u89c6\uff09\u6781\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8fb9\u7f18\u8ba1\u7b97\u4e0e\u673a\u8f7d\u89c4\u5212\u7ed3\u5408\uff0c\u89e3\u51b3\u8fd9\u4e00\u96be\u9898\u3002", "method": "\u91c7\u7528AI\u8f85\u52a9\u7684\u89c6\u89c9\u53cd\u5e94\u89c4\u5212\u65b9\u6cd5\uff0c\u5c06\u5bfc\u822a\u4efb\u52a1\u5206\u4e3a\u4e24\u90e8\u5206\uff1a\u8fb9\u7f18\u786c\u4ef6\u8fd0\u884c\u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\uff0c\u673a\u8f7d\u6267\u884c\u89c4\u5212\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u65e0\u4eba\u673a\u80fd\u4ee5\u7ea6\u6bcf\u79d28\u5e27\u7684\u901f\u5ea6\u5904\u7406\u6307\u4ee4\uff0c\u6a21\u578b\u6027\u80fd\u8fbeCOCO mAP 60.8\uff0c\u5e76\u5728\u5b9e\u5730\u6d4b\u8bd5\u4e2d\u4ee51\u7c73/\u79d2\u901f\u5ea6\u907f\u969c\u5e76\u62b5\u8fbe\u76ee\u6807\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u901a\u4fe1\u5ef6\u8fdf\u548c\u6a21\u578b\u6027\u80fd\u4e0a\u6ee1\u8db3\u5b9e\u65f6\u5bfc\u822a\u9700\u6c42\uff0c\u4e3a\u7eb3\u7c73\u65e0\u4eba\u673a\u5b8c\u5168\u673a\u8f7d\u5b9e\u73b0\u63d0\u4f9b\u4e86\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u53ef\u62d3\u5c55\u81f3\u81ea\u4e3b\u63a2\u7d22\u4efb\u52a1\u3002"}}
{"id": "2505.05446", "pdf": "https://arxiv.org/pdf/2505.05446", "abs": "https://arxiv.org/abs/2505.05446", "authors": ["Han Xiao", "Yina Xie", "Guanxin Tan", "Yinghao Chen", "Rui Hu", "Ke Wang", "Aojun Zhou", "Hao Li", "Hao Shao", "Xudong Lu", "Peng Gao", "Yafei Wen", "Xiaoxin Chen", "Shuai Ren", "Hongsheng Li"], "title": "Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding", "categories": ["cs.CV", "cs.CL"], "comment": "CVPR2025", "summary": "Visual Document Understanding has become essential with the increase of\ntext-rich visual content. This field poses significant challenges due to the\nneed for effective integration of visual perception and textual comprehension,\nparticularly across diverse document types with complex layouts. Moreover,\nexisting fine-tuning datasets for this domain often fall short in providing the\ndetailed contextual information for robust understanding, leading to\nhallucinations and limited comprehension of spatial relationships among visual\nelements. To address these challenges, we propose an innovative pipeline that\nutilizes adaptive generation of markup languages, such as Markdown, JSON, HTML,\nand TiKZ, to build highly structured document representations and deliver\ncontextually-grounded responses. We introduce two fine-grained structured\ndatasets: DocMark-Pile, comprising approximately 3.8M pretraining data pairs\nfor document parsing, and DocMark-Instruct, featuring 624k fine-tuning data\nannotations for grounded instruction following. Extensive experiments\ndemonstrate that our proposed model significantly outperforms existing\nstate-of-theart MLLMs across a range of visual document understanding\nbenchmarks, facilitating advanced reasoning and comprehension capabilities in\ncomplex visual scenarios. Our code and models are released at https://github.\ncom/Euphoria16/DocMark.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u6d41\u6c34\u7ebf\uff0c\u5229\u7528Markdown\u3001JSON\u3001HTML\u7b49\u6807\u8bb0\u8bed\u8a00\u751f\u6210\u7ed3\u6784\u5316\u6587\u6863\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u6587\u6863\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u89c6\u89c9\u6587\u6863\u7406\u89e3\u9762\u4e34\u6574\u5408\u89c6\u89c9\u611f\u77e5\u548c\u6587\u672c\u7406\u89e3\u7684\u6311\u6218\uff0c\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u8be6\u7ec6\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u81ea\u9002\u5e94\u751f\u6210\u7684\u6807\u8bb0\u8bed\u8a00\u6784\u5efa\u7ed3\u6784\u5316\u6587\u6863\u8868\u793a\uff0c\u5e76\u5f15\u5165\u4e24\u4e2a\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6DocMark-Pile\u548cDocMark-Instruct\u3002", "result": "\u6a21\u578b\u5728\u591a\u4e2a\u89c6\u89c9\u6587\u6863\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f73MLLMs\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u89c6\u89c9\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u5148\u8fdb\u7684\u63a8\u7406\u548c\u7406\u89e3\u80fd\u529b\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.04977", "pdf": "https://arxiv.org/pdf/2505.04977", "abs": "https://arxiv.org/abs/2505.04977", "authors": ["Brian Choi", "Shu Wang", "Isabelle Choi", "Kun Sun"], "title": "ChainMarks: Securing DNN Watermark with Cryptographic Chain", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted In ACM ASIA Conference on Computer and Communications\n  Security (ASIA CCS '25), August 25-29, 2025, Ha Noi, Vietnam", "summary": "With the widespread deployment of deep neural network (DNN) models, dynamic\nwatermarking techniques are being used to protect the intellectual property of\nmodel owners. However, recent studies have shown that existing watermarking\nschemes are vulnerable to watermark removal and ambiguity attacks. Besides, the\nvague criteria for determining watermark presence further increase the\nlikelihood of such attacks. In this paper, we propose a secure DNN watermarking\nscheme named ChainMarks, which generates secure and robust watermarks by\nintroducing a cryptographic chain into the trigger inputs and utilizes a\ntwo-phase Monte Carlo method for determining watermark presence. First,\nChainMarks generates trigger inputs as a watermark dataset by repeatedly\napplying a hash function over a secret key, where the target labels associated\nwith trigger inputs are generated from the digital signature of model owner.\nThen, the watermarked model is produced by training a DNN over both the\noriginal and watermark datasets. To verify watermarks, we compare the predicted\nlabels of trigger inputs with the target labels and determine ownership with a\nmore accurate decision threshold that considers the classification probability\nof specific models. Experimental results show that ChainMarks exhibits higher\nlevels of robustness and security compared to state-of-the-art watermarking\nschemes. With a better marginal utility, ChainMarks provides a higher\nprobability guarantee of watermark presence in DNN models with the same level\nof watermark accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a ChainMarks \u7684\u5b89\u5168 DNN \u6c34\u5370\u65b9\u6848\uff0c\u901a\u8fc7\u5f15\u5165\u5bc6\u7801\u94fe\u751f\u6210\u89e6\u53d1\u5668\u8f93\u5165\uff0c\u5e76\u4f7f\u7528\u4e24\u9636\u6bb5\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u786e\u5b9a\u6c34\u5370\u5b58\u5728\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u5370\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709 DNN \u6c34\u5370\u6280\u672f\u5bb9\u6613\u53d7\u5230\u79fb\u9664\u548c\u6a21\u7cca\u653b\u51fb\uff0c\u4e14\u6c34\u5370\u5b58\u5728\u5224\u5b9a\u6807\u51c6\u6a21\u7cca\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u5b89\u5168\u3001\u53ef\u9760\u7684\u6c34\u5370\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u54c8\u5e0c\u51fd\u6570\u548c\u5bc6\u94a5\u751f\u6210\u89e6\u53d1\u5668\u8f93\u5165\u4f5c\u4e3a\u6c34\u5370\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u6a21\u578b\u6240\u6709\u8005\u7684\u6570\u5b57\u7b7e\u540d\u751f\u6210\u76ee\u6807\u6807\u7b7e\uff0c\u8bad\u7ec3\u5e26\u6c34\u5370\u7684 DNN \u6a21\u578b\u3002\u6c34\u5370\u9a8c\u8bc1\u65f6\uff0c\u57fa\u4e8e\u5206\u7c7b\u6982\u7387\u548c\u66f4\u7cbe\u786e\u7684\u51b3\u7b56\u9608\u503c\u5224\u5b9a\u6240\u6709\u6743\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cChainMarks \u5728\u76f8\u540c\u6c34\u5370\u7cbe\u5ea6\u4e0b\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u3001\u5b89\u5168\u6027\uff0c\u5e76\u80fd\u66f4\u53ef\u9760\u5730\u5224\u5b9a\u6c34\u5370\u5b58\u5728\u3002", "conclusion": "ChainMarks \u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b89\u5168\u7684 DNN \u6c34\u5370\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2505.05467", "pdf": "https://arxiv.org/pdf/2505.05467", "abs": "https://arxiv.org/abs/2505.05467", "authors": ["Haibo Wang", "Bo Feng", "Zhengfeng Lai", "Mingze Xu", "Shiyu Li", "Weifeng Ge", "Afshin Dehghan", "Meng Cao", "Ping Huang"], "title": "StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.", "AI": {"tldr": "StreamBridge\u662f\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u79bb\u7ebfVideo-LLMs\u6539\u8fdb\u4e3a\u652f\u6301\u6d41\u5f0f\u5904\u7406\u7684\u6a21\u578b\u3002\u5b83\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u591a\u8f6e\u5b9e\u65f6\u7406\u89e3\u548c\u4e3b\u52a8\u54cd\u5e94\u673a\u5236\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u5185\u5b58\u7f13\u51b2\u533a\u548c\u8f7b\u91cf\u7ea7\u6fc0\u6d3b\u6a21\u578b\u589e\u5f3a\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cStreamBridge\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8eGPT-4o\u548cGemini 1.5 Pro\u7b49\u4e13\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709Video-LLMs\u5728\u591a\u8f6e\u5b9e\u65f6\u7406\u89e3\u548c\u4e3b\u52a8\u54cd\u5e94\u65b9\u9762\u80fd\u529b\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u5728\u7ebf\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u8be5\u7814\u7a76\u7684\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "StreamBridge\u901a\u8fc7\u5f15\u5165\uff081\uff09\u7ed3\u5408\u964d\u8f6e\u8870\u51cf\u538b\u7f29\u7b56\u7565\u7684\u5185\u5b58\u7f13\u51b2\u533a\u4ee5\u652f\u6301\u591a\u8f6e\u957f\u6587\u672c\u4ea4\u4e92\uff1b\uff082\uff09\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709Video-LLMs\u4e2d\u7684\u8f7b\u91cf\u7ea7\u6fc0\u6d3b\u6a21\u578b\u6765\u5b9e\u73b0\u4e3b\u52a8\u54cd\u5e94\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8005\u8fd8\u6784\u5efa\u4e86\u4e13\u95e8\u7684\u6570\u636e\u96c6Stream-IT\u6765\u652f\u6301\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cStreamBridge\u663e\u8457\u63d0\u5347\u4e86\u79bb\u7ebfVideo-LLMs\u5728\u6d41\u5f0f\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4f18\u4e8eGPT-4o\u548cGemini 1.5 Pro\u7b49\u4e13\u6709\u6a21\u578b\uff0c\u5e76\u5728\u6807\u51c6\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u4e0a\u8fbe\u5230\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u6c34\u5e73\u3002", "conclusion": "StreamBridge\u6709\u6548\u89e3\u51b3\u4e86\u5c06\u79bb\u7ebfVideo-LLMs\u9002\u914d\u4e3a\u6d41\u5f0f\u6a21\u578b\u7684\u6311\u6218\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u5b9e\u65f6\u4ea4\u4e92\u80fd\u529b\uff0c\u8fd8\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u591a\u79cd\u73b0\u6709\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.04983", "pdf": "https://arxiv.org/pdf/2505.04983", "abs": "https://arxiv.org/abs/2505.04983", "authors": ["Yuta Kawakami", "Jin Tian"], "title": "Decomposition of Probabilities of Causation with Two Mediators", "categories": ["stat.ME", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2412.14491", "summary": "Mediation analysis for probabilities of causation (PoC) provides a\nfundamental framework for evaluating the necessity and sufficiency of treatment\nin provoking an event through different causal pathways. One of the primary\nobjectives of causal mediation analysis is to decompose the total effect into\npath-specific components. In this study, we investigate the path-specific\nprobability of necessity and sufficiency (PNS) to decompose the total PNS into\npath-specific components along distinct causal pathways between treatment and\noutcome, incorporating two mediators. We define the path-specific PNS for\ndecomposition and provide an identification theorem. Furthermore, we conduct\nnumerical experiments to assess the properties of the proposed estimators from\nfinite samples and demonstrate their practical application using a real-world\neducational dataset.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u901a\u8fc7\u4e0d\u540c\u56e0\u679c\u8def\u5f84\u5206\u89e3\u5fc5\u8981\u6027\u548c\u5145\u5206\u6027\u6982\u7387\uff08PNS\uff09\u7684\u8def\u5f84\u7279\u5b9a\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u5b9a\u4e49\u4e86\u8def\u5f84\u7279\u5b9aPNS\u53ca\u5176\u8bc6\u522b\u5b9a\u7406\u3002\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u548c\u771f\u5b9e\u6559\u80b2\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e2d\u4ecb\u5206\u6790\u5206\u89e3\u6cbb\u7597\u5bf9\u4e8b\u4ef6\u5f71\u54cd\u7684\u5fc5\u8981\u6027\u548c\u5145\u5206\u6027\u6982\u7387\uff08PoC\uff09\uff0c\u6cbf\u4e0d\u540c\u56e0\u679c\u8def\u5f84\u8bc4\u4f30\u8def\u5f84\u7279\u5b9a\u7684PNS\uff0c\u586b\u8865\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u89e3\u591a\u4e2d\u4ecb\u8def\u5f84PNS\u4e0a\u7684\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u5b9a\u4e49\u4e86\u8def\u5f84\u7279\u5b9aPNS\u7684\u5206\u89e3\u65b9\u6cd5\uff0c\u63d0\u51fa\u8bc6\u522b\u5b9a\u7406\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u68c0\u9a8c\u4f30\u8ba1\u91cf\u7684\u6709\u9650\u6837\u672c\u6027\u8d28\uff0c\u6700\u7ec8\u7528\u771f\u5b9e\u6559\u80b2\u6570\u636e\u96c6\u9a8c\u8bc1\u5b9e\u7528\u6027\u3002", "result": "\u63d0\u51fa\u7684\u8def\u5f84\u7279\u5b9aPNS\u5206\u89e3\u65b9\u6cd5\u5728\u6570\u503c\u5b9e\u9a8c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u80fd\u591f\u6709\u6548\u5206\u89e3\u591a\u4e2d\u4ecb\u8def\u5f84\u7684\u5f71\u54cd\uff0c\u4e14\u5728\u5b9e\u9645\u6570\u636e\u5e94\u7528\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u56e0\u679c\u4e2d\u4ecb\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u6cbf\u4e0d\u540c\u8def\u5f84\u5206\u89e3\u5fc5\u8981\u6027\u548c\u5145\u5206\u6027\u6982\u7387\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.05001", "pdf": "https://arxiv.org/pdf/2505.05001", "abs": "https://arxiv.org/abs/2505.05001", "authors": ["Lang Nie", "Chunyu Lin", "Kang Liao", "Yun Zhang", "Shuaicheng Liu", "Yao Zhao"], "title": "StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps", "categories": ["cs.CV", "cs.AI"], "comment": "TPAMI2025; https://github.com/nie-lang/StabStitch2. arXiv admin note:\n  text overlap with arXiv:2403.06378", "summary": "We retarget video stitching to an emerging issue, named warping shake, which\nunveils the temporal content shakes induced by sequentially unsmooth warps when\nextending image stitching to video stitching. Even if the input videos are\nstable, the stitched video can inevitably cause undesired warping shakes and\naffect the visual experience. To address this issue, we propose StabStitch++, a\nnovel video stitching framework to realize spatial stitching and temporal\nstabilization with unsupervised learning simultaneously. First, different from\nexisting learning-based image stitching solutions that typically warp one image\nto align with another, we suppose a virtual midplane between original image\nplanes and project them onto it. Concretely, we design a differentiable\nbidirectional decomposition module to disentangle the homography transformation\nand incorporate it into our spatial warp, evenly spreading alignment burdens\nand projective distortions across two views. Then, inspired by camera paths in\nvideo stabilization, we derive the mathematical expression of stitching\ntrajectories in video stitching by elaborately integrating spatial and temporal\nwarps. Finally, a warp smoothing model is presented to produce stable stitched\nvideos with a hybrid loss to simultaneously encourage content alignment,\ntrajectory smoothness, and online collaboration. Compared with StabStitch that\nsacrifices alignment for stabilization, StabStitch++ makes no compromise and\noptimizes both of them simultaneously, especially in the online mode. To\nestablish an evaluation benchmark and train the learning framework, we build a\nvideo stitching dataset with a rich diversity in camera motions and scenes.\nExperiments exhibit that StabStitch++ surpasses current solutions in stitching\nperformance, robustness, and efficiency, offering compelling advancements in\nthis field by building a real-time online video stitching system.", "AI": {"tldr": "StabStitch++ \u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u9891\u62fc\u63a5\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u2018warping shake\u2019\u95ee\u9898\uff0c\u901a\u8fc7\u865a\u62df\u4e2d\u95f4\u9762\u8bbe\u8ba1\u548c\u53cc\u5411\u5206\u89e3\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u7a7a\u95f4\u62fc\u63a5\u548c\u65f6\u95f4\u7a33\u5b9a\u7684\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u62fc\u63a5\u65b9\u6cd5\u5728\u4ece\u56fe\u50cf\u6269\u5c55\u5230\u89c6\u9891\u65f6\uff0c\u4f1a\u56e0\u4e3a\u65f6\u95f4\u4e0a\u7684\u4e0d\u7a33\u5b9a\u800c\u5bfc\u81f4\u2018warping shake\u2019\uff0c\u5c3d\u7ba1\u8f93\u5165\u89c6\u9891\u7a33\u5b9a\uff0c\u62fc\u63a5\u89c6\u9891\u4ecd\u4f1a\u51fa\u73b0\u4e0d\u5fc5\u8981\u7684\u6296\u52a8\u3002", "method": "\u63d0\u51faStabStitch++\u6846\u67b6\uff0c\u91c7\u7528\u865a\u62df\u4e2d\u95f4\u9762\u8bbe\u8ba1\uff0c\u901a\u8fc7\u53cc\u5411\u5206\u89e3\u6a21\u5757\u5747\u5300\u5206\u914d\u5bf9\u9f50\u8d1f\u62c5\u548c\u6295\u5f71\u53d8\u5f62\uff0c\u5e76\u7ed3\u5408\u65f6\u95f4\u548c\u7a7a\u95f4\u53d8\u6362\u6a21\u578b\u751f\u6210\u7a33\u5b9a\u62fc\u63a5\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u8868\u660eStabStitch++\u5728\u62fc\u63a5\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5728\u7ebf\u89c6\u9891\u62fc\u63a5\u7cfb\u7edf\u3002", "conclusion": "StabStitch++ \u5728\u4e0d\u727a\u7272\u5bf9\u9f50\u548c\u7a33\u5b9a\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u62fc\u63a5\u7684\u6574\u4f53\u8d28\u91cf\u3002"}}
{"id": "2505.04713", "pdf": "https://arxiv.org/pdf/2505.04713", "abs": "https://arxiv.org/abs/2505.04713", "authors": ["Luis F. Gomez", "Gonzalo Garrido-Lopez", "Julian Fierrez", "Aythami Morales", "Ruben Tolosana", "Javier Rueda", "Enrique Navarro"], "title": "Comparison of Visual Trackers for Biomechanical Analysis of Running", "categories": ["cs.CV", "cs.LG"], "comment": "Preprint of the paper presented to the Third Workshop on Learning\n  with Few or Without Annotated Face, Body, and Gesture Data on 19th IEEE\n  Conference on Automatic Face and Gesture Recognition 2025", "summary": "Human pose estimation has witnessed significant advancements in recent years,\nmainly due to the integration of deep learning models, the availability of a\nvast amount of data, and large computational resources. These developments have\nled to highly accurate body tracking systems, which have direct applications in\nsports analysis and performance evaluation.\n  This work analyzes the performance of six trackers: two point trackers and\nfour joint trackers for biomechanical analysis in sprints. The proposed\nframework compares the results obtained from these pose trackers with the\nmanual annotations of biomechanical experts for more than 5870 frames. The\nexperimental framework employs forty sprints from five professional runners,\nfocusing on three key angles in sprint biomechanics: trunk inclination, hip\nflex extension, and knee flex extension. We propose a post-processing module\nfor outlier detection and fusion prediction in the joint angles.\n  The experimental results demonstrate that using joint-based models yields\nroot mean squared errors ranging from 11.41{\\deg} to 4.37{\\deg}. When\nintegrated with the post-processing modules, these errors can be reduced to\n6.99{\\deg} and 3.88{\\deg}, respectively. The experimental findings suggest that\nhuman pose tracking approaches can be valuable resources for the biomechanical\nanalysis of running. However, there is still room for improvement in\napplications where high accuracy is required.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u516d\u79cd\u59ff\u6001\u8ddf\u8e2a\u5668\u5728\u77ed\u8dd1\u751f\u7269\u529b\u5b66\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u540e\u5904\u7406\u6a21\u5757\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u7ed3\u679c\u663e\u793a\u57fa\u4e8e\u5173\u8282\u7684\u6a21\u578b\u8bef\u5dee\u8f83\u5c0f\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u59ff\u6001\u4f30\u8ba1\u6280\u672f\u7684\u8fdb\u6b65\u4f7f\u5176\u5728\u4f53\u80b2\u5206\u6790\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u4e0d\u540c\u59ff\u6001\u8ddf\u8e2a\u5668\u5728\u77ed\u8dd1\u751f\u7269\u529b\u5b66\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4f18\u5316\u65b9\u6848\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u516d\u79cd\u8ddf\u8e2a\u5668\uff08\u4e24\u79cd\u57fa\u4e8e\u70b9\uff0c\u56db\u79cd\u57fa\u4e8e\u5173\u8282\uff09\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u4e13\u5bb6\u624b\u52a8\u6807\u6ce8\u76845870\u5e27\u6570\u636e\u8fdb\u884c\u5bf9\u6bd4\u3002\u5b9e\u9a8c\u5bf9\u4e94\u540d\u804c\u4e1a\u77ed\u8dd1\u8fd0\u52a8\u5458\u768440\u6b21\u77ed\u8dd1\u8fdb\u884c\u5173\u952e\u89d2\u5ea6\u5206\u6790\uff08\u8eaf\u5e72\u503e\u659c\u3001\u9acb\u5173\u8282\u5c48\u4f38\u3001\u819d\u5173\u8282\u5c48\u4f38\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u540e\u5904\u7406\u6a21\u5757\u7528\u4e8e\u5f02\u5e38\u503c\u68c0\u6d4b\u548c\u878d\u5408\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u5173\u8282\u7684\u6a21\u578b\u5747\u65b9\u6839\u8bef\u5dee\u572811.41\u00b0\u81f34.37\u00b0\u4e4b\u95f4\uff0c\u52a0\u5165\u540e\u5904\u7406\u6a21\u5757\u540e\u53ef\u964d\u81f36.99\u00b0\u81f33.88\u00b0\uff0c\u663e\u793a\u51fa\u59ff\u6001\u8ddf\u8e2a\u5728\u751f\u7269\u529b\u5b66\u5206\u6790\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u4f46\u9ad8\u7cbe\u5ea6\u5e94\u7528\u4ecd\u9700\u6539\u8fdb\u3002", "conclusion": "\u59ff\u6001\u8ddf\u8e2a\u65b9\u6cd5\u5bf9\u77ed\u8dd1\u751f\u7269\u529b\u5b66\u5206\u6790\u5177\u6709\u4ef7\u503c\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u6ee1\u8db3\u9ad8\u7cbe\u5ea6\u9700\u6c42\u3002"}}
{"id": "2505.04718", "pdf": "https://arxiv.org/pdf/2505.04718", "abs": "https://arxiv.org/abs/2505.04718", "authors": ["Divyansh Srivastava", "Xiang Zhang", "He Wen", "Chenru Wen", "Zhuowen Tu"], "title": "Lay-Your-Scene: Natural Scene Layout Generation with Diffusion Transformers", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We present Lay-Your-Scene (shorthand LayouSyn), a novel text-to-layout\ngeneration pipeline for natural scenes. Prior scene layout generation methods\nare either closed-vocabulary or use proprietary large language models for\nopen-vocabulary generation, limiting their modeling capabilities and broader\napplicability in controllable image generation. In this work, we propose to use\nlightweight open-source language models to obtain scene elements from text\nprompts and a novel aspect-aware diffusion Transformer architecture trained in\nan open-vocabulary manner for conditional layout generation. Extensive\nexperiments demonstrate that LayouSyn outperforms existing methods and achieves\nstate-of-the-art performance on challenging spatial and numerical reasoning\nbenchmarks. Additionally, we present two applications of LayouSyn. First, we\nshow that coarse initialization from large language models can be seamlessly\ncombined with our method to achieve better results. Second, we present a\npipeline for adding objects to images, demonstrating the potential of LayouSyn\nin image editing applications.", "AI": {"tldr": "LayouSyn\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6587\u672c\u5230\u5e03\u5c40\u751f\u6210\u7ba1\u9053\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u548c\u521b\u65b0\u7684\u6269\u6563Transformer\u67b6\u6784\uff0c\u5728\u5f00\u653e\u8bcd\u6c47\u6761\u4ef6\u4e0b\u751f\u6210\u573a\u666f\u5e03\u5c40\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u56fe\u50cf\u7f16\u8f91\u4e2d\u5c55\u793a\u4e86\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u573a\u666f\u5e03\u5c40\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u8bcd\u6c47\u5c01\u95ed\uff0c\u8981\u4e48\u4f9d\u8d56\u4e13\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u9650\u5236\u4e86\u53ef\u63a7\u56fe\u50cf\u751f\u6210\u7684\u5efa\u6a21\u80fd\u529b\u548c\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u4ece\u6587\u672c\u63d0\u793a\u4e2d\u63d0\u53d6\u573a\u666f\u5143\u7d20\uff0c\u7ed3\u5408\u65b0\u578b\u7684\u9762\u5411\u957f\u5bbd\u6bd4\u7684\u6269\u6563Transformer\u67b6\u6784\u8fdb\u884c\u5f00\u653e\u8bcd\u6c47\u6761\u4ef6\u4e0b\u7684\u5e03\u5c40\u751f\u6210\u3002", "result": "\u5728\u7a7a\u95f4\u548c\u6570\u503c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u73b0\u4e86\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u53ca\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "LayouSyn\u4e0d\u4ec5\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd8\u4e3a\u53ef\u63a7\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2505.05054", "pdf": "https://arxiv.org/pdf/2505.05054", "abs": "https://arxiv.org/abs/2505.05054", "authors": ["Navya Sonal Agarwal", "Jan Philipp Schneider", "Kanchana Vaishnavi Gandikota", "Syed Muhammad Kazim", "John Meshreki", "Ivo Ihrke", "Michael Moeller"], "title": "Direct Image Classification from Fourier Ptychographic Microscopy Measurements without Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "ISCS 2025", "summary": "The computational imaging technique of Fourier Ptychographic Microscopy (FPM)\nenables high-resolution imaging with a wide field of view and can serve as an\nextremely valuable tool, e.g. in the classification of cells in medical\napplications. However, reconstructing a high-resolution image from tens or even\nhundreds of measurements is computationally expensive, particularly for a wide\nfield of view. Therefore, in this paper, we investigate the idea of classifying\nthe image content in the FPM measurements directly without performing a\nreconstruction step first. We show that Convolutional Neural Networks (CNN) can\nextract meaningful information from measurement sequences, significantly\noutperforming the classification on a single band-limited image (up to 12 %)\nwhile being significantly more efficient than a reconstruction of a\nhigh-resolution image. Furthermore, we demonstrate that a learned multiplexing\nof several raw measurements allows maintaining the classification accuracy\nwhile reducing the amount of data (and consequently also the acquisition time)\nsignificantly.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u65e0\u9700\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5373\u53ef\u76f4\u63a5\u4eceFPM\u6d4b\u91cf\u4e2d\u5bf9\u7ec6\u80de\u8fdb\u884c\u5206\u7c7b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5e76\u51cf\u5c11\u4e86\u6570\u636e\u91cf\u3002", "motivation": "\u73b0\u6709\u7684FPM\u6280\u672f\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u7279\u522b\u662f\u5728\u5927\u89c6\u573a\u4e0b\uff0c\u56e0\u6b64\u63a2\u7d22\u76f4\u63a5\u5728\u6d4b\u91cf\u9636\u6bb5\u5206\u7c7b\u56fe\u50cf\u5185\u5bb9\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\u3002", "method": "\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u76f4\u63a5\u4eceFPM\u6d4b\u91cf\u5e8f\u5217\u4e2d\u63d0\u53d6\u4fe1\u606f\u5206\u7c7b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5b66\u4e60\u591a\u8def\u590d\u7528\u7684\u65b9\u6cd5\u6765\u51cf\u5c11\u6570\u636e\u91cf\u3002", "result": "CNN\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff08\u63d0\u534712%\uff09\uff0c\u4e14\u907f\u514d\u4e86\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u91cd\u5efa\u6b65\u9aa4\uff0c\u6570\u636e\u91cf\u4e5f\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u76f4\u63a5\u5728\u6d4b\u91cf\u9636\u6bb5\u8fdb\u884c\u56fe\u50cf\u5206\u7c7b\u7684\u9ad8\u6548\u65b9\u6cd5\u53ef\u884c\uff0c\u4e3a\u533b\u5b66\u5e94\u7528\u4e2d\u7684\u5feb\u901f\u5206\u7c7b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.05071", "pdf": "https://arxiv.org/pdf/2505.05071", "abs": "https://arxiv.org/abs/2505.05071", "authors": ["Chunyu Xie", "Bin Wang", "Fanjing Kong", "Jincheng Li", "Dawei Liang", "Gengshen Zhang", "Dawei Leng", "Yuhui Yin"], "title": "FG-CLIP: Fine-Grained Visual and Textual Alignment", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at ICML 2025", "summary": "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks\nsuch as image-text retrieval and zero-shot classification but struggles with\nfine-grained understanding due to its focus on coarse-grained short captions.\nTo address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances\nfine-grained understanding through three key innovations. First, we leverage\nlarge multimodal models to generate 1.6 billion long caption-image pairs for\ncapturing global-level semantic details. Second, a high-quality dataset is\nconstructed with 12 million images and 40 million region-specific bounding\nboxes aligned with detailed captions to ensure precise, context-rich\nrepresentations. Third, 10 million hard fine-grained negative samples are\nincorporated to improve the model's ability to distinguish subtle semantic\ndifferences. Corresponding training methods are meticulously designed for these\ndata. Extensive experiments demonstrate that FG-CLIP outperforms the original\nCLIP and other state-of-the-art methods across various downstream tasks,\nincluding fine-grained understanding, open-vocabulary object detection,\nimage-text retrieval, and general multimodal benchmarks. These results\nhighlight FG-CLIP's effectiveness in capturing fine-grained image details and\nimproving overall model performance. The related data, code, and models are\navailable at https://github.com/360CVGroup/FG-CLIP.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFG-CLIP\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5927\u89c4\u6a21\u957f\u6807\u9898-\u56fe\u50cf\u5bf9\u3001\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u5f15\u5165\u56f0\u96be\u8d1f\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86CLIP\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "CLIP\u867d\u7136\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e3b\u8981\u56e0\u4e3a\u5176\u4f9d\u8d56\u7c97\u7c92\u5ea6\u7684\u77ed\u6807\u9898\u3002FG-CLIP\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u6570\u636e\u548c\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e09\u79cd\u5173\u952e\u521b\u65b0\uff1a1) \u5229\u7528\u5927\u6a21\u578b\u751f\u6210\u957f\u6807\u9898-\u56fe\u50cf\u5bf9\uff1b2) \u6784\u5efa\u5305\u542b\u533a\u57df\u6807\u6ce8\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff1b3) \u5f15\u5165\u56f0\u96be\u8d1f\u6837\u672c\u4ee5\u4f18\u5316\u6a21\u578b\u533a\u5206\u80fd\u529b\u3002", "result": "FG-CLIP\u5728\u7ec6\u7c92\u5ea6\u7406\u89e3\u3001\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e2d\u4f18\u4e8e\u539f\u59cbCLIP\u548c\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "FG-CLIP\u901a\u8fc7\u6539\u8fdb\u6570\u636e\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2505.04831", "pdf": "https://arxiv.org/pdf/2505.04831", "abs": "https://arxiv.org/abs/2505.04831", "authors": ["Nicholas Pfaff", "Hongkai Dai", "Sergey Zakharov", "Shun Iwase", "Russ Tedrake"], "title": "Steerable Scene Generation with Post Training and Inference-Time Search", "categories": ["cs.RO", "cs.GR", "cs.LG"], "comment": "Project website: https://steerable-scene-generation.github.io/", "summary": "Training robots in simulation requires diverse 3D scenes that reflect the\nspecific challenges of downstream tasks. However, scenes that satisfy strict\ntask requirements, such as high-clutter environments with plausible spatial\narrangement, are rare and costly to curate manually. Instead, we generate\nlarge-scale scene data using procedural models that approximate realistic\nenvironments for robotic manipulation, and adapt it to task-specific goals. We\ndo this by training a unified diffusion-based generative model that predicts\nwhich objects to place from a fixed asset library, along with their SE(3)\nposes. This model serves as a flexible scene prior that can be adapted using\nreinforcement learning-based post training, conditional generation, or\ninference-time search, steering generation toward downstream objectives even\nwhen they differ from the original data distribution. Our method enables\ngoal-directed scene synthesis that respects physical feasibility and scales\nacross scene types. We introduce a novel MCTS-based inference-time search\nstrategy for diffusion models, enforce feasibility via projection and\nsimulation, and release a dataset of over 44 million SE(3) scenes spanning five\ndiverse environments. Website with videos, code, data, and model weights:\nhttps://steerable-scene-generation.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7edf\u4e00\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u751f\u6210\u7b26\u5408\u7269\u7406\u53ef\u884c\u6027\u76843D\u573a\u666f\u6570\u636e\uff0c\u652f\u6301\u4efb\u52a1\u5bfc\u5411\u7684\u573a\u666f\u5408\u6210\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b4400\u4e07SE(3)\u573a\u666f\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u624b\u52a8\u7b56\u5212\u6ee1\u8db3\u7279\u5b9a\u4efb\u52a1\u9700\u6c42\u7684\u9ad8\u590d\u6742\u5ea63D\u573a\u666f\u6210\u672c\u9ad8\u6602\u4e14\u7a00\u7f3a\uff0c\u56e0\u6b64\u9700\u81ea\u52a8\u5316\u751f\u6210\u6b64\u7c7b\u573a\u666f\u6570\u636e\u4ee5\u652f\u6301\u673a\u5668\u4eba\u4eff\u771f\u8bad\u7ec3\u3002", "method": "\u8bad\u7ec3\u4e00\u4e2a\u7edf\u4e00\u7684\u6269\u6563\u751f\u6210\u6a21\u578b\u9884\u6d4b\u7269\u4f53\u53ca\u5176SE(3)\u4f4d\u59ff\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u3001\u6761\u4ef6\u751f\u6210\u6216\u63a8\u7406\u65f6\u641c\u7d22\uff08MCTS\u7b56\u7565\uff09\u8fdb\u884c\u9002\u5e94\u6027\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u6295\u5f71\u548c\u4eff\u771f\u786e\u4fdd\u7269\u7406\u53ef\u884c\u6027\u3002", "result": "\u751f\u6210\u4e86\u8986\u76d65\u79cd\u73af\u5883\u76844400\u4e07SE(3)\u573a\u666f\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u4efb\u52a1\u5bfc\u5411\u573a\u666f\u5408\u6210\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u7269\u7406\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u573a\u666f\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u6570\u636e\u4e0e\u6a21\u578b\u63a8\u52a8\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2505.04845", "pdf": "https://arxiv.org/pdf/2505.04845", "abs": "https://arxiv.org/abs/2505.04845", "authors": ["Binesh Sadanandan", "Bahareh Arghavani Nobar", "Vahid Behzadan"], "title": "Comparative Study of Generative Models for Early Detection of Failures in Medical Devices", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "The medical device industry has significantly advanced by integrating\nsophisticated electronics like microchips and field-programmable gate arrays\n(FPGAs) to enhance the safety and usability of life-saving devices. These\ncomplex electro-mechanical systems, however, introduce challenging failure\nmodes that are not easily detectable with conventional methods. Effective fault\ndetection and mitigation become vital as reliance on such electronics grows.\nThis paper explores three generative machine learning-based approaches for\nfault detection in medical devices, leveraging sensor data from surgical\nstaplers,a class 2 medical device. Historically considered low-risk, these\ndevices have recently been linked to an increasing number of injuries and\nfatalities. The study evaluates the performance and data requirements of these\nmachine-learning approaches, highlighting their potential to enhance device\nsafety.", "AI": {"tldr": "\u8bba\u6587\u63a2\u7d22\u4e86\u4e09\u79cd\u57fa\u4e8e\u751f\u6210\u5f0f\u673a\u5668\u5b66\u4e60\u7684\u533b\u7597\u8bbe\u5907\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u624b\u672f\u7f1d\u5408\u5668\u7b49\u4e8c\u7ea7\u533b\u7597\u8bbe\u5907\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u533b\u7597\u8bbe\u5907\u7535\u5b50\u5316\u7684\u590d\u6742\u5316\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u68c0\u6d4b\u5176\u6545\u969c\u6a21\u5f0f\uff0c\u56e0\u6b64\u5bfb\u6c42\u66f4\u6709\u6548\u7684\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u5229\u7528\u4e86\u4e09\u79cd\u751f\u6210\u5f0f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u624b\u672f\u7f1d\u5408\u5668\u7684\u4f20\u611f\u5668\u6570\u636e\u8fdb\u884c\u6545\u969c\u68c0\u6d4b\u3002", "result": "\u7814\u7a76\u8868\u660e\u8fd9\u4e9b\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u63d0\u5347\u8bbe\u5907\u5b89\u5168\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u6027\u80fd\u4e0e\u6570\u636e\u9700\u6c42\u3002", "conclusion": "\u751f\u6210\u5f0f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4e3a\u533b\u7597\u8bbe\u5907\u6545\u969c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u51cf\u5c11\u56e0\u8bbe\u5907\u6545\u969c\u5f15\u53d1\u7684\u4f24\u5bb3\u548c\u6b7b\u4ea1\u3002"}}
{"id": "2505.05138", "pdf": "https://arxiv.org/pdf/2505.05138", "abs": "https://arxiv.org/abs/2505.05138", "authors": ["Steven Jorgensen", "Erik Hemberg", "Jamal Toutouh", "Una-May O'Reilly"], "title": "Guiding Evolutionary AutoEncoder Training with Activation-Based Pruning Operators", "categories": ["cs.NE", "cs.AI"], "comment": "Accepted to The Genetic and Evolutionary Computation Conference\n  (GECCO 2025)", "summary": "This study explores a novel approach to neural network pruning using\nevolutionary computation, focusing on simultaneously pruning the encoder and\ndecoder of an autoencoder. We introduce two new mutation operators that use\nlayer activations to guide weight pruning. Our findings reveal that one of\nthese activation-informed operators outperforms random pruning, resulting in\nmore efficient autoencoders with comparable performance to canonically trained\nmodels. Prior work has established that autoencoder training is effective and\nscalable with a spatial coevolutionary algorithm that cooperatively coevolves a\npopulation of encoders with a population of decoders, rather than one\nautoencoder. We evaluate how the same activity-guided mutation operators\ntransfer to this context. We find that random pruning is better than guided\npruning, in the coevolutionary setting. This suggests activation-based guidance\nproves more effective in low-dimensional pruning environments, where\nconstrained sample spaces can lead to deviations from true uniformity in\nrandomization. Conversely, population-driven strategies enhance robustness by\nexpanding the total pruning dimensionality, achieving statistically uniform\nrandomness that better preserves system dynamics. We experiment with pruning\naccording to different schedules and present best combinations of operator and\nschedule for the canonical and coevolving populations cases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u8ba1\u7b97\u7684\u795e\u7ecf\u7f51\u7edc\u526a\u679d\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6fc0\u6d3b\u4fe1\u606f\u6307\u5bfc\u526a\u679d\uff0c\u5728\u81ea\u7f16\u7801\u5668\u4e0a\u8868\u73b0\u4f18\u4e8e\u968f\u673a\u526a\u679d\uff0c\u4f46\u5728\u534f\u540c\u8fdb\u5316\u573a\u666f\u4e2d\u968f\u673a\u526a\u679d\u66f4\u4f18\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528\u8fdb\u5316\u8ba1\u7b97\u548c\u6fc0\u6d3b\u4fe1\u606f\u6307\u5bfc\u81ea\u7f16\u7801\u5668\u7684\u526a\u679d\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u6548\u7387\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u57fa\u4e8e\u6fc0\u6d3b\u7684\u7a81\u53d8\u7b97\u5b50\uff0c\u5206\u522b\u5728\u7ecf\u5178\u81ea\u7f16\u7801\u5668\u548c\u534f\u540c\u8fdb\u5316\u79cd\u7fa4\u4e2d\u8fdb\u884c\u526a\u679d\u5b9e\u9a8c\u3002", "result": "\u6fc0\u6d3b\u6307\u5bfc\u7684\u526a\u679d\u5728\u7ecf\u5178\u81ea\u7f16\u7801\u5668\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u534f\u540c\u8fdb\u5316\u573a\u666f\u4e2d\u968f\u673a\u526a\u679d\u6548\u679c\u66f4\u597d\uff0c\u8868\u660e\u9ad8\u7ef4\u73af\u5883\u4e0b\u968f\u673a\u6027\u66f4\u7a33\u5065\u3002", "conclusion": "\u6fc0\u6d3b\u6307\u5bfc\u526a\u679d\u9002\u7528\u4e8e\u4f4e\u7ef4\u73af\u5883\uff0c\u800c\u534f\u540c\u8fdb\u5316\u7684\u9ad8\u7ef4\u6027\u589e\u5f3a\u4e86\u968f\u673a\u526a\u679d\u7684\u9c81\u68d2\u6027\uff0c\u9700\u7ed3\u5408\u4e0d\u540c\u7b56\u7565\u4f18\u5316\u526a\u679d\u6548\u679c\u3002"}}
{"id": "2505.05170", "pdf": "https://arxiv.org/pdf/2505.05170", "abs": "https://arxiv.org/abs/2505.05170", "authors": ["Elizabeth Ankrah", "Stephanie Nyairo", "Mercy Muchai", "Kagonya Awori", "Millicent Ochieng", "Mark Kariuki", "Jacki O'Neill"], "title": "Dukawalla: Voice Interfaces for Small Businesses in Africa", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Small and medium sized businesses often struggle with data driven decision\nmaking do to a lack of advanced analytics tools, especially in African\ncountries where they make up a majority of the workforce. Though many tools\nexist they are not designed to fit into the ways of working of SMB workers who\nare mobile first, have limited time to learn new workflows, and for whom social\nand business are tightly coupled. To address this, the Dukawalla prototype was\ncreated. This intelligent assistant bridges the gap between raw business data,\nand actionable insights by leveraging voice interaction and the power of\ngenerative AI. Dukawalla provides an intuitive way for business owners to\ninteract with their data, aiding in informed decision making. This paper\nexamines Dukawalla's deployment across SMBs in Nairobi, focusing on their\nexperiences using this voice based assistant to streamline data collection and\nprovide business insights", "AI": {"tldr": "\u6458\u8981\u603b\u7ed3\u4e86\u975e\u6d32\u4e2d\u5c0f\u4f01\u4e1a\u5728\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u4e0a\u7684\u56f0\u96be\uff0c\u5e76\u4ecb\u7ecd\u4e86Dukawalla\u8bed\u97f3\u52a9\u624b\u539f\u578b\u5982\u4f55\u901a\u8fc7\u751f\u6210\u5f0fAI\u548c\u8bed\u97f3\u4ea4\u4e92\u5e2e\u52a9\u8fd9\u4e9b\u4f01\u4e1a\u7b80\u5316\u6570\u636e\u5904\u7406\u548c\u83b7\u53d6\u5546\u4e1a\u6d1e\u5bdf\u3002", "motivation": "\u975e\u6d32\u4e2d\u5c0f\u4f01\u4e1a\u7531\u4e8e\u7f3a\u4e4f\u9002\u5408\u7684\u5de5\u5177\uff0c\u96be\u4ee5\u8fdb\u884c\u6570\u636e\u9a71\u52a8\u7684\u51b3\u7b56\uff0c\u5c24\u5176\u662f\u8fd9\u4e9b\u4f01\u4e1a\u5458\u5de5\u591a\u4e3a\u79fb\u52a8\u4f18\u5148\u3001\u65f6\u95f4\u6709\u9650\u4e14\u793e\u4ea4\u4e0e\u5546\u4e1a\u7d27\u5bc6\u5173\u8054\u7684\u4eba\u7fa4\u3002", "method": "\u5f00\u53d1\u4e86Dukawalla\u539f\u578b\uff0c\u4e00\u4e2a\u57fa\u4e8e\u8bed\u97f3\u4ea4\u4e92\u548c\u751f\u6210\u5f0fAI\u7684\u667a\u80fd\u52a9\u624b\uff0c\u65e8\u5728\u4e3a\u7528\u6237\u63d0\u4f9b\u76f4\u89c2\u7684\u6570\u636e\u4ea4\u4e92\u65b9\u5f0f\u3002", "result": "Dukawalla\u5728\u5185\u7f57\u6bd5\u7684\u4e2d\u5c0f\u4f01\u4e1a\u4e2d\u5f97\u5230\u90e8\u7f72\uff0c\u9a8c\u8bc1\u4e86\u5176\u7b80\u5316\u6570\u636e\u6536\u96c6\u548c\u63d0\u4f9b\u5546\u4e1a\u6d1e\u5bdf\u7684\u6709\u6548\u6027\u3002", "conclusion": "Dukawalla\u901a\u8fc7\u8bed\u97f3\u4ea4\u4e92\u548cAI\u6280\u672f\uff0c\u6210\u529f\u5e2e\u52a9\u975e\u6d32\u4e2d\u5c0f\u4f01\u4e1a\u63d0\u5347\u4e86\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u7684\u80fd\u529b\u3002"}}
{"id": "2505.04875", "pdf": "https://arxiv.org/pdf/2505.04875", "abs": "https://arxiv.org/abs/2505.04875", "authors": ["Conor Rowan", "Kurt Maute", "Alireza Doostan"], "title": "Physics-informed solution reconstruction in elasticity and heat transfer using the explicit constraint force method", "categories": ["cs.CE", "cs.LG"], "comment": null, "summary": "One use case of ``physics-informed neural networks'' (PINNs) is solution\nreconstruction, which aims to estimate the full-field state of a physical\nsystem from sparse measurements. Parameterized governing equations of the\nsystem are used in tandem with the measurements to regularize the regression\nproblem. However, in real-world solution reconstruction problems, the\nparameterized governing equation may be inconsistent with the physical\nphenomena that give rise to the measurement data. We show that due to assuming\nconsistency between the true and parameterized physics, PINNs-based approaches\nmay fail to satisfy three basic criteria of interpretability, robustness, and\ndata consistency. As we argue, these criteria ensure that (i) the quality of\nthe reconstruction can be assessed, (ii) the reconstruction does not depend\nstrongly on the choice of physics loss, and (iii) that in certain situations,\nthe physics parameters can be uniquely recovered. In the context of elasticity\nand heat transfer, we demonstrate how standard formulations of the physics loss\nand techniques for constraining the solution to respect the measurement data\nlead to different ``constraint forces\" -- which we define as additional source\nterms arising from the constraints -- and that these constraint forces can\nsignificantly influence the reconstructed solution. To avoid the potentially\nsubstantial influence of the choice of physics loss and method of constraint\nenforcement on the reconstructed solution, we propose the ``explicit constraint\nforce method'' (ECFM) to gain control of the source term introduced by the\nconstraint. We then show that by satisfying the criteria of interpretability,\nrobustness, and data consistency, this approach leads to more predictable and\ncustomizable reconstructions from noisy measurement data, even when the\nparameterization of the missing physics is inconsistent with the measured\nsystem.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u5728\u89e3\u51b3\u91cd\u5efa\u95ee\u9898\u65f6\u53ef\u80fd\u56e0\u7269\u7406\u6a21\u578b\u4e0e\u6570\u636e\u4e0d\u4e00\u81f4\u800c\u5bfc\u81f4\u7684\u4e09\u4e2a\u95ee\u9898\uff1a\u53ef\u89e3\u91ca\u6027\u3001\u9c81\u68d2\u6027\u548c\u6570\u636e\u4e00\u81f4\u6027\u4e0d\u8db3\u3002\u901a\u8fc7\u5f15\u5165\u201c\u663e\u5f0f\u7ea6\u675f\u529b\u65b9\u6cd5\u201d\uff08ECFM\uff09\uff0c\u6709\u6548\u63a7\u5236\u4e86\u7ea6\u675f\u5f15\u5165\u7684\u6e90\u9879\uff0c\u63d0\u5347\u4e86\u91cd\u5efa\u7684\u53ef\u9884\u6d4b\u6027\u548c\u53ef\u5b9a\u5236\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3PINNs\u5728\u91cd\u5efa\u95ee\u9898\u4e2d\u56e0\u7269\u7406\u6a21\u578b\u4e0e\u771f\u5b9e\u6570\u636e\u4e0d\u4e00\u81f4\u800c\u5bfc\u81f4\u7684\u4e0d\u53ef\u89e3\u91ca\u3001\u4e0d\u9c81\u68d2\u548c\u6570\u636e\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u201c\u663e\u5f0f\u7ea6\u675f\u529b\u65b9\u6cd5\u201d\uff08ECFM\uff09\uff0c\u901a\u8fc7\u663e\u5f0f\u63a7\u5236\u7ea6\u675f\u5f15\u5165\u7684\u6e90\u9879\u6765\u4f18\u5316\u91cd\u5efa\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eECFM\u5728\u5f39\u6027\u548c\u70ed\u4f20\u9012\u95ee\u9898\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7684\u53ef\u9884\u6d4b\u6027\u548c\u53ef\u5b9a\u5236\u6027\uff0c\u5373\u4f7f\u7269\u7406\u6a21\u578b\u4e0e\u6570\u636e\u4e0d\u4e00\u81f4\u3002", "conclusion": "ECFM\u901a\u8fc7\u6ee1\u8db3\u53ef\u89e3\u91ca\u6027\u3001\u9c81\u68d2\u6027\u548c\u6570\u636e\u4e00\u81f4\u6027\u7684\u6807\u51c6\uff0c\u4e3a\u590d\u6742\u7269\u7406\u7cfb\u7edf\u7684\u91cd\u5efa\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05189", "pdf": "https://arxiv.org/pdf/2505.05189", "abs": "https://arxiv.org/abs/2505.05189", "authors": ["Wei Peng", "Kang Liu", "Jianchen Hu", "Meng Zhang"], "title": "Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Prompt learning is one of the most effective paradigms for adapting\npre-trained vision-language models (VLMs) to the biomedical image\nclassification tasks in few shot scenarios. However, most of the current prompt\nlearning methods only used the text prompts and ignored the particular\nstructures (such as the complex anatomical structures and subtle pathological\nfeatures) in the biomedical images. In this work, we propose Biomed-DPT, a\nknowledge-enhanced dual modality prompt tuning technique. In designing the text\nprompt, Biomed-DPT constructs a dual prompt including the template-driven\nclinical prompts and the large language model (LLM)-driven domain-adapted\nprompts, then extracts the clinical knowledge from the domain-adapted prompts\nthrough the knowledge distillation technique. In designing the vision prompt,\nBiomed-DPT introduces the zero vector as a soft prompt to leverage attention\nre-weighting so that the focus on non-diagnostic regions and the recognition of\nnon-critical pathological features are avoided. Biomed-DPT achieves an average\nclassification accuracy of 66.14\\% across 11 biomedical image datasets covering\n9 modalities and 10 organs, with performance reaching 78.06\\% in base classes\nand 75.97\\% in novel classes, surpassing the Context Optimization (CoOp) method\nby 6.20\\%, 3.78\\%, and 8.04\\%, respectively. Our code are available at\n\\underline{https://github.com/Kanyooo/Biomed-DPT}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u77e5\u8bc6\u589e\u5f3a\u7684\u53cc\u6a21\u6001\u63d0\u793a\u8c03\u4f18\u6280\u672fBiomed-DPT\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u63d0\u793a\uff0c\u9488\u5bf9\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u63d0\u793a\uff0c\u800c\u5ffd\u7565\u4e86\u751f\u7269\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u7279\u6b8a\u7ed3\u6784\uff08\u5982\u590d\u6742\u7684\u89e3\u5256\u7ed3\u6784\u548c\u7ec6\u5fae\u7684\u75c5\u7406\u7279\u5f81\uff09\u3002", "method": "Biomed-DPT\u8bbe\u8ba1\u4e86\u53cc\u6a21\u6001\u63d0\u793a\uff1a1) \u6587\u672c\u63d0\u793a\u7ed3\u5408\u6a21\u677f\u9a71\u52a8\u7684\u4e34\u5e8a\u63d0\u793a\u548cLLM\u9a71\u52a8\u7684\u9886\u57df\u9002\u5e94\u63d0\u793a\uff1b2) \u89c6\u89c9\u63d0\u793a\u5f15\u5165\u96f6\u5411\u91cf\u4f5c\u4e3a\u8f6f\u63d0\u793a\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u91cd\u52a0\u6743\u907f\u514d\u5173\u6ce8\u975e\u8bca\u65ad\u533a\u57df\u548c\u975e\u5173\u952e\u75c5\u7406\u7279\u5f81\u3002", "result": "Biomed-DPT\u572811\u4e2a\u751f\u7269\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523066.14%\uff0c\u5728\u57fa\u7840\u7c7b\u548c\u65b0\u7c7b\u4e2d\u5206\u522b\u8fbe\u523078.06%\u548c75.97%\uff0c\u4f18\u4e8eCoOp\u65b9\u6cd56.20%\u30013.78%\u548c8.04%\u3002", "conclusion": "Biomed-DPT\u901a\u8fc7\u53cc\u6a21\u6001\u63d0\u793a\u548c\u77e5\u8bc6\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2505.04886", "pdf": "https://arxiv.org/pdf/2505.04886", "abs": "https://arxiv.org/abs/2505.04886", "authors": ["Mukund Telukunta", "Venkata Sriram Siddhardh Nadendla", "Morgan Stuart", "Casey Canfield"], "title": "Fairness Perceptions in Regression-based Predictive Models", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Regression-based predictive analytics used in modern kidney transplantation\nis known to inherit biases from training data. This leads to social\ndiscrimination and inefficient organ utilization, particularly in the context\nof a few social groups. Despite this concern, there is limited research on\nfairness in regression and its impact on organ utilization and placement. This\npaper introduces three novel divergence-based group fairness notions: (i)\nindependence, (ii) separation, and (iii) sufficiency to assess the fairness of\nregression-based analytics tools. In addition, fairness preferences are\ninvestigated from crowd feedback, in order to identify a socially accepted\ngroup fairness criterion for evaluating these tools. A total of 85 participants\nwere recruited from the Prolific crowdsourcing platform, and a Mixed-Logit\ndiscrete choice model was used to model fairness feedback and estimate social\nfairness preferences. The findings clearly depict a strong preference towards\nthe separation and sufficiency fairness notions, and that the predictive\nanalytics is deemed fair with respect to gender and race groups, but unfair in\nterms of age groups.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u65b0\u7684\u57fa\u4e8e\u5dee\u5f02\u7684\u7fa4\u4f53\u516c\u5e73\u6027\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u4f17\u5305\u53cd\u9988\u8c03\u67e5\u516c\u5e73\u6027\u504f\u597d\uff0c\u53d1\u73b0\u9884\u6d4b\u5206\u6790\u5728\u6027\u522b\u548c\u79cd\u65cf\u7fa4\u4f53\u4e2d\u88ab\u89c6\u4e3a\u516c\u5e73\uff0c\u4f46\u5728\u5e74\u9f84\u7fa4\u4f53\u4e2d\u4e0d\u516c\u5e73\u3002", "motivation": "\u56de\u5f52\u9884\u6d4b\u5206\u6790\u5728\u80be\u810f\u79fb\u690d\u4e2d\u7684\u5e94\u7528\u53ef\u80fd\u56e0\u8bad\u7ec3\u6570\u636e\u7684\u504f\u89c1\u5bfc\u81f4\u793e\u4f1a\u6b67\u89c6\u548c\u5668\u5b98\u5229\u7528\u6548\u7387\u4f4e\u4e0b\uff0c\u5c24\u5176\u5728\u90e8\u5206\u793e\u4f1a\u7fa4\u4f53\u4e2d\u3002\u76ee\u524d\u5173\u4e8e\u56de\u5f52\u516c\u5e73\u6027\u53ca\u5176\u5bf9\u5668\u5b98\u5229\u7528\u548c\u5206\u914d\u5f71\u54cd\u7684\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u4e09\u79cd\u65b0\u7684\u57fa\u4e8e\u5dee\u5f02\u7684\u7fa4\u4f53\u516c\u5e73\u6027\u6982\u5ff5\uff08\u72ec\u7acb\u6027\u3001\u5206\u79bb\u6027\u548c\u5145\u5206\u6027\uff09\uff0c\u5e76\u901a\u8fc7Prolific\u4f17\u5305\u5e73\u53f0\u62db\u52df85\u540d\u53c2\u4e0e\u8005\uff0c\u4f7f\u7528\u6df7\u5408Logit\u79bb\u6563\u9009\u62e9\u6a21\u578b\u5efa\u6a21\u516c\u5e73\u6027\u53cd\u9988\uff0c\u4f30\u8ba1\u793e\u4f1a\u516c\u5e73\u6027\u504f\u597d\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u660e\u786e\u663e\u793a\u53c2\u4e0e\u8005\u66f4\u503e\u5411\u4e8e\u5206\u79bb\u6027\u548c\u5145\u5206\u6027\u516c\u5e73\u6027\u6982\u5ff5\uff0c\u9884\u6d4b\u5206\u6790\u5728\u6027\u522b\u548c\u79cd\u65cf\u7fa4\u4f53\u4e2d\u88ab\u89c6\u4e3a\u516c\u5e73\uff0c\u4f46\u5728\u5e74\u9f84\u7fa4\u4f53\u4e2d\u4e0d\u516c\u5e73\u3002", "conclusion": "\u8bba\u6587\u4e3a\u56de\u5f52\u9884\u6d4b\u5206\u6790\u7684\u516c\u5e73\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u793e\u4f1a\u5bf9\u516c\u5e73\u6027\u504f\u597d\u7684\u5177\u4f53\u503e\u5411\uff0c\u6709\u52a9\u4e8e\u6539\u5584\u5668\u5b98\u79fb\u690d\u4e2d\u7684\u516c\u5e73\u6027\u3002"}}
{"id": "2505.04897", "pdf": "https://arxiv.org/pdf/2505.04897", "abs": "https://arxiv.org/abs/2505.04897", "authors": ["Taisuke Kobayashi"], "title": "CubeDAgger: Improved Robustness of Interactive Imitation Learning without Violation of Dynamic Stability", "categories": ["cs.RO", "cs.LG"], "comment": "7 pages, 4 figures", "summary": "Interactive imitation learning makes an agent's control policy robust by\nstepwise supervisions from an expert. The recent algorithms mostly employ\nexpert-agent switching systems to reduce the expert's burden by limitedly\nselecting the supervision timing. However, the precise selection is difficult\nand such a switching causes abrupt changes in actions, damaging the dynamic\nstability. This paper therefore proposes a novel method, so-called CubeDAgger,\nwhich improves robustness while reducing dynamic stability violations by making\nthree improvements to a baseline method, EnsembleDAgger. The first improvement\nadds a regularization to explicitly activate the threshold for deciding the\nsupervision timing. The second transforms the expert-agent switching system to\nan optimal consensus system of multiple action candidates. Third,\nautoregressive colored noise to the actions is introduced to make the\nstochastic exploration consistent over time. These improvements are verified by\nsimulations, showing that the learned policies are sufficiently robust while\nmaintaining dynamic stability during interaction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCubeDAgger\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdbEnsembleDAgger\uff0c\u589e\u5f3a\u4e86\u4ea4\u4e92\u6a21\u4eff\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u5e76\u51cf\u5c11\u52a8\u6001\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5177\u4f53\u5305\u62ec\u6fc0\u6d3b\u76d1\u7763\u65f6\u673a\u9608\u503c\u3001\u4f18\u5316\u52a8\u4f5c\u5019\u9009\u5171\u8bc6\u7cfb\u7edf\u548c\u5f15\u5165\u81ea\u56de\u5f52\u566a\u58f0\u3002", "motivation": "\u73b0\u6709\u4ea4\u4e92\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u901a\u8fc7\u4e13\u5bb6-\u4ee3\u7406\u5207\u6362\u7cfb\u7edf\u51cf\u5c11\u4e13\u5bb6\u8d1f\u62c5\uff0c\u4f46\u5207\u6362\u65f6\u673a\u9009\u62e9\u56f0\u96be\u4e14\u6613\u5bfc\u81f4\u52a8\u4f5c\u7a81\u53d8\uff0c\u635f\u5bb3\u52a8\u6001\u7a33\u5b9a\u6027\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u5728\u4fdd\u8bc1\u9c81\u68d2\u6027\u7684\u540c\u65f6\u51cf\u5c11\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "CubeDAgger\u6539\u8fdb\u4e86EnsembleDAgger\uff1a1) \u663e\u5f0f\u6fc0\u6d3b\u76d1\u7763\u65f6\u673a\u9608\u503c\uff1b2) \u5c06\u5207\u6362\u7cfb\u7edf\u8f6c\u53d8\u4e3a\u591a\u52a8\u4f5c\u5019\u9009\u7684\u6700\u4f18\u5171\u8bc6\u7cfb\u7edf\uff1b3) \u5f15\u5165\u81ea\u56de\u5f52\u566a\u58f0\u4fdd\u8bc1\u968f\u673a\u63a2\u7d22\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u8868\u660e\uff0cCubeDAgger\u5b66\u4e60\u5230\u7684\u7b56\u7565\u5728\u4ea4\u4e92\u4e2d\u65e2\u80fd\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u53c8\u80fd\u7ef4\u62a4\u52a8\u6001\u7a33\u5b9a\u6027\u3002", "conclusion": "CubeDAgger\u901a\u8fc7\u4e09\u9879\u6539\u8fdb\u6709\u6548\u5e73\u8861\u4e86\u4ea4\u4e92\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u9c81\u68d2\u6027\u4e0e\u52a8\u6001\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.05203", "pdf": "https://arxiv.org/pdf/2505.05203", "abs": "https://arxiv.org/abs/2505.05203", "authors": ["Wangkun Xu", "Zhongda Chu", "Fei Teng"], "title": "LAPSO: A Unified Optimization View for Learning-Augmented Power System Operations", "categories": ["eess.SY", "cs.AI", "cs.SY"], "comment": null, "summary": "With the high penetration of renewables, traditional model-based power system\noperation is challenged to deliver economic, stable, and robust decisions.\nMachine learning has emerged as a powerful modeling tool for capturing complex\ndynamics to address these challenges. However, its separate design often lacks\nsystematic integration with existing methods. To fill the gap, this paper\nproposes a holistic framework of Learning-Augmented Power System Operations\n(LAPSO, pronounced as Lap-So). Adopting a native optimization perspective,\nLAPSO is centered on the operation stage and aims to break the boundary between\ntemporally siloed power system tasks, such as forecast, operation and control,\nwhile unifying the objectives of machine learning and model-based optimizations\nat both training and inference stages. Systematic analysis and simulations\ndemonstrate the effectiveness of applying LAPSO in designing new integrated\nalgorithms, such as stability-constrained optimization (SCO) and\nobjective-based forecasting (OBF), while enabling end-to-end tracing of\ndifferent sources of uncertainties. In addition, a dedicated Python\npackage-lapso is introduced to automatically augment existing power system\noptimization models with learnable components. All code and data are available\nat https://github.com/xuwkk/lapso_exp.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLAPSO\u7684\u5b66\u4e60\u589e\u5f3a\u578b\u7535\u529b\u7cfb\u7edf\u64cd\u4f5c\u6846\u67b6\uff0c\u65e8\u5728\u6574\u5408\u673a\u5668\u5b66\u4e60\u548c\u4f20\u7edf\u6a21\u578b\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u9ad8\u6bd4\u4f8b\u53ef\u518d\u751f\u80fd\u6e90\u5e26\u6765\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u7edf\u4e00\u76ee\u6807\u548c\u6253\u7834\u4efb\u52a1\u58c1\u5792\u63d0\u5347\u7cfb\u7edf\u64cd\u4f5c\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\u7684\u9ad8\u6e17\u900f\u7387\uff0c\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u7535\u529b\u7cfb\u7edf\u64cd\u4f5c\u65b9\u6cd5\u5728\u7ecf\u6d4e\u6027\u3001\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002\u673a\u5668\u5b66\u4e60\u867d\u80fd\u6355\u6349\u590d\u6742\u52a8\u6001\uff0c\u4f46\u7f3a\u4e4f\u4e0e\u73b0\u6709\u65b9\u6cd5\u7684\u7cfb\u7edf\u6574\u5408\u3002", "method": "\u63d0\u51faLAPSO\u6846\u67b6\uff0c\u4ece\u4f18\u5316\u89d2\u5ea6\u51fa\u53d1\uff0c\u6574\u5408\u7535\u529b\u7cfb\u7edf\u7684\u9884\u6d4b\u3001\u64cd\u4f5c\u548c\u63a7\u5236\u4efb\u52a1\uff0c\u7edf\u4e00\u673a\u5668\u5b66\u4e60\u548c\u6a21\u578b\u4f18\u5316\u7684\u76ee\u6807\u3002\u901a\u8fc7\u7a33\u5b9a\u6027\u7ea6\u675f\u4f18\u5316\uff08SCO\uff09\u548c\u76ee\u6807\u5bfc\u5411\u9884\u6d4b\uff08OBF\uff09\u7b49\u65b0\u7b97\u6cd5\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u4e0d\u786e\u5b9a\u6027\u8ffd\u8e2a\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cLAPSO\u80fd\u6709\u6548\u63d0\u5347\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u589e\u5f3a\u7535\u529b\u7cfb\u7edf\u64cd\u4f5c\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002\u540c\u65f6\u53d1\u5e03\u4e86Python\u5de5\u5177\u5305lapso\uff0c\u652f\u6301\u73b0\u6709\u6a21\u578b\u7684\u53ef\u5b66\u4e60\u7ec4\u4ef6\u81ea\u52a8\u589e\u5f3a\u3002", "conclusion": "LAPSO\u4e3a\u7535\u529b\u7cfb\u7edf\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u4e0e\u4f18\u5316\u6574\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u76ee\u6807\u548c\u7aef\u5230\u7aef\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u5f00\u6e90\u5de5\u5177\u4ee5\u4fc3\u8fdb\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2505.04937", "pdf": "https://arxiv.org/pdf/2505.04937", "abs": "https://arxiv.org/abs/2505.04937", "authors": ["Nong Minh Hieu", "Antoine Ledent"], "title": "Generalization Analysis for Contrastive Representation Learning under Non-IID Settings", "categories": ["stat.ML", "cs.LG"], "comment": "To Appear in ICML, 2025", "summary": "Contrastive Representation Learning (CRL) has achieved impressive success in\nvarious domains in recent years. Nevertheless, the theoretical understanding of\nthe generalization behavior of CRL is limited. Moreover, to the best of our\nknowledge, the current literature only analyzes generalization bounds under the\nassumption that the data tuples used for contrastive learning are independently\nand identically distributed. However, in practice, we are often limited to a\nfixed pool of reusable labeled data points, making it inevitable to recycle\ndata across tuples to create sufficiently large datasets. Therefore, the\ntuple-wise independence condition imposed by previous works is invalidated. In\nthis paper, we provide a generalization analysis for the CRL framework under\nnon-$i.i.d.$ settings that adheres to practice more realistically. Drawing\ninspiration from the literature on U-statistics, we derive generalization\nbounds which indicate the required number of samples in each class scales as\nthe logarithm of the covering number of the class of learnable feature\nrepresentations associated to each class. Next, we apply our main results to\nderive excess risk bounds for common function classes such as linear maps and\nneural networks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\uff08CRL\uff09\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-i.i.d.\uff09\u6570\u636e\u4e0b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u586b\u8865\u4e86\u7406\u8bba\u7a7a\u767d\uff0c\u5e76\u4e3a\u5e38\u89c1\u51fd\u6570\u7c7b\uff08\u5982\u7ebf\u6027\u6620\u5c04\u548c\u795e\u7ecf\u7f51\u7edc\uff09\u63d0\u4f9b\u4e86\u8fc7\u98ce\u9669\u754c\u3002", "motivation": "\u5f53\u524dCRL\u7684\u6cdb\u5316\u7406\u8bba\u4ec5\u9488\u5bf9\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\uff0c\u4f46\u5b9e\u9645\u4e2d\u6570\u636e\u590d\u7528\u4e0d\u53ef\u907f\u514d\uff0c\u5bfc\u81f4\u72ec\u7acb\u6027\u5047\u8bbe\u5931\u6548\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u8131\u8282\u95ee\u9898\u3002", "method": "\u57fa\u4e8eU\u7edf\u8ba1\u91cf\u6587\u732e\uff0c\u63a8\u5bfc\u975ei.i.d.\u8bbe\u7f6e\u4e0b\u7684\u6cdb\u5316\u754c\uff0c\u91cd\u70b9\u5206\u6790\u6bcf\u7c7b\u6837\u672c\u6570\u4e0e\u7279\u5f81\u8868\u793a\u7c7b\u8986\u76d6\u6570\u7684\u5bf9\u6570\u5173\u7cfb\u3002", "result": "\u5f97\u51fa\u975ei.i.d.\u6570\u636e\u4e0b\u7684\u6cdb\u5316\u754c\uff0c\u5e76\u5c06\u7ed3\u679c\u5e94\u7528\u4e8e\u7ebf\u6027\u6620\u5c04\u548c\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed9\u51fa\u5176\u8fc7\u98ce\u9669\u754c\u7684\u5b9a\u91cf\u5206\u6790\u3002", "conclusion": "\u7814\u7a76\u4e3a\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u7684CRL\u6cdb\u5316\u7406\u8bba\u63d0\u4f9b\u4e86\u9996\u4e2a\u5b9e\u7528\u6846\u67b6\uff0c\u9a8c\u8bc1\u4e86\u6837\u672c\u590d\u6742\u5ea6\u4e0e\u7279\u5f81\u8868\u793a\u590d\u6742\u5ea6\u7684\u5bf9\u6570\u5173\u7cfb\u3002"}}
{"id": "2505.05211", "pdf": "https://arxiv.org/pdf/2505.05211", "abs": "https://arxiv.org/abs/2505.05211", "authors": ["Chara Podimata"], "title": "Incentive-Aware Machine Learning; Robustness, Fairness, Improvement & Causality", "categories": ["cs.GT", "cs.AI"], "comment": "This literature review was published in SIGEcom Exchanges in 2025", "summary": "The article explores the emerging domain of incentive-aware machine learning\n(ML), which focuses on algorithmic decision-making in contexts where\nindividuals can strategically modify their inputs to influence outcomes. It\ncategorizes the research into three perspectives: robustness, aiming to design\nmodels resilient to \"gaming\"; fairness, analyzing the societal impacts of such\nsystems; and improvement/causality, recognizing situations where strategic\nactions lead to genuine personal or societal improvement. The paper introduces\na unified framework encapsulating models for these perspectives, including\noffline, online, and causal settings, and highlights key challenges such as\ndifferentiating between gaming and improvement and addressing heterogeneity\namong agents. By synthesizing findings from diverse works, we outline\ntheoretical advancements and practical solutions for robust, fair, and\ncausally-informed incentive-aware ML systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u6fc0\u52b1\u611f\u77e5\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u9886\u57df\uff0c\u5206\u7c7b\u4e3a\u4e09\u4e2a\u89c6\u89d2\uff1a\u9c81\u68d2\u6027\u3001\u516c\u5e73\u6027\u548c\u6539\u8fdb/\u56e0\u679c\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u603b\u7ed3\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u8fdb\u5c55\u3002", "motivation": "\u7814\u7a76\u6fc0\u52b1\u611f\u77e5ML\u7684\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u4e2a\u4f53\u53ef\u80fd\u901a\u8fc7\u7b56\u7565\u6027\u4fee\u6539\u8f93\u5165\u6765\u5f71\u54cd\u7b97\u6cd5\u51b3\u7b56\u7684\u95ee\u9898\uff0c\u4ee5\u8bbe\u8ba1\u66f4\u5065\u58ee\u3001\u516c\u5e73\u4e14\u80fd\u4fc3\u8fdb\u771f\u6b63\u6539\u8fdb\u7684\u7cfb\u7edf\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5bf9\u73b0\u6709\u7814\u7a76\u7684\u5206\u7c7b\uff08\u9c81\u68d2\u6027\u3001\u516c\u5e73\u6027\u3001\u6539\u8fdb/\u56e0\u679c\u6027\uff09\u548c\u5206\u6790\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u6db5\u76d6\u79bb\u7ebf\u548c\u5728\u7ebf\u8bbe\u7f6e\u4ee5\u53ca\u56e0\u679c\u6a21\u578b\u3002", "result": "\u7814\u7a76\u603b\u7ed3\u4e86\u4e2a\u4f53\u7b56\u7565\u884c\u4e3a\u5728ML\u7cfb\u7edf\u4e2d\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u533a\u5206\u201c\u64cd\u63a7\u201d\u548c\u201c\u6539\u8fdb\u201d\u7684\u65b9\u6cd5\uff0c\u5e76\u5f3a\u8c03\u4e86\u4ee3\u7406\u5f02\u6784\u6027\u7684\u6311\u6218\u3002", "conclusion": "\u8bba\u6587\u4e3a\u6fc0\u52b1\u611f\u77e5ML\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u5728\u9c81\u68d2\u6027\u3001\u516c\u5e73\u6027\u548c\u56e0\u679c\u6027\u65b9\u9762\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u9700\u6c42\u3002"}}
{"id": "2505.04954", "pdf": "https://arxiv.org/pdf/2505.04954", "abs": "https://arxiv.org/abs/2505.04954", "authors": ["Lei Xin", "Baike She", "Qi Dou", "George Chiu", "Shreyas Sundaram"], "title": "Learning Linearized Models from Nonlinear Systems under Initialization Constraints with Finite Data", "categories": ["stat.ML", "cs.LG", "cs.SY", "eess.SY"], "comment": "12 pages, 5 figurues. arXiv admin note: substantial text overlap with\n  arXiv:2309.08805", "summary": "The identification of a linear system model from data has wide applications\nin control theory. The existing work that provides finite sample guarantees for\nlinear system identification typically uses data from a single long system\ntrajectory under i.i.d. random inputs, and assumes that the underlying dynamics\nis truly linear. In contrast, we consider the problem of identifying a\nlinearized model when the true underlying dynamics is nonlinear, given that\nthere is a certain constraint on the region where one can initialize the\nexperiments. We provide a multiple trajectories-based deterministic data\nacquisition algorithm followed by a regularized least squares algorithm, and\nprovide a finite sample error bound on the learned linearized dynamics. Our\nerror bound shows that one can consistently learn the linearized dynamics, and\ndemonstrates a trade-off between the error due to nonlinearity and the error\ndue to noise. We validate our results through numerical experiments, where we\nalso show the potential insufficiency of linear system identification using a\nsingle trajectory with i.i.d. random inputs, when nonlinearity does exist.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u6027\u5316\u6a21\u578b\u8bc6\u522b\u4e2d\u5904\u7406\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u8f68\u8ff9\u6570\u636e\u91c7\u96c6\u548c\u6b63\u5219\u5316\u6700\u5c0f\u4e8c\u4e58\u7b97\u6cd5\uff0c\u63d0\u4f9b\u4e86\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\u9650\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7ebf\u6027\u7cfb\u7edf\u8bc6\u522b\u5728\u5b58\u5728\u975e\u7ebf\u6027\u65f6\u7684\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u73b0\u6709\u7ebf\u6027\u7cfb\u7edf\u8bc6\u522b\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u52a8\u529b\u5b66\u662f\u4e25\u683c\u7ebf\u6027\u7684\uff0c\u4e14\u4f9d\u8d56\u5355\u4e00\u957f\u8f68\u8ff9\u6570\u636e\u3002\u672c\u6587\u5219\u63a2\u8ba8\u5728\u771f\u5b9e\u52a8\u529b\u5b66\u975e\u7ebf\u6027\u4e14\u5b9e\u9a8c\u521d\u59cb\u5316\u533a\u57df\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u6709\u6548\u8bc6\u522b\u7ebf\u6027\u5316\u6a21\u578b\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u591a\u8f68\u8ff9\u786e\u5b9a\u6027\u6570\u636e\u91c7\u96c6\u7b97\u6cd5\u548c\u6b63\u5219\u5316\u6700\u5c0f\u4e8c\u4e58\u7b97\u6cd5\uff0c\u7ed3\u5408\u6709\u9650\u6837\u672c\u8bef\u5dee\u5206\u6790\u3002", "result": "\u7ed3\u679c\u663e\u793a\u53ef\u4ee5\u4e00\u81f4\u5730\u5b66\u4e60\u7ebf\u6027\u5316\u52a8\u529b\u5b66\uff0c\u5e76\u5c55\u793a\u4e86\u975e\u7ebf\u6027\u548c\u566a\u58f0\u8bef\u5dee\u4e4b\u95f4\u7684\u6743\u8861\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u6307\u51fa\u5355\u4e00\u8f68\u8ff9i.i.d.\u968f\u673a\u8f93\u5165\u7684\u4e0d\u8db3\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u4e0b\u80fd\u6709\u6548\u8bc6\u522b\u7ebf\u6027\u5316\u6a21\u578b\uff0c\u4e14\u591a\u8f68\u8ff9\u6570\u636e\u91c7\u96c6\u4f18\u4e8e\u5355\u4e00\u8f68\u8ff9\u65b9\u6cd5\u3002"}}
{"id": "2505.04967", "pdf": "https://arxiv.org/pdf/2505.04967", "abs": "https://arxiv.org/abs/2505.04967", "authors": ["Li Ni", "Ziqi Deng", "Lin Mu", "Lei Zhang", "Wenjian Luo", "Yiwen Zhang"], "title": "Community and hyperedge inference in multiple hypergraphs", "categories": ["cs.SI", "cs.LG"], "comment": null, "summary": "Hypergraphs, capable of representing high-order interactions via hyperedges,\nhave become a powerful tool for modeling real-world biological and social\nsystems. Inherent relationships within these real-world systems, such as the\nencoding relationship between genes and their protein products, drive the\nestablishment of interconnections between multiple hypergraphs. Here, we\ndemonstrate how to utilize those interconnections between multiple hypergraphs\nto synthesize integrated information from multiple higher-order systems,\nthereby enhancing understanding of underlying structures. We propose a model\nbased on the stochastic block model, which integrates information from multiple\nhypergraphs to reveal latent high-order structures. Real-world hyperedges\nexhibit preferential attachment, where certain nodes dominate hyperedge\nformation. To characterize this phenomenon, our model introduces hyperedge\ninternal degree to quantify nodes' contributions to hyperedge formation. This\nmodel is capable of mining communities, predicting missing hyperedges of\narbitrary sizes within hypergraphs, and inferring inter-hypergraph edges\nbetween hypergraphs. We apply our model to high-order datasets to evaluate its\nperformance. Experimental results demonstrate strong performance of our model\nin community detection, hyperedge prediction, and inter-hypergraph edge\nprediction tasks. Moreover, we show that our model enables analysis of multiple\nhypergraphs of different types and supports the analysis of a single hypergraph\nin the absence of inter-hypergraph edges. Our work provides a practical and\nflexible tool for analyzing multiple hypergraphs, greatly advancing the\nunderstanding of the organization in real-world high-order systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u5757\u6a21\u578b\u7684\u8d85\u56fe\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u8d85\u56fe\u7684\u4fe1\u606f\u63ed\u793a\u6f5c\u5728\u9ad8\u9636\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u8d85\u8fb9\u5185\u90e8\u5ea6\u91cf\u5316\u8282\u70b9\u8d21\u732e\uff0c\u7528\u4e8e\u793e\u533a\u53d1\u73b0\u3001\u8d85\u8fb9\u9884\u6d4b\u53ca\u8d85\u56fe\u95f4\u8fb9\u7f18\u63a8\u65ad\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u751f\u7269\u548c\u793e\u4f1a\u7cfb\u7edf\u5b58\u5728\u590d\u6742\u7684\u9ad8\u9636\u4ea4\u4e92\u5173\u7cfb\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6574\u5408\u591a\u4e2a\u8d85\u56fe\u7684\u4fe1\u606f\u4ee5\u63ed\u793a\u5176\u6f5c\u5728\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u968f\u673a\u5757\u6a21\u578b\u7684\u6846\u67b6\uff0c\u5f15\u5165\u8d85\u8fb9\u5185\u90e8\u5ea6\u91cf\u5316\u8282\u70b9\u5bf9\u8d85\u8fb9\u5f62\u6210\u7684\u8d21\u732e\uff0c\u652f\u6301\u591a\u8d85\u56fe\u4fe1\u606f\u6574\u5408\u3001\u793e\u533a\u53d1\u73b0\u548c\u8d85\u8fb9\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u793e\u533a\u68c0\u6d4b\u3001\u8d85\u8fb9\u9884\u6d4b\u548c\u8d85\u56fe\u95f4\u8fb9\u7f18\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u5206\u6790\u4e0d\u540c\u7c7b\u578b\u8d85\u56fe\u6216\u5355\u8d85\u56fe\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5206\u6790\u591a\u8d85\u56fe\u63d0\u4f9b\u4e86\u7075\u6d3b\u5b9e\u7528\u7684\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u73b0\u5b9e\u9ad8\u9636\u7cfb\u7edf\u7ec4\u7ec7\u7684\u7406\u89e3\u3002"}}
{"id": "2505.05283", "pdf": "https://arxiv.org/pdf/2505.05283", "abs": "https://arxiv.org/abs/2505.05283", "authors": ["Kaixin Wang", "Tianlin Li", "Xiaoyu Zhang", "Chong Wang", "Weisong Sun", "Yang Liu", "Bin Shi"], "title": "Software Development Life Cycle Perspective: A Survey of Benchmarks for CodeLLMs and Agents", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Code large language models (CodeLLMs) and agents have shown great promise in\ntackling complex software engineering tasks.Compared to traditional software\nengineering methods, CodeLLMs and agents offer stronger abilities, and can\nflexibly process inputs and outputs in both natural and code. Benchmarking\nplays a crucial role in evaluating the capabilities of CodeLLMs and agents,\nguiding their development and deployment. However, despite their growing\nsignificance, there remains a lack of comprehensive reviews of benchmarks for\nCodeLLMs and agents. To bridge this gap, this paper provides a comprehensive\nreview of existing benchmarks for CodeLLMs and agents, studying and analyzing\n181 benchmarks from 461 relevant papers, covering the different phases of the\nsoftware development life cycle (SDLC). Our findings reveal a notable imbalance\nin the coverage of current benchmarks, with approximately 60% focused on the\nsoftware development phase in SDLC, while requirements engineering and software\ndesign phases receive minimal attention at only 5% and 3%, respectively.\nAdditionally, Python emerges as the dominant programming language across the\nreviewed benchmarks. Finally, this paper highlights the challenges of current\nresearch and proposes future directions, aiming to narrow the gap between the\ntheoretical capabilities of CodeLLMs and agents and their application in\nreal-world scenarios.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86181\u4e2a\u9488\u5bf9\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\uff08CodeLLMs\uff09\u548c\u667a\u80fd\u4f53\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6d4b\u8bd5\u5728\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\uff08SDLC\uff09\u4e2d\u5206\u5e03\u4e0d\u5747\uff0c\u4e3b\u8981\u96c6\u4e2d\u5f00\u53d1\u9636\u6bb5\uff0c\u800c\u5bf9\u9700\u6c42\u5de5\u7a0b\u548c\u8f6f\u4ef6\u8bbe\u8ba1\u9636\u6bb5\u5173\u6ce8\u8f83\u5c11\u3002\u6b64\u5916\uff0cPython\u662f\u4e3b\u8981\u7f16\u7a0b\u8bed\u8a00\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9CodeLLMs\u548c\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u5e76\u6307\u5bfc\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "method": "\u5206\u6790\u4e86461\u7bc7\u76f8\u5173\u8bba\u6587\u4e2d\u7684181\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d6SDLC\u4e0d\u540c\u9636\u6bb5\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u5728SDLC\u4e2d\u5206\u5e03\u4e0d\u5747\uff0c60%\u96c6\u4e2d\u5728\u5f00\u53d1\u9636\u6bb5\uff0c\u9700\u6c42\u5de5\u7a0b\u548c\u8f6f\u4ef6\u8bbe\u8ba1\u4ec5\u53605%\u548c3%\uff0c\u4e14Python\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002", "conclusion": "\u5f53\u524d\u7814\u7a76\u5b58\u5728\u4e0d\u5e73\u8861\uff0c\u672a\u6765\u9700\u7f29\u5c0f\u7406\u8bba\u80fd\u529b\u4e0e\u5b9e\u9645\u5e94\u7528\u7684\u5dee\u8ddd\u3002"}}
{"id": "2505.04986", "pdf": "https://arxiv.org/pdf/2505.04986", "abs": "https://arxiv.org/abs/2505.04986", "authors": ["Qian Peng", "Yajie Bao", "Haojie Ren", "Zhaojun Wang", "Changliang Zou"], "title": "Conformal Prediction with Cellwise Outliers: A Detect-then-Impute Approach", "categories": ["stat.ML", "cs.LG"], "comment": "23 pages, 15 figures", "summary": "Conformal prediction is a powerful tool for constructing prediction intervals\nfor black-box models, providing a finite sample coverage guarantee for\nexchangeable data. However, this exchangeability is compromised when some\nentries of the test feature are contaminated, such as in the case of cellwise\noutliers. To address this issue, this paper introduces a novel framework called\ndetect-then-impute conformal prediction. This framework first employs an\noutlier detection procedure on the test feature and then utilizes an imputation\nmethod to fill in those cells identified as outliers. To quantify the\nuncertainty in the processed test feature, we adaptively apply the detection\nand imputation procedures to the calibration set, thereby constructing\nexchangeable features for the conformal prediction interval of the test label.\nWe develop two practical algorithms, PDI-CP and JDI-CP, and provide a\ndistribution-free coverage analysis under some commonly used detection and\nimputation procedures. Notably, JDI-CP achieves a finite sample $1-2\\alpha$\ncoverage guarantee. Numerical experiments on both synthetic and real datasets\ndemonstrate that our proposed algorithms exhibit robust coverage properties and\ncomparable efficiency to the oracle baseline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6'\u68c0\u6d4b-\u586b\u8865\u4fdd\u5f62\u9884\u6d4b'\uff0c\u7528\u4e8e\u89e3\u51b3\u6d4b\u8bd5\u7279\u5f81\u4e2d\u6570\u636e\u6c61\u67d3\u5bfc\u81f4\u7684\u4ea4\u6362\u6027\u95ee\u9898\u3002\u901a\u8fc7\u7ed3\u5408\u68c0\u6d4b\u5f02\u5e38\u503c\u548c\u586b\u8865\u7f3a\u5931\u503c\u7684\u6b65\u9aa4\uff0c\u5e76\u57fa\u4e8e\u6821\u51c6\u96c6\u6784\u5efa\u9884\u6d4b\u533a\u95f4\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5728\u6d4b\u8bd5\u7279\u5f81\u53d7\u5230\u6c61\u67d3\uff08\u5982\u5355\u5143\u5f02\u5e38\u503c\uff09\u65f6\uff0c\u4fdd\u5f62\u9884\u6d4b\u7684\u4ea4\u6362\u6027\u5047\u8bbe\u88ab\u7834\u574f\uff0c\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u8bc1\u8986\u76d6\u7387\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u80fd\u81ea\u9002\u5e94\u5904\u7406\u6c61\u67d3\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa'\u68c0\u6d4b-\u586b\u8865\u4fdd\u5f62\u9884\u6d4b'\u6846\u67b6\uff1a\u5148\u68c0\u6d4b\u5f02\u5e38\u503c\uff0c\u518d\u586b\u8865\u7f3a\u5931\u503c\uff0c\u5e76\u901a\u8fc7\u6821\u51c6\u96c6\u751f\u6210\u4ea4\u6362\u6027\u7279\u5f81\u3002\u5f00\u53d1\u4e86PDI-CP\u548cJDI-CP\u4e24\u79cd\u7b97\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5206\u5e03\u65e0\u5173\u7684\u8986\u76d6\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cJDI-CP\u80fd\u5728\u6709\u9650\u6837\u672c\u4e0b\u5b9e\u73b01-2\u03b1\u7684\u8986\u76d6\u7387\u4fdd\u8bc1\uff0c\u4e14\u7b97\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u5747\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u4e0e\u57fa\u51c6\u65b9\u6cd5\u7684\u53ef\u6bd4\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6c61\u67d3\u6570\u636e\u4e0b\u7684\u9884\u6d4b\u533a\u95f4\u6784\u5efa\u95ee\u9898\uff0c\u4e3a\u9ed1\u76d2\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8986\u76d6\u7387\u4fdd\u8bc1\uff0c\u4e14\u5b9e\u7528\u6027\u5f3a\u3002"}}
{"id": "2505.05288", "pdf": "https://arxiv.org/pdf/2505.05288", "abs": "https://arxiv.org/abs/2505.05288", "authors": ["Ahmed Abdelreheem", "Filippo Aleotti", "Jamie Watson", "Zawar Qureshi", "Abdelrahman Eldesokey", "Peter Wonka", "Gabriel Brostow", "Sara Vicente", "Guillermo Garcia-Hernando"], "title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Tech report. Project page: https://nianticlabs.github.io/placeit3d/", "summary": "We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u2018\u8bed\u8a00\u5f15\u5bfc\u7684\u7269\u4f53\u5728\u771f\u5b9e3D\u573a\u666f\u4e2d\u7684\u653e\u7f6e\u2019\u65b0\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5173\u57fa\u51c6\u3001\u6570\u636e\u96c6\u548c\u521d\u6b65\u65b9\u6cd5\u3002", "motivation": "3D\u573a\u666f\u4e2d\u8bed\u8a00\u5f15\u5bfc\u7684\u4efb\u52a1\uff08\u5982\u7269\u4f53\u653e\u7f6e\uff09\u5177\u6709\u6a21\u7cca\u6027\u548c\u51e0\u4f55\u5173\u7cfb\u590d\u6742\u7684\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u63d0\u51fa\u65b0\u57fa\u51c6\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u6784\u5efa\u8bad\u7ec33D\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u9996\u4e2a\u975e\u5e73\u51e1\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "\u5efa\u7acb\u4e86\u65b0\u57fa\u51c6\u548c\u6570\u636e\u96c6\uff0c\u4e3a\u672a\u67653D\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "conclusion": "\u8be5\u4efb\u52a1\u548c\u57fa\u51c6\u6709\u671b\u6210\u4e3a\u8bc4\u4f30\u901a\u75283D\u5927\u8bed\u8a00\u6a21\u578b\u7684\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2505.04992", "pdf": "https://arxiv.org/pdf/2505.04992", "abs": "https://arxiv.org/abs/2505.04992", "authors": ["Jialong Jiang", "Wenkang Hu", "Jian Huang", "Yuling Jiao", "Xu Liu"], "title": "Boosting Statistic Learning with Synthetic Data from Pretrained Large Models", "categories": ["stat.ML", "cs.LG", "stat.AP"], "comment": null, "summary": "The rapid advancement of generative models, such as Stable Diffusion, raises\na key question: how can synthetic data from these models enhance predictive\nmodeling? While they can generate vast amounts of datasets, only a subset\nmeaningfully improves performance. We propose a novel end-to-end framework that\ngenerates and systematically filters synthetic data through domain-specific\nstatistical methods, selectively integrating high-quality samples for effective\naugmentation. Our experiments demonstrate consistent improvements in predictive\nperformance across various settings, highlighting the potential of our\nframework while underscoring the inherent limitations of generative models for\ndata augmentation. Despite the ability to produce large volumes of synthetic\ndata, the proportion that effectively improves model performance is limited.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u7edf\u8ba1\u65b9\u6cd5\u751f\u6210\u5e76\u7b5b\u9009\u5408\u6210\u6570\u636e\uff0c\u9009\u62e9\u6027\u6574\u5408\u9ad8\u8d28\u91cf\u6837\u672c\u4ee5\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6539\u8fdb\u9884\u6d4b\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u751f\u6210\u6a21\u578b\u5728\u6570\u636e\u589e\u5f3a\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76\u751f\u6210\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u4ea7\u751f\u7684\u5408\u6210\u6570\u636e\u5982\u4f55\u6709\u6548\u589e\u5f3a\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u5927\u91cf\u6570\u636e\u4f46\u4ec5\u6709\u90e8\u5206\u80fd\u771f\u6b63\u63d0\u5347\u6027\u80fd\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u7edf\u8ba1\u65b9\u6cd5\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e76\u7cfb\u7edf\u6027\u7b5b\u9009\u9ad8\u8d28\u91cf\u6837\u672c\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u573a\u666f\u4e2d\u5747\u80fd\u4e00\u81f4\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u663e\u793a\u751f\u6210\u6a21\u578b\u7684\u6570\u636e\u589e\u5f3a\u6548\u679c\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u6837\u672c\u7684\u6bd4\u4f8b\u3002", "conclusion": "\u5c3d\u7ba1\u751f\u6210\u6a21\u578b\u80fd\u751f\u6210\u5927\u91cf\u5408\u6210\u6570\u636e\uff0c\u4f46\u5b9e\u9645\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u6570\u636e\u6bd4\u4f8b\u6709\u9650\uff0c\u6846\u67b6\u7684\u9009\u62e9\u6027\u6574\u5408\u7b56\u7565\u662f\u5173\u952e\u3002"}}
{"id": "2505.05291", "pdf": "https://arxiv.org/pdf/2505.05291", "abs": "https://arxiv.org/abs/2505.05291", "authors": ["Benjamin A. Cohen", "Jonathan Fhima", "Meishar Meisel", "Baskin Meital", "Luis Filipe Nakayama", "Eran Berkowitz", "Joachim A. Behar"], "title": "Benchmarking Ophthalmology Foundation Models for Clinically Significant Age Macular Degeneration Detection", "categories": ["eess.IV", "cs.AI", "cs.CV", "q-bio.TO"], "comment": "10 pages, 3 figures", "summary": "Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to\nlearn robust representations from large-scale natural image datasets, enhancing\ntheir generalization across domains. In retinal imaging, foundation models\npretrained on either natural or ophthalmic data have shown promise, but the\nbenefits of in-domain pretraining remain uncertain. To investigate this, we\nbenchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets\ntotaling 70,000 expert-annotated images for the task of moderate-to-late\nage-related macular degeneration (AMD) identification. Our results show that\niBOT pretrained on natural images achieves the highest out-of-distribution\ngeneralization, with AUROCs of 0.80-0.97, outperforming domain-specific models,\nwhich achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining,\nwhich achieved AUROCs of 0.68-0.91. These findings highlight the value of\nfoundation models in improving AMD identification and challenge the assumption\nthat in-domain pretraining is necessary. Furthermore, we release BRAMD, an\nopen-access dataset (n=587) of DFIs with AMD labels from Brazil.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u516d\u79cd\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u5728\u4e03\u4e2a\u89c6\u7f51\u819c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u81ea\u7136\u56fe\u50cf\u9884\u8bad\u7ec3\u7684iBOT\u6a21\u578b\u5728\u8de8\u57df\u6cdb\u5316\u4e0a\u4f18\u4e8e\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff0c\u6311\u6218\u4e86\u9886\u57df\u5185\u9884\u8bad\u7ec3\u7684\u5fc5\u8981\u6027\u5047\u8bbe\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u5728\u89c6\u7f51\u819c\u56fe\u50cf\u9886\u57df\u4e2d\uff0c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff08SSL\uff09\u5bf9\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u7684\u6027\u80fd\u5f71\u54cd\uff0c\u7279\u522b\u662f\u9a8c\u8bc1\u9886\u57df\u5185\u9884\u8bad\u7ec3\u662f\u5426\u6bd4\u81ea\u7136\u56fe\u50cf\u9884\u8bad\u7ec3\u66f4\u6709\u6548\u3002", "method": "\u65b9\u6cd5\u662f\u5728\u4e03\u4e2a\u5171\u8ba170,000\u5f20\u4e13\u5bb6\u6807\u6ce8\u7684\u773c\u5e95\u56fe\u50cf\uff08DFI\uff09\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u516d\u79cdSSL\u9884\u8bad\u7ec3\u7684ViT\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4efb\u52a1\u662f\u8bc6\u522b\u4e2d\u665a\u671f\u5e74\u9f84\u76f8\u5173\u6027\u9ec4\u6591\u53d8\u6027\uff08AMD\uff09\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u81ea\u7136\u56fe\u50cf\u9884\u8bad\u7ec3\u7684iBOT\u6a21\u578b\u5728\u8de8\u57df\u6cdb\u5316\u4e0a\u8868\u73b0\u6700\u4f73\uff08AUROC\u4e3a0.80-0.97\uff09\uff0c\u4f18\u4e8e\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff08AUROC\u4e3a0.78-0.96\uff09\u548c\u672a\u9884\u8bad\u7ec3\u7684ViT-L\u57fa\u7ebf\uff08AUROC\u4e3a0.68-0.91\uff09\u3002", "conclusion": "\u7ed3\u8bba\u662f\u57fa\u7840\u6a21\u578b\uff08\u5982iBOT\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u5347AMD\u8bc6\u522b\u6027\u80fd\uff0c\u4e14\u81ea\u7136\u56fe\u50cf\u9884\u8bad\u7ec3\u7684\u6548\u679c\u4f18\u4e8e\u9886\u57df\u5185\u9884\u8bad\u7ec3\u3002\u8bba\u6587\u8fd8\u53d1\u5e03\u4e86\u5df4\u897fAMD\u6570\u636e\u96c6BRAMD\uff08n=587\uff09\u3002"}}
{"id": "2505.04999", "pdf": "https://arxiv.org/pdf/2505.04999", "abs": "https://arxiv.org/abs/2505.04999", "authors": ["Anthony Liang", "Pavel Czempin", "Matthew Hong", "Yutai Zhou", "Erdem Biyik", "Stephen Tu"], "title": "CLAM: Continuous Latent Action Models for Robot Learning from Unlabeled Demonstrations", "categories": ["cs.RO", "cs.LG"], "comment": "Latent Action Models, Self-supervised Pretraining, Learning from\n  Videos", "summary": "Learning robot policies using imitation learning requires collecting large\namounts of costly action-labeled expert demonstrations, which fundamentally\nlimits the scale of training data. A promising approach to address this\nbottleneck is to harness the abundance of unlabeled observations-e.g., from\nvideo demonstrations-to learn latent action labels in an unsupervised way.\nHowever, we find that existing methods struggle when applied to complex robot\ntasks requiring fine-grained motions. We design continuous latent action models\n(CLAM) which incorporate two key ingredients we find necessary for learning to\nsolve complex continuous control tasks from unlabeled observation data: (a)\nusing continuous latent action labels instead of discrete representations, and\n(b) jointly training an action decoder to ensure that the latent action space\ncan be easily grounded to real actions with relatively few labeled examples.\nImportantly, the labeled examples can be collected from non-optimal play data,\nenabling CLAM to learn performant policies without access to any action-labeled\nexpert data. We demonstrate on continuous control benchmarks in DMControl\n(locomotion) and MetaWorld (manipulation), as well as on a real WidowX robot\narm that CLAM significantly outperforms prior state-of-the-art methods,\nremarkably with a 2-3x improvement in task success rate compared to the best\nbaseline. Videos and code can be found at clamrobot.github.io.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCLAM\u7684\u8fde\u7eed\u6f5c\u5728\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u5229\u7528\u672a\u6807\u6ce8\u89c2\u5bdf\u6570\u636e\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\u6807\u7b7e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u65e0\u9700\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u4e13\u5bb6\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u4e13\u5bb6\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u8bba\u6587\u65e8\u5728\u5229\u7528\u4e30\u5bcc\u7684\u672a\u6807\u6ce8\u89c2\u5bdf\u6570\u636e\uff08\u5982\u89c6\u9891\u6f14\u793a\uff09\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7cbe\u7ec6\u52a8\u4f5c\u7684\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u8fde\u7eed\u6f5c\u5728\u52a8\u4f5c\u6a21\u578b\uff08CLAM\uff09\uff0c\u5176\u6838\u5fc3\u7279\u70b9\u662f\uff1a\uff08a\uff09\u4f7f\u7528\u8fde\u7eed\u6f5c\u5728\u52a8\u4f5c\u6807\u7b7e\u800c\u975e\u79bb\u6563\u8868\u793a\uff0c\uff08b\uff09\u8054\u5408\u8bad\u7ec3\u52a8\u4f5c\u89e3\u7801\u5668\uff0c\u4f7f\u6f5c\u5728\u52a8\u4f5c\u7a7a\u95f4\u80fd\u8f7b\u677e\u6620\u5c04\u5230\u771f\u5b9e\u52a8\u4f5c\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\u6570\u636e\uff08\u751a\u81f3\u975e\u6700\u4f18\u6570\u636e\uff09\u5373\u53ef\u5b66\u4e60\u9ad8\u6027\u80fd\u7b56\u7565\u3002", "result": "\u5728DMControl\uff08\u8fd0\u52a8\u63a7\u5236\uff09\u548cMetaWorld\uff08\u64cd\u4f5c\u4efb\u52a1\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ee5\u53ca\u771f\u5b9e\u7684WidowX\u673a\u68b0\u81c2\u4e0a\uff0cCLAM\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e862-3\u500d\u3002", "conclusion": "CLAM\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\u6807\u7b7e\u548c\u8054\u5408\u8bad\u7ec3\u89e3\u7801\u5668\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u590d\u6742\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u7684\u6570\u636e\u74f6\u9888\u95ee\u9898\uff0c\u4e14\u65e0\u9700\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\uff0c\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.05004", "pdf": "https://arxiv.org/pdf/2505.05004", "abs": "https://arxiv.org/abs/2505.05004", "authors": ["Hendrik M\u00f6ller", "Hanna Sch\u00f6n", "Alina Dima", "Benjamin Keinert-Weth", "Robert Graf", "Matan Atad", "Johannes Paetzold", "Friederike Jungmann", "Rickmer Braren", "Florian Kofler", "Bjoern Menze", "Daniel Rueckert", "Jan S. Kirschke"], "title": "Automated Thoracolumbar Stump Rib Detection and Analysis in a Large CT Cohort", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Thoracolumbar stump ribs are one of the essential indicators of thoracolumbar\ntransitional vertebrae or enumeration anomalies. While some studies manually\nassess these anomalies and describe the ribs qualitatively, this study aims to\nautomate thoracolumbar stump rib detection and analyze their morphology\nquantitatively. To this end, we train a high-resolution deep-learning model for\nrib segmentation and show significant improvements compared to existing models\n(Dice score 0.997 vs. 0.779, p-value < 0.01). In addition, we use an iterative\nalgorithm and piece-wise linear interpolation to assess the length of the ribs,\nshowing a success rate of 98.2%. When analyzing morphological features, we show\nthat stump ribs articulate more posteriorly at the vertebrae (-19.2 +- 3.8 vs\n-13.8 +- 2.5, p-value < 0.01), are thinner (260.6 +- 103.4 vs. 563.6 +- 127.1,\np-value < 0.01), and are oriented more downwards and sideways within the first\ncentimeters in contrast to full-length ribs. We show that with partially\nvisible ribs, these features can achieve an F1-score of 0.84 in differentiating\nstump ribs from regular ones. We publish the model weights and masks for public\nuse.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u68c0\u6d4b\u80f8\u8170\u690e\u6b8b\u7aef\u808b\u9aa8\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u5f62\u6001\u5b66\u5b9a\u91cf\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u6548\u679c\u3002", "motivation": "\u80f8\u8170\u690e\u6b8b\u7aef\u808b\u9aa8\u662f\u80f8\u8170\u690e\u8fc7\u6e21\u690e\u6216\u8ba1\u6570\u5f02\u5e38\u7684\u91cd\u8981\u6307\u6807\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u5b9a\u6027\u8bc4\u4f30\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5b9e\u73b0\u81ea\u52a8\u5316\u68c0\u6d4b\u5e76\u63d0\u4f9b\u5b9a\u91cf\u5f62\u6001\u5206\u6790\u3002", "method": "\u91c7\u7528\u9ad8\u5206\u8fa8\u7387\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u808b\u9aa8\u5206\u5272\uff0c\u5e76\u7ed3\u5408\u8fed\u4ee3\u7b97\u6cd5\u548c\u5206\u6bb5\u7ebf\u6027\u63d2\u503c\u8bc4\u4f30\u808b\u9aa8\u957f\u5ea6\uff0c\u540c\u65f6\u5206\u6790\u5f62\u6001\u7279\u5f81\u5982\u4f4d\u7f6e\u3001\u539a\u5ea6\u548c\u65b9\u5411\u3002", "result": "\u6a21\u578b\u5206\u5272\u6548\u679c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08Dice\u5f97\u52060.997 vs. 0.779\uff09\uff0c\u808b\u9aa8\u957f\u5ea6\u8bc4\u4f30\u6210\u529f\u7387\u8fbe98.2%\uff0c\u5f62\u6001\u7279\u5f81\u5206\u6790\u663e\u793a\u6b8b\u7aef\u808b\u9aa8\u5728\u4f4d\u7f6e\u3001\u539a\u5ea6\u548c\u65b9\u5411\u4e0a\u4e0e\u5b8c\u6574\u808b\u9aa8\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff08p<0.01\uff09\uff0c\u533a\u5206\u6b8b\u7aef\u808b\u9aa8\u7684F1-score\u4e3a0.84\u3002", "conclusion": "\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u80f8\u8170\u690e\u6b8b\u7aef\u808b\u9aa8\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u548c\u5b9a\u91cf\u5206\u6790\uff0c\u6a21\u578b\u6743\u91cd\u548c\u63a9\u7801\u5df2\u516c\u5f00\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2505.05318", "pdf": "https://arxiv.org/pdf/2505.05318", "abs": "https://arxiv.org/abs/2505.05318", "authors": ["Agnese Chiatti", "Sara Bernardini", "Lara Shibelski Godoy Piccolo", "Viola Schiaffonati", "Matteo Matteucci"], "title": "Mapping User Trust in Vision Language Models: Research Landscape, Challenges, and Prospects", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.HC", "cs.RO"], "comment": null, "summary": "The rapid adoption of Vision Language Models (VLMs), pre-trained on large\nimage-text and video-text datasets, calls for protecting and informing users\nabout when to trust these systems. This survey reviews studies on trust\ndynamics in user-VLM interactions, through a multi-disciplinary taxonomy\nencompassing different cognitive science capabilities, collaboration modes, and\nagent behaviours. Literature insights and findings from a workshop with\nprospective VLM users inform preliminary requirements for future VLM trust\nstudies.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u5173\u4e8e\u7528\u6237\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e92\u52a8\u4e2d\u4fe1\u4efb\u52a8\u6001\u7684\u7814\u7a76\uff0c\u901a\u8fc7\u591a\u5b66\u79d1\u5206\u7c7b\u6cd5\u603b\u7ed3\u4e86\u8ba4\u77e5\u79d1\u5b66\u80fd\u529b\u3001\u534f\u4f5c\u6a21\u5f0f\u548c\u884c\u4e3a\uff0c\u5e76\u4e3a\u672a\u6765VLM\u4fe1\u4efb\u7814\u7a76\u63d0\u51fa\u4e86\u521d\u6b65\u9700\u6c42\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u5e94\u7528\uff0c\u4fdd\u62a4\u7528\u6237\u5e76\u544a\u77e5\u4f55\u65f6\u4fe1\u4efb\u8fd9\u4e9b\u7cfb\u7edf\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u4e0e\u6f5c\u5728VLM\u7528\u6237\u7684\u7814\u8ba8\u4f1a\uff0c\u5206\u6790\u4e86\u4fe1\u4efb\u52a8\u6001\u7684\u591a\u5b66\u79d1\u5206\u7c7b\u6cd5\u3002", "result": "\u603b\u7ed3\u4e86\u8ba4\u77e5\u79d1\u5b66\u80fd\u529b\u3001\u534f\u4f5c\u6a21\u5f0f\u548c\u884c\u4e3a\u7684\u591a\u5b66\u79d1\u89c1\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u521d\u6b65\u9700\u6c42\u3002", "conclusion": "\u4e3a\u672a\u6765VLM\u4fe1\u4efb\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u591a\u5b66\u79d1\u5206\u7c7b\u6cd5\u548c\u7528\u6237\u9700\u6c42\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.05321", "pdf": "https://arxiv.org/pdf/2505.05321", "abs": "https://arxiv.org/abs/2505.05321", "authors": ["Chintan B. Maniyar", "Minakshi Kumar", "Gengchen Mai"], "title": "Feature-Augmented Deep Networks for Multiscale Building Segmentation in High-Resolution UAV and Satellite Imagery", "categories": ["cs.CV", "cs.AI", "I.4.6; I.4.10; I.5.1; I.2.10"], "comment": "in preparation for journal submission, 25 pages, 11 figures", "summary": "Accurate building segmentation from high-resolution RGB imagery remains\nchallenging due to spectral similarity with non-building features, shadows, and\nirregular building geometries. In this study, we present a comprehensive deep\nlearning framework for multiscale building segmentation using RGB aerial and\nsatellite imagery with spatial resolutions ranging from 0.4m to 2.7m. We curate\na diverse, multi-sensor dataset and introduce feature-augmented inputs by\nderiving secondary representations including Principal Component Analysis\n(PCA), Visible Difference Vegetation Index (VDVI), Morphological Building Index\n(MBI), and Sobel edge filters from RGB channels. These features guide a\nRes-U-Net architecture in learning complex spatial patterns more effectively.\nWe also propose training policies incorporating layer freezing, cyclical\nlearning rates, and SuperConvergence to reduce training time and resource\nusage. Evaluated on a held-out WorldView-3 image, our model achieves an overall\naccuracy of 96.5%, an F1-score of 0.86, and an Intersection over Union (IoU) of\n0.80, outperforming existing RGB-based benchmarks. This study demonstrates the\neffectiveness of combining multi-resolution imagery, feature augmentation, and\noptimized training strategies for robust building segmentation in remote\nsensing applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u5c3a\u5ea6RGB\u5f71\u50cf\u3001\u7279\u5f81\u589e\u5f3a\u548c\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u7b51\u5206\u5272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387RGB\u5f71\u50cf\u4e2d\u7684\u5efa\u7b51\u5206\u5272\u56e0\u5149\u8c31\u76f8\u4f3c\u6027\u3001\u9634\u5f71\u548c\u4e0d\u89c4\u5219\u51e0\u4f55\u5f62\u72b6\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528Res-U-Net\u67b6\u6784\uff0c\u7ed3\u5408PCA\u3001VDVI\u3001MBI\u548cSobel\u8fb9\u7f18\u6ee4\u6ce2\u5668\u7b49\u7279\u5f81\u589e\u5f3a\u8f93\u5165\uff0c\u5e76\u5e94\u7528\u5c42\u51bb\u7ed3\u3001\u5faa\u73af\u5b66\u4e60\u7387\u548c\u8d85\u7ea7\u6536\u655b\u7b49\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u6a21\u578b\u5728WorldView-3\u5f71\u50cf\u6d4b\u8bd5\u4e2d\u8fbe\u523096.5%\u7684\u603b\u4f53\u51c6\u786e\u7387\u30010.86\u7684F1\u5206\u6570\u548c0.80\u7684IoU\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u8bc1\u660e\u4e86\u591a\u5206\u8fa8\u7387\u5f71\u50cf\u3001\u7279\u5f81\u589e\u5f3a\u548c\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\u5728\u9065\u611f\u5efa\u7b51\u5206\u5272\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.05354", "pdf": "https://arxiv.org/pdf/2505.05354", "abs": "https://arxiv.org/abs/2505.05354", "authors": ["Pungponhavoan Tep", "Marc Bernacki"], "title": "High-fidelity Grain Growth Modeling: Leveraging Deep Learning for Fast Computations", "categories": ["cond-mat.mtrl-sci", "cs.AI"], "comment": null, "summary": "Grain growth simulation is crucial for predicting metallic material\nmicrostructure evolution during annealing and resulting final mechanical\nproperties, but traditional partial differential equation-based methods are\ncomputationally expensive, creating bottlenecks in materials design and\nmanufacturing. In this work, we introduce a machine learning framework that\ncombines a Convolutional Long Short-Term Memory networks with an Autoencoder to\nefficiently predict grain growth evolution. Our approach captures both spatial\nand temporal aspects of grain evolution while encoding high-dimensional grain\nstructure data into a compact latent space for pattern learning, enhanced by a\nnovel composite loss function combining Mean Squared Error, Structural\nSimilarity Index Measurement, and Boundary Preservation to maintain structural\nintegrity of grain boundary topology of the prediction. Results demonstrated\nthat our machine learning approach accelerates grain growth prediction by up to\n\\SI{89}{\\times} faster, reducing computation time from \\SI{10}{\\minute} to\napproximately \\SI{10}{\\second} while maintaining high-fidelity predictions. The\nbest model (S-30-30) achieving a structural similarity score of\n\\SI{86.71}{\\percent} and mean grain size error of just \\SI{0.07}{\\percent}. All\nmodels accurately captured grain boundary topology, morphology, and size\ndistributions. This approach enables rapid microstructural prediction for\napplications where conventional simulations are prohibitively time-consuming,\npotentially accelerating innovation in materials science and manufacturing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5377\u79ef\u957f\u77ed\u65f6\u8bb0\u5fc6\u7f51\u7edc\u4e0e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u9884\u6d4b\u91d1\u5c5e\u6750\u6599\u6676\u7c92\u751f\u957f\u6f14\u5316\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u534789\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u6676\u7c92\u751f\u957f\u6a21\u62df\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u6750\u6599\u8bbe\u8ba1\u4e0e\u5236\u9020\u6548\u7387\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5377\u79efLSTM\u7f51\u7edc\u4e0e\u81ea\u52a8\u7f16\u7801\u5668\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u590d\u5408\u635f\u5931\u51fd\u6570\uff08MSE\u3001SSIM\u548c\u8fb9\u754c\u4fdd\u6301\uff09\u6355\u83b7\u65f6\u7a7a\u6f14\u5316\u7279\u5f81\u5e76\u538b\u7f29\u6570\u636e\u7ef4\u5ea6\u3002", "result": "\u6a21\u578b\uff08S-30-30\uff09\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u4e8689\u500d\uff08\u4ece10\u5206\u949f\u964d\u81f310\u79d2\uff09\uff0c\u7ed3\u6784\u76f8\u4f3c\u6027\u8fbe86.71%\uff0c\u5e73\u5747\u6676\u7c92\u5c3a\u5bf8\u8bef\u5dee\u4ec50.07%\uff0c\u5b8c\u7f8e\u4fdd\u7559\u4e86\u6676\u754c\u62d3\u6251\u548c\u5f62\u6001\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u52a0\u901f\u4e86\u5fae\u89c2\u7ed3\u6784\u9884\u6d4b\uff0c\u4e3a\u4f20\u7edf\u65b9\u6cd5\u8017\u65f6\u8fc7\u957f\u7684\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u6750\u6599\u79d1\u5b66\u7684\u521b\u65b0\u3002"}}
{"id": "2505.05085", "pdf": "https://arxiv.org/pdf/2505.05085", "abs": "https://arxiv.org/abs/2505.05085", "authors": ["Gary Froyland", "Kevin K\u00fchl"], "title": "Learning dynamically inspired invariant subspaces for Koopman and transfer operator approximation", "categories": ["math.DS", "cs.LG", "cs.NA", "math.NA", "47A15, 37C30, 47A58, 68T07"], "comment": "20 pages, 13 figures", "summary": "Transfer and Koopman operator methods offer a framework for representing\ncomplex, nonlinear dynamical systems via linear transformations, enabling for a\ndeeper understanding of the underlying dynamics. The spectrum of these\noperators provide important insights into system predictability and emergent\nbehaviour, although efficiently estimating them from data can be challenging.\nWe tackle this issue through the lens of general operator and representational\nlearning, in which we approximate these linear operators using efficient\nfinite-dimensional representations. Specifically, we machine-learn orthonormal,\nlocally supported basis functions that are dynamically tailored to the system.\nThis learned basis provides a particularly accurate approximation of the\noperator's action as well as a nearly invariant finite-dimensional subspace. We\nillustrate our approach with examples that showcase the retrieval of spectral\nproperties from the estimated operator, and emphasise the dynamically adaptive\nquality of the machine-learned basis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5b66\u4e60\u6b63\u4ea4\u3001\u5c40\u90e8\u652f\u6301\u7684\u57fa\u51fd\u6570\u6765\u8fd1\u4f3c\u7ebf\u6027\u7b97\u5b50\uff08\u5982\u8f6c\u79fb\u548cKoopman\u7b97\u5b50\uff09\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u590d\u6742\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7684\u9884\u6d4b\u548c\u5206\u6790\u80fd\u529b\u3002", "motivation": "\u8f6c\u79fb\u548cKoopman\u7b97\u5b50\u65b9\u6cd5\u4e3a\u590d\u6742\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7ebf\u6027\u8868\u793a\u6846\u67b6\uff0c\u4f46\u5176\u8c31\u7684\u9ad8\u6548\u4f30\u8ba1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u548c\u4f18\u5316\u7684\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5b66\u4e60\u6b63\u4ea4\u4e14\u5c40\u90e8\u652f\u6301\u7684\u57fa\u51fd\u6570\uff0c\u8fd9\u4e9b\u57fa\u51fd\u6570\u52a8\u6001\u9002\u5e94\u7cfb\u7edf\u7279\u6027\uff0c\u4ece\u800c\u63d0\u4f9b\u5bf9\u7ebf\u6027\u7b97\u5b50\u884c\u4e3a\u7684\u51c6\u786e\u8fd1\u4f3c\u548c\u8fd1\u4f3c\u4e0d\u53d8\u7684\u6709\u9650\u7ef4\u5b50\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6062\u590d\u7b97\u5b50\u7684\u8c31\u7279\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5b66\u4e60\u57fa\u51fd\u6570\u7684\u52a8\u6001\u9002\u5e94\u6027\u3002", "conclusion": "\u901a\u8fc7\u5b66\u4e60\u7684\u57fa\u51fd\u6570\u53ef\u4ee5\u6709\u6548\u8fd1\u4f3c\u7ebf\u6027\u7b97\u5b50\uff0c\u4e3a\u590d\u6742\u52a8\u529b\u7cfb\u7edf\u7684\u5206\u6790\u548c\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002"}}
{"id": "2505.05356", "pdf": "https://arxiv.org/pdf/2505.05356", "abs": "https://arxiv.org/abs/2505.05356", "authors": ["Runfeng Li", "Mikhail Okunev", "Zixuan Guo", "Anh Ha Duong", "Christian Richardt", "Matthew O'Toole", "James Tompkin"], "title": "Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "We present a method to reconstruct dynamic scenes from monocular\ncontinuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that\nachieves similar or better accuracy than neural volumetric approaches and is\n100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a\nsingle viewpoint is a significant challenge in computer vision. In C-ToF\nradiance field reconstruction, the property of interest-depth-is not directly\nmeasured, causing an additional challenge. This problem has a large and\nunderappreciated impact upon the optimization when using a fast primitive-based\nscene representation like 3D Gaussian splatting, which is commonly used with\nmulti-view data to produce satisfactory results and is brittle in its\noptimization otherwise. We incorporate two heuristics into the optimization to\nimprove the accuracy of scene geometry represented by Gaussians. Experimental\nresults show that our approach produces accurate reconstructions under\nconstrained C-ToF sensing conditions, including for fast motions like swinging\nbaseball bats. https://visual.cs.brown.edu/gftorf", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u76ee\u8fde\u7eed\u6ce2\u98de\u884c\u65f6\u95f4\uff08C-ToF\uff09\u76f8\u673a\u7684\u539f\u59cb\u4f20\u611f\u5668\u6837\u672c\u4e2d\u91cd\u5efa\u52a8\u6001\u573a\u666f\u7684\u65b9\u6cd5\uff0c\u5176\u7cbe\u5ea6\u4e0e\u795e\u7ecf\u4f53\u79ef\u65b9\u6cd5\u76f8\u4f3c\u6216\u66f4\u597d\uff0c\u4e14\u901f\u5ea6\u63d0\u9ad8100\u500d\u3002", "motivation": "\u4ece\u5355\u4e00\u89c6\u89d2\u5feb\u901f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u52a8\u60013D\u91cd\u5efa\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002\u5728C-ToF\u8f90\u5c04\u573a\u91cd\u5efa\u4e2d\uff0c\u6df1\u5ea6\u8fd9\u4e00\u5173\u952e\u5c5e\u6027\u5e76\u975e\u76f4\u63a5\u6d4b\u91cf\uff0c\u8fd9\u589e\u52a0\u4e86\u4f18\u5316\u96be\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u4f7f\u7528\u57fa\u4e8e\u5feb\u901f\u57fa\u5143\u7684\u573a\u666f\u8868\u793a\uff08\u59823D\u9ad8\u65af\u629b\u96ea\u7403\uff09\u65f6\u3002", "method": "\u6211\u4eec\u5728\u4f18\u5316\u4e2d\u5f15\u5165\u4e86\u4e24\u79cd\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u9ad8\u65af\u8868\u793a\u7684\u573a\u666f\u51e0\u4f55\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u53d7\u9650\u7684C-ToF\u4f20\u611f\u6761\u4ef6\u4e0b\uff08\u5305\u62ec\u5feb\u901f\u8fd0\u52a8\u5982\u6325\u68d2\uff09\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u4ea7\u751f\u51c6\u786e\u7684\u91cd\u5efa\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u6027\uff0c\u4e3a\u5355\u89c6\u89d2C-ToF\u6570\u636e\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.05091", "pdf": "https://arxiv.org/pdf/2505.05091", "abs": "https://arxiv.org/abs/2505.05091", "authors": ["Shashank Agnihotri", "Amaan Ansari", "Annika Dackermann", "Fabian R\u00f6sch", "Margret Keuper"], "title": "DispBench: Benchmarking Disparity Estimation to Synthetic Corruptions", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at CVPR 2025 Workshop on Synthetic Data for Computer Vision", "summary": "Deep learning (DL) has surpassed human performance on standard benchmarks,\ndriving its widespread adoption in computer vision tasks. One such task is\ndisparity estimation, estimating the disparity between matching pixels in\nstereo image pairs, which is crucial for safety-critical applications like\nmedical surgeries and autonomous navigation. However, DL-based disparity\nestimation methods are highly susceptible to distribution shifts and\nadversarial attacks, raising concerns about their reliability and\ngeneralization. Despite these concerns, a standardized benchmark for evaluating\nthe robustness of disparity estimation methods remains absent, hindering\nprogress in the field.\n  To address this gap, we introduce DispBench, a comprehensive benchmarking\ntool for systematically assessing the reliability of disparity estimation\nmethods. DispBench evaluates robustness against synthetic image corruptions\nsuch as adversarial attacks and out-of-distribution shifts caused by 2D Common\nCorruptions across multiple datasets and diverse corruption scenarios. We\nconduct the most extensive performance and robustness analysis of disparity\nestimation methods to date, uncovering key correlations between accuracy,\nreliability, and generalization. Open-source code for DispBench:\nhttps://github.com/shashankskagnihotri/benchmarking_robustness/tree/disparity_estimation/final/disparity_estimation", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DispBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u5dee\u4f30\u8ba1\u65b9\u6cd5\u9c81\u68d2\u6027\u7684\u6807\u51c6\u5316\u57fa\u51c6\u5de5\u5177\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u89c6\u5dee\u4f30\u8ba1\u65b9\u6cd5\u5bf9\u5206\u5e03\u504f\u79fb\u548c\u5bf9\u6297\u653b\u51fb\u9ad8\u5ea6\u654f\u611f\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u5176\u9c81\u68d2\u6027\uff0c\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3002", "method": "DispBench\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u89c6\u5dee\u4f30\u8ba1\u65b9\u6cd5\u5728\u5bf9\u6297\u653b\u51fb\u548c\u5206\u5e03\u504f\u79fb\uff08\u59822D Common Corruptions\uff09\u4e0b\u7684\u8868\u73b0\uff0c\u8fdb\u884c\u4e86\u8fc4\u4eca\u6700\u5168\u9762\u7684\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u5206\u6790\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u89c6\u5dee\u4f30\u8ba1\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3001\u53ef\u9760\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u5173\u952e\u5173\u8054\u3002", "conclusion": "DispBench\u4e3a\u89c6\u5dee\u4f30\u8ba1\u9886\u57df\u7684\u9c81\u68d2\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u5e76\u5f00\u6e90\u4e86\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.05375", "pdf": "https://arxiv.org/pdf/2505.05375", "abs": "https://arxiv.org/abs/2505.05375", "authors": ["Kejie Zhao", "Wenjia Hua", "Aiersi Tuerhong", "Luziwei Leng", "Yuxin Ma", "Qinghua Guo"], "title": "Threshold Modulation for Online Test-Time Adaptation of Spiking Neural Networks", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "comment": "Accepted by IJCNN 2025. \\c{opyright} 2025 IEEE. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses, including reprinting/republishing this material for advertising or\n  promotional purposes, collecting new collected works for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Recently, spiking neural networks (SNNs), deployed on neuromorphic chips,\nprovide highly efficient solutions on edge devices in different scenarios.\nHowever, their ability to adapt to distribution shifts after deployment has\nbecome a crucial challenge. Online test-time adaptation (OTTA) offers a\npromising solution by enabling models to dynamically adjust to new data\ndistributions without requiring source data or labeled target samples.\nNevertheless, existing OTTA methods are largely designed for traditional\nartificial neural networks and are not well-suited for SNNs. To address this\ngap, we propose a low-power, neuromorphic chip-friendly online test-time\nadaptation framework, aiming to enhance model generalization under distribution\nshifts. The proposed approach is called Threshold Modulation (TM), which\ndynamically adjusts the firing threshold through neuronal dynamics-inspired\nnormalization, being more compatible with neuromorphic hardware. Experimental\nresults on benchmark datasets demonstrate the effectiveness of this method in\nimproving the robustness of SNNs against distribution shifts while maintaining\nlow computational cost. The proposed method offers a practical solution for\nonline test-time adaptation of SNNs, providing inspiration for the design of\nfuture neuromorphic chips. The demo code is available at\ngithub.com/NneurotransmitterR/TM-OTTA-SNN.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u9608\u503c\u8c03\u5236\uff08TM\uff09\u7684\u4f4e\u529f\u8017\u3001\u9002\u7528\u4e8e\u795e\u7ecf\u5f62\u6001\u82af\u7247\u7684\u5728\u7ebf\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5728\u7ebf\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\uff08OTTA\uff09\u65b9\u6cd5\u5927\u591a\u9488\u5bf9\u4f20\u7edf\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\uff0c\u4e0d\u9002\u5408\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86TM\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u795e\u7ecf\u5143\u52a8\u6001\u542f\u53d1\u7684\u5f52\u4e00\u5316\u52a8\u6001\u8c03\u6574\u8109\u51b2\u53d1\u653e\u9608\u503c\uff0c\u79f0\u4e3a\u9608\u503c\u8c03\u5236\uff08TM\uff09\uff0c\u8be5\u65b9\u6cd5\u66f4\u9002\u5408\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86SNN\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "TM\u65b9\u6cd5\u4e3aSNN\u7684\u5728\u7ebf\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u795e\u7ecf\u5f62\u6001\u82af\u7247\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u542f\u53d1\u3002"}}
{"id": "2505.05118", "pdf": "https://arxiv.org/pdf/2505.05118", "abs": "https://arxiv.org/abs/2505.05118", "authors": ["Makbule Gulcin Ozsoy"], "title": "Enhancing Text2Cypher with Schema Filtering", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Knowledge graphs represent complex data using nodes, relationships, and\nproperties. Cypher, a powerful query language for graph databases, enables\nefficient modeling and querying. Recent advancements in large language models\nallow translation of natural language questions into Cypher queries -\nText2Cypher. A common approach is incorporating database schema into prompts.\nHowever, complex schemas can introduce noise, increase hallucinations, and\nraise computational costs. Schema filtering addresses these challenges by\nincluding only relevant schema elements, improving query generation while\nreducing token costs. This work explores various schema filtering methods for\nText2Cypher task and analyzes their impact on token length, performance, and\ncost. Results show that schema filtering effectively optimizes Text2Cypher,\nespecially for smaller models. Consistent with prior research, we find that\nlarger models benefit less from schema filtering due to their longer context\ncapabilities. However, schema filtering remains valuable for both larger and\nsmaller models in cost reduction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728Text2Cypher\u4efb\u52a1\u4e2d\u6a21\u5f0f\u8fc7\u6ee4\u5bf9\u4f18\u5316\u81ea\u7136\u8bed\u8a00\u5230Cypher\u67e5\u8be2\u8f6c\u6362\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5bf9\u8f83\u5c0f\u6a21\u578b\u7684\u6548\u679c\u3002", "motivation": "\u77e5\u8bc6\u56fe\u8c31\u7684\u590d\u6742\u6a21\u5f0f\u53ef\u80fd\u5728\u81ea\u7136\u8bed\u8a00\u8f6c\u67e5\u8be2\u4efb\u52a1\u4e2d\u5f15\u5165\u566a\u58f0\u548c\u589e\u52a0\u8ba1\u7b97\u6210\u672c\uff0c\u6a21\u5f0f\u8fc7\u6ee4\u901a\u8fc7\u4ec5\u5305\u542b\u76f8\u5173\u6a21\u5f0f\u5143\u7d20\u6765\u4f18\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e0d\u540c\u6a21\u5f0f\u8fc7\u6ee4\u65b9\u6cd5\u5728Text2Cypher\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff0c\u5206\u6790\u4e86\u5176\u5bf9\u4ee4\u724c\u957f\u5ea6\u3001\u6027\u80fd\u548c\u6210\u672c\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\u6a21\u5f0f\u8fc7\u6ee4\u6709\u6548\u4f18\u5316\u4e86Text2Cypher\uff0c\u5c24\u5176\u5bf9\u8f83\u5c0f\u6a21\u578b\u6548\u679c\u663e\u8457\uff0c\u4e14\u80fd\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u5bf9\u5927\u5c0f\u6a21\u578b\u5747\u6709\u5e2e\u52a9\u3002", "conclusion": "\u5c3d\u7ba1\u5927\u578b\u6a21\u578b\u56e0\u5176\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u53d7\u76ca\u8f83\u5c11\uff0c\u6a21\u5f0f\u8fc7\u6ee4\u5728\u964d\u4f4e\u6210\u672c\u65b9\u9762\u5bf9\u6240\u6709\u6a21\u578b\u4ecd\u5177\u4ef7\u503c\u3002"}}
{"id": "2505.05121", "pdf": "https://arxiv.org/pdf/2505.05121", "abs": "https://arxiv.org/abs/2505.05121", "authors": ["Jasper Rou"], "title": "Error Analysis of Deep PDE Solvers for Option Pricing", "categories": ["q-fin.CP", "cs.LG", "q-fin.MF", "91G20, 91G60, 68T07"], "comment": "15 pages, 19 figures", "summary": "Option pricing often requires solving partial differential equations (PDEs).\nAlthough deep learning-based PDE solvers have recently emerged as quick\nsolutions to this problem, their empirical and quantitative accuracy remain not\nwell understood, hindering their real-world applicability. In this research,\nour aim is to offer actionable insights into the utility of deep PDE solvers\nfor practical option pricing implementation. Through comparative experiments in\nboth the Black--Scholes and the Heston model, we assess the empirical\nperformance of two neural network algorithms to solve PDEs: the Deep Galerkin\nMethod and the Time Deep Gradient Flow method (TDGF). We determine their\nempirical convergence rates and training time as functions of (i) the number of\nsampling stages, (ii) the number of samples, (iii) the number of layers, and\n(iv) the number of nodes per layer. For the TDGF, we also consider the order of\nthe discretization scheme and the number of time steps.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5b66\u4e60PDE\u6c42\u89e3\u5668\u5728\u5b9e\u9645\u671f\u6743\u5b9a\u4ef7\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e24\u79cd\u795e\u7ecf\u7f51\u7edc\u7b97\u6cd5\uff08Deep Galerkin Method\u548cTDGF\uff09\u5728Black-Scholes\u548cHeston\u6a21\u578b\u4e2d\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u4e86\u5176\u7ecf\u9a8c\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60PDE\u6c42\u89e3\u5668\u5728\u671f\u6743\u5b9a\u4ef7\u4e2d\u7684\u5b9e\u8bc1\u548c\u91cf\u5316\u51c6\u786e\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5728Black-Scholes\u548cHeston\u6a21\u578b\u4e2d\u8fdb\u884c\u6bd4\u8f83\u5b9e\u9a8c\uff0c\u8bc4\u4f30Deep Galerkin Method\u548cTDGF\u7684\u6027\u80fd\uff0c\u6d4b\u8bd5\u53c2\u6570\u5305\u62ec\u91c7\u6837\u9636\u6bb5\u6570\u3001\u6837\u672c\u6570\u3001\u5c42\u6570\u3001\u8282\u70b9\u6570\u7b49\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u5305\u62ec\u4e24\u79cd\u7b97\u6cd5\u7684\u7ecf\u9a8c\u6536\u655b\u901f\u5ea6\u548c\u8bad\u7ec3\u65f6\u95f4\u7684\u91cf\u5316\u6570\u636e\uff0c\u5c24\u5176\u662fTDGF\u8fd8\u8003\u5bdf\u4e86\u79bb\u6563\u5316\u65b9\u6848\u9636\u6570\u548c\u65f6\u95f4\u6b65\u6570\u7684\u5f71\u54cd\u3002", "conclusion": "\u7ed3\u8bba\u662f\u63d0\u4f9b\u4e86\u5173\u4e8e\u6df1\u5ea6\u5b66\u4e60PDE\u6c42\u89e3\u5668\u5728\u671f\u6743\u5b9a\u4ef7\u4e2d\u5b9e\u7528\u6027\u7684\u5177\u4f53\u89c1\u89e3\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2505.05122", "pdf": "https://arxiv.org/pdf/2505.05122", "abs": "https://arxiv.org/abs/2505.05122", "authors": ["Makbule Gulcin Ozsoy"], "title": "Text2Cypher: Data Pruning using Hard Example Selection", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Database query languages such as SQL for relational databases and Cypher for\ngraph databases have been widely adopted. Recent advancements in large language\nmodels (LLMs) enable natural language interactions with databases through\nmodels like Text2SQL and Text2Cypher. Fine-tuning these models typically\nrequires large, diverse datasets containing non-trivial examples. However, as\ndataset size increases, the cost of fine-tuning also rises. This makes smaller,\nhigh-quality datasets essential for reducing costs for the same or better\nperformance. In this paper, we propose five hard-example selection techniques\nfor pruning the Text2Cypher dataset, aiming to preserve or improve performance\nwhile reducing resource usage. Our results show that these hard-example\nselection approaches can halve training time and costs with minimal impact on\nperformance, and demonstrates that hard-example selection provides a\ncost-effective solution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e94\u79cd\u786c\u6837\u672c\u9009\u62e9\u6280\u672f\uff0c\u7528\u4e8e\u4fee\u526aText2Cypher\u6570\u636e\u96c6\uff0c\u4ee5\u51cf\u5c11\u8d44\u6e90\u4f7f\u7528\u5e76\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u5c06\u8bad\u7ec3\u65f6\u95f4\u548c\u6210\u672c\u51cf\u534a\uff0c\u4e14\u5bf9\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4e0e\u6570\u636e\u5e93\u4ea4\u4e92\uff08\u5982Text2SQL\u548cText2Cypher\uff09\u6210\u4e3a\u53ef\u80fd\u3002\u7136\u800c\uff0c\u5fae\u8c03\u8fd9\u4e9b\u6a21\u578b\u9700\u8981\u5927\u91cf\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0c\u6210\u672c\u9ad8\u6602\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u6570\u636e\u96c6\u6765\u964d\u4f4e\u6210\u672c\u6210\u4e3a\u5173\u952e\u3002", "method": "\u63d0\u51fa\u4e86\u4e94\u79cd\u786c\u6837\u672c\u9009\u62e9\u6280\u672f\uff0c\u7528\u4e8e\u4eceText2Cypher\u6570\u636e\u96c6\u4e2d\u7b5b\u9009\u51fa\u5bf9\u6a21\u578b\u8bad\u7ec3\u6700\u6709\u4ef7\u503c\u7684\u6837\u672c\uff0c\u4ece\u800c\u5728\u4e0d\u663e\u8457\u5f71\u54cd\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u51cf\u5c11\u8bad\u7ec3\u8d44\u6e90\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u786c\u6837\u672c\u9009\u62e9\u65b9\u6cd5\u80fd\u591f\u5c06\u8bad\u7ec3\u65f6\u95f4\u548c\u6210\u672c\u964d\u4f4e50%\uff0c\u540c\u65f6\u6027\u80fd\u51e0\u4e4e\u4e0d\u53d7\u5f71\u54cd\u3002", "conclusion": "\u786c\u6837\u672c\u9009\u62e9\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u4f18\u5316LLM\u5728\u6570\u636e\u5e93\u67e5\u8be2\u4efb\u52a1\u4e2d\u7684\u5fae\u8c03\u8fc7\u7a0b\u3002"}}
{"id": "2505.05151", "pdf": "https://arxiv.org/pdf/2505.05151", "abs": "https://arxiv.org/abs/2505.05151", "authors": ["Chuangtao Chen", "Qinglin Zhao", "MengChu Zhou", "Zhimin He", "Haozhen Situ"], "title": "Overcoming Dimensional Factorization Limits in Discrete Diffusion Models through Quantum Joint Distribution Learning", "categories": ["quant-ph", "cs.LG"], "comment": "Comments are welcome", "summary": "This study explores quantum-enhanced discrete diffusion models to overcome\nclassical limitations in learning high-dimensional distributions. We rigorously\nprove that classical discrete diffusion models, which calculate per-dimension\ntransition probabilities to avoid exponential computational cost, exhibit\nworst-case linear scaling of Kullback-Leibler (KL) divergence with data\ndimension. To address this, we propose a Quantum Discrete Denoising Diffusion\nProbabilistic Model (QD3PM), which enables joint probability learning through\ndiffusion and denoising in exponentially large Hilbert spaces. By deriving\nposterior states through quantum Bayes' theorem, similar to the crucial role of\nposterior probabilities in classical diffusion models, and by learning the\njoint probability, we establish a solid theoretical foundation for\nquantum-enhanced diffusion models. For denoising, we design a quantum circuit\nusing temporal information for parameter sharing and learnable\nclassical-data-controlled rotations for encoding. Exploiting joint distribution\nlearning, our approach enables single-step sampling from pure noise,\neliminating iterative requirements of existing models. Simulations demonstrate\nthe proposed model's superior accuracy in modeling complex distributions\ncompared to factorization methods. Hence, this paper establishes a new\ntheoretical paradigm in generative models by leveraging the quantum advantage\nin joint distribution learning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50\u79bb\u6563\u6269\u6563\u6a21\u578b\uff08QD3PM\uff09\uff0c\u901a\u8fc7\u91cf\u5b50\u65b9\u6cd5\u89e3\u51b3\u7ecf\u5178\u79bb\u6563\u6269\u6563\u6a21\u578b\u5728\u9ad8\u7ef4\u5206\u5e03\u5b66\u4e60\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5c55\u793a\u4e86\u91cf\u5b50\u4f18\u52bf\u5728\u8054\u5408\u5206\u5e03\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u7ecf\u5178\u79bb\u6563\u6269\u6563\u6a21\u578b\u5728\u9ad8\u7ef4\u6570\u636e\u5206\u5e03\u5b66\u4e60\u4e2d\u5b58\u5728KL\u6563\u5ea6\u968f\u7ef4\u5ea6\u7ebf\u6027\u589e\u957f\u7684\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u7814\u7a76\u63d0\u51fa\u91cf\u5b50\u589e\u5f3a\u65b9\u6cd5\u4ee5\u514b\u670d\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u4e86QD3PM\u6a21\u578b\uff0c\u5229\u7528\u91cf\u5b50\u8d1d\u53f6\u65af\u5b9a\u7406\u63a8\u5bfc\u540e\u9a8c\u6001\uff0c\u5e76\u901a\u8fc7\u91cf\u5b50\u7535\u8def\u5b9e\u73b0\u5355\u6b65\u91c7\u6837\uff0c\u907f\u514d\u8fed\u4ee3\u9700\u6c42\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\uff0cQD3PM\u5728\u590d\u6742\u5206\u5e03\u5efa\u6a21\u4e0a\u6bd4\u56e0\u5b50\u5206\u89e3\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u751f\u6210\u6a21\u578b\u5efa\u7acb\u4e86\u65b0\u7684\u7406\u8bba\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u91cf\u5b50\u8054\u5408\u5206\u5e03\u5b66\u4e60\u7684\u4f18\u52bf\u3002"}}
{"id": "2505.05163", "pdf": "https://arxiv.org/pdf/2505.05163", "abs": "https://arxiv.org/abs/2505.05163", "authors": ["Aishwarya Venkataramanan", "Paul Bodesheim", "Joachim Denzler"], "title": "Probabilistic Embeddings for Frozen Vision-Language Models: Uncertainty Quantification with Gaussian Process Latent Variable Models", "categories": ["cs.CV", "cs.LG"], "comment": "UAI 2025, 22 pages", "summary": "Vision-Language Models (VLMs) learn joint representations by mapping images\nand text into a shared latent space. However, recent research highlights that\ndeterministic embeddings from standard VLMs often struggle to capture the\nuncertainties arising from the ambiguities in visual and textual descriptions\nand the multiple possible correspondences between images and texts. Existing\napproaches tackle this by learning probabilistic embeddings during VLM\ntraining, which demands large datasets and does not leverage the powerful\nrepresentations already learned by large-scale VLMs like CLIP. In this paper,\nwe propose GroVE, a post-hoc approach to obtaining probabilistic embeddings\nfrom frozen VLMs. GroVE builds on Gaussian Process Latent Variable Model\n(GPLVM) to learn a shared low-dimensional latent space where image and text\ninputs are mapped to a unified representation, optimized through single-modal\nembedding reconstruction and cross-modal alignment objectives. Once trained,\nthe Gaussian Process model generates uncertainty-aware probabilistic\nembeddings. Evaluation shows that GroVE achieves state-of-the-art uncertainty\ncalibration across multiple downstream tasks, including cross-modal retrieval,\nvisual question answering, and active learning.", "AI": {"tldr": "GroVE proposes a post-hoc method to generate probabilistic embeddings from frozen VLMs using GPLVM, improving uncertainty calibration in vision-language tasks.", "motivation": "Standard VLMs struggle with capturing uncertainties in visual and textual ambiguities, and existing probabilistic approaches require large datasets and retraining.", "method": "GroVE uses GPLVM to learn a shared latent space from frozen VLMs, optimizing for single-modal reconstruction and cross-modal alignment to generate uncertainty-aware embeddings.", "result": "GroVE achieves state-of-the-art uncertainty calibration in tasks like cross-modal retrieval, visual question answering, and active learning.", "conclusion": "GroVE effectively enhances frozen VLMs with probabilistic embeddings without retraining, addressing uncertainty in vision-language tasks."}}
{"id": "2505.05168", "pdf": "https://arxiv.org/pdf/2505.05168", "abs": "https://arxiv.org/abs/2505.05168", "authors": ["M. D. Ruiz-Medina", "A. Torres--Signes"], "title": "Local linear Fr\u00e9chet curve regression in manifolds", "categories": ["math.ST", "cs.LG", "stat.ML", "stat.TH"], "comment": null, "summary": "Global Fr\\'echet functional regression has been recently addressed from time\ncorrelated bivariate curve data evaluated in a manifold (see Torres et al.\n2025). For this type of curve data sets, the present paper solves the problem\nof local linear approximation of the Fr\\'echet conditional mean in an extrinsic\nand intrinsic way. The extrinsic local linear Fr\\'echet functional regression\npredictor is obtained in the time varying tangent space by projection into an\northornormal basis of the ambient Hilbert space. The conditions assumed ensure\nthe existence and uniqueness of this predictor, and its computation via\nexponential and logarithmic maps. A weighted Fr\\'echet mean approach is adopted\nin the computation of an intrinsic local linear Fr\\'echet functional regression\npredictor. The asymptotic optimality of this intrinsic local approximation is\nalso proved. The performance of the empirical version of both, extrinsic and\nintrinsic functional predictors, and of a Nadaraya-Watson type Fr\\'echet curve\npredictor is illustrated in the simulation study undertaken. The finite-sample\nsize properties are also tested in a real-data application via\ncross-validation. Specifically, functional prediction of the magnetic vector\nfield from the time-varying geocentric latitude and longitude of the satellite\nNASA's MAGSAT spacecraft is addressed.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5c40\u90e8\u7ebf\u6027Fr\u00e9chet\u529f\u80fd\u56de\u5f52\u7684\u4e24\u79cd\u65b9\u6cd5\uff08\u5916\u5728\u548c\u5185\u5728\uff09\uff0c\u8bc1\u660e\u4e86\u5176\u6700\u4f18\u6027\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4ece\u65f6\u95f4\u76f8\u5173\u7684\u53cc\u53d8\u91cf\u66f2\u7ebf\u6570\u636e\u4e2d\u5c40\u90e8\u7ebf\u6027\u8fd1\u4f3cFr\u00e9chet\u6761\u4ef6\u5747\u503c\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6d41\u5f62\u6570\u636e\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5916\u5728\u65b9\u6cd5\u5728\u65f6\u53d8\u5207\u7a7a\u95f4\u4e2d\u6295\u5f71\u6c42\u89e3\uff0c\u4ee5\u53ca\u5185\u5728\u65b9\u6cd5\u91c7\u7528\u52a0\u6743Fr\u00e9chet\u5747\u503c\u8ba1\u7b97\uff0c\u5e76\u8bc1\u660e\u4e86\u6e10\u8fd1\u6700\u4f18\u6027\u3002", "result": "\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\uff08NASA\u7684MAGSAT\u536b\u661f\u78c1\u573a\u5411\u91cf\u9884\u6d4b\uff09\u9a8c\u8bc1\u4e86\u4e24\u79cd\u9884\u6d4b\u5668\u7684\u6027\u80fd\u548c\u5c0f\u6837\u672c\u6027\u8d28\u3002", "conclusion": "\u5916\u5728\u548c\u5185\u5728\u5c40\u90e8\u7ebf\u6027Fr\u00e9chet\u529f\u80fd\u56de\u5f52\u9884\u6d4b\u5668\u5747\u6709\u6548\uff0c\u9002\u7528\u4e8e\u6d41\u5f62\u6570\u636e\u7684\u529f\u80fd\u9884\u6d4b\u95ee\u9898\u3002"}}
{"id": "2505.05183", "pdf": "https://arxiv.org/pdf/2505.05183", "abs": "https://arxiv.org/abs/2505.05183", "authors": ["Elad Feldman", "Jacob Shams", "Dudi Biton", "Alfred Chen", "Shaoyuan Xie", "Satoru Koda", "Yisroel Mirsky", "Asaf Shabtai", "Yuval Elovici", "Ben Nassi"], "title": "PaniCar: Securing the Perception of Advanced Driving Assistance Systems Against Emergency Vehicle Lighting", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The safety of autonomous cars has come under scrutiny in recent years,\nespecially after 16 documented incidents involving Teslas (with autopilot\nengaged) crashing into parked emergency vehicles (police cars, ambulances, and\nfiretrucks). While previous studies have revealed that strong light sources\noften introduce flare artifacts in the captured image, which degrade the image\nquality, the impact of flare on object detection performance remains unclear.\nIn this research, we unveil PaniCar, a digital phenomenon that causes an object\ndetector's confidence score to fluctuate below detection thresholds when\nexposed to activated emergency vehicle lighting. This vulnerability poses a\nsignificant safety risk, and can cause autonomous vehicles to fail to detect\nobjects near emergency vehicles. In addition, this vulnerability could be\nexploited by adversaries to compromise the security of advanced driving\nassistance systems (ADASs). We assess seven commercial ADASs (Tesla Model 3,\n\"manufacturer C\", HP, Pelsee, AZDOME, Imagebon, Rexing), four object detectors\n(YOLO, SSD, RetinaNet, Faster R-CNN), and 14 patterns of emergency vehicle\nlighting to understand the influence of various technical and environmental\nfactors. We also evaluate four SOTA flare removal methods and show that their\nperformance and latency are insufficient for real-time driving constraints. To\nmitigate this risk, we propose Caracetamol, a robust framework designed to\nenhance the resilience of object detectors against the effects of activated\nemergency vehicle lighting. Our evaluation shows that on YOLOv3 and Faster\nRCNN, Caracetamol improves the models' average confidence of car detection by\n0.20, the lower confidence bound by 0.33, and reduces the fluctuation range by\n0.33. In addition, Caracetamol is capable of processing frames at a rate of\nbetween 30-50 FPS, enabling real-time ADAS car detection.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u7d27\u6025\u8f66\u8f86\u706f\u5149\u7167\u5c04\u4e0b\uff0c\u7269\u4f53\u68c0\u6d4b\u6027\u80fd\u4f1a\u56e0\u56fe\u50cf\u5149\u6591\u800c\u53d7\u5f71\u54cd\u3002\u63d0\u51fa\u7684Caracetamol\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7a33\u5b9a\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u7d27\u6025\u8f66\u8f86\u706f\u5149\u7167\u5c04\u4e0b\u53ef\u80fd\u65e0\u6cd5\u68c0\u6d4b\u7269\u4f53\uff0c\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002", "method": "\u8bc4\u4f30\u591a\u79cdADAS\u548c\u7269\u4f53\u68c0\u6d4b\u5668\uff0c\u6d4b\u8bd5\u4e0d\u540c\u706f\u5149\u6a21\u5f0f\uff0c\u5e76\u63d0\u51faCaracetamol\u6846\u67b6\u3002", "result": "Caracetamol\u5c06YOLOv3\u548cFaster RCNN\u7684\u8f66\u8f86\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\u63d0\u9ad8\u4e860.20\uff0c\u6ce2\u52a8\u8303\u56f4\u51cf\u5c110.33\u3002", "conclusion": "Caracetamol\u80fd\u6709\u6548\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5728\u7d27\u6025\u706f\u5149\u4e0b\u7684\u68c0\u6d4b\u7a33\u5b9a\u6027\uff0c\u6ee1\u8db3\u5b9e\u65f6\u6027\u9700\u6c42\u3002"}}
{"id": "2505.05470", "pdf": "https://arxiv.org/pdf/2505.05470", "abs": "https://arxiv.org/abs/2505.05470", "authors": ["Jie Liu", "Gongye Liu", "Jiajun Liang", "Yangguang Li", "Jiaheng Liu", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Wanli Ouyang"], "title": "Flow-GRPO: Training Flow Matching Models via Online RL", "categories": ["cs.CV", "cs.AI"], "comment": "Code: https://github.com/yifan123/flow_grpo", "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, its accuracy\nimproves from $59\\%$ to $92\\%$, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments.", "AI": {"tldr": "Flow-GRPO\u662f\u4e00\u79cd\u5c06\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e0e\u6d41\u5339\u914d\u6a21\u578b\u7ed3\u5408\u7684\u9996\u4e2a\u65b9\u6cd5\uff0c\u901a\u8fc7ODE-to-SDE\u8f6c\u6362\u548c\u964d\u566a\u7b56\u7565\u63d0\u5347\u91c7\u6837\u6548\u7387\u548c\u6027\u80fd\uff0c\u5728\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u6d41\u5339\u914d\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u63d0\u9ad8\u751f\u6210\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528ODE-to-SDE\u8f6c\u6362\u548c\u964d\u566a\u7b56\u7565\uff0c\u524d\u8005\u5b9e\u73b0\u7edf\u8ba1\u91c7\u6837\u4ee5\u652f\u6301RL\u63a2\u7d22\uff0c\u540e\u8005\u51cf\u5c11\u8bad\u7ec3\u6b65\u9aa4\u4f46\u4e0d\u5f71\u54cd\u63a8\u7406\u6027\u80fd\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5982GenEval\u51c6\u786e\u6027\u4ece63%\u63d0\u5347\u81f395%\uff0c\u89c6\u89c9\u6587\u672c\u6e32\u67d3\u4ece59%\u63d0\u5347\u81f392%\uff0c\u4e14\u672a\u727a\u7272\u56fe\u50cf\u8d28\u91cf\u6216\u591a\u6837\u6027\u3002", "conclusion": "Flow-GRPO\u4e0d\u4ec5\u9ad8\u6548\u63d0\u5347\u4e86\u751f\u6210\u4efb\u52a1\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u4fdd\u6301\u4e86\u56fe\u50cf\u8d28\u91cf\u4e0e\u591a\u6837\u6027\uff0c\u5c55\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u6d41\u5339\u914d\u6a21\u578b\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.05223", "pdf": "https://arxiv.org/pdf/2505.05223", "abs": "https://arxiv.org/abs/2505.05223", "authors": ["Hendrik Surmann", "Jorge de Heuvel", "Maren Bennewitz"], "title": "Multi-Objective Reinforcement Learning for Adaptive Personalized Autonomous Driving", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Human drivers exhibit individual preferences regarding driving style.\nAdapting autonomous vehicles to these preferences is essential for user trust\nand satisfaction. However, existing end-to-end driving approaches often rely on\npredefined driving styles or require continuous user feedback for adaptation,\nlimiting their ability to support dynamic, context-dependent preferences. We\npropose a novel approach using multi-objective reinforcement learning (MORL)\nwith preference-driven optimization for end-to-end autonomous driving that\nenables runtime adaptation to driving style preferences. Preferences are\nencoded as continuous weight vectors to modulate behavior along interpretable\nstyle objectives$\\unicode{x2013}$including efficiency, comfort, speed, and\naggressiveness$\\unicode{x2013}$without requiring policy retraining. Our\nsingle-policy agent integrates vision-based perception in complex mixed-traffic\nscenarios and is evaluated in diverse urban environments using the CARLA\nsimulator. Experimental results demonstrate that the agent dynamically adapts\nits driving behavior according to changing preferences while maintaining\nperformance in terms of collision avoidance and route completion.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff08MORL\uff09\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\uff0c\u901a\u8fc7\u504f\u597d\u9a71\u52a8\u4f18\u5316\u5b9e\u73b0\u8fd0\u884c\u65f6\u9a7e\u9a76\u98ce\u683c\u52a8\u6001\u9002\u914d\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7b56\u7565\u3002", "motivation": "\u4eba\u7c7b\u9a7e\u9a76\u5458\u6709\u5404\u81ea\u7684\u9a7e\u9a76\u98ce\u683c\u504f\u597d\uff0c\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u9884\u5b9a\u4e49\u98ce\u683c\u6216\u9700\u6301\u7eed\u7528\u6237\u53cd\u9988\uff0c\u65e0\u6cd5\u652f\u6301\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u504f\u597d\u9002\u914d\u3002", "method": "\u4f7f\u7528MORL\u6846\u67b6\uff0c\u5c06\u504f\u597d\u7f16\u7801\u4e3a\u8fde\u7eed\u6743\u91cd\u5411\u91cf\uff0c\u8c03\u8282\u53ef\u89e3\u91ca\u7684\u9a7e\u9a76\u98ce\u683c\u76ee\u6807\uff08\u5982\u6548\u7387\u3001\u8212\u9002\u6027\u3001\u901f\u5ea6\u548c\u6fc0\u8fdb\u6027\uff09\uff0c\u7ed3\u5408\u89c6\u89c9\u611f\u77e5\u5904\u7406\u590d\u6742\u6df7\u5408\u4ea4\u901a\u573a\u666f\u3002", "result": "\u5728CARLA\u6a21\u62df\u5668\u4e2d\u9a8c\u8bc1\uff0c\u5355\u7b56\u7565\u667a\u80fd\u4f53\u80fd\u6839\u636e\u53d8\u5316\u504f\u597d\u52a8\u6001\u8c03\u6574\u9a7e\u9a76\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u6301\u78b0\u649e\u907f\u514d\u548c\u8def\u7ebf\u5b8c\u6210\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u9002\u914d\u7528\u6237\u9a7e\u9a76\u98ce\u683c\u504f\u597d\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u4e2a\u6027\u5316\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2505.05261", "pdf": "https://arxiv.org/pdf/2505.05261", "abs": "https://arxiv.org/abs/2505.05261", "authors": ["Yu Liu", "Fabricio Oliveira"], "title": "ICNN-enhanced 2SP: Leveraging input convex neural networks for solving two-stage stochastic programming", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "Two-stage stochastic programming (2SP) offers a basic framework for modelling\ndecision-making under uncertainty, yet scalability remains a challenge due to\nthe computational complexity of recourse function evaluation. Existing\nlearning-based methods like Neural Two-Stage Stochastic Programming (Neur2SP)\nemploy neural networks (NNs) as recourse function surrogates but rely on\ncomputationally intensive mixed-integer programming (MIP) formulations. We\npropose ICNN-enhanced 2SP, a method that leverages Input Convex Neural Networks\n(ICNNs) to exploit linear programming (LP) representability in convex 2SP\nproblems. By architecturally enforcing convexity and enabling exact inference\nthrough LP, our approach eliminates the need for integer variables inherent to\nthe conventional MIP-based formulation while retaining an exact embedding of\nthe ICNN surrogate within the 2SP framework. This results in a more\ncomputationally efficient alternative that maintains solution quality.\nComprehensive experiments reveal that ICNNs incur only marginally longer\ntraining times while achieving validation accuracy on par with their MIP-based\ncounterparts. Across benchmark problems, ICNN-enhanced 2SP often exhibits\nconsiderably faster solution times than the MIP-based formulations while\npreserving solution quality, with these advantages becoming significantly more\npronounced as problem scale increases. For the most challenging instances, the\nmethod achieves speedups of up to 100$\\times$ and solution quality superior to\nMIP-based formulations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ICNN\u589e\u5f3a\u7684\u4e24\u9636\u6bb5\u968f\u673a\u89c4\u5212\u65b9\u6cd5\uff0c\u5229\u7528\u8f93\u5165\u51f8\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3\u4f20\u7edf\u6df7\u5408\u6574\u6570\u89c4\u5212\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5982Neur2SP\u4f9d\u8d56\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u6df7\u5408\u6574\u6570\u89c4\u5212\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5229\u7528ICNN\u7684\u7ebf\u6027\u89c4\u5212\u53ef\u8868\u793a\u6027\uff0c\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u8f93\u5165\u51f8\u795e\u7ecf\u7f51\u7edc\uff08ICNN\uff09\u5728\u51f8\u4e24\u9636\u6bb5\u968f\u673a\u89c4\u5212\u95ee\u9898\u4e2d\u66ff\u4ee3\u6df7\u5408\u6574\u6570\u89c4\u5212\uff0c\u901a\u8fc7LP\u5b9e\u73b0\u7cbe\u786e\u63a8\u7406\uff0c\u65e0\u9700\u6574\u6570\u53d8\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cICNN\u65b9\u6cd5\u5728\u4fdd\u6301\u9a8c\u8bc1\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u6700\u9ad8\u53ef\u8fbe\u5230100\u500d\u7684\u52a0\u901f\uff0c\u4e14\u968f\u7740\u95ee\u9898\u89c4\u6a21\u589e\u5927\u4f18\u52bf\u66f4\u4e3a\u663e\u8457\u3002", "conclusion": "ICNN\u589e\u5f3a\u7684\u4e24\u9636\u6bb5\u968f\u673a\u89c4\u5212\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edfMIP\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u95ee\u9898\u3002"}}
{"id": "2505.05269", "pdf": "https://arxiv.org/pdf/2505.05269", "abs": "https://arxiv.org/abs/2505.05269", "authors": ["Jingbin Xu", "Chen Qian", "Meimei Liu", "Feng Guo"], "title": "A Two-Sample Test of Text Generation Similarity", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "The surge in digitized text data requires reliable inferential methods on\nobserved textual patterns. This article proposes a novel two-sample text test\nfor comparing similarity between two groups of documents. The hypothesis is\nwhether the probabilistic mapping generating the textual data is identical\nacross two groups of documents. The proposed test aims to assess text\nsimilarity by comparing the entropy of the documents. Entropy is estimated\nusing neural network-based language models. The test statistic is derived from\nan estimation-and-inference framework, where the entropy is first approximated\nusing an estimation set, followed by inference on the remaining data set. We\nshowed theoretically that under mild conditions, the test statistic\nasymptotically follows a normal distribution. A multiple data-splitting\nstrategy is proposed to enhance test power, which combines p-values into a\nunified decision. Various simulation studies and a real data example\ndemonstrated that the proposed two-sample text test maintains the nominal Type\none error rate while offering greater power compared to existing methods. The\nproposed method provides a novel solution to assert differences in document\nclasses, particularly in fields where large-scale textual information is\ncrucial.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u71b5\u7684\u4e24\u6837\u672c\u6587\u672c\u76f8\u4f3c\u6027\u68c0\u9a8c\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u8bed\u8a00\u6a21\u578b\u4f30\u8ba1\u71b5\u5e76\u63a8\u5bfc\u68c0\u9a8c\u7edf\u8ba1\u91cf\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u6e10\u8fd1\u6b63\u6001\u5206\u5e03\uff0c\u591a\u91cd\u6570\u636e\u5206\u5272\u7b56\u7565\u63d0\u5347\u68c0\u9a8c\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6570\u5b57\u5316\u6587\u672c\u6570\u636e\u6fc0\u589e\u9700\u8981\u53ef\u9760\u7684\u63a8\u65ad\u65b9\u6cd5\u6bd4\u8f83\u6587\u6863\u76f8\u4f3c\u6027\uff0c\u68c0\u9a8c\u4e24\u7ec4\u6587\u6863\u662f\u5426\u6765\u81ea\u540c\u4e00\u6982\u7387\u751f\u6210\u6a21\u578b\u3002", "method": "\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u71b5\u4f30\u8ba1\uff0c\u901a\u8fc7\u4f30\u8ba1-\u63a8\u65ad\u6846\u67b6\u6784\u5efa\u68c0\u9a8c\u7edf\u8ba1\u91cf\uff0c\u91c7\u7528\u591a\u91cd\u6570\u636e\u5206\u5272\u5408\u5e76p\u503c\u63d0\u5347\u68c0\u9a8c\u529b\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u68c0\u9a8c\u7edf\u8ba1\u91cf\u6e10\u8fd1\u6b63\u6001\uff0c\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1\u5176\u63a7\u5236\u4e00\u7c7b\u9519\u8bef\u7387\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u68c0\u9a8c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u6587\u672c\u5206\u7c7b\u5dee\u5f02\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05273", "pdf": "https://arxiv.org/pdf/2505.05273", "abs": "https://arxiv.org/abs/2505.05273", "authors": ["Alexander Soen"], "title": "A Connection Between Learning to Reject and Bhattacharyya Divergences", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "Learning to reject provide a learning paradigm which allows for our models to\nabstain from making predictions. One way to learn the rejector is to learn an\nideal marginal distribution (w.r.t. the input domain) - which characterizes a\nhypothetical best marginal distribution - and compares it to the true marginal\ndistribution via a density ratio. In this paper, we consider learning a joint\nideal distribution over both inputs and labels; and develop a link between\nrejection and thresholding different statistical divergences. We further find\nthat when one considers a variant of the log-loss, the rejector obtained by\nconsidering the joint ideal distribution corresponds to the thresholding of the\nskewed Bhattacharyya divergence between class-probabilities. This is in\ncontrast to the marginal case - that is equivalent to a typical\ncharacterization of optimal rejection, Chow's Rule - which corresponds to a\nthresholding of the Kullback-Leibler divergence. In general, we find that\nrejecting via a Bhattacharyya divergence is less aggressive than Chow's Rule.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u8054\u5408\u7406\u60f3\u5206\u5e03\u5b66\u4e60\u62d2\u7edd\u673a\u5236\uff0c\u5e76\u4e0e\u7edf\u8ba1\u5dee\u5f02\u9608\u503c\u76f8\u5173\u8054\uff0c\u53d1\u73b0\u57fa\u4e8eBhattacharyya\u5dee\u5f02\u7684\u62d2\u7edd\u6bd4Chow\u89c4\u5219\u66f4\u6e29\u548c\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u5b66\u4e60\u8054\u5408\u7406\u60f3\u5206\u5e03\u6765\u6539\u8fdb\u6a21\u578b\u7684\u62d2\u7edd\u673a\u5236\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u786e\u5b9a\u60c5\u51b5\u4e0b\u7684\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u8054\u5408\u7406\u60f3\u5206\u5e03\u4e0e\u771f\u5b9e\u5206\u5e03\u7684\u5bc6\u5ea6\u6bd4\uff0c\u5efa\u7acb\u62d2\u7edd\u673a\u5236\u4e0e\u7edf\u8ba1\u5dee\u5f02\u9608\u503c\u7684\u8054\u7cfb\uff0c\u5e76\u4f7f\u7528\u53d8\u4f53\u5bf9\u6570\u635f\u5931\u51fd\u6570\u3002", "result": "\u53d1\u73b0\u57fa\u4e8eBhattacharyya\u5dee\u5f02\u7684\u62d2\u7edd\u673a\u5236\u6bd4\u4f20\u7edf\u7684Chow\u89c4\u5219\uff08\u57fa\u4e8eKL\u5dee\u5f02\uff09\u66f4\u6e29\u548c\uff0c\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8054\u5408\u7406\u60f3\u5206\u5e03\u4e0e\u7edf\u8ba1\u5dee\u5f02\u9608\u503c\u7ed3\u5408\u7684\u65b9\u6cd5\u4e3a\u62d2\u7edd\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u4e14Bhattacharyya\u5dee\u5f02\u7684\u9002\u7528\u6027\u66f4\u5f3a\u3002"}}
{"id": "2505.05287", "pdf": "https://arxiv.org/pdf/2505.05287", "abs": "https://arxiv.org/abs/2505.05287", "authors": ["Zechu Li", "Yufeng Jin", "Daniel Ordonez Apraez", "Claudio Semini", "Puze Liu", "Georgia Chalvatzaki"], "title": "Morphologically Symmetric Reinforcement Learning for Ambidextrous Bimanual Manipulation", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Humans naturally exhibit bilateral symmetry in their gross manipulation\nskills, effortlessly mirroring simple actions between left and right hands.\nBimanual robots-which also feature bilateral symmetry-should similarly exploit\nthis property to perform tasks with either hand. Unlike humans, who often favor\na dominant hand for fine dexterous skills, robots should ideally execute\nambidextrous manipulation with equal proficiency. To this end, we introduce\nSYMDEX (SYMmetric DEXterity), a reinforcement learning framework for\nambidextrous bi-manipulation that leverages the robot's inherent bilateral\nsymmetry as an inductive bias. SYMDEX decomposes complex bimanual manipulation\ntasks into per-hand subtasks and trains dedicated policies for each. By\nexploiting bilateral symmetry via equivariant neural networks, experience from\none arm is inherently leveraged by the opposite arm. We then distill the\nsubtask policies into a global ambidextrous policy that is independent of the\nhand-task assignment. We evaluate SYMDEX on six challenging simulated\nmanipulation tasks and demonstrate successful real-world deployment on two of\nthem. Our approach strongly outperforms baselines on complex task in which the\nleft and right hands perform different roles. We further demonstrate SYMDEX's\nscalability by extending it to a four-arm manipulation setup, where our\nsymmetry-aware policies enable effective multi-arm collaboration and\ncoordination. Our results highlight how structural symmetry as inductive bias\nin policy learning enhances sample efficiency, robustness, and generalization\nacross diverse dexterous manipulation tasks.", "AI": {"tldr": "SYMDEX\u662f\u4e00\u4e2a\u5229\u7528\u673a\u5668\u4eba\u53cc\u4fa7\u5bf9\u79f0\u6027\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u53cc\u624b\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u7b56\u7565\u548c\u5bf9\u79f0\u795e\u7ecf\u7f51\u7edc\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u6a21\u62df\u548c\u5b9e\u9645\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u673a\u5668\u4eba\u5e94\u50cf\u4eba\u7c7b\u4e00\u6837\u5229\u7528\u53cc\u4fa7\u5bf9\u79f0\u6027\u5b9e\u73b0\u53cc\u624b\u7075\u5de7\u64cd\u4f5c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6709\u6548\u5229\u7528\u5bf9\u79f0\u6027\u7684\u7b56\u7565\uff0c\u5bfc\u81f4\u4efb\u52a1\u6548\u7387\u4e0d\u8db3\u3002", "method": "\u65b9\u6cd5\u662f\u5c06\u53cc\u624b\u4efb\u52a1\u5206\u89e3\u4e3a\u5355\u624b\u5b50\u4efb\u52a1\uff0c\u5229\u7528\u5bf9\u79f0\u795e\u7ecf\u7f51\u7edc\u5171\u4eab\u53cc\u81c2\u7ecf\u9a8c\uff0c\u5e76\u5c06\u5176\u84b8\u998f\u4e3a\u5168\u5c40\u7b56\u7565\uff0c\u4ee5\u72ec\u7acb\u4e8e\u624b\u90e8\u4efb\u52a1\u5206\u914d\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u516d\u9879\u6a21\u62df\u4efb\u52a1\u548c\u4e24\u9879\u5b9e\u9645\u4efb\u52a1\u4e2d\uff0cSYMDEX\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u53cc\u624b\u5206\u5de5\u590d\u6742\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u4e14\u80fd\u6269\u5c55\u5230\u56db\u81c2\u534f\u4f5c\u573a\u666f\u3002", "conclusion": "\u7ed3\u8bba\u662f\u901a\u8fc7\u5bf9\u79f0\u6027\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\uff0cSYMDEX\u5728\u6837\u672c\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2505.05301", "pdf": "https://arxiv.org/pdf/2505.05301", "abs": "https://arxiv.org/abs/2505.05301", "authors": ["Jiaqi Leng", "Zhiyan Ding", "Zherui Chen", "Lin Lin"], "title": "Operator-Level Quantum Acceleration of Non-Logconcave Sampling", "categories": ["quant-ph", "cs.LG", "math.OC"], "comment": "43 pages, 7 figures", "summary": "Sampling from probability distributions of the form $\\sigma \\propto e^{-\\beta\nV}$, where $V$ is a continuous potential, is a fundamental task across physics,\nchemistry, biology, computer science, and statistics. However, when $V$ is\nnon-convex, the resulting distribution becomes non-logconcave, and classical\nmethods such as Langevin dynamics often exhibit poor performance. We introduce\nthe first quantum algorithm that provably accelerates a broad class of\ncontinuous-time sampling dynamics. For Langevin dynamics, our method encodes\nthe target Gibbs measure into the amplitudes of a quantum state, identified as\nthe kernel of a block matrix derived from a factorization of the Witten\nLaplacian operator. This connection enables Gibbs sampling via singular value\nthresholding and yields the first provable quantum advantage with respect to\nthe Poincar\\'e constant in the non-logconcave setting. Building on this\nframework, we further develop the first quantum algorithm that accelerates\nreplica exchange Langevin diffusion, a widely used method for sampling from\ncomplex, rugged energy landscapes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50\u7b97\u6cd5\uff0c\u7528\u4e8e\u52a0\u901f\u975e\u51f8\u52bf\u80fd\u4e0b\u7684\u8fde\u7eed\u65f6\u95f4\u91c7\u6837\u52a8\u6001\uff0c\u8bc1\u660e\u4e86\u5728\u975e\u5bf9\u6570\u51f9\u60c5\u51b5\u4e0b\u76f8\u5bf9\u4e8e\u7ecf\u5178\u65b9\u6cd5\u7684\u91cf\u5b50\u4f18\u52bf\u3002", "motivation": "\u5f53\u52bf\u80fd\u51fd\u6570\u975e\u51f8\u65f6\uff0c\u7ecf\u5178\u91c7\u6837\u65b9\u6cd5\uff08\u5982Langevin\u52a8\u529b\u5b66\uff09\u6027\u80fd\u8f83\u5dee\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u91cf\u5b50\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u96be\u9898\u3002", "method": "\u8be5\u7b97\u6cd5\u5c06\u76ee\u6807Gibbs\u6d4b\u5ea6\u7f16\u7801\u4e3a\u91cf\u5b50\u6001\u7684\u632f\u5e45\uff0c\u5229\u7528Witten Laplacian\u7b97\u5b50\u7684\u5206\u89e3\u6784\u5efa\u5757\u77e9\u9635\uff0c\u5e76\u901a\u8fc7\u5947\u5f02\u503c\u9608\u503c\u5b9e\u73b0Gibbs\u91c7\u6837\u3002", "result": "\u5728\u975e\u5bf9\u6570\u51f9\u60c5\u51b5\u4e0b\uff0c\u8be5\u7b97\u6cd5\u9996\u6b21\u5b9e\u73b0\u4e86\u76f8\u5bf9\u4e8e\u7ecf\u5178\u65b9\u6cd5\u7684\u91cf\u5b50\u4f18\u52bf\uff0c\u5e76\u4e3a\u590d\u6742\u80fd\u91cf\u666f\u89c2\u7684\u91c7\u6837\u63d0\u4f9b\u4e86\u66f4\u5feb\u7684\u91cf\u5b50\u52a0\u901f\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u91cf\u5b50\u8ba1\u7b97\u5728\u975e\u51f8\u4f18\u5316\u548c\u91c7\u6837\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u5c55\u793a\u4e86\u91cf\u5b50\u52a0\u901f\u5728\u5b9e\u9645\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.05371", "pdf": "https://arxiv.org/pdf/2505.05371", "abs": "https://arxiv.org/abs/2505.05371", "authors": ["Niklas Grieger", "Siamak Mehrkanoon", "Philipp Ritter", "Stephan Bialonski"], "title": "From Sleep Staging to Spindle Detection: Evaluating End-to-End Automated Sleep Analysis", "categories": ["eess.SP", "cs.LG", "q-bio.NC"], "comment": "10 pages, 4 figures, 2 tables", "summary": "Automation of sleep analysis, including both macrostructural (sleep stages)\nand microstructural (e.g., sleep spindles) elements, promises to enable\nlarge-scale sleep studies and to reduce variance due to inter-rater\nincongruencies. While individual steps, such as sleep staging and spindle\ndetection, have been studied separately, the feasibility of automating\nmulti-step sleep analysis remains unclear. Here, we evaluate whether a fully\nautomated analysis using state-of-the-art machine learning models for sleep\nstaging (RobustSleepNet) and subsequent spindle detection (SUMOv2) can\nreplicate findings from an expert-based study of bipolar disorder. The\nautomated analysis qualitatively reproduced key findings from the expert-based\nstudy, including significant differences in fast spindle densities between\nbipolar patients and healthy controls, accomplishing in minutes what previously\ntook months to complete manually. While the results of the automated analysis\ndiffered quantitatively from the expert-based study, possibly due to biases\nbetween expert raters or between raters and the models, the models individually\nperformed at or above inter-rater agreement for both sleep staging and spindle\ndetection. Our results demonstrate that fully automated approaches have the\npotential to facilitate large-scale sleep research. We are providing public\naccess to the tools used in our automated analysis by sharing our code and\nintroducing SomnoBot, a privacy-preserving sleep analysis platform.", "AI": {"tldr": "\u81ea\u52a8\u5316\u7761\u7720\u5206\u6790\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08RobustSleepNet\u548cSUMOv2\uff09\u5b9e\u73b0\u4e86\u7761\u7720\u5206\u671f\u548c\u7eba\u9524\u6ce2\u68c0\u6d4b\uff0c\u6210\u529f\u590d\u5236\u4e86\u4e13\u5bb6\u7814\u7a76\u4e2d\u53cc\u76f8\u60c5\u611f\u969c\u788d\u60a3\u8005\u7684\u5feb\u7eba\u9524\u6ce2\u5bc6\u5ea6\u5dee\u5f02\uff0c\u5c3d\u7ba1\u5b9a\u91cf\u7ed3\u679c\u7565\u6709\u4e0d\u540c\u3002", "motivation": "\u81ea\u52a8\u5316\u7761\u7720\u5206\u6790\u53ef\u51cf\u5c11\u4eba\u4e3a\u8bef\u5dee\u5e76\u652f\u6301\u5927\u89c4\u6a21\u7814\u7a76\uff0c\u4f46\u76ee\u524d\u591a\u6b65\u9aa4\u5206\u6790\u7684\u53ef\u884c\u6027\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u7528RobustSleepNet\u8fdb\u884c\u7761\u7720\u5206\u671f\uff0cSUMOv2\u68c0\u6d4b\u7eba\u9524\u6ce2\uff0c\u5b8c\u5168\u81ea\u52a8\u5316\u5206\u6790\u53cc\u76f8\u969c\u788d\u60a3\u8005\u4e0e\u5065\u5eb7\u5bf9\u7167\u7684\u7761\u7720\u6570\u636e\u3002", "result": "\u81ea\u52a8\u5316\u5206\u6790\u5728\u5b9a\u6027\u4e0a\u590d\u73b0\u4e86\u4e13\u5bb6\u7814\u7a76\u7684\u5173\u952e\u53d1\u73b0\uff08\u5982\u5feb\u7eba\u9524\u6ce2\u5bc6\u5ea6\u5dee\u5f02\uff09\uff0c\u4f46\u5b9a\u91cf\u7ed3\u679c\u5b58\u5728\u5dee\u5f02\uff1b\u6a21\u578b\u6027\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7\u4eba\u5de5\u8bc4\u5206\u8005\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "\u5168\u81ea\u52a8\u5316\u65b9\u6cd5\u53ef\u63a8\u52a8\u5927\u89c4\u6a21\u7761\u7720\u7814\u7a76\uff0c\u4f5c\u8005\u516c\u5f00\u4e86\u5206\u6790\u5de5\u5177\u4ee3\u7801\u5e76\u63a8\u51fa\u4e86\u9690\u79c1\u4fdd\u62a4\u5e73\u53f0SomnoBot\u3002"}}
{"id": "2505.05404", "pdf": "https://arxiv.org/pdf/2505.05404", "abs": "https://arxiv.org/abs/2505.05404", "authors": ["Michelangelo Domina", "Filippo Bigi", "Paolo Pegolo", "Michele Ceriotti"], "title": "Representing spherical tensors with scalar-based machine-learning models", "categories": ["physics.chem-ph", "cs.LG", "stat.ML"], "comment": null, "summary": "Rotational symmetry plays a central role in physics, providing an elegant\nframework to describe how the properties of 3D objects -- from atoms to the\nmacroscopic scale -- transform under the action of rigid rotations. Equivariant\nmodels of 3D point clouds are able to approximate structure-property relations\nin a way that is fully consistent with the structure of the rotation group, by\ncombining intermediate representations that are themselves spherical tensors.\nThe symmetry constraints however make this approach computationally demanding\nand cumbersome to implement, which motivates increasingly popular unconstrained\narchitectures that learn approximate symmetries as part of the training\nprocess. In this work, we explore a third route to tackle this learning\nproblem, where equivariant functions are expressed as the product of a scalar\nfunction of the point cloud coordinates and a small basis of tensors with the\nappropriate symmetry. We also propose approximations of the general expressions\nthat, while lacking universal approximation properties, are fast, simple to\nimplement, and accurate in practical settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5904\u7406\u70b9\u4e91\u6570\u636e\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7b49\u53d8\u51fd\u6570\u8868\u793a\u4e3a\u70b9\u4e91\u5750\u6807\u7684\u6807\u91cf\u51fd\u6570\u4e0e\u5177\u6709\u9002\u5f53\u5bf9\u79f0\u6027\u7684\u5c0f\u5f20\u91cf\u57fa\u7684\u4e58\u79ef\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u8fd1\u4f3c\u65b9\u6cd5\u4ee5\u7b80\u5316\u8ba1\u7b97\u3002", "motivation": "\u65cb\u8f6c\u5bf9\u79f0\u6027\u5728\u7269\u7406\u5b66\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u7684\u7b49\u53d8\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5b9e\u73b0\u590d\u6742\uff0c\u800c\u65e0\u7ea6\u675f\u67b6\u6784\u5219\u5b66\u4e60\u8fd1\u4f3c\u5bf9\u79f0\u6027\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u7b2c\u4e09\u79cd\u8def\u5f84\uff0c\u65e2\u4fdd\u6301\u5bf9\u79f0\u6027\u53c8\u7b80\u5316\u8ba1\u7b97\u3002", "method": "\u5c06\u7b49\u53d8\u51fd\u6570\u8868\u793a\u4e3a\u70b9\u4e91\u5750\u6807\u7684\u6807\u91cf\u51fd\u6570\u4e0e\u4e00\u7ec4\u5c0f\u5bf9\u79f0\u5f20\u91cf\u57fa\u7684\u4e58\u79ef\uff0c\u5e76\u63d0\u51fa\u8fd1\u4f3c\u8868\u8fbe\u5f0f\uff0c\u5c3d\u7ba1\u4e0d\u5177\u5907\u666e\u9002\u6027\uff0c\u4f46\u8ba1\u7b97\u901f\u5ea6\u5feb\u4e14\u5b9e\u73b0\u7b80\u5355\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51c6\u786e\u4e14\u9ad8\u6548\uff0c\u7b80\u5316\u4e86\u4f20\u7edf\u7b49\u53d8\u6a21\u578b\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u517c\u987e\u5bf9\u79f0\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7684\u65b0\u65b9\u6cd5\uff0c\u4e3a\u70b9\u4e91\u6570\u636e\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05420", "pdf": "https://arxiv.org/pdf/2505.05420", "abs": "https://arxiv.org/abs/2505.05420", "authors": ["Mario U. Gaimann", "Miriam Klopotek"], "title": "Robustly optimal dynamics for active matter reservoir computing", "categories": ["nlin.AO", "cond-mat.soft", "cs.LG", "physics.comp-ph"], "comment": "55 pages, 30 figures. Supplementary Videos:\n  https://doi.org/10.18419/DARUS-4619. Replication Data:\n  https://doi.org/10.18419/DARUS-4620", "summary": "We study the information processing abilities of active matter in the\nreservoir computing (RC) paradigm, using a model that is externally driven to\ninfer the future state of a chaotic signal. The simulated system closely\nfollows a previously reported model. We uncover an exceptional dynamical regime\nof agent dynamics that has been overlooked heretofore. It appears robustly\noptimal across varying physical parameters and inference tasks, thus providing\nvaluable insights into computation and inference with physical systems more\ngenerally. The ability to form effective mechanisms for information processing\nare primarily determined by the system's own intrinsic relaxation abilities.\nThese are identifiable when probing the system without a specific inference\ngoal and manifest when testing minimalistic single-particle reservoirs. The\nregime that achieves optimal computation is situated just below the critical\ndamping threshold, involving a microscopic dynamical relaxation with multiple\nstages. The optimal system is adaptable under chaotic external driving, due to\na diversity in response mechanisms that emerge like rapid alternations between\nquasi-stationary and highly nonlinear dynamical states. Both coherent and\nincoherent dynamics contribute to their operation, partly at dissimilar scales\nof space and delay time. Correlations on agent dynamics can indicate the\nbest-performing regimes and onsets of tight relationships between the\nresponding system and the fluctuating driver. As this model of computation is\ninterpretable in physical terms, it facilitates re-framing inquiries regarding\nlearning and unconventional computing with a fresh rationale for many-body\nphysics out of equilibrium.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6d3b\u6027\u7269\u8d28\u5728\u50a8\u5907\u8ba1\u7b97\uff08RC\uff09\u8303\u5f0f\u4e2d\u7684\u4fe1\u606f\u5904\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u4e86\u4e00\u79cd\u6b64\u524d\u88ab\u5ffd\u89c6\u7684\u52a8\u6001\u673a\u5236\uff0c\u5176\u5728\u4e0d\u540c\u7269\u7406\u53c2\u6570\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u63ed\u793a\u4e86\u7269\u7406\u7cfb\u7edf\u4e2d\u8ba1\u7b97\u4e0e\u63a8\u7406\u7684\u901a\u7528\u6027\u3002", "motivation": "\u63a2\u7d22\u6d3b\u6027\u7269\u8d28\u5982\u4f55\u901a\u8fc7\u5185\u90e8\u52a8\u6001\u7279\u6027\u5b9e\u73b0\u9ad8\u6548\u4fe1\u606f\u5904\u7406\uff0c\u63ed\u793a\u7269\u7406\u7cfb\u7edf\u4e2d\u8ba1\u7b97\u80fd\u529b\u7684\u901a\u7528\u673a\u5236\u3002", "method": "\u4f7f\u7528\u5916\u90e8\u9a71\u52a8\u7684\u6a21\u62df\u7cfb\u7edf\u63a8\u65ad\u6df7\u6c8c\u4fe1\u53f7\u672a\u6765\u72b6\u6001\uff0c\u5206\u6790\u52a8\u6001\u653e\u677e\u80fd\u529b\u5bf9\u5176\u4fe1\u606f\u5904\u7406\u6548\u7387\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u6700\u4f18\u8ba1\u7b97\u673a\u5236\u4f4d\u4e8e\u4e34\u754c\u963b\u5c3c\u9608\u503c\u4ee5\u4e0b\uff0c\u6d89\u53ca\u591a\u9636\u6bb5\u5fae\u89c2\u52a8\u6001\u653e\u677e\uff0c\u4e14\u80fd\u9002\u5e94\u6df7\u6c8c\u9a71\u52a8\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u7406\u89e3\u975e\u5e73\u8861\u591a\u4f53\u7269\u7406\u4e2d\u7684\u5b66\u4e60\u548c\u975e\u5e38\u89c4\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2505.05471", "pdf": "https://arxiv.org/pdf/2505.05471", "abs": "https://arxiv.org/abs/2505.05471", "authors": ["Jarren Briscoe", "Assefaw Gebremedhin"], "title": "Facets of Disparate Impact: Evaluating Legally Consistent Bias in Machine Learning", "categories": ["cs.CY", "cs.LG"], "comment": "CIKM 2024", "summary": "Leveraging current legal standards, we define bias through the lens of\nmarginal benefits and objective testing with the novel metric \"Objective\nFairness Index\". This index combines the contextual nuances of objective\ntesting with metric stability, providing a legally consistent and reliable\nmeasure. Utilizing the Objective Fairness Index, we provide fresh insights into\nsensitive machine learning applications, such as COMPAS (recidivism\nprediction), highlighting the metric's practical and theoretical significance.\nThe Objective Fairness Index allows one to differentiate between discriminatory\ntests and systemic disparities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u2018\u5ba2\u89c2\u516c\u5e73\u6307\u6570\u2019\u7684\u65b0\u6307\u6807\uff0c\u7ed3\u5408\u76ee\u6807\u6d4b\u8bd5\u548c\u7a33\u5b9a\u6027\uff0c\u7528\u4e8e\u8861\u91cf\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u504f\u89c1\uff0c\u5e76\u5728COMPAS\u7b49\u654f\u611f\u5e94\u7528\u4e2d\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u6cd5\u5f8b\u6807\u51c6\u4e0b\u7f3a\u4e4f\u7edf\u4e00\u7684\u504f\u89c1\u8861\u91cf\u65b9\u6cd5\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u53cd\u6620\u4e0a\u4e0b\u6587\u53c8\u80fd\u4fdd\u6301\u7a33\u5b9a\u6027\u7684\u6307\u6807\u3002", "method": "\u57fa\u4e8e\u8fb9\u9645\u6548\u76ca\u548c\u5ba2\u89c2\u6d4b\u8bd5\uff0c\u5b9a\u4e49\u5e76\u5f00\u53d1\u4e86\u2018\u5ba2\u89c2\u516c\u5e73\u6307\u6570\u2019\uff0c\u7528\u4e8e\u533a\u5206\u6b67\u89c6\u6027\u6d4b\u8bd5\u548c\u7cfb\u7edf\u6027\u5dee\u5f02\u3002", "result": "\u65b0\u6307\u6807\u5728COMPAS\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5177\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002", "conclusion": "\u2018\u5ba2\u89c2\u516c\u5e73\u6307\u6570\u2019\u4e3a\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u516c\u5e73\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6cd5\u5f8b\u4e00\u81f4\u4e14\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
