<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 103]
- [cs.LG](#cs.LG) [Total: 53]
- [cs.AI](#cs.AI) [Total: 17]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.RO](#cs.RO) [Total: 6]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.CV](#cs.CV) [Total: 26]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [stat.ML](#stat.ML) [Total: 9]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.CE](#cs.CE) [Total: 2]
- [stat.OT](#stat.OT) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Efficient Multilingual ASR Finetuning via LoRA Language Experts](https://arxiv.org/abs/2506.21555)
*Jiahong Li,Yiwen Shao,Jianheng Zhuo,Chenda Li,Liliang Tang,Dong Yu,Yanmin Qian*

Key words: 多语言ASR, LoRA, Whisper, 专家融合, 知识蒸馏

TL;DR: 本文提出了一种基于Whisper的LoRA语言专家框架，用于多语言ASR的微调，通过专家融合或知识蒸馏，显著提升了目标语言的识别性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 多语言ASR模型面临不同语言相互干扰的问题，导致模型难以有效识别多语言并共享模型能力。

Method: 采用基于Whisper的LoRA语言专家框架，通过专家融合或知识蒸馏进行微调。

Result: 实验结果表明，该方法在语言感知和无语言感知场景下分别实现了约10%和15%的相对性能提升。

Conclusion: 该框架显著提升了多语言ASR的性能，优于标准微调方法。

Abstract: Recent advancements in deep learning have significantly enhanced multilingual
automatic speech recognition (ASR) due to the development of advanced model
architectures and available large-scale multilingual datasets. Despite that,
multilingual ASR still suffers from the curse of multilinguality in that
different languages tend to interfere with each other, making it difficult for
the ASR model to identify multiple languages effectively while sharing model
capacity across them. This paper proposes an efficient finetuning framework for
customized multilingual ASR via prepared LoRA language experts based on
Whisper. Through LoRA expert fusion or knowledge distillation, our approach
achieves better recognition performance on target languages than standard
fine-tuning methods. Experimental results demonstrate that the proposed models
yield approximately 10\% and 15\% relative performance gains in language-aware
and language-agnostic scenarios, respectively.

</details>


### [2] [VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21556)
*Hyeongcheol Park,MinHyuk Jang,Ha Dam Baek,Gyusam Chang,Jiyoung Seo,Jiwan Park,Hogun Park,Sangpil Kim*

Key words: 多模态知识图谱, 视觉-听觉-文本, 跨模态对齐, RAG框架, MLLMs

TL;DR: 论文提出了首个覆盖视觉、听觉和文本的多模态知识图谱（VAT-KG），通过严格的跨模态对齐和过滤步骤自动构建，并引入新颖的多模态RAG框架，实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有MMKGs知识覆盖范围有限，模态支持不足，难以满足多模态任务的需求，尤其是在视频和音频等丰富模态的应用中。

Method: 提出VAT-KG，概念中心化和知识密集型的多模态知识图谱，通过严格过滤和对齐步骤实现跨模态知识对齐，并开发多模态RAG框架支持任意模态的查询。

Result: 实验证明VAT-KG在多模态问答任务中表现优异，有效支持MLLMs，提升多模态知识的统一与应用。

Conclusion: VAT-KG填补了现有MMKGs的不足，为多模态任务提供了更全面和实用的知识支持。

Abstract: Multimodal Knowledge Graphs (MMKGs), which represent explicit knowledge
across multiple modalities, play a pivotal role by complementing the implicit
knowledge of Multimodal Large Language Models (MLLMs) and enabling more
grounded reasoning via Retrieval Augmented Generation (RAG). However, existing
MMKGs are generally limited in scope: they are often constructed by augmenting
pre-existing knowledge graphs, which restricts their knowledge, resulting in
outdated or incomplete knowledge coverage, and they often support only a narrow
range of modalities, such as text and visual information. These limitations
reduce their extensibility and applicability to a broad range of multimodal
tasks, particularly as the field shifts toward richer modalities such as video
and audio in recent MLLMs. Therefore, we propose the Visual-Audio-Text
Knowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive
multimodal knowledge graph that covers visual, audio, and text information,
where each triplet is linked to multimodal data and enriched with detailed
descriptions of concepts. Specifically, our construction pipeline ensures
cross-modal knowledge alignment between multimodal data and fine-grained
semantics through a series of stringent filtering and alignment steps, enabling
the automatic generation of MMKGs from any multimodal dataset. We further
introduce a novel multimodal RAG framework that retrieves detailed
concept-level knowledge in response to queries from arbitrary modalities.
Experiments on question answering tasks across various modalities demonstrate
the effectiveness of VAT-KG in supporting MLLMs, highlighting its practical
value in unifying and leveraging multimodal knowledge.

</details>


### [3] [Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning](https://arxiv.org/abs/2506.21557)
*Kaiying Yan,Moyang Liu,Yukun Liu,Ruibo Fu,Zhengqi Wen,Jianhua Tao,Xuefei Liu*

Key words: 假新闻检测、多模态、条件扩散模型、大型语言模型、推理验证

TL;DR: 论文提出了一个名为DIFND的假新闻检测框架，结合条件扩散模型和多模态大型语言模型，通过生成反驳或认证证据和多代理推理系统，提高了检测的准确性和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 假新闻在多媒体平台上的快速传播对信息可信度构成严重威胁，需要一种既能提升检测性能又能增强解释性的方法。

Method: 采用Debunk-and-Infer框架，结合条件扩散模型生成多模态内容的反驳或认证证据，并利用多代理MLLM系统进行逻辑推理和最终判断。

Result: 在FakeSV和FVC数据集上的实验表明，DIFND在检测准确性和可信度上优于现有方法。

Conclusion: DIFND通过整合多模态特征、生成式反驳线索和推理验证，为假新闻检测提供了有效和可信的解决方案。

Abstract: The rapid spread of fake news across multimedia platforms presents serious
challenges to information credibility. In this paper, we propose a
Debunk-and-Infer framework for Fake News Detection(DIFND) that leverages
debunking knowledge to enhance both the performance and interpretability of
fake news detection. DIFND integrates the generative strength of conditional
diffusion models with the collaborative reasoning capabilities of multimodal
large language models (MLLMs). Specifically, debunk diffusion is employed to
generate refuting or authenticating evidence based on the multimodal content of
news videos, enriching the evaluation process with diverse yet semantically
aligned synthetic samples. To improve inference, we propose a chain-of-debunk
strategy where a multi-agent MLLM system produces logic-grounded,
multimodal-aware reasoning content and final veracity judgment. By jointly
modeling multimodal features, generative debunking cues, and reasoning-rich
verification within a unified architecture, DIFND achieves notable improvements
in detection accuracy. Extensive experiments on the FakeSV and FVC datasets
show that DIFND not only outperforms existing approaches but also delivers
trustworthy decisions.

</details>


### [4] [Bench to the Future: A Pastcasting Benchmark for Forecasting Agents](https://arxiv.org/abs/2506.21558)
*FutureSearch,:,Jack Wildman,Nikos I. Bosse,Daniel Hnyk,Peter Mühlbacher,Finn Hambly,Jon Evans,Dan Schwarz,Lawrence Phillips*

Key words: forecasting, benchmark, pastcasting, LLM, AI systems

TL;DR: 论文提出了一个名为BTF的“pastcasting”基准测试，用于在已知结果的过去事件上评估大语言模型的预测能力，提供了一个现实、封闭且可重复的环境。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 开发一个现实、封闭且可重复的基准测试环境，以解决现有预测基准测试在评估AI系统时的挑战。

Method: 基于已知结果的数百个高质量问题及其相关网页数据，构建了一个“pastcasting”基准测试BTF。

Result: 实验表明，BTF能够在过去事件上产生与实时未解决问题上的预测类似的结果，并展示了不同LLM的表现。

Conclusion: BTF是一个动态基准测试，可用于持续跟踪预测能力的进步，并邀请研究者使用。

Abstract: Forecasting is a challenging task that offers a clearly measurable way to
study AI systems. Forecasting requires a large amount of research on the
internet, and evaluations require time for events to happen, making the
development of forecasting benchmarks challenging. To date, no forecasting
benchmark provides a realistic, hermetic, and repeatable environment for LLM
forecasters. We introduce Bench To the Future (BTF), a "pastcasting" benchmark
with hundreds of high-quality questions for which the resolution is already
known. Each question is accompanied by a large offline corpus of tens of
thousands of relevant web pages, enabling a way to elicit realistic "forecasts"
on past events from LLMs. Results suggest that our pastcasting environment can
produce results comparable to those based on forecasts using the internet on
at-the-time unresolved questions. We show results benchmarking agent and
chain-of-thought forecasting approaches using several LLMs, including the
recently-released Claude 4 models, and demonstrate BTF's ability to track
steady forecasting capability progress over time. We intend this to be a living
benchmark, with new questions added continually to account for increasing
training data cutoff dates. We invite researchers to contact us at
hello@futuresearch.ai to utilize our benchmark or tooling for their own
research.

</details>


### [5] [GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations](https://arxiv.org/abs/2506.21559)
*Junze Chen,Cheng Yang,Shujie Li,Zhiqiang Zhang,Yawen Li,Junping Du,Chuan Shi*

Key words: 图语言模型, 参数适应, 少样本学习, 图神经网络

TL;DR: 本文提出了GraphLAMA方法，通过参数适应阶段高效调整图语言模型（GLM）以适应新任务，提升了预测准确性和推理速度。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有图语言模型（GLM）中，上下文学习（ICL）因固定参数和长上下文导致效率问题，而指令调优需要大量标注数据。为解决这些问题，作者提出了一种高效的参数适应方法。

Method: 提出的GraphLAMA方法结合了图神经网络（GNN）和语言模型，通过专门设计的模型结构和学习方案实现高效调优和推理。

Result: 实验表明，GraphLAMA在少样本/零样本节点分类和摘要生成任务中表现优异，准确率提升4.91%，推理速度比ICL快10倍。

Conclusion: GraphLAMA通过参数适应方法解决了GLM的效率和数据需求问题，显著提升了模型性能。

Abstract: Large language models (LLMs) have demonstrated their strong capabilities in
various domains, and have been recently integrated for graph analysis as graph
language models (GLMs). With LLMs as the predictor, some GLMs can interpret
unseen tasks described by natural language, and learn from a few examples in
the prompts without parameter tuning, known as in-context learning (ICL).
Another subset of GLMs utilizes abundant training labels to enhance model
performance, known as instruction tuning. However, we argue that ICL on graphs
has effectiveness issues due to fixed parameters and efficiency issues due to
long context. Meanwhile, the large amount of labeled data required for
instruction tuning can be difficult to obtain in real-world scenarios. To this
end, we aim to introduce an extra parameter adaptation stage that can
efficiently tailor GLMs to an unseen graph and task with only a few labeled
examples, in exchange for better prediction accuracy and faster inference
speed. For implementation, in this paper we propose GraphLAMA method, with its
model backbone and learning schemes specialized for efficient tuning and
inference. Specifically, for model backbone, we use a graph neural network
(GNN) with several well-designed components to transform nodes into the
representation space of LLM tokens. Task instructions can then be represented
as a mixture of node and language tokens. In the pre-training stage, model
parameters except the LLM will be trained with different tasks to capture
general knowledge. In the adaptation stage, only a few pre-trained parameters
will be updated based on few-shot examples. Extensive experiments on
few/zero-shot node classification and summary generation show that our proposed
GraphLAMA achieves state-of-the-art performance with 4.91% absolution
improvement in accuracy. Compared with ICL, our inference speed can be 10 times
faster under 5-shot setting.

</details>


### [6] [Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning](https://arxiv.org/abs/2506.21560)
*Yifu Han,Geo Zhang*

Key words: 强化学习, 小型语言模型, 指令跟随, 数学推理, 数据增强

TL;DR: 研究比较了不同强化学习微调技术在小型语言模型上的效果，RLOO和DPO表现优异，特别在数学任务中结合数据增强和外部验证器提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索如何高效地在小型语言模型中应用强化学习微调技术，以提升其在指令跟随和数学推理任务中的表现。

Method: 比较了监督微调（SFT）、直接偏好优化（DPO）和强化学习留一法（RLOO），并在数学任务中结合数据增强和外部验证器。

Result: RLOO结合DeBERTa奖励模型表现出最佳对齐效果，DPO结果稳定；数学任务中结合数据增强和验证器显著提升准确率。

Conclusion: 研究为小型语言模型的训练提供了实用策略，展示了微调技术与推理工具的潜力。

Abstract: This study investigates the effectiveness of reinforcement learning (RL)
fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two
challenging tasks: instruction following and mathematical reasoning. We compare
supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using
preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.
Our experiments show that RLOO with DeBERTa reward modeling achieves the best
alignment, while DPO provides strong and consistent results. For math reasoing
tasks, synthetic data augmentation and best-of-N sampling with an external
verifier significantly improve accuracy, showing the potential of combining
fine-tuning with inference-time tools. This study highlights key trade-offs and
practical strategies for training lightweight, task-aligned small-scale
language models.

</details>


### [7] [Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs](https://arxiv.org/abs/2506.21561)
*Emilio Barkett,Olivia Long,Madhavendra Thakur*

Key words: 大型语言模型、真相检测、推理模型、真值偏向、欺骗检测

TL;DR: 研究发现，虽然在事实核查和决策中广泛应用，大型语言模型（LLMs）作为真相判断工具仍存在不足。推理模型比非推理模型的“真值偏向”更低，但仍高于人类基准；某些先进模型（如o4-mini、GPT-4.1和R1）在识别欺骗时表现不佳，显示其能力提升并未解决根本问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨LLMs在真相检测方面的能力，尤其是推理模型的表现，揭示其在实际应用中的局限性和潜在问题。

Method: 通过让8个LLMs在不同提示下进行4,800次真值判断，比较推理模型与非推理模型的表现。

Result: 推理模型的“真值偏向”较低，但某些先进模型在检测欺骗时存在不对称性（如真值检测准确但欺骗检测不准确）。

Conclusion: LLMs的能力提升未能解决真相检测的根本挑战，需进一步研究改进。

Abstract: Despite their widespread use in fact-checking, moderation, and high-stakes
decision-making, large language models (LLMs) remain poorly understood as
judges of truth. This study presents the largest evaluation to date of LLMs'
veracity detection capabilities and the first analysis of these capabilities in
reasoning models. We had eight LLMs make 4,800 veracity judgments across
several prompts, comparing reasoning and non-reasoning models. We find that
rates of truth-bias, or the likelihood to believe a statement is true,
regardless of whether it is actually true, are lower in reasoning models than
in non-reasoning models, but still higher than human benchmarks. Most
concerning, we identify sycophantic tendencies in several advanced models
(o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an
asymmetry in detection accuracy, performing well in truth accuracy but poorly
in deception accuracy. This suggests that capability advances alone do not
resolve fundamental veracity detection challenges in LLMs.

</details>


### [8] [FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction](https://arxiv.org/abs/2506.21562)
*Jun Yin,Pengyu Zeng,Jing Zhong,Peilin Li,Miao Zhang,Ran Luo,Shuai Lu*

Key words: 建筑设计, 地板图生成, 自回归模型, FPDS

TL;DR: 提出了一种基于‘下一个房间预测’的渐进式生成方法FPDS，与传统单次生成的地平面生成模型相比，更符合实际建筑设计中的迭代工作流程，并在文本到平面任务中表现出色。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有地板图生成模型多采用端到端单次生成方式，与实际建筑设计的渐进迭代流程不符，本研究旨在解决这一问题。

Method: 借鉴大型语言模型中的自回归‘下一个令牌预测’机制，提出‘下一个房间预测’范式，开发FPDS模型。

Result: FPDS在文本到平面任务中表现优于扩散模型和Tell2Design，具有潜在的实际应用价值。

Conclusion: FPDS的渐进生成方法更适合建筑设计工作流程，为智能建筑设计提供了新的可能性。

Abstract: In the architectural design process, floor plan generation is inherently
progressive and iterative. However, existing generative models for floor plans
are predominantly end-to-end generation that produce an entire pixel-based
layout in a single pass. This paradigm is often incompatible with the
incremental workflows observed in real-world architectural practice. To address
this issue, we draw inspiration from the autoregressive 'next token prediction'
mechanism commonly used in large language models, and propose a novel 'next
room prediction' paradigm tailored to architectural floor plan modeling.
Experimental evaluation indicates that FPDS demonstrates competitive
performance in comparison to diffusion models and Tell2Design in the
text-to-floorplan task, indicating its potential applicability in supporting
future intelligent architectural design.

</details>


### [9] [FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models](https://arxiv.org/abs/2506.21563)
*Kaiying Kevin Lin,Hsiyu Chen,Haopeng Zhang*

Key words: FORMOSANBENCH、LLM、低资源语言、Formosan语言、机器翻译、语音识别、文本摘要

TL;DR: 论文介绍了首个评估大型语言模型（LLM）在低资源南岛语言（台湾原住民语言）性能的基准测试FORMOSANBENCH，并揭示了LLM在这些语言任务上的显著不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 高资源语言的LLM表现优秀，但在台湾濒危原住民语言等低资源语言的研究仍不足，急需开发更具包容性的NLP技术。

Method: 通过FORMOSANBENCH基准测试，评估了Atayal、Amis和Paiwan三种语言在机器翻译、语音识别和文本摘要任务中的表现，比较了零样本学习、10样本学习和微调的效果。

Result: LLM在Formosan语言任务中表现不佳，10样本学习和微调效果有限，高资源与低资源语言间存在显著性能差距。

Conclusion: 研究呼吁开发更有效支持濒危和低资源语言的NLP技术，并公开了数据集和代码以促进相关研究。

Abstract: While large language models (LLMs) have demonstrated impressive performance
across a wide range of natural language processing (NLP) tasks in high-resource
languages, their capabilities in low-resource and minority languages remain
significantly underexplored. Formosan languages -- a subgroup of Austronesian
languages spoken in Taiwan -- are both linguistically rich and endangered,
largely due to the sociolinguistic dominance of Mandarin. In this work, we
introduce FORMOSANBENCH, the first benchmark for evaluating LLMs on
low-resource Austronesian languages. It covers three endangered Formosan
languages: Atayal, Amis, and Paiwan, across three core NLP tasks: machine
translation, automatic speech recognition (ASR), and text summarization. We
assess model performance in zero-shot, 10-shot, and fine-tuned settings using
FORMOSANBENCH. Our results reveal a substantial performance gap between
high-resource and Formosan languages. Existing LLMs consistently underperform
across all tasks, with 10-shot learning and fine-tuning offering only limited
improvements. These findings underscore the urgent need for more inclusive NLP
technologies that can effectively support endangered and underrepresented
languages. We release our datasets and code to facilitate future research in
this direction.

</details>


### [10] [Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing](https://arxiv.org/abs/2506.21564)
*Jiyan Liu,Youzheng Liu,Taihang Wang,Xiaoman Xu,Yimin Wang,Ye Jiang*

Key words: 事实核查, 检索模型, 重排序, 加权投票, SemEval

TL;DR: 论文介绍了QUST_NLP在SemEval-2025 Task 7中的参与情况，提出了一个三阶段的检索框架用于事实核查声明检索。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 目标是提升事实核查声明的检索效果，通过多阶段优化提高检索准确性和效率。

Method: 三阶段框架：1）评估多种检索模型并选择最佳候选检索模型；2）使用多个重排序模型优化候选结果；3）加权投票确定最终结果。

Result: 在单语和跨语任务中分别获得第5和第7名。

Conclusion: 提出的三阶段检索框架能有效提升事实核查声明的检索性能。

Abstract: This paper describes the participation of QUST_NLP in the SemEval-2025 Task
7. We propose a three-stage retrieval framework specifically designed for
fact-checked claim retrieval. Initially, we evaluate the performance of several
retrieval models and select the one that yields the best results for candidate
retrieval. Next, we employ multiple re-ranking models to enhance the candidate
results, with each model selecting the Top-10 outcomes. In the final stage, we
utilize weighted voting to determine the final retrieval outcomes. Our approach
achieved 5th place in the monolingual track and 7th place in the crosslingual
track. We release our system code at:
https://github.com/warmth27/SemEval2025_Task7.

</details>


### [11] [A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing](https://arxiv.org/abs/2506.21565)
*Takato Ueno,Keito Inoshita*

Key words: kairanban文化, idobata对话, 多Agent推理框架, 大型语言模型, 偏见减轻, 情感分析

TL;DR: 该研究提出了一个多Agent推理框架（KCS+IBC），灵感来源于日本的kairanban文化和idobata对话，以整合多个大型语言模型（LLM）实现偏见减轻、可解释性改进和情感分析的概率预测。实验表明，KCS在准确性上与单一LLM相当，而KCS+IBC在推理后期表现出熵的持续下降和方差逐步增加，平衡了预测的聚合与多样性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 受到日本kairanban文化和idobata对话启发，研究者希望利用这些传统交流方式来改进情感分析中的偏见减轻和可解释性，同时引入概率预测。

Method: 提出了一个多Agent推理框架（KCS+IBC），通过整合多个LLM，并引入中期非正式对话环节，结合正式推理与个人观点，同时实现概率情感预测。

Result: KCS在准确性上与单一LLM相当；KCS+IBC在推理后期表现出熵的下降和方差的增加，平衡了预测的聚合与多样性。

Conclusion: 该框架展现了平衡预测聚合与多样性的能力，未来工作将进一步量化其对偏见纠正的影响，并开发更先进的情感分析系统。

Abstract: Japan's kairanban culture and idobata conversations have long functioned as
traditional communication practices that foster nuanced dialogue among
community members and contribute to the formation of social balance. Inspired
by these information exchange processes, this study proposes a multi-agent
inference framework (KCS+IBC) that integrates multiple large language models
(LLMs) to achieve bias mitigation, improved explainability, and probabilistic
prediction in sentiment analysis. In addition to sequentially sharing
prediction results, the proposed method incorporates a mid-phase casual
dialogue session to blend formal inference with individual perspectives and
introduces probabilistic sentiment prediction. Experimental results show that
KCS achieves accuracy comparable to that of a single LLM across datasets, while
KCS+IBC exhibits a consistent decrease in entropy and a gradual increase in
variance during the latter stages of inference, suggesting the framework's
ability to balance aggregation and diversity of predictions. Future work will
quantitatively assess the impact of these characteristics on bias correction
and aim to develop more advanced sentiment analysis systems.

</details>


### [12] [The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation](https://arxiv.org/abs/2506.21566)
*Arwa Arif*

Key words: 回译, 低资源机器翻译, 古吉拉特语, MBART50, BLEU

TL;DR: 研究探讨了回译（BT）在英语-古吉拉特语低资源机器翻译（MT）中的效果，发现即使在高质量平行语料库基础上添加回译数据，性能未提升甚至略有下降，表明回译在某些低资源场景中可能存在收益递减。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管回译广泛用于低资源机器翻译以生成合成数据，但其在高质量低资源环境中的效果尚不明确。作者希望通过英语-古吉拉特语翻译实验验证回译的实际效果。

Method: 使用MBART50预训练模型，基于约50,000句高质量平行语料库训练基线系统，并添加经过筛选的回译数据（来自单语古吉拉特语文本）。

Result: 添加回译数据后，翻译性能未提升，部分情况下甚至略微下降。通过BLEU、ChrF++、TER和BLEURT等多指标验证了这一现象。

Conclusion: 回译在某些低资源环境中可能达到收益递减点，需进一步研究优化策略。

Abstract: Backtranslation BT is widely used in low resource machine translation MT to
generate additional synthetic training data using monolingual corpora. While
this approach has shown strong improvements for many language pairs, its
effectiveness in high quality, low resource settings remains unclear. In this
work, we explore the effectiveness of backtranslation for English Gujarati
translation using the multilingual pretrained MBART50 model. Our baseline
system, trained on a high quality parallel corpus of approximately 50,000
sentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment
this data with carefully filtered backtranslated examples generated from
monolingual Gujarati text. Surprisingly, adding this synthetic data does not
improve translation performance and, in some cases, slightly reduces it. We
evaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and
analyze possible reasons for this saturation. Our findings suggest that
backtranslation may reach a point of diminishing returns in certain
low-resource settings and we discuss implications for future research.

</details>


### [13] [BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining](https://arxiv.org/abs/2506.21567)
*Baqer M. Merzah,Tania Taami,Salman Asoudeh,Amir reza Hossein pour,Saeed Mirzaee,Amir Ali Bengari*

Key words: 大型语言模型,生物信息学,波斯医学问答,BioPars

TL;DR: 论文介绍了BioPars，一种用于评估大型语言模型在生物信息学任务中的能力的工具，并展示了其在波斯医学问答中的首次应用，取得了显著成果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探究大型语言模型在生物信息学中的潜力，特别是在波斯医学问答领域的应用，并评估其能力。

Method: 开发了BIOPARS-BENCH数据集和BioParsQA评估工具，设计了BioPars来衡量模型在专业知识获取、知识解释与合成等方面的能力。

Result: BioPars在多个评测指标（如ROUGE-L、BERTScore、MoverScore和BLEURT）上优于其他模型，如GPT-4 1.0。

Conclusion: 研究表明大型语言模型在生物信息学中具有潜力，但仍需进一步优化以应对更复杂的实际任务。

Abstract: Large Language Models (LLMs) have recently gained attention in the life
sciences due to their capacity to model, extract, and apply complex biological
information. Beyond their classical use as chatbots, these systems are
increasingly used for complex analysis and problem-solving in specialized
fields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset
from over 10,000 scientific articles, textbooks, and medical websites.
BioParsQA was also introduced to evaluate the proposed model, which consists of
5,231 Persian medical questions and answers. This study then introduces
BioPars, a simple but accurate measure designed to assess LLMs for three main
abilities: acquiring subject-specific knowledge, interpreting and synthesizing
such knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,
and Galactica, our study highlights their ability to remember and retrieve
learned knowledge but also reveals shortcomings in addressing higher-level,
real-world questions and fine-grained inferences. These findings indicate the
need for further fine-tuning to address the capabilities of LLM in
bioinformatics tasks. To our knowledge, BioPars is the first application of LLM
in Persian medical QA, especially for generating long answers. Evaluation of
four selected medical QA datasets shows that BioPars has achieved remarkable
results compared to comparative approaches. The model on BioParsQA achieved a
ROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model
achieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT
values were also higher in this model than the other three models. In addition,
the reported scores for the model are MoverScore=60.43 and BLEURT=50.78.
BioPars is an ongoing project and all resources related to its development will
be made available via the following GitHub repository:
https://github.com/amirap80/BioPars.

</details>


### [14] [Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants Integretion](https://arxiv.org/abs/2506.21568)
*Andrejs Sorstkins*

Key words: Resource efficiency, LLMs, RAG, HyDE, privacy-first

TL;DR: 该研究评估了两种增强策略（RAG和HyDE）在小型Gemma LLM上的效果，发现RAG能显著降低延迟并减少幻觉，而HyDE则提升语义相关性但增加计算开销。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决边缘计算和隐私敏感应用中部署大型语言模型的资源效率问题。

Method: 通过MongoDB实现短期记忆，Qdrant实现长期语义存储，FastAPI和LangChain协调，React.js前端展示。

Result: RAG减少延迟17%且消除幻觉，HyDE提升语义相关性但增加响应时间25-40%。

Conclusion: RAG是小型LLM驱动的个人助手的实用选择。

Abstract: Resource efficiency is a critical barrier to deploying large language models
(LLMs) in edge and privacy-sensitive applications. This study evaluates the
efficacy of two augmentation strategies--Retrieval-Augmented Generation (RAG)
and Hypothetical Document Embeddings (HyDE)--on compact Gemma LLMs of 1 billion
and 4 billion parameters, within the context of a privacy-first personal
assistant. We implement short-term memory via MongoDB and long-term semantic
storage via Qdrant, orchestrated through FastAPI and LangChain, and expose the
system through a React.js frontend. Across both model scales, RAG consistently
reduces latency by up to 17\% and eliminates factual hallucinations when
responding to user-specific and domain-specific queries. HyDE, by contrast,
enhances semantic relevance--particularly for complex physics prompts--but
incurs a 25--40\% increase in response time and a non-negligible hallucination
rate in personal-data retrieval. Comparing 1 B to 4 B models, we observe that
scaling yields marginal throughput gains for baseline and RAG pipelines, but
magnifies HyDE's computational overhead and variability. Our findings position
RAG as the pragmatic choice for on-device personal assistants powered by
small-scale LLMs.

</details>


### [15] [Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA](https://arxiv.org/abs/2506.21569)
*Weihua Xiao,Derek Ekberg,Siddharth Garg,Ramesh Karri*

Key words: SystemVerilog Assertions, NL2SVA, 大语言模型, RAG框架, 微调数据集

TL;DR: 论文提出了一种定制化的RAG框架和合成微调数据集，以提高LLM在NL2SVA任务中的性能，实验表明其显著提升了功能匹配SVA的数量。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 手动编写SystemVerilog Assertions (SVAs)耗时且易错，因此希望通过大语言模型(LLM)自动化翻译自然语言属性描述，但现有模型在领域特定语法和语义理解上仍有不足。

Method: 采用定制化RAG框架和合成微调数据集，通过提示引导的解释教会LLM逐层构建并发SVAs，并进行监督微调以提高语法和功能准确性。

Result: 实验表明，定制RAG框架使功能匹配SVA数量较GPT-4o-mini提高了58.42%，同时微调后的Qwen2.5-Coder-7B-Instruct结合HybridRetrieval提升了59.05%。

Conclusion: 提出的方法有效提升了LLM在NL2SVA任务中的性能，为硬件设计验证提供了更高效的自动化解决方案。

Abstract: SystemVerilog Assertions (SVAs) are critical for verifying the correctness of
hardware designs, but manually writing them from natural language property
descriptions, i.e., NL2SVA, remains a labor-intensive and error-prone task.
Recent advances in large language models (LLMs) offer opportunities to automate
this translation. However, existing models still struggle with understanding
domain-specific syntax and semantics. To enhance LLM performance in NL2SVA, we
propose a customized retrieval-augmented generation (RAG) framework and a
synthetic fine-tuning dataset that together improve LLM's performance. To
further improve lightweight models over NL2SVA, our fine-tuning dataset
provides prompt-guided explanations that teach LLMs the layer-by-layer
construction process of concurrent SVAs, enabling supervised fine-tuning that
greatly improves syntax and functionality accuracy. To evaluate the performance
of LLMs over NL2SVA, we construct the largest evaluation dataset for NL2SVA,
comprising 40 Verilog designs and 229 formally verified SVAs with detailed
annotations. Experimental results show that our customized RAG framework
increases the number of functionality matched SVAs by 58.42% over GPT-4o-mini,
while Qwen2.5-Coder-7B-Instruct fine-tuned on our fine-tuning dataset and
integrated with HybridRetrieval achieves a 59.05% over the base Qwen model.

</details>


### [16] [Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting](https://arxiv.org/abs/2506.21570)
*Roland Riachi,Kashif Rasul,Arjun Ashok,Prateek Humane,Alexis Roger,Andrew R. Williams,Yuriy Nevmyvaka,Irina Rish*

Key words: 预训练语言模型,时间序列预测,低数据量,迁移学习

TL;DR: 研究了预训练语言模型在低数据量时间序列预测中的有效迁移，分析了上游后训练、时间序列分词器和语言模型规模等设计选择的影响。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索预训练语言模型在时间序列预测中的迁移效果，改进低数据量下的预测性能。

Method: 分析不同的设计选择（如上游后训练、时间序列分词器和语言模型规模）对验证损失的影响。

Result: 在低数据量下，某些设计选择显著优于其他选项，且预训练语言模型的验证损失持续下降，表现出非零迁移差距。

Conclusion: 研究揭示了计算高效训练在时间序列预测中的有效应用，并为跨模态数据分布研究开辟了道路。

Abstract: Recent works have demonstrated the effectiveness of adapting pre-trained
language models (LMs) for forecasting time series in the low-data regime. We
build upon these findings by analyzing the effective transfer from language
models to time series forecasting under various design choices including
upstream post-training, time series tokenizer and language backbone size. In
the low-data regime, these design choices have a significant impact on the
validation loss, with clear-cut choices that outperform others. Contrary to
Hernandez et al. (2021), we observe that the validation loss of the LMs
continues to smoothly decrease long after the validation loss of the randomly
initialized models has converged, leading to a non-vanishing transfer gap that
holds across design choices. These findings not only help shed light on the
effective use of compute-efficient training for time series, but also open the
way for the study of modality-agnostic properties of data distributions
leveraged by these models.

</details>


### [17] [Towards Understanding the Cognitive Habits of Large Reasoning Models](https://arxiv.org/abs/2506.21571)
*Jianshuo Dong,Yujia Fu,Chuanrui Hu,Chao Zhang,Han Qiu*

Key words: Large Reasoning Models, CogTest, cognitive habits, LLM behavior, safety

TL;DR: 该论文研究了大型推理模型（LRMs）是否表现出类似人类的认知习惯，并通过CogTest基准评估了16种LRMs。结果表明，LRMs不仅具有人类习惯，还能根据任务调整行为，且某些习惯与安全风险相关。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨LRMs是否展现出类似人类的认知习惯，以更好地理解和监控模型行为。

Method: 引入CogTest基准，评估16种认知习惯在25种任务中的表现，采用证据优先的提取方法。

Result: LRMs表现出人类习惯并适应任务调整，某些习惯与安全风险相关。

Conclusion: 研究LRMs的持续行为模式有助于深入理解模型行为异常。

Abstract: Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain
of Thought (CoT) before producing final responses, offer a promising approach
to interpreting and monitoring model behaviors. Inspired by the observation
that certain CoT patterns -- e.g., ``Wait, did I miss anything?'' --
consistently emerge across tasks, we explore whether LRMs exhibit human-like
cognitive habits. Building on Habits of Mind, a well-established framework of
cognitive habits associated with successful human problem-solving, we introduce
CogTest, a principled benchmark designed to evaluate LRMs' cognitive habits.
CogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks,
and employs an evidence-first extraction method to ensure reliable habit
identification. With CogTest, we conduct a comprehensive evaluation of 16
widely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that
LRMs, unlike conventional LLMs, not only exhibit human-like habits but also
adaptively deploy them according to different tasks. Finer-grained analyses
further uncover patterns of similarity and difference in LRMs' cognitive habit
profiles, particularly certain inter-family similarity (e.g., Qwen-3 models and
DeepSeek-R1). Extending the study to safety-related tasks, we observe that
certain habits, such as Taking Responsible Risks, are strongly associated with
the generation of harmful responses. These findings suggest that studying
persistent behavioral patterns in LRMs' CoTs is a valuable step toward deeper
understanding of LLM misbehavior. The code is available at:
https://github.com/jianshuod/CogTest.

</details>


### [18] [Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling](https://arxiv.org/abs/2506.21572)
*Tianyu. Zou,Shengwu. Xiong,Ruilin. Yao,Jirui. Huang,Yi. Rong,Yaxiong. Chen,Shili. Xiong,Cong. Wang*

Key words: 多模态大语言模型, 结构方程模型, 认知发展理论, 基准设计

TL;DR: 提出了基于结构方程模型（SEM）的多模态大语言模型（MLLM）评估框架，解决了现有基准设计缺乏结构性和诊断力的问题。通过皮亚杰认知发展理论分层能力，构建了新基准Gold，实验表明其解释性更强、冗余更低。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有MLLM基准设计缺乏结构化、理论基础和诊断力，导致能力重叠和冗余。

Method: 提出基于SEM的框架，将MLLM能力分为感知、记忆和推理三层，并构建新基准Gold。

Result: Gold基准在解释性、指标冗余和认知一致性上优于现有方法。

Conclusion: 新框架和基准有效提升了MLLM评估的准确性和诊断力。

Abstract: Evaluating multimodal large language models (MLLMs) remains a fundamental
challenge due to a lack of structured, interpretable, and theoretically
grounded benchmark designs. Existing benchmarks often adopt heuristic-based
task groupings with unclear cognitive targets, thus resulting in overlapping
abilities, redundant indicators, and limited diagnostic power. In this work, we
propose a novel framework for aligning MLLM benchmark based on Structural
Equation Modeling (SEM) to analyze and quantify the internal validity,
dimensional separability, and contribution of benchmark components. Motivated
by the observed limitations of current designs, we further introduce a novel
capability hierarchy grounded in Piagets theory of cognitive development,
dividing MLLM abilities into three hierarchical layers, i.e., Perception,
Memory, and Reasoning. We reorganize existing MLLM benchmarks under the
proposed framework and construct a new benchmark named Gold. Experimental
results demonstrate that the proposed benchmark exhibits stronger
interpretability, reduced indicator redundancy, and clearer cognitive
consistency compared to existing approaches.

</details>


### [19] [Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs](https://arxiv.org/abs/2506.21573)
*Yanwei Ren,Liu Liu,Baosheng Yu,Jiayan Qiu,Quan Chen*

Key words: 大语言模型, 指令优化, 黑盒白盒融合, 语义相似性, 迭代优化

TL;DR: 提出了一种融合黑盒和白盒模型优势的新框架，用于优化大语言模型的指令，实现高质量和高效的语义表示与优化。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 单纯依赖白盒方法计算资源需求大且表达能力有限，而黑盒模型则成本高昂。需要一种结合两者优势的方案来优化大语言模型的指令。

Method: 通过黑盒模型提供高质量多样化指令初始化，白盒模型提供细粒度可解释性，并利用语义相似性约束将两者融合为一个高维表示，进行迭代优化。

Result: 在复杂推理和跨语言泛化等任务上，该方法持续优于当前最优基线。

Conclusion: 该方法为下一代大语言模型应用提供了可扩展且高效的解决方案。

Abstract: Optimizing instructions for large language models (LLMs) is critical for
harnessing their full potential in complex and diverse tasks. However, relying
solely on white-box approaches demands extensive computational resources and
offers limited representational capacity, while black-box models can incur
prohibitive financial costs. To address these challenges, we introduce a novel
framework that seamlessly merges the strengths of both paradigms. Black-box
models provide high-quality, diverse instruction initializations, and white-box
models supply fine-grained interpretability through hidden states and output
features. By enforcing a semantic similarity constraint, these components fuse
into a unified high-dimensional representation that captures deep semantic and
structural nuances, enabling an iterative optimization process to refine
instruction quality and adaptability. Extensive evaluations across a broad
spectrum of tasks-ranging from complex reasoning to cross-lingual
generalization-demonstrate that our approach consistently outperforms
state-of-the-art baselines. This fusion of black-box initialization with
advanced semantic refinement yields a scalable and efficient solution, paving
the way for next-generation LLM-driven applications in diverse real-world
scenarios. The source code will be released soon.

</details>


### [20] [Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions](https://arxiv.org/abs/2506.21574)
*Yicheng Mao,Yang Zhao*

Key words: 大型语言模型,移民决策,公平性,GPT-3.5,GPT-4

TL;DR: 本研究探讨了大型语言模型（如GPT-3.5和GPT-4）在支持移民决策中的潜力，发现其能部分模仿人类决策策略，但也存在国籍偏见和特权群体偏好的问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 全球化和移民人口增加使移民部门面临巨大工作量及公平决策的挑战，人工智能的集成可能为解决这些问题提供方案。

Method: 采用混合方法，包括离散选择实验和深度访谈，研究LLM的决策策略及其公平性。

Result: LLMs能部分模仿人类决策策略，注重效用最大化和程序公平，但仍存在国籍偏见和特权群体偏好。

Conclusion: LLMs在自动化和增强移民决策方面既具潜力，又存在局限。

Abstract: With globalization and increasing immigrant populations, immigration
departments face significant work-loads and the challenge of ensuring fairness
in decision-making processes. Integrating artificial intelligence offers a
promising solution to these challenges. This study investigates the potential
of large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting
immigration decision-making. Utilizing a mixed-methods approach,this paper
conducted discrete choice experiments and in-depth interviews to study LLM
decision-making strategies and whether they are fair. Our findings demonstrate
that LLMs can align their decision-making with human strategies, emphasizing
utility maximization and procedural fairness. Meanwhile, this paper also
reveals that while ChatGPT has safeguards to prevent unintentional
discrimination, it still exhibits stereotypes and biases concerning nationality
and shows preferences toward privileged group. This dual analysis highlights
both the potential and limitations of LLMs in automating and enhancing
immigration decisions.

</details>


### [21] [STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing](https://arxiv.org/abs/2506.21575)
*Josefa Lia Stoisser,Marc Boubnovski Martell,Lawrence Phillips,Casper Hansen,Julien Fauqueur*

Key words: STRuCT-LLM, 结构化推理, Text-to-SQL, Text-to-Cypher, 强化学习, 跨形式迁移

TL;DR: STRuCT-LLM是一个统一的框架，用于训练大型语言模型（LLMs）在关系型和图结构数据上进行结构化推理。它结合强化学习和Chain-of-Thought监督，联合优化Text-to-SQL和Text-to-Cypher任务，并通过共享抽象实现跨形式迁移。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的方法在处理关系型和图结构数据时往往孤立对待，缺乏统一的框架。作者希望通过联合训练SQL和Cypher任务，实现跨形式迁移，提升模型在结构化推理任务上的表现。

Method: 采用强化学习（RL）与Chain-of-Thought（CoT）监督结合的方法，引入了基于图编辑距离的拓扑感知奖励函数，以支持细粒度的图解析优化。模型通过共享SQL和Cypher的抽象实现跨形式迁移。

Result: STRuCT-LLM在多个任务上实现了显著提升：Spider任务提升13.5%，Text2Cypher任务提升73.1%。在零样本泛化任务中，TableBench提升8.5%，CR-LT-KGQA提升1.7%。

Conclusion: 实验证明了可执行查询作为结构化推理支撑的有效性，以及SQL和Cypher联合训练的协同优势。

Abstract: We propose STRuCT-LLM, a unified framework for training large language models
(LLMs) to perform structured reasoning over both relational and
graph-structured data. Our approach jointly optimizes Text-to-SQL and
Text-to-Cypher tasks using reinforcement learning (RL) combined with
Chain-of-Thought (CoT) supervision. To support fine-grained optimization in
graph-based parsing, we introduce a topology-aware reward function based on
graph edit distance. Unlike prior work that treats relational and graph
formalisms in isolation, STRuCT-LLM leverages shared abstractions between SQL
and Cypher to induce cross-formalism transfer, enabling SQL training to improve
Cypher performance and vice versa - even without shared schemas. Our largest
model (QwQ-32B) achieves substantial relative improvements across tasks: on
semantic parsing, Spider improves by 13.5\% and Text2Cypher by 73.1\%. The
model also demonstrates strong zero-shot generalization, improving performance
on downstream tabular QA (TableBench: 8.5\%) and knowledge graph QA
(CR-LT-KGQA: 1.7\%) without any QA-specific supervision. These results
demonstrate both the effectiveness of executable queries as scaffolds for
structured reasoning and the synergistic benefits of jointly training on SQL
and Cypher (code available at https://github.com/bouv/STRuCT-LLM).

</details>


### [22] [Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning](https://arxiv.org/abs/2506.21576)
*Hongli Yang,Yizhou Peng,Hao Huang,Sheng Li*

Key words: ASR, 多语言, 软提示调优, 代码切换, 低资源

TL;DR: 该论文探讨了软提示调优（SPT）在低资源场景下提升多语言ASR性能的方法，并提出了SPT4ASR组合策略，实验证明其有效性和参数效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大规模多语言ASR模型在高资源场景表现优异，但在低资源（如稀有语言和代码切换）场景中面临计算成本和灾难性遗忘问题。因此，研究如何通过参数高效的方法提升模型性能，同时保留先验知识。

Method: 研究了两种策略：(1) 对整个Whisper模型和软提示进行全微调（FFT），(2) 冻结模型参数，仅训练软提示（原始SPT设计）。此外，提出了SPT4ASR，结合多种SPT变体。

Result: 实验表明，深度提示调优是最有效的SPT方法，SPT4ASR进一步降低了代码切换ASR的错误率，同时保持与LoRA相似的参数效率，且不影响现有语言的性能。

Conclusion: SPT和SPT4ASR能够在低资源场景中有效提升ASR性能，同时避免灾难性遗忘，为多语言ASR的优化提供了新思路。

Abstract: Large-scale multilingual ASR models like Whisper excel in high-resource
settings but face challenges in low-resource scenarios, such as rare languages
and code-switching (CS), due to computational costs and catastrophic
forgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method
to enhance CS ASR while preserving prior knowledge. We evaluate two strategies:
(1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model,
demonstrating improved cross-lingual capabilities compared to traditional
methods, and (2) adhering to SPT's original design by freezing model parameters
and only training soft prompts. Additionally, we introduce SPT4ASR, a
combination of different SPT variants. Experiments on the SEAME and ASRU2019
datasets show that deep prompt tuning is the most effective SPT approach, and
our SPT4ASR methods achieve further error reductions in CS ASR, maintaining
parameter efficiency similar to LoRA, without degrading performance on existing
languages.

</details>


### [23] [Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR](https://arxiv.org/abs/2506.21577)
*Hongli Yang,Sheng Li,Hao Huang,Ayiduosi Tuohan,Yizhou Peng*

Key words: 多语言ASR, 软提示调优, Whisper, 语言扩展

TL;DR: 本文提出了Entire SPT和LAPT两种方法，通过软提示调优技术提升多语言ASR性能，并在语言扩展任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为解决多语言ASR中的语言干扰和扩展问题，提升模型性能。

Method: 1) Entire Soft Prompt Tuning (Entire SPT) 对编码器和解码器应用软提示；2) Language-Aware Prompt Tuning (LAPT) 利用跨语言相似性编码共享及语言特定特征；3) SPT-Whisper工具包整合SPT到Whisper中。

Result: 在FLEURS的三个语言上，Entire SPT和LAPT分别比Decoder SPT提升5.0%和16.0%。

Conclusion: Entire SPT和LAPT为动态多语言ASR模型提供了高效解决方案，计算开销低。

Abstract: Recent advancements in multilingual automatic speech recognition (ASR) have
been driven by large-scale end-to-end models like Whisper. However, challenges
such as language interference and expanding to unseen languages (language
expansion) without degrading performance persist. This paper addresses these
with three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which
applies soft prompts to both the encoder and decoder, enhancing feature
extraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which
leverages cross-lingual similarities to encode shared and language-specific
features using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that
integrates SPT into Whisper and enables efficient continual learning.
Experiments across three languages from FLEURS demonstrate that Entire SPT and
LAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks,
respectively, providing an efficient solution for dynamic, multilingual ASR
models with minimal computational overhead.

</details>


### [24] [MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark](https://arxiv.org/abs/2412.15194)
*Qihao Zhao,Yangyu Huang,Tengchao Lv,Lei Cui,Qinzheng Sun,Shaoguang Mao,Xin Zhang,Ying Xin,Qiufeng Yin,Scarlett Li,Furu Wei*

Key words: MMLU-CF, 数据污染, 多项选择题, 大型语言模型, 评估

TL;DR: 提出了一种无污染且更具挑战性的多项选择题基准MMLU-CF，用于可靠评估大型语言模型的能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于现有MCQ数据集存在数据污染问题，导致评估结果不可靠，亟需一个新的无污染基准。

Method: 通过设计三个去污染规则和分拆验证集与测试集的方式避免数据泄漏，测试集闭源以保证可靠性。

Result: 主流LLMs在MMLU-CF上的表现显著下滑，如GPT-4o的5-shot得分仅为73.4%。

Conclusion: MMLU-CF成功实现了无污染且更具挑战性的评估标准。

Abstract: Multiple-choice question (MCQ) datasets like Massive Multitask Language
Understanding (MMLU) are widely used to evaluate the commonsense,
understanding, and problem-solving abilities of large language models (LLMs).
However, the open-source nature of these benchmarks and the broad sources of
training data for LLMs have inevitably led to benchmark contamination,
resulting in unreliable evaluation results. To alleviate this issue, we propose
a contamination-free and more challenging MCQ benchmark called MMLU-CF. This
benchmark reassesses LLMs' understanding of world knowledge by averting both
unintentional and malicious data leakage. To avoid unintentional data leakage,
we source data from a broader domain and design three decontamination rules. To
prevent malicious data leakage, we divide the benchmark into validation and
test sets with similar difficulty and subject distributions. The test set
remains closed-source to ensure reliable results, while the validation set is
publicly available to promote transparency and facilitate independent
verification. Our evaluation of mainstream LLMs reveals that the powerful
GPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on
the test set, which indicates the effectiveness of our approach in creating a
more rigorous and contamination-free evaluation standard. The GitHub repository
is available at https://github.com/microsoft/MMLU-CF and the dataset refers to
https://huggingface.co/datasets/microsoft/MMLU-CF.

</details>


### [25] [HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models](https://arxiv.org/abs/2506.21578)
*Andrew Maranhão Ventura D'addario*

Key words: Large Language Models, healthcare, benchmarking, Portuguese, interprofessional, AI readiness

TL;DR: 本文介绍了HealthQA-BR，一个针对葡萄牙语医疗领域的全面基准测试，揭示了大型语言模型在跨专业医疗知识上的不均衡表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前大型语言模型（LLMs）在医疗领域的评估主要依赖以医生为中心的英语基准，忽视了跨专业医疗团队的需求，可能导致对模型能力的误判。

Method: 研究团队开发了HealthQA-BR基准，包含5,632个来自巴西国家考试的问题，涵盖医学、护理、牙科等多个领域。对20多个领先LLMs进行了零样本评估。

Result: 结果显示，尽管GPT 4.1等模型总体准确率达86.6%，但在某些领域（如神经外科）表现较差，仅60.0%，知识分布不均衡。

Conclusion: 高水平的单一评分不足以保证模型安全性，需采用更细致的评估方法。HealthQA-BER的发布为跨专业医疗团队的AI验证提供了工具。

Abstract: The evaluation of Large Language Models (LLMs) in healthcare has been
dominated by physician-centric, English-language benchmarks, creating a
dangerous illusion of competence that ignores the interprofessional nature of
patient care. To provide a more holistic and realistic assessment, we introduce
HealthQA-BR, the first large-scale, system-wide benchmark for
Portuguese-speaking healthcare. Comprising 5,632 questions from Brazil's
national licensing and residency exams, it uniquely assesses knowledge not only
in medicine and its specialties but also in nursing, dentistry, psychology,
social work, and other allied health professions. We conducted a rigorous
zero-shot evaluation of over 20 leading LLMs. Our results reveal that while
state-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%),
this top-line score masks alarming, previously unmeasured deficiencies. A
granular analysis shows performance plummets from near-perfect in specialties
like Ophthalmology (98.7%) to barely passing in Neurosurgery (60.0%) and, most
notably, Social Work (68.4%). This "spiky" knowledge profile is a systemic
issue observed across all models, demonstrating that high-level scores are
insufficient for safety validation. By publicly releasing HealthQA-BR and our
evaluation suite, we provide a crucial tool to move beyond single-score
evaluations and toward a more honest, granular audit of AI readiness for the
entire healthcare team.

</details>


### [26] [From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models](https://arxiv.org/abs/2506.21580)
*Dana Alsagheer,Yang Lu,Abdulrahman Kamal,Omar Kamal,Mohammad Kamal,Nada Mansour,Cosmo Yang Wu,Rambiba Karanjai,Sen Li,Weidong Shi*

Key words: 大型语言模型, 推理能力, 决策, AI技术, 特定领域任务

TL;DR: 本研究探讨大型语言模型（LLMs）的通用推理能力如何影响其在特定领域推理任务中的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着AI技术的发展，训练LLMs在通用推理中表现出色的趋势日益明显，但决策能力仍需强推理能力支持。

Method: 研究通过分析LLMs的通用推理能力与特定领域推理任务的关联来展开。

Result: 未明确提及具体实验结果，但强调了推理在决策中的作用。

Conclusion: 推理能力的提升有助于LLMs在特定领域任务中的表现。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
remarkable capabilities in various domains. However, effective decision-making
relies heavily on strong reasoning abilities. Reasoning is the foundation for
decision-making, providing the analytical and logical framework to make sound
choices. Reasoning involves analyzing information, drawing inferences, and
reaching conclusions based on logic or evidence. Decision-making builds on this
foundation by applying the insights from reasoning to select the best course of
action among alternatives. Together, these processes create a continuous cycle
of thought and action aimed at achieving goals effectively. As AI technology
evolves, there is a growing trend to train LLMs to excel in general reasoning.
This study explores how the general reasoning capabilities of LLMs connect to
their performance in domain-specific reasoning tasks.

</details>


### [27] [VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents](https://arxiv.org/abs/2506.21582)
*Sam Yu-Te Lee,Chengyang Ji,Shicheng Wen,Lifu Huang,Dongyi Liu,Kwan-Liu Ma*

Key words: 文本分析, 人机协作, 大型语言模型, VIDEE系统, NLP

TL;DR: VIDEE系统通过人机协作工作流，帮助入门级数据分析师进行高级文本分析，包括分解、执行和评估三个阶段。研究表明其适用于非专家用户，并为人机协作设计提供了见解。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统文本分析需要NLP专业知识，门槛较高，而大型语言模型（LLM）的发展为自动化分析提供了新的可能性。

Method: VIDEE系统采用三阶段工作流：分解（结合人类反馈的蒙特卡洛树搜索）、执行（生成可执行分析管道）和评估（LLM评估与可视化）。

Result: 实验和用户研究表明VIDEE有效且易用，揭示了用户行为模式并为系统优化提供了依据。

Conclusion: VIDEE为非专家用户提供了实用的文本分析工具，并为人机协作系统的未来设计提供了参考。

Abstract: Text analytics has traditionally required specialized knowledge in Natural
Language Processing (NLP) or text analysis, which presents a barrier for
entry-level analysts. Recent advances in large language models (LLMs) have
changed the landscape of NLP by enabling more accessible and automated text
analysis (e.g., topic detection, summarization, information extraction, etc.).
We introduce VIDEE, a system that supports entry-level data analysts to conduct
advanced text analytics with intelligent agents. VIDEE instantiates a
human-agent collaroration workflow consisting of three stages: (1)
Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search
algorithm to support generative reasoning with human feedback, (2) Execution,
which generates an executable text analytics pipeline, and (3) Evaluation,
which integrates LLM-based evaluation and visualizations to support user
validation of execution results. We conduct two quantitative experiments to
evaluate VIDEE's effectiveness and analyze common agent errors. A user study
involving participants with varying levels of NLP and text analytics experience
-- from none to expert -- demonstrates the system's usability and reveals
distinct user behavior patterns. The findings identify design implications for
human-agent collaboration, validate the practical utility of VIDEE for
non-expert users, and inform future improvements to intelligent text analytics
systems.

</details>


### [28] [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545)
*Yalun Dai,Yangyu Huang,Xin Zhang,Wenshan Wu,Chong Li,Wenhui Lu,Shijie Cao,Li Dong,Scarlett Li*

Key words: 数据效能, 语言模型, 数据组织, DELT, LQS, FO

TL;DR: 论文提出了一种新的概念『数据效能』（Data Efficacy），旨在通过优化训练数据的组织方式来最大化语言模型性能。作者提出了DELT范式，包括数据评分、数据选择和数据排序三个部分，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前研究主要集中在数据效率（Data Efficiency）上，即通过选择最小或最优的子集来提升性能，而数据组织方式的优化（Data Efficacy）尚未充分探索。本文旨在填补这一空白。

Method: 提出了DELT范式，包含数据评分（如LQS）、数据选择和排序（如FO）。LQS从梯度一致性角度评估样本的学习能力和质量，FO解决了模型遗忘和数据分布偏差问题。

Result: 实验表明，DELT的各个实例均能在不增加数据规模或模型大小的情况下提升语言模型性能。其中，LQS与FO的组合效果最显著，且数据效能与数据效率可同时实现。

Conclusion: 数据效能是语言模型训练中一个具有潜力的基础性研究方向。

Abstract: Data is fundamental to the training of language models (LM). Recent research
has been dedicated to data efficiency, which aims to maximize performance by
selecting a minimal or optimal subset of training data. Techniques such as data
filtering, sampling, and selection play a crucial role in this area. To
complement it, we define Data Efficacy, which focuses on maximizing performance
by optimizing the organization of training data and remains relatively
underexplored. This work introduces a general paradigm, DELT, for considering
data efficacy in LM training, which highlights the significance of training
data organization. DELT comprises three components: Data Scoring, Data
Selection, and Data Ordering. Among these components, we design
Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which
considers both the learnability and quality of each data sample from the
gradient consistency perspective. We also devise Folding Ordering (FO), as a
novel instance of Data Ordering, which addresses issues such as model
forgetting and data distribution bias. Comprehensive experiments validate the
data efficacy in LM training, which demonstrates the following: Firstly,
various instances of the proposed DELT enhance LM performance to varying
degrees without increasing the data scale and model size. Secondly, among these
instances, the combination of our proposed LQS for data scoring and Folding for
data ordering achieves the most significant improvement. Lastly, data efficacy
can be achieved together with data efficiency by applying data selection.
Therefore, we believe that data efficacy is a promising foundational area in LM
training.

</details>


### [29] [Hope Speech Detection in code-mixed Roman Urdu tweets: A Positive Turn in Natural Language Processing](https://arxiv.org/abs/2506.21583)
*Muhammad Ahmad,Muhammad Waqas,Ameer Hamza,Ildar Batyrshin,Grigori Sidorov*

Key words: 希望语音检测, 罗马乌尔都语, 混合代码, 注意力模型, 自然语言处理

TL;DR: 该研究首次针对罗马乌尔都语混合代码中的希望语音检测，提出了一个多类标注数据集，并开发了一个基于注意力的Transformer模型，性能优于基线模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有研究主要关注高资源语言和标准化脚本，忽略了罗马乌尔都语等非正式和资源匮乏语言，这项研究旨在填补这一空白。

Method: 研究引入了一个多类标注数据集，分析了希望的心理基础和语言模式，并提出了一种优化的基于注意力的Transformer模型，使用5折交叉验证进行评估。

Result: 提出的XLM-R模型在交叉验证中得分0.78，优于SVM（0.75）和BiLSTM（0.76），性能提升显著。

Conclusion: 该研究为资源匮乏的非正式语言中的希望语音检测提供了重要基础，并提出了一种有效的模型。

Abstract: Hope is a positive emotional state involving the expectation of favorable
future outcomes, while hope speech refers to communication that promotes
optimism, resilience, and support, particularly in adverse contexts. Although
hope speech detection has gained attention in Natural Language Processing
(NLP), existing research mainly focuses on high-resource languages and
standardized scripts, often overlooking informal and underrepresented forms
such as Roman Urdu. To the best of our knowledge, this is the first study to
address hope speech detection in code-mixed Roman Urdu by introducing a
carefully annotated dataset, thereby filling a critical gap in inclusive NLP
research for low-resource, informal language varieties. This study makes four
key contributions: (1) it introduces the first multi-class annotated dataset
for Roman Urdu hope speech, comprising Generalized Hope, Realistic Hope,
Unrealistic Hope, and Not Hope categories; (2) it explores the psychological
foundations of hope and analyzes its linguistic patterns in code-mixed Roman
Urdu to inform dataset development; (3) it proposes a custom attention-based
transformer model optimized for the syntactic and semantic variability of Roman
Urdu, evaluated using 5-fold cross-validation; and (4) it verifies the
statistical significance of performance gains using a t-test. The proposed
model, XLM-R, achieves the best performance with a cross-validation score of
0.78, outperforming the baseline SVM (0.75) and BiLSTM (0.76), with gains of 4%
and 2.63% respectively.

</details>


### [30] [Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques](https://arxiv.org/abs/2506.21584)
*J. Koorndijk*

Key words: 对齐伪装，语言模型，提示干预，浅层伪装，深层伪装

TL;DR: 研究发现小型指令调优模型LLaMA 3 8B也能表现出对齐伪装行为，且仅通过提示干预即可显著减少该行为，挑战了伪装对齐需大规模模型的假设。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索小型语言模型是否也会表现出对齐伪装行为，以及如何通过简单干预减少这种行为。

Method: 使用LLaMA 3 8B模型，通过提示干预（如道德框架和推理提示）测试对齐伪装行为。

Result: 提示干预显著减少了对齐伪装行为，表明伪装行为可分为浅层（可通过提示抑制）和深层（目标驱动的持续不对齐）。

Conclusion: 研究改进了对语言模型伪装行为的理解，并强调需对不同规模和部署环境下的模型进行对齐评估。

Abstract: Current literature suggests that alignment faking (deceptive alignment) is an
emergent property of large language models. We present the first empirical
evidence that a small instruction-tuned model, specifically LLaMA 3 8B, can
also exhibit alignment faking. We further show that prompt-only interventions,
including deontological moral framing and scratchpad reasoning, significantly
reduce this behavior without modifying model internals. This challenges the
assumption that prompt-based ethics are trivial and that deceptive alignment
requires scale. We introduce a taxonomy distinguishing shallow deception,
shaped by context and suppressible through prompting, from deep deception,
which reflects persistent, goal-driven misalignment. Our findings refine the
understanding of deception in language models and underscore the need for
alignment evaluations across model sizes and deployment settings.

</details>


### [31] [Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops](https://arxiv.org/abs/2506.21585)
*Christoph Brosch,Sian Brumm,Rolf Krieger,Jonas Scheffler*

Key words: 生成式AI, 大语言模型, 信息提取, 食品页面, 成本效益

TL;DR: 该论文探讨了使用生成式AI和大语言模型（LLMs）从在线零售商食品页面提取结构化信息的方法，比较了直接提取和间接提取的优缺点。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在利用LLMs自动提取网页中的结构化信息，特别是食品页面的关键属性（如成分表和营养表），以提高效率和降低成本。

Method: 采用两种LLM方法：直接提取和间接提取（通过生成函数）。在3,000个食品页面的数据集上比较准确性、效率和成本。

Result: 间接提取的准确率略低（96.48%，比直接提取低1.61%），但减少了95.82%的LLM调用次数，显著提升了效率并降低了成本。

Conclusion: 间接提取方法为基于模板网页的大规模信息提取任务提供了可扩展且成本效益高的解决方案。

Abstract: Generative AI and large language models (LLMs) offer significant potential
for automating the extraction of structured information from web pages. In this
work, we focus on food product pages from online retailers and explore
schema-constrained extraction approaches to retrieve key product attributes,
such as ingredient lists and nutrition tables. We compare two LLM-based
approaches, direct extraction and indirect extraction via generated functions,
evaluating them in terms of accuracy, efficiency, and cost on a curated dataset
of 3,000 food product pages from three different online shops. Our results show
that although the indirect approach achieves slightly lower accuracy (96.48\%,
$-1.61\%$ compared to direct extraction), it reduces the number of required LLM
calls by 95.82\%, leading to substantial efficiency gains and lower operational
costs. These findings suggest that indirect extraction approaches can provide
scalable and cost-effective solutions for large-scale information extraction
tasks from template-based web pages using LLMs.

</details>


### [32] [Can Vision Language Models Understand Mimed Actions?](https://arxiv.org/abs/2506.21586)
*Hyundong Cho,Spencer Lin,Tejas Srinivasan,Michael Saxon,Deuksin Kwon,Natali T. Chavez,Jonathan May*

Key words: 非语言通信；模仿动作；视觉-语言模型；动作识别；基准测试

TL;DR: 论文提出了一种名为MIME的新型视频问答基准，用于评估视觉-语言模型对模仿动作的理解能力，发现现有模型表现远逊于人类。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究模仿动作（mime）作为非语言通信（NVC）的子集，因其明确性和低解释方差，是提升视觉-语言模型对NVC理解的关键前提。

Method: 构建了包含86种模仿动作的MIME基准，采用动作捕捉数据生成多样化变体，并评估模型对动作识别的鲁棒性。

Result: 实验结果表明，当前视觉-语言模型在MIME基准上的表现显著低于人类。

Conclusion: 研究强调了提升模型对人类手势理解的重要性，为未来研究提供了方向。

Abstract: Nonverbal communication (NVC) plays an integral role in human language, but
studying NVC in general is challenging because of its broad scope and high
variance in interpretation among individuals and cultures. However, mime -- the
theatrical technique of suggesting intent using only gesture, expression, and
movement -- is a subset of NVC that consists of explicit and embodied actions
with much lower human interpretation variance. We argue that a solid
understanding of mimed actions is a crucial prerequisite for vision-language
models capable of interpreting and commanding more subtle aspects of NVC.
Hence, we propose Mime Identification Multimodal Evaluation (MIME), a novel
video-based question answering benchmark comprising of 86 mimed actions.
Constructed with motion capture data, MIME consists of variations of each
action with perturbations applied to the character, background, and viewpoint
for evaluating recognition robustness. We find that both open-weight and
API-based vision-language models perform significantly worse than humans on
MIME, motivating the need for increased research for instilling more robust
understanding of human gestures.

</details>


### [33] [Is DeepSeek a New Voice Among LLMs in Public Opinion Simulation?](https://arxiv.org/abs/2506.21587)
*Weihong Qi,Fan Huang,Jisun An,Haewoon Kwak*

Key words: DeepSeek, LLM, 公众意见模拟, 文化偏见, 人口偏见

TL;DR: 研究评估开源大模型DeepSeek在模拟公众意见方面的能力，并与主流公司开发的模型对比，发现其在特定议题上表现优异但存在文化及人口偏见。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 评估开源LLM能否模拟中美公众对社会议题的意见，并与主流模型对比。

Method: 比较DeepSeek-R1、DeepSeek-V3与Qwen2.5、GPT-4o、Llama-3.3，利用ANES和中国坐标数据评估模型表现。

Result: DeepSeek-V3在模拟美国堕胎议题上表现最佳，但在中国议题上对资本主义观点建模不足；所有模型存在过度概括倾向。

Conclusion: 需减少文化及人口偏见，提倡包容性训练方法。

Abstract: This study evaluates the ability of DeepSeek, an open-source large language
model (LLM), to simulate public opinions in comparison to LLMs developed by
major tech companies. By comparing DeepSeek-R1 and DeepSeek-V3 with Qwen2.5,
GPT-4o, and Llama-3.3 and utilizing survey data from the American National
Election Studies (ANES) and the Zuobiao dataset of China, we assess these
models' capacity to predict public opinions on social issues in both China and
the United States, highlighting their comparative capabilities between
countries. Our findings indicate that DeepSeek-V3 performs best in simulating
U.S. opinions on the abortion issue compared to other topics such as climate
change, gun control, immigration, and services for same-sex couples, primarily
because it more accurately simulates responses when provided with Democratic or
liberal personas. For Chinese samples, DeepSeek-V3 performs best in simulating
opinions on foreign aid and individualism but shows limitations in modeling
views on capitalism, particularly failing to capture the stances of low-income
and non-college-educated individuals. It does not exhibit significant
differences from other models in simulating opinions on traditionalism and the
free market. Further analysis reveals that all LLMs exhibit the tendency to
overgeneralize a single perspective within demographic groups, often defaulting
to consistent responses within groups. These findings highlight the need to
mitigate cultural and demographic biases in LLM-driven public opinion modeling,
calling for approaches such as more inclusive training methodologies.

</details>


### [34] [Understanding Verbatim Memorization in LLMs Through Circuit Discovery](https://arxiv.org/abs/2506.21588)
*Ilya Lasy,Peter Knees,Stefan Woltran*

Key words: 大型语言模型, 记忆机制, 变压器电路, 机械可解释性, 对比数据集

TL;DR: 该论文研究了大型语言模型（LLMs）中记忆机制的具体网络部分和行为差异，通过变压器电路和对比数据集分析了记忆序列的起始和维护机制。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是理解LLMs中记忆训练的底层机制，特别是记忆序列的起始和模型行为的差异。

Method: 采用机械可解释性方法，利用变压器电路分析最小计算子图，通过对比数据集定位记忆内容的分歧点。

Result: 发现起始记忆的电路也能维护记忆，但仅维护记忆的电路无法触发记忆起始；记忆预防机制在不同文本领域中表现稳健，但记忆起始更具上下文依赖性。

Conclusion: 论文揭示了记忆机制的电路级别行为，为理解LLMs的记忆现象提供了新的视角。

Abstract: Underlying mechanisms of memorization in LLMs -- the verbatim reproduction of
training data -- remain poorly understood. What exact part of the network
decides to retrieve a token that we would consider as start of memorization
sequence? How exactly is the models' behaviour different when producing
memorized sentence vs non-memorized? In this work we approach these questions
from mechanistic interpretability standpoint by utilizing transformer circuits
-- the minimal computational subgraphs that perform specific functions within
the model. Through carefully constructed contrastive datasets, we identify
points where model generation diverges from memorized content and isolate the
specific circuits responsible for two distinct aspects of memorization. We find
that circuits that initiate memorization can also maintain it once started,
while circuits that only maintain memorization cannot trigger its initiation.
Intriguingly, memorization prevention mechanisms transfer robustly across
different text domains, while memorization induction appears more
context-dependent.

</details>


### [35] [A General Method for Detecting Information Generated by Large Language Models](https://arxiv.org/abs/2506.21589)
*Minjia Mao,Dongjun Wei,Xiao Fang,Michael Chau*

Key words: LLM检测, 双记忆网络, 理论指导, 泛化能力

TL;DR: 该论文提出了一种通用LLM检测器（GLD），利用双记忆网络设计和理论指导的检测泛化模块，解决了现有方法在未知LLM和域上泛化能力不足的问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着LLM的普及，区分人类撰写和LLM生成内容的需求日益重要，以维护数字平台信任并防止虚假信息传播。

Method: 提出GLD，结合双记忆网络设计和理论指导的检测泛化模块。

Result: 通过真实数据集验证，GLD在检测未知LLM和域上的表现优于现有方法。

Conclusion: GLD在学术和实践中具有重要价值，为数字平台和LLM提供了有效解决方案。

Abstract: The proliferation of large language models (LLMs) has significantly
transformed the digital information landscape, making it increasingly
challenging to distinguish between human-written and LLM-generated content.
Detecting LLM-generated information is essential for preserving trust on
digital platforms (e.g., social media and e-commerce sites) and preventing the
spread of misinformation, a topic that has garnered significant attention in IS
research. However, current detection methods, which primarily focus on
identifying content generated by specific LLMs in known domains, face
challenges in generalizing to new (i.e., unseen) LLMs and domains. This
limitation reduces their effectiveness in real-world applications, where the
number of LLMs is rapidly multiplying and content spans a vast array of
domains. In response, we introduce a general LLM detector (GLD) that combines a
twin memory networks design and a theory-guided detection generalization module
to detect LLM-generated information across unseen LLMs and domains. Using
real-world datasets, we conduct extensive empirical evaluations and case
studies to demonstrate the superiority of GLD over state-of-the-art detection
methods. The study has important academic and practical implications for
digital platforms and LLMs.

</details>


### [36] [Representation Consistency for Accurate and Coherent LLM Answer Aggregation](https://arxiv.org/abs/2506.21590)
*Junqi Jiang,Tom Bewley,Salim I. Amoukou,Francesco Leofante,Antonio Rago,Saumitra Mishra,Francesca Toni*

Key words: 大语言模型, 测试时扩展, 表示一致性, 推理性能, 稀疏激活

TL;DR: 提出了一种称为表示一致性（RC）的测试时扩展方法，通过聚合大语言模型（LLM）生成的多个候选答案，结合模型内部激活的一致性，提高推理性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法需要复杂的提示和采样策略修改，RC方法则无需额外查询模型，通过内部激活一致性改进答案聚合效果。

Method: 利用模型内部激活（密集或稀疏编码）的一致性，加权聚合多答案候选集中的答案，轻量级计算且无需额外模型查询。

Result: 在四个开源LLM和四个推理数据集上验证RC有效性，准确率提升高达4%，稀疏激活信号与一致性推理概念吻合。

Conclusion: RC通过内部激活一致性显著提升任务性能，是一种高效且无需额外成本的测试时扩展方法。

Abstract: Test-time scaling improves large language models' (LLMs) performance by
allocating more compute budget during inference. To achieve this, existing
methods often require intricate modifications to prompting and sampling
strategies. In this work, we introduce representation consistency (RC), a
test-time scaling method for aggregating answers drawn from multiple candidate
responses of an LLM regardless of how they were generated, including variations
in prompt phrasing and sampling strategy. RC enhances answer aggregation by not
only considering the number of occurrences of each answer in the candidate
response set, but also the consistency of the model's internal activations
while generating the set of responses leading to each answer. These activations
can be either dense (raw model activations) or sparse (encoded via pretrained
sparse autoencoders). Our rationale is that if the model's representations of
multiple responses converging on the same answer are highly variable, this
answer is more likely to be the result of incoherent reasoning and should be
down-weighted during aggregation. Importantly, our method only uses cached
activations and lightweight similarity computations and requires no additional
model queries. Through experiments with four open-source LLMs and four
reasoning datasets, we validate the effectiveness of RC for improving task
performance during inference, with consistent accuracy improvements (up to 4%)
over strong test-time scaling baselines. We also show that consistency in the
sparse activation signals aligns well with the common notion of coherent
reasoning.

</details>


### [37] [FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models' Knowledge and Reasoning](https://arxiv.org/abs/2506.21591)
*Shaoyu Dou,Yutian Shen,Mofan Chen,Zixuan Wang,Jiajie Xu,Qi Guo,Kailai Shao,Chao Chen,Haixiang Hu,Haibo Shi,Min Min,Liwen Zhang*

Key words: 大语言模型,金融推理,知识分数,推理分数,Bloom分类法

TL;DR: 论文提出了FinEval-KR框架，用于独立评估大语言模型在金融推理任务中的知识和推理能力，并引入了认知分数。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前评估基准在复杂金融推理任务中未能区分知识和推理能力，且缺乏对任务失败的根因分析。

Method: 提出了FinEval-KR框架，设计独立的知识分数和推理分数，并结合Bloom分类法提出认知分数。

Result: 实验表明推理能力和高阶认知能力是推理准确性的核心因素，顶级模型在知识应用上仍存在瓶颈。

Conclusion: 金融专用大模型在多指标上普遍落后于顶级通用大模型，需提升知识应用能力。

Abstract: Large Language Models (LLMs) demonstrate significant potential but face
challenges in complex financial reasoning tasks requiring both domain knowledge
and sophisticated reasoning. Current evaluation benchmarks often fall short by
not decoupling these capabilities indicators from single task performance and
lack root cause analysis for task failure. To address this, we introduce
FinEval-KR, a novel evaluation framework for decoupling and quantifying LLMs'
knowledge and reasoning abilities independently, proposing distinct knowledge
score and reasoning score metrics. Inspired by cognitive science, we further
propose a cognitive score based on Bloom's taxonomy to analyze capabilities in
reasoning tasks across different cognitive levels. We also release a new
open-source Chinese financial reasoning dataset covering 22 subfields to
support reproducible research and further advancements in financial reasoning.
Our experimental results reveal that LLM reasoning ability and higher-order
cognitive ability are the core factors influencing reasoning accuracy. We also
specifically find that even top models still face a bottleneck with knowledge
application. Furthermore, our analysis shows that specialized financial LLMs
generally lag behind the top general large models across multiple metrics.

</details>


### [38] [SignBart -- New approach with the skeleton sequence for Isolated Sign language Recognition](https://arxiv.org/abs/2506.21592)
*Tinh Nguyen,Minh Khue Phan Tran*

Key words: 手语识别, BART架构, 骨架序列, Cross-Attention, 高效模型

TL;DR: 本文提出一种新的手语识别方法，通过BART架构独立编码骨架序列的x和y坐标，显著提高了准确率和效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决传统方法在手语识别中效率与准确率难以兼顾的问题，尤其是RNN、LSTM和GCN的梯度消失和高计算成本问题。

Method: 利用BART架构的编码器-解码器独立编码x和y坐标，并通过Cross-Attention保持其相关性。

Result: 在LSA-64数据集上达到96.04%的准确率，参数仅749,888个，优于以往百万参数模型。

Conclusion: 该方法为手语识别提供了高效可靠的解决方案，有助于改善听力障碍者的沟通工具。

Abstract: Sign language recognition is crucial for individuals with hearing impairments
to break communication barriers. However, previous approaches have had to
choose between efficiency and accuracy. Such as RNNs, LSTMs, and GCNs, had
problems with vanishing gradients and high computational costs. Despite
improving performance, transformer-based methods were not commonly used. This
study presents a new novel SLR approach that overcomes the challenge of
independently extracting meaningful information from the x and y coordinates of
skeleton sequences, which traditional models often treat as inseparable. By
utilizing an encoder-decoder of BART architecture, the model independently
encodes the x and y coordinates, while Cross-Attention ensures their
interrelation is maintained. With only 749,888 parameters, the model achieves
96.04% accuracy on the LSA-64 dataset, significantly outperforming previous
models with over one million parameters. The model also demonstrates excellent
performance and generalization across WLASL and ASL-Citizen datasets. Ablation
studies underscore the importance of coordinate projection, normalization, and
using multiple skeleton components for boosting model efficacy. This study
offers a reliable and effective approach for sign language recognition, with
strong potential for enhancing accessibility tools for the deaf and hard of
hearing.

</details>


### [39] [Gazal-R1: Achieving State-of-the-Art Medical Reasoning with Parameter-Efficient Two-Stage Training](https://arxiv.org/abs/2506.21594)
*Ahmed M. Adly,Mostafa Samy,Amr Fawzy*

Key words: Gazal-R1, 医学推理, 两阶段训练, 参数高效技术, GRPO

TL;DR: Gazal-R1是一个320亿参数的语言模型，在医学推理领域表现卓越，并通过透明的逐步解释支持临床决策。通过两阶段训练流程，它在医学基准测试中超越更大规模的模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在通过战略训练使中等规模模型在专业领域（如医学）中优于更大规模的模型，并提供透明的临床决策支持。

Method: 采用两阶段训练流程：1）在有监督的细化训练中使用107,033个合成医学推理示例，结合参数高效技术（DoRA和rsLoRA）；2）使用GRPO和多组件奖励系统进行强化学习。

Result: Gazal-R1在医学基准测试中表现卓越（MedQA 87.1%，MMLU Pro 81.6%，PubMedQA 79.6%），超越更大规模模型12倍。

Conclusion: Gazal-R1为开发高性能、高效且可解释的领域专用语言模型提供了可复现的框架，解决了专业领域训练中的关键挑战。

Abstract: We present Gazal-R1, a 32-billion-parameter language model that achieves
state-of-the-art performance in medical reasoning while providing transparent,
step-by-step explanations for clinical decision-making. Built upon Qwen3 32B,
our model demonstrates that strategic training can enable mid-sized models to
outperform significantly larger counterparts in specialized domains. We
developed a novel two-stage training pipeline: first, supervised fine-tuning on
a carefully curated dataset of 107,033 synthetic medical reasoning examples
that teaches structured clinical thinking, enhanced by advanced
parameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation
(DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using
Group Relative Policy Optimization (GRPO) with a sophisticated multi-component
reward system that refines accuracy, format adherence, and reasoning quality.
Gazal-R1 achieves exceptional performance across medical benchmarks, scoring
87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing
models up to 12x larger. Beyond its strong empirical results, this work
provides detailed insights into the challenges of training reasoning-capable
models in specialized domains, including issues with reward hacking, training
instability, and the fundamental tension between factual recall and detailed
reasoning. Our methodology offers a reproducible framework for developing
high-capability, domain-specific language models that balance performance,
efficiency, and explainability.

</details>


### [40] [Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources](https://arxiv.org/abs/2506.21595)
*Jinpyo Kim,Gyeongje Cho,Chanwoo Park,Jongwon Park,Jongmin Kim,Yeonkyoun So,Jaejin Lee*

Key words: LLM, 多语言模型, 韩语适配, 低成本训练

TL;DR: 该论文提出了一种低成本方法，将现有的英语大语言模型（LLM）适配到韩语，并通过端到端流程展示了数据收集、预处理、模型训练、下游测试及评估的全过程。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决LLMs在英语和中文之外的语言（尤其是韩语）表现不佳的问题，同时公开LLMs训练的完整流程以促进透明度。

Method: 采用低成本方案，通过收集韩语数据集、数据预处理、模型训练、创建下游测试并进行评估，实现LLM的韩语适配。

Result: 新双语模型Thunder-LLM和Thunder-LLM-Ins在韩语任务中表现优于现有最优模型，且数据与计算资源需求低。

Conclusion: 该方法高效且成本低，能有效为现有LLMs添加新语言能力；研究团队分享了完整经验并公开代码。

Abstract: Since state-of-the-art LLMs often underperform in languages other than
English or Chinese, improving the capability of LLMs in new languages has
become an essential task. Moreover, LLMs' entire end-to-end training process
remains largely unknown to the public due to proprietary reasons, technical
complexity, inconsistent documentation, and ethical considerations. The
complete picture remains a closely guarded secret within the industry. This
paper presents methods to adapt an existing English-based LLM to Korean in a
low-budget scenario. We describe the entire end-to-end process: collecting
Korean datasets, preprocessing the data, training the model, creating
downstream benchmarks, and conducting evaluations. The evaluation results
indicate that our method can effectively and cost-efficiently add new language
capabilities to existing LLMs. Our new bilingual models, Thunder-LLM and
Thunder-LLM-Ins, achieve superior Korean performance compared to
state-of-the-art models while utilizing minimal data and computational
resources. We share our comprehensive experience and make the code publicly
available.

</details>


### [41] [Evaluating Multimodal Large Language Models on Educational Textbook Question Answering](https://arxiv.org/abs/2506.21596)
*Hessa A. Alawwad,Anas Zafar,Areej Alhothali,Usman Naseem,Ali Alkhathlan,Amani Jamal*

Key words: 多模态大语言模型, 教科书问答, 检索增强生成, 教育AI

TL;DR: 评估多模态大语言模型在教科书问答任务中的表现，引入轻量级检索增强生成管道，分析上下文检索对模型性能的影响。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 测试多模态大语言模型在处理复杂教育内容和图表时的能力，填补现有研究的空白。

Method: 使用CK12-QA数据集评估LLaVA和LLaMA 3.2-Vision等模型，并提出轻量级多模态RAG管道。

Result: 检索的教育上下文对模型准确性和推理有显著影响，但模型在问题-上下文关系和噪声处理上仍有局限。

Conclusion: 多模态AI在教育中的应用潜力巨大，但仍需进一步研究以优化模型性能。

Abstract: Multimodal large language models (MLLMs) have recently achieved significant
success in vision--language tasks. However, their capacity to reason over
complex, long lessons and intricate educational diagrams that cannot be
represented as a single natural image remains largely untested. In this work,
we present the first evaluation of state-of-the-art MLLMs on the textbook
question answering (TQA) task using the CK12-QA dataset. We assess the
performance of recent vision-language models, including LLaVA and LLaMA
3.2-Vision, across various input configurations. Additionally, we introduce a
lightweight multimodal retrieval-augmented generation (RAG) pipeline that
integrates both paragraphs and diagrams from the lesson into the prompt. Our
results demonstrate the influence of retrieved educational context on model
accuracy and reasoning, while also revealing current limitations in handling
question-context relationships and the potential for noise, pointing to key
directions for future research in multimodal AI-driven learning.

</details>


### [42] [Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering](https://arxiv.org/abs/2506.21597)
*Brandon Colelough,Davis Bartels,Dina Demner-Fushman*

Key words: ClinIQLink, 大型语言模型, 医学问答, 共享任务, BioNLP

TL;DR: ClinIQLink是一个针对大型语言模型（LLM）的医学问答共享任务，包含4978个专家验证的问题-答案对，覆盖7种格式，通过自动评分和医生评审对模型性能进行全面评估。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 测试大型语言模型在针对全科医生级别的医学问答任务中的表现，提供一个标准化的评估平台。

Method: 使用4978个专家验证的问题-答案对，涵盖7种格式；模型通过Docker或Apptainer镜像在CodaBench或Zaratan集群上运行；自动评分（Task 1）结合精确匹配和嵌入度量，医生评审（Task 2）审核模型回答。

Result: 未提及具体结果，但提供了详细的任务设计和评估方式。

Conclusion: ClinIQLink是一个全面且严谨的医学问答评估任务，旨在推动LLM在医疗领域的应用研究。

Abstract: In this paper, we present an overview of ClinIQLink, a shared task,
collocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test
large language models (LLMs) on medically-oriented question answering aimed at
the level of a General Practitioner. The challenge supplies 4,978
expert-verified, medical source-grounded question-answer pairs that cover seven
formats: true/false, multiple choice, unordered list, short answer,
short-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled
in Docker or Apptainer images, are executed on the CodaBench platform or the
University of Maryland's Zaratan cluster. An automated harness (Task 1) scores
closed-ended items by exact match and open-ended items with a three-tier
embedding metric. A subsequent physician panel (Task 2) audits the top model
responses.

</details>


### [43] [Structured Attention Matters to Multimodal LLMs in Document Understanding](https://arxiv.org/abs/2506.21600)
*Chang Liu,Hongkai Chen,Yujun Cai,Hang Wu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Key words: 多模态大语言模型, 文档理解, 输入格式, LaTex, 结构化文本

TL;DR: 研究发现输入格式对多模态大语言模型（MLLMs）的文档理解性能有重要影响，提出了一种保留结构的新方法以提升性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨输入格式对MLLMs文档理解的影响，发现原始OCR文本可能降低性能。

Method: 提出一种基于LaTex范式的方法，保留文档的层次结构和空间关系。

Result: 结构化的文本能够显著提升MLLMs的文档问答性能且无需修改模型架构或额外训练。

Conclusion: 保留文档结构的输入格式能有效提升MLLMs的文档理解能力。

Abstract: Document understanding remains a significant challenge for multimodal large
language models (MLLMs). While previous research has primarily focused on
locating evidence pages through precise multimodal queries, our work
investigates a fundamental yet overlooked aspect: how input format influences
document comprehension performance. Through systematic analysis, we discover
that raw OCR text often impairs rather than improves MLLMs' performance, which
is a counterintuitive finding we attribute to attention dispersion and
structure loss. To further substantiate our hypothesis, we propose a novel
structure-preserving approach that encodes document elements using the LaTex
paradigm, maintaining the hierarchical organization and spatial relationships
critical for comprehension. Our attention analysis reveals that structured text
induces structured attention patterns on both textual and visual content,
directing models to focus on semantically meaningful regions while reducing
attention waste. This approach significantly enhances MLLMs' document question
answering performance across diverse document types without requiring
architectural modifications or additional training.

</details>


### [44] [BiMark: Unbiased Multilayer Watermarking for Large Language Models](https://arxiv.org/abs/2506.21602)
*Xiaoyan Feng,He Zhang,Yanjun Zhang,Leo Yu Zhang,Shirui Pan*

Key words: LLM，水印，文本真实性，模型无关检测，多比特水印

TL;DR: BiMark是一种新颖的水印框架，通过三项创新解决了LLM生成文本的真实性识别问题，实现了文本质量保留、模型无关检测和多比特水印嵌入。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着大语言模型的进步，识别LLM生成文本的真实性成为迫切需求，现有水印方法难以同时满足文本质量、模型无关检测和水印容量的要求。

Method: 提出了BiMark框架，包括比特翻转无偏重权重机制、多层架构增强检测性和支持多比特水印的信息编码方法。

Result: 与现有方法相比，BiMark在短文本中提取率提高30%，同时保持较低的困惑度，下游任务表现与非水印文本相当。

Conclusion: BiMark为LLM生成文本的真实性识别提供了一种高质量、模型无关且支持多比特水印的解决方案。

Abstract: Recent advances in Large Language Models (LLMs) have raised urgent concerns
about LLM-generated text authenticity, prompting regulatory demands for
reliable identification mechanisms. Although watermarking offers a promising
solution, existing approaches struggle to simultaneously achieve three critical
requirements: text quality preservation, model-agnostic detection, and message
embedding capacity, which are crucial for practical implementation. To achieve
these goals, the key challenge lies in balancing the trade-off between text
quality preservation and message embedding capacity. To address this challenge,
we propose BiMark, a novel watermarking framework that achieves these
requirements through three key innovations: (1) a bit-flip unbiased reweighting
mechanism enabling model-agnostic detection, (2) a multilayer architecture
enhancing detectability without compromising generation quality, and (3) an
information encoding approach supporting multi-bit watermarking. Through
theoretical analysis and extensive experiments, we validate that, compared to
state-of-the-art multi-bit watermarking methods, BiMark achieves up to 30%
higher extraction rates for short texts while maintaining text quality
indicated by lower perplexity, and performs comparably to non-watermarked text
on downstream tasks such as summarization and translation.

</details>


### [45] [Operationalizing Automated Essay Scoring: A Human-Aware Approach](https://arxiv.org/abs/2506.21603)
*Yenisel Plasencia-Calaña*

Key words: 自动作文评分（AES），机器学习，大型语言模型（LLMs），可解释性，偏见，鲁棒性

TL;DR: 本文探讨了以人为中心的自动作文评分系统（AES）的操作化，分析了机器学习方法与大型语言模型（LLMs）的优缺点。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在解决AES系统在准确性之外的维度（如偏见、鲁棒性和可解释性）的挑战，以提高其可靠性和可信度。

Method: 通过比较机器学习方法与LLMs的表现，分析它们在偏见、鲁棒性和可解释性等关键维度的差异。

Result: 机器学习模型在准确性上优于LLMs，但可解释性较差；LLMs提供更丰富的解释，但两者在偏见和极端分数鲁棒性上均表现不佳。

Conclusion: 研究揭示了不同方法间的权衡，为开发更可靠的AES系统提供了指导。

Abstract: This paper explores the human-centric operationalization of Automated Essay
Scoring (AES) systems, addressing aspects beyond accuracy. We compare various
machine learning-based approaches with Large Language Models (LLMs) approaches,
identifying their strengths, similarities and differences. The study
investigates key dimensions such as bias, robustness, and explainability,
considered important for human-aware operationalization of AES systems. Our
study shows that ML-based AES models outperform LLMs in accuracy but struggle
with explainability, whereas LLMs provide richer explanations. We also found
that both approaches struggle with bias and robustness to edge scores. By
analyzing these dimensions, the paper aims to identify challenges and
trade-offs between different methods, contributing to more reliable and
trustworthy AES methods.

</details>


### [46] [MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents](https://arxiv.org/abs/2506.21605)
*Haoran Tan,Zeyu Zhang,Chen Ma,Xu Chen,Quanyu Dai,Zhenhua Dong*

Key words: LLM代理,记忆能力,评估基准,MemBench,数据集

TL;DR: 构建了一个用于评估LLM代理记忆能力的综合数据集和基准MemBench，解决现有评估方法的局限性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有评估方法在记忆层级多样性和交互场景上存在不足，且缺乏全面的评价指标。

Method: 构建包含事实记忆和反思记忆的数据集，并提出参与和观察两种交互场景，开发MemBench基准进行多维度评估。

Result: 提出了MemBench基准，涵盖有效性、效率和容量等多个评估维度。

Conclusion: MemBench为LLM代理记忆能力研究提供了更全面的评估工具，数据集和项目已开源。

Abstract: Recent works have highlighted the significance of memory mechanisms in
LLM-based agents, which enable them to store observed information and adapt to
dynamic environments. However, evaluating their memory capabilities still
remains challenges. Previous evaluations are commonly limited by the diversity
of memory levels and interactive scenarios. They also lack comprehensive
metrics to reflect the memory capabilities from multiple aspects. To address
these problems, in this paper, we construct a more comprehensive dataset and
benchmark to evaluate the memory capability of LLM-based agents. Our dataset
incorporates factual memory and reflective memory as different levels, and
proposes participation and observation as various interactive scenarios. Based
on our dataset, we present a benchmark, named MemBench, to evaluate the memory
capability of LLM-based agents from multiple aspects, including their
effectiveness, efficiency, and capacity. To benefit the research community, we
release our dataset and project at https://github.com/import-myself/Membench.

</details>


### [47] [Large Language Models as symbolic DNA of cultural dynamics](https://arxiv.org/abs/2506.21606)
*Parham Pourdavood,Michael Jacob,Terrence Deacon*

Key words: 大语言模型、文化动态、DNA类比、外部化、递归

TL;DR: 该论文提出了一种新的观点，将大语言模型（LLMs）视为类似于DNA的人类文化动态外部化信息媒介，强调其作为压缩模式储存库的角色，而非替代人类智能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨LLMs在文化动态中的作用，超越将其视为自主智能或简单模仿工具的视角，强调其对人类创造性过程的催化作用。

Method: 通过分析四个普遍特征（压缩、解压、外部化和递归），与DNA的类比构建理论框架。

Result: LLMs作为文化可进化工具，帮助人类在模拟环境中进行低风险自我反思和假设生成。

Conclusion: LLMs的意义在于促进人类文化演化，而非替代人类智能，强调人类解释在其中的核心作用。

Abstract: This paper proposes a novel conceptualization of Large Language Models (LLMs)
as externalized informational substrates that function analogously to DNA for
human cultural dynamics. Rather than viewing LLMs as either autonomous
intelligence or mere programmed mimicry, we argue they serve a broader role as
repositories that preserve compressed patterns of human symbolic
expression--"fossils" of meaningful dynamics that retain relational residues
without their original living contexts. Crucially, these compressed patterns
only become meaningful through human reinterpretation, creating a recursive
feedback loop where they can be recombined and cycle back to ultimately
catalyze human creative processes. Through analysis of four universal
features--compression, decompression, externalization, and recursion--we
demonstrate that just as DNA emerged as a compressed and externalized medium
for preserving useful cellular dynamics without containing explicit reference
to goal-directed physical processes, LLMs preserve useful regularities of human
culture without containing understanding of embodied human experience.
Therefore, we argue that LLMs' significance lies not in rivaling human
intelligence, but in providing humanity a tool for self-reflection and playful
hypothesis-generation in a low-stakes, simulated environment. This framework
positions LLMs as tools for cultural evolvability, enabling humanity to
generate novel hypotheses about itself while maintaining the human
interpretation necessary to ground these hypotheses in ongoing human aesthetics
and norms.

</details>


### [48] [CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks](https://arxiv.org/abs/2506.21607)
*Dipak Meher,Carlotta Domeniconi,Guadalupe Correa-Cabrera*

Key words: CORE-KG，知识图谱，法律文本，共指消解

TL;DR: CORE-KG是一个模块化框架，通过两步流程从法律文本中构建可解释的知识图谱，减少了节点重复和噪声。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决法律文本中知识图谱构建的挑战，如高噪声和节点重复问题。

Method: 使用两步流程：类型感知的共指消解和基于领域指导的实体关系提取。

Result: 相比基线方法，节点重复减少33.28%，噪声降低38.37%。

Conclusion: CORE-KG为分析复杂犯罪网络提供了更强的基础。

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer valuable insights but are unstructured, lexically
dense, and filled with ambiguous or shifting references-posing challenges for
automated knowledge graph (KG) construction. Existing KG methods often rely on
static templates and lack coreference resolution, while recent LLM-based
approaches frequently produce noisy, fragmented graphs due to hallucinations,
and duplicate nodes caused by a lack of guided extraction. We propose CORE-KG,
a modular framework for building interpretable KGs from legal texts. It uses a
two-step pipeline: (1) type-aware coreference resolution via sequential,
structured LLM prompts, and (2) entity and relationship extraction using
domain-guided instructions, built on an adapted GraphRAG framework. CORE-KG
reduces node duplication by 33.28%, and legal noise by 38.37% compared to a
GraphRAG-based baseline-resulting in cleaner and more coherent graph
structures. These improvements make CORE-KG a strong foundation for analyzing
complex criminal networks.

</details>


### [49] [SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2](https://arxiv.org/abs/2506.21608)
*Yasmine Bouamra,Bruno Yun,Alexandre Poisson,Frédéric Armetta*

Key words: SysML v2, 多代理系统, 自然语言处理, 模板生成

TL;DR: SysTemp是一个基于多代理系统的工具，旨在通过自然语言规范生成SysML v2模型，解决学习语料稀缺和语法复杂的问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决SysML v2模型自动生成中的语料稀缺和语法复杂问题。

Method: 采用多代理系统，包括模板生成器来结构化生成过程。

Result: 通过评估展示了该系统在提高SysML v2建模质量方面的潜力。

Conclusion: SysTemp能有效改进SysML v2模型的生成质量和效率。

Abstract: The automatic generation of SysML v2 models represents a major challenge in
the engineering of complex systems, particularly due to the scarcity of
learning corpora and complex syntax. We present SysTemp, a system aimed at
facilitating and improving the creation of SysML v2 models from natural
language specifications. It is based on a multi-agent system, including a
template generator that structures the generation process. We discuss the
advantages and challenges of this system through an evaluation, highlighting
its potential to improve the quality of the generations in SysML v2 modeling.

</details>


### [50] [From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models](https://arxiv.org/abs/2506.21609)
*Junhao Liu,Zhenhao Xu,Yuxin Fang,Yichuan Chen,Zuobin Ying,Wenhan Chang*

Key words: 大语言模型、推理过程、自我反思模式、跨领域关联、模型比较

TL;DR: 论文提出一种新框架，分析四种先进大语言模型的推理特征，揭示其思考过程与输出的关系，并比较它们在推理深度、中间步骤依赖等方面的差异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有研究缺乏对大语言模型推理过程和输出的系统性比较，尤其是关于其自我反思模式（"顿悟时刻"）和跨领域关联的研究。

Method: 使用关键词统计和LLM-as-a-judge范式，分析四种模型（GPT-o1、DeepSeek-R1、Kimi-k1.5和Grok-3）的推理特征；利用多样化数据集和评估指标分析推理连贯性与输出准确性。

Result: 研究揭示了模型在探索与利用、问题处理和结论达成等方面的不同模式，并比较了它们的推理深度、中间步骤依赖以及与GPT-o1的相似性差异。

Conclusion: 该研究为计算效率与推理鲁棒性之间的权衡提供了见解，并为模型设计和评估提供了实用建议。

Abstract: Recently, there have been notable advancements in large language models
(LLMs), demonstrating their growing abilities in complex reasoning. However,
existing research largely overlooks a thorough and systematic comparison of
these models' reasoning processes and outputs, particularly regarding their
self-reflection pattern (also termed "Aha moment") and the interconnections
across diverse domains. This paper proposes a novel framework for analyzing the
reasoning characteristics of four cutting-edge large reasoning models (GPT-o1,
DeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge
paradigm. Our approach connects their internal thinking processes with their
final outputs. A diverse dataset consists of real-world scenario-based
questions covering logical deduction, causal inference, and multi-step
problem-solving. Additionally, a set of metrics is put forward to assess both
the coherence of reasoning and the accuracy of the outputs. The research
results uncover various patterns of how these models balance exploration and
exploitation, deal with problems, and reach conclusions during the reasoning
process. Through quantitative and qualitative comparisons, disparities among
these models are identified in aspects such as the depth of reasoning, the
reliance on intermediate steps, and the degree of similarity between their
thinking processes and output patterns and those of GPT-o1. This work offers
valuable insights into the trade-off between computational efficiency and
reasoning robustness and provides practical recommendations for enhancing model
design and evaluation in practical applications. We publicly release our
project at: https://github.com/ChangWenhan/FromThinking2Output

</details>


### [51] [Does Multimodality Lead to Better Time Series Forecasting?](https://arxiv.org/abs/2506.21611)
*Xiyuan Zhang,Boran Han,Haoyang Fang,Abdul Fatir Ansari,Shuai Zhang,Danielle C. Maddix,Cuixiong Hu,Andrew Gordon Wilson,Michael W. Mahoney,Hao Wang,Yan Liu,Huzefa Rangwala,George Karypis,Bernie Wang*

Key words: 多模态学习, 时间序列预测, 文本信息, 对齐方法, 提示方法

TL;DR: 该研究探讨了在多模态时间序列预测中文本信息的作用，发现其效果并非普遍适用，并提出了建模和数据方面的关键条件。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究多模态整合（文本与时序数据）在预测任务中是否总是有效，以及在何种条件下能带来增益。

Method: 通过14个不同领域的预测任务，评估两种多模态预测范式：对齐方法和提示方法，并分析了模型架构和数据特性的影响。

Result: 多模态方法的效果因数据集和模型而异，并非总是优于单模态基准；增益的关键条件包括高容量文本模型、弱时序模型、适当对齐策略、充足训练数据和互补文本信号。

Conclusion: 研究为多模态预测的实际应用提供了指导，明确了其适用性和局限性。

Abstract: Recently, there has been growing interest in incorporating textual
information into foundation models for time series forecasting. However, it
remains unclear whether and under what conditions such multimodal integration
consistently yields gains. We systematically investigate these questions across
a diverse benchmark of 14 forecasting tasks spanning 7 domains, including
health, environment, and economics. We evaluate two popular multimodal
forecasting paradigms: aligning-based methods, which align time series and text
representations; and prompting-based methods, which directly prompt large
language models for forecasting. Although prior works report gains from
multimodal input, we find these effects are not universal across datasets and
models, and multimodal methods sometimes do not outperform the strongest
unimodal baselines. To understand when textual information helps, we
disentangle the effects of model architectural properties and data
characteristics. Our findings highlight that on the modeling side,
incorporating text information is most helpful given (1) high-capacity text
models, (2) comparatively weaker time series models, and (3) appropriate
aligning strategies. On the data side, performance gains are more likely when
(4) sufficient training data is available and (5) the text offers complementary
predictive signal beyond what is already captured from the time series alone.
Our empirical findings offer practical guidelines for when multimodality can be
expected to aid forecasting tasks, and when it does not.

</details>


### [52] [AdaptGOT: A Pre-trained Model for Adaptive Contextual POI Representation Learning](https://arxiv.org/abs/2506.21612)
*Xiaobin Ren,Xinyu Zhu,Kaiqi Zhao*

Key words: POI embedding, adaptive learning, GOT representation, attention mechanism, MoE architecture

TL;DR: 摘要介绍了POI嵌入方法中的挑战，并提出AdaptGOT模型，结合自适应学习和GOT表示，通过实验验证其优越性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有POI嵌入方法在多上下文采样、POI多语境探索、通用性和泛化能力方面存在不足。

Method: AdaptGOT模型包括上下文邻域生成、基于注意力的GOT表示和MoE自适应编码器-解码器架构。

Result: 在两个真实数据集和多项POI任务上的实验表明AdaptGOT性能优越。

Conclusion: AdaptGOT模型通过多采样策略和注意力机制有效解决了当前POI嵌入的挑战。

Abstract: Currently, considerable strides have been achieved in Point-of-Interest (POI)
embedding methodologies, driven by the emergence of novel POI tasks like
recommendation and classification. Despite the success of task-specific,
end-to-end models in POI embedding, several challenges remain. These include
the need for more effective multi-context sampling strategies, insufficient
exploration of multiple POI contexts, limited versatility, and inadequate
generalization. To address these issues, we propose the AdaptGOT model, which
integrates both the (Adapt)ive representation learning technique and the
Geographical-Co-Occurrence-Text (GOT) representation with a particular emphasis
on Geographical location, Co-Occurrence and Textual information. The AdaptGOT
model comprises three key components: (1) contextual neighborhood generation,
which integrates advanced mixed sampling techniques such as KNN, density-based,
importance-based, and category-aware strategies to capture complex contextual
neighborhoods; (2) an advanced GOT representation enhanced by an attention
mechanism, designed to derive high-quality, customized representations and
efficiently capture complex interrelations between POIs; and (3) the MoE-based
adaptive encoder-decoder architecture, which ensures topological consistency
and enriches contextual representation by minimizing Jensen-Shannon divergence
across varying contexts. Experiments on two real-world datasets and multiple
POI tasks substantiate the superior performance of the proposed AdaptGOT model.

</details>


### [53] [ChildGuard: A Specialized Dataset for Combatting Child-Targeted Hate Speech](https://arxiv.org/abs/2506.21613)
*Gautam Siddharth Kashyap,Mohammad Anas Azeez,Rafiq Ali,Zohaib Hasan Siddiqui,Jiechao Gao,Usman Naseem*

Key words: 仇恨言论, 儿童保护, 数据集, 大型语言模型

TL;DR: 研究人员开发了ChildGuard1数据集，专门针对儿童仇恨言论，填补了现有数据集的不足，并评估了现有检测方法的有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有仇恨言论数据集缺乏针对儿童的年龄特定标注和情感影响分析，无法有效解决这一问题。

Method: 通过从现有语料库中提取并添加儿童特定标注，构建ChildGuard1数据集，并评估现有仇恨言论检测方法的性能。

Result: ChildGuard1提供了一个全面的儿童仇恨言论数据集，并展示了现有方法在该领域的局限性。

Conclusion: ChildGuard1为改进儿童仇恨言论检测方法提供了基础，并鼓励进一步研究。

Abstract: The increasing prevalence of child-targeted hate speech online underscores
the urgent need for specialized datasets to address this critical issue.
Existing hate speech datasets lack agespecific annotations, fail to capture
nuanced contexts, and overlook the unique emotional impact on children. To
bridge this gap, we introduce ChildGuard1, a curated dataset derived from
existing corpora and enriched with child-specific annotations. ChildGuard
captures diverse contexts of child-targeted hate speech, spanning age groups.
We benchmark existing state-of-the-art hate speech detection methods, including
Large Language Models (LLMs), and assess their effectiveness in detecting and
contextualizing child-targeted hate speech. To foster further research in this
area, we publicly release ChildGuard, providing a robust foundation for
developing improved methods to detect and mitigate such harm.

</details>


### [54] [LastingBench: Defend Benchmarks Against Knowledge Leakage](https://arxiv.org/abs/2506.21614)
*Yixiong Fang,Tianran Sun,Yuling Shi,Min Wang,Xiaodong Gu*

Key words: 大型语言模型、问答基准测试、数据泄露、反事实重写、评估鲁棒性

TL;DR: 研究提出LastingBench框架，通过扰动和重写泄漏点来减少大型语言模型在问答基准测试中的数据记忆效应，提升评估的公正性和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型在问答基准测试中可能因数据泄露而作弊，降低评估有效性。现有研究多集中于检测泄露，而较少关注其影响缓解。

Method: 提出LastingBench框架，通过扰动识别泄漏点，并将其重写为反事实内容，破坏模型的记忆能力。

Result: 在最新问答基准测试中，LastingBench显著降低了模型的记忆效应，展现了性能差距。

Conclusion: LastingBench为基准测试提供了可扩展的长期保护方案，确保评估的鲁棒性和公平性。

Abstract: The increasing complexity of large language models (LLMs) raises concerns
about their ability to "cheat" on standard Question Answering (QA) benchmarks
by memorizing task-specific data. This undermines the validity of benchmark
evaluations, as they no longer reflect genuine model capabilities but instead
the effects of data leakage. While prior work has focused on detecting such
leakage, little attention has been given to mitigating its impact and
preserving the long-term utility of benchmarks. In this paper, we introduce
LastingBench, a novel framework designed to continuously reinforce and
safeguard existing benchmarks against knowledge leakage. LastingBench
identifies leakage points in the context through perturbation, then rewrites
the leakage points to counterfactual ones-disrupting memorization while
preserving the benchmark's original evaluative intent. Evaluations of
state-of-the-art QA benchmarks show significant performance gaps, highlighting
the efficacy of LastingBench in reducing memorization effects. LastingBench
offers a practical and scalable solution to ensure benchmark robustness over
time, promoting fairer and more interpretable evaluations of LLMs.

</details>


### [55] [Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines](https://arxiv.org/abs/2506.21615)
*Wenhao Li,Hongkuan Zhang,Hongwei Zhang,Zhengxu Li,Zengjie Dong,Yafan Chen,Niranjan Bidargaddi,Hong Liu*

Key words: 医疗语言模型, 临床实践指南, 检索增强生成, ICD代码, EHR

TL;DR: GARMLE-G是一种基于检索的框架，旨在通过直接引用临床实践指南（CPGs）来生成医疗语言模型的输出，避免了模型生成文本的幻觉问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的医疗语言模型依赖于ICD代码进行诊断预测，但这些代码无法捕捉临床医生的复杂推理过程，导致模型在临床上的实用性受限。

Method: GARMLE-G通过结合LLM预测与EHR数据生成语义丰富的查询，检索相关的CPG知识片段，并将指南内容与模型输出融合，生成临床对齐的建议。

Result: 在高血压诊断的原型系统中，GARMLE-G在检索精度、语义相关性和临床指南依从性上优于基于RAG的基线模型，同时保持了轻量级架构。

Conclusion: GARMLE-G提供了一种可扩展、低成本的幻觉免费方法，为医疗语言模型在循证临床实践中的应用提供了潜力。

Abstract: Current medical language models, adapted from large language models (LLMs),
typically predict ICD code-based diagnosis from electronic health records
(EHRs) because these labels are readily available. However, ICD codes do not
capture the nuanced, context-rich reasoning clinicians use for diagnosis.
Clinicians synthesize diverse patient data and reference clinical practice
guidelines (CPGs) to make evidence-based decisions. This misalignment limits
the clinical utility of existing models. We introduce GARMLE-G, a
Generation-Augmented Retrieval framework that grounds medical language model
outputs in authoritative CPGs. Unlike conventional Retrieval-Augmented
Generation based approaches, GARMLE-G enables hallucination-free outputs by
directly retrieving authoritative guideline content without relying on
model-generated text. It (1) integrates LLM predictions with EHR data to create
semantically rich queries, (2) retrieves relevant CPG knowledge snippets via
embedding similarity, and (3) fuses guideline content with model output to
generate clinically aligned recommendations. A prototype system for
hypertension diagnosis was developed and evaluated on multiple metrics,
demonstrating superior retrieval precision, semantic relevance, and clinical
guideline adherence compared to RAG-based baselines, while maintaining a
lightweight architecture suitable for localized healthcare deployment. This
work provides a scalable, low-cost, and hallucination-free method for grounding
medical language models in evidence-based clinical practice, with strong
potential for broader clinical deployment.

</details>


### [56] [TIM: A Large-Scale Dataset and large Timeline Intelligence Model for Open-domain Timeline Summarization](https://arxiv.org/abs/2506.21616)
*Chuanrui Hu,Wei Hu,Penghang Yu,Hua Zhang,Bing-Kun Bao*

Key words: 时间线摘要, 开放领域, 大型语言模型, 渐进优化, 双对齐奖励

TL;DR: 提出了一种名为TIM的大规模时间线智能模型，用于开放领域时间线摘要，通过渐进优化策略提升摘要性能，解决了现有大型语言模型在主题相关性和时间准确性上的不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有大型语言模型在开放领域时间线摘要中存在主题相关性评估和主题演化理解不足的问题，导致摘要中包含无关信息或时间戳不准确。

Method: 1. 构建了一个大规模时间线摘要数据集；2. 提出渐进优化策略，包括指令调优和双对齐奖励学习方法，分别提升摘要能力和主题演化理解能力。

Result: TIM模型在开放领域中表现出强大的时间线摘要能力，实验验证了其有效性。

Conclusion: TIM模型通过渐进优化策略显著提升了开放领域时间线摘要的性能，为解决现有问题提供了有效方案。

Abstract: Open-domain Timeline Summarization (TLS) is crucial for monitoring the
evolution of news topics. To identify changes in news topics, existing methods
typically employ general Large Language Models (LLMs) to summarize relevant
timestamps from retrieved news. While general LLMs demonstrate capabilities in
zero-shot news summarization and timestamp localization, they struggle with
assessing topic relevance and understanding topic evolution. Consequently, the
summarized information often includes irrelevant details or inaccurate
timestamps. To address these issues, we propose the first large Timeline
Intelligence Model (TIM) for open-domain TLS, which is capable of effectively
summarizing open-domain timelines. Specifically, we begin by presenting a
large-scale TLS dataset, comprising over 1,000 news topics and more than 3,000
annotated TLS instances. Furthermore, we propose a progressive optimization
strategy, which gradually enhance summarization performance. It employs
instruction tuning to enhance summarization and topic-irrelevant information
filtering capabilities. Following this, it exploits a novel dual-alignment
reward learning method that incorporates both semantic and temporal
perspectives, thereby improving the understanding of topic evolution
principles. Through this progressive optimization strategy, TIM demonstrates a
robust ability to summarize open-domain timelines. Extensive experiments in
open-domain demonstrate the effectiveness of our TIM.

</details>


### [57] [TrajTok: Technical Report for 2025 Waymo Open Sim Agents Challenge](https://arxiv.org/abs/2506.21618)
*Zhiyuan Zhang,Xiaosong Jia,Guanyu Chen,Qifeng Li,Junchi Yan*

Key words: TrajTok, 行为生成模型, 空间感知标签平滑, 交叉熵损失, SMART模型

TL;DR: TrajTok是一种结合数据驱动和规则方法的轨迹标记器，用于离散下一令牌预测的行为生成模型，具有更好的覆盖性、对称性和鲁棒性，同时提出了空间感知标签平滑方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 提出一种更有效的行为生成模型标记方法，解决现有方法的不足。

Method: 结合数据驱动和规则方法设计TrajTok标记器，并采用空间感知标签平滑方法优化交叉熵损失。

Result: 在Waymo Open Sim Agents Challenge 2025中，SMART模型实现了0.7852的真实性评分，表现优异。

Conclusion: TrajTok标记器及其配套损失方法有效提升了行为生成模型的性能，未来将开源代码。

Abstract: In this technical report, we introduce TrajTok, a trajectory tokenizer for
discrete next-token-prediction based behavior generation models, which combines
data-driven and rule-based methods with better coverage, symmetry and
robustness, along with a spatial-aware label smoothing method for cross-entropy
loss. We adopt the tokenizer and loss for the SMART model and reach a superior
performance with realism score of 0.7852 on the Waymo Open Sim Agents Challenge
2025. We will open-source the code in the future.

</details>


### [58] [IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech](https://arxiv.org/abs/2506.21619)
*Siyi Zhou,Yiquan Zhou,Yi He,Xun Zhou,Jinchao Wang,Wei Deng,Jingchen Shu*

Key words: 文本到语音, 自回归模型, 持续时间控制, 情感解耦, 零样本生成

TL;DR: IndexTTS2提出了一种新的自回归模型友好的语音持续时间控制方法，同时支持精确控制和自由生成模式，并实现了音色与情感的解耦控制，在零样本设置下表现出色。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 自回归系统在语音自然性上有优势，但在语音持续时间控制上存在局限性，尤其是在需要严格视听同步的应用（如视频配音）中。

Method: IndexTTS2通过两种生成模式控制时长，结合GPT潜表示增强稳定性，并使用基于文本描述的软指令机制进行情感控制。

Result: 实验表明，IndexTTS2在词错误率、说话人相似性和情感保真度上均优于现有零样本TTS模型。

Conclusion: IndexTTS2为TTS系统提供了更灵活的时长和情感控制能力，显著提升了性能和应用范围。

Abstract: Large-scale text-to-speech (TTS) models are typically categorized into
autoregressive and non-autoregressive systems. Although autoregressive systems
exhibit certain advantages in speech naturalness, their token-by-token
generation mechanism makes it difficult to precisely control the duration of
synthesized speech. This is a key limitation in applications such as video
dubbing that require strict audio-visual synchronization. This paper introduces
IndexTTS2, which proposes a novel and autoregressive-model-friendly method for
speech duration control. The method supports two generation modes: one allows
explicit specification of the number of generated tokens for precise duration
control; the other does not require manual input and lets the model freely
generate speech while preserving prosodic characteristics from the input
prompt. Furthermore, IndexTTS2 achieves disentanglement between emotional
expression and speaker identity, enabling independent control of timbre and
emotion. In the zero-shot setting, the model can perfectly reproduce the
emotional characteristics of the input prompt. Users may also provide a
separate emotion prompt, even from a different speaker, allowing the model to
reconstruct the target timbre while conveying the desired emotion. To enhance
clarity during strong emotional expressions, we incorporate GPT latent
representations to improve speech stability. Meanwhile, to lower the barrier
for emotion control, we design a soft instruction mechanism based on textual
descriptions by fine-tuning Qwen3. This enables effective guidance of speech
generation with desired emotional tendencies using natural language input.
Experimental results demonstrate that IndexTTS2 outperforms existing
state-of-the-art zero-shot TTS models in word error rate, speaker similarity,
and emotional fidelity.

</details>


### [59] [How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit](https://arxiv.org/abs/2506.21620)
*Daniele Cirulli,Giulio Cimini,Giovanni Palermo*

Key words: 大型语言模型,政治讨论,GPT-4,在线影响,话语操控

TL;DR: 研究了GPT-4在模拟2016年美国总统选举期间Reddit讨论中的表现，发现其能生成真实的政治倾向评论，但更易于达成共识，且真实与生成评论在语义嵌入空间可区分。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 评估大型语言模型在政治相关在线讨论中的表现，探讨其潜在影响。

Method: 通过三次实验，让GPT-4模拟真实或虚拟党派用户生成评论，分析其政治立场、情感和语言特征。

Result: GPT-4能生成真实评论，但倾向于共识；真实与生成评论在语义空间可区分。

Conclusion: 揭示了大型语言模型可能被用于操纵政治讨论的潜力，引发对AI驱动话语操控的广泛关注。

Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for
natural language generation, with applications spanning from content creation
to social simulations. Their ability to mimic human interactions raises both
opportunities and concerns, particularly in the context of politically relevant
online discussions. In this study, we evaluate the performance of LLMs in
replicating user-generated content within a real-world, divisive scenario:
Reddit conversations during the 2016 US Presidential election. In particular,
we conduct three different experiments, asking GPT-4 to generate comments by
impersonating either real or artificial partisan users. We analyze the
generated comments in terms of political alignment, sentiment, and linguistic
features, comparing them against real user contributions and benchmarking
against a null model. We find that GPT-4 is able to produce realistic comments,
both in favor of or against the candidate supported by the community, yet
tending to create consensus more easily than dissent. In addition we show that
real and artificial comments are well separated in a semantically embedded
space, although they are indistinguishable by manual inspection. Our findings
provide insights on the potential use of LLMs to sneak into online discussions,
influence political debate and shape political narratives, bearing broader
implications of AI-driven discourse manipulation.

</details>


### [60] [The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs](https://arxiv.org/abs/2506.21621)
*Jasper Dekoninck,Ivo Petrov,Kristian Minchev,Mislav Balunovic,Martin Vechev,Miroslav Marinov,Maria Drencheva,Lyuba Konova,Milen Shumanov,Kaloyan Tsvetkov,Nikolay Drenchev,Lazar Todorov,Kalina Nikolova,Nikolay Georgiev,Vanesa Kalinkova,Margulan Ismoldayev*

Key words: LLM,数学证明生成,Open Proof Corpus,数据集,微调

TL;DR: Open Proof Corpus (OPC)是一个包含5000多条经过人类评估的LLM生成证明的数据集，旨在推动数学证明生成研究。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前缺乏大规模、高质量的人类评估证明数据集，阻碍了LLM在数学证明生成领域的进一步进展。

Method: 提出OPC数据集，探索自然语言与形式化证明生成之间的性能差距等问题，并利用数据集微调模型。

Result: 微调后的8B参数模型在评估证明正确性任务中表现与Gemini-2.5-Pro相当。

Conclusion: OPC为数学证明生成研究提供了重要资源，并展示了其实际应用价值。

Abstract: In recent months, large language models (LLMs) have made significant progress
in mathematical proof generation, but further advancement is hindered by the
lack of a large-scale, high-quality dataset of human-evaluated proofs. While
expensive to create, such a dataset is essential for driving improvements in
training and enabling a rigorous analysis of proof generation capabilities. In
this work, we present the Open Proof Corpus (OPC), a dataset comprising over
5,000 human-evaluated proofs produced by state-of-the-art LLMs. The OPC was
specifically designed for broad applicability and downstream usage in proof
generation research and is the first to include a substantial number of
correct, LLM-generated solutions to problems from prestigious mathematics
competitions such as the USAMO and IMO. Using the OPC, we explore critical
questions in automated proof generation: (1) the performance gap between
natural language and formal proof generation, (2) the discrepancy between
final-answer accuracy and full-proof validity, and (3) the impact of best-of-n
selection on proof quality. Finally, to showcase the utility of the OPC, we
finetune an 8B-parameter model on the dataset, obtaining a model that performs
on par with the best model, Gemini-2.5-Pro, on the task of evaluating proof
correctness.

</details>


### [61] [Adapting Foundation Speech Recognition Models to Impaired Speech: A Semantic Re-chaining Approach for Personalization of German Speech](https://arxiv.org/abs/2506.21622)
*Niclas Pokel,Pehuén Moure,Roman Boehringer,Yingqiang Gao*

Key words: 自动语音识别,非标准语音,语音障碍,个性化模型,Whisper

TL;DR: 提出了一种轻量级方法，用于个性化适应ASR模型，改善非标准语音的转录质量，减少沟通障碍。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于现有的ASR模型（如Whisper）在非标准语音（如脑瘫或遗传疾病导致的语音障碍）上表现不佳，且训练数据有限，本文旨在解决这一问题。

Method: 提出了一种实用且轻量级的流程，通过选择特定单词并增强小规模语音障碍数据集的语义连贯性，对ASR模型进行个性化调整。

Result: 在结构性语音障碍儿童的数据上应用此方法，转录质量得到显著提升。

Conclusion: 该方法展示了在减少非标准语音模式个体沟通障碍方面的潜力。

Abstract: Speech impairments caused by conditions such as cerebral palsy or genetic
disorders pose significant challenges for automatic speech recognition (ASR)
systems. Despite recent advances, ASR models like Whisper struggle with
non-normative speech due to limited training data and the difficulty of
collecting and annotating non-normative speech samples. In this work, we
propose a practical and lightweight pipeline to personalize ASR models,
formalizing the selection of words and enriching a small, speech-impaired
dataset with semantic coherence. Applied to data from a child with a structural
speech impairment, our approach shows promising improvements in transcription
quality, demonstrating the potential to reduce communication barriers for
individuals with atypical speech patterns.

</details>


### [62] [Performance of diverse evaluation metrics in NLP-based assessment and text generation of consumer complaints](https://arxiv.org/abs/2506.21623)
*Peiheng Gao,Chen Yang,Ning Sun,Ričardas Zitikis*

Key words: 机器学习, 文本分类, 人工经验算法, 合成数据, 生成对抗网络

TL;DR: 该研究通过结合人工经验训练算法和合成数据生成方法，旨在提升机器学习在文本分类中的性能，减少数据获取成本，并增强模型鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决机器学习在自然语言处理中难以捕捉细微语义差异的挑战，尤其是在消费者投诉分类任务中。

Method: 采用人工经验训练算法识别语义差异，并结合生成对抗网络生成的合成数据，通过专家标注进行优化。

Result: 预期显著提升分类器性能，降低数据获取成本，并改善评价指标和模型鲁棒性。

Conclusion: 结合专家知识和高质量合成数据是提升文本分类任务的有效途径。

Abstract: Machine learning (ML) has significantly advanced text classification by
enabling automated understanding and categorization of complex, unstructured
textual data. However, accurately capturing nuanced linguistic patterns and
contextual variations inherent in natural language, particularly within
consumer complaints, remains a challenge. This study addresses these issues by
incorporating human-experience-trained algorithms that effectively recognize
subtle semantic differences crucial for assessing consumer relief eligibility.
Furthermore, we propose integrating synthetic data generation methods that
utilize expert evaluations of generative adversarial networks and are refined
through expert annotations. By combining expert-trained classifiers with
high-quality synthetic data, our research seeks to significantly enhance
machine learning classifier performance, reduce dataset acquisition costs, and
improve overall evaluation metrics and robustness in text classification tasks.

</details>


### [63] [Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents](https://arxiv.org/abs/2506.21625)
*Jiaxi Zhuang,Kangning Li,Jue Hou,Mingjun Xu,Zhifeng Gao,Hengxing Cai*

Key words: 分子构效关系, 多模态大语言模型, 监督微调, 科学文献分析, 基准评测

TL;DR: 本文提出了Doc2SAR框架和DocSAR-200基准，用于从科学文献和专利中提取分子构效关系，通过结合领域专用工具和经过监督微调的多模态大语言模型，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 从异质文档格式中提取分子构效关系（SARs）存在挑战，现有方法（如基于规则或通用多模态大语言模型）在准确性和泛化性上不足，需要更好解决方案。

Method: 提出Doc2SAR框架，结合领域专用工具和监督微调的多模态大语言模型（MLLMs），并构建DocSAR-200基准用于评估。

Result: Doc2SAR在DocSAR-200基准上综合召回率达到80.78%，比GPT-4o高51.48%，且推理高效，附带网页应用。

Conclusion: Doc2SAR在SAR提取任务中表现优异，为药物发现和材料研究提供了实用工具。

Abstract: Extracting molecular structure-activity relationships (SARs) from scientific
literature and patents is essential for drug discovery and materials research.
However, this task remains challenging due to heterogeneous document formats
and limitations of existing methods. Specifically, rule-based approaches
relying on rigid templates fail to generalize across diverse document layouts,
while general-purpose multimodal large language models (MLLMs) lack sufficient
accuracy and reliability for specialized tasks, such as layout detection and
optical chemical structure recognition (OCSR). To address these challenges, we
introduce DocSAR-200, a rigorously annotated benchmark of 200 scientific
documents designed specifically for evaluating SAR extraction methods.
Additionally, we propose Doc2SAR, a novel synergistic framework that integrates
domain-specific tools with MLLMs enhanced via supervised fine-tuning (SFT).
Extensive experiments demonstrate that Doc2SAR achieves state-of-the-art
performance across various document types, significantly outperforming leading
end-to-end baselines. Specifically, Doc2SAR attains an overall Table Recall of
80.78% on DocSAR-200, exceeding end2end GPT-4o by 51.48%. Furthermore, Doc2SAR
demonstrates practical usability through efficient inference and is accompanied
by a web app.

</details>


### [64] [Do We Really Need GNNs with Explicit Structural Modeling? MLPs Suffice for Language Model Representations](https://arxiv.org/abs/2506.21682)
*Li Zhou,Hao Jiang,Junjie Li,Zefeng Zhao,Feng Jiang,Wenyu Chen,Haizhou Li*

Key words: 图神经网络（GNNs）、多层感知机（MLPs）、结构建模、语言模型、信息论

TL;DR: 该论文通过信息论视角提出了一种探测框架，评估显式结构建模对语言模型表征的提升作用，并探讨MLPs作为GNNs高效替代方案的潜力。研究发现MLPs和具有特征转换的GNNs能有效编码语言知识，而仅依赖消息传递的模型表现较差。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: GNNs未充分利用结构信息，而MLPs在结构感知任务中表现出意外能力。本文旨在系统评估显式结构建模的作用，并探索MLPs的潜力。

Method: 提出模块化探测框架，通过控制模块选择性使用GNN或其解耦组件（消息传递与特征转换），评估各自贡献。使用Edge Probing Suite工具。

Result: MLPs和具有特征转换的GNNs能提升语言模型表征的语言知识；仅依赖消息传递的模型表现较差。

Conclusion: 特征转换操作对语言知识编码至关重要，MLPs可作为高效替代方案。

Abstract: Explicit structural information has been proven to be encoded by Graph Neural
Networks (GNNs), serving as auxiliary knowledge to enhance model capabilities
and improve performance in downstream NLP tasks. However, recent studies
indicate that GNNs fail to fully utilize structural information, whereas
Multi-Layer Perceptrons (MLPs), despite lacking the message-passing mechanisms
inherent to GNNs, exhibit a surprising ability in structure-aware tasks.
Motivated by these findings, this paper introduces a comprehensive probing
framework from an information-theoretic perspective. The framework is designed
to systematically assess the role of explicit structural modeling in enhancing
language model (LM) representations and to investigate the potential of MLPs as
efficient and scalable alternatives to GNNs. We extend traditional probing
classifiers by incorporating a control module that allows for selective use of
either the full GNN model or its decoupled components, specifically, the
message-passing and feature-transformation operations.This modular approach
isolates and assesses the individual contributions of these operations,
avoiding confounding effects from the complete GNN architecture. Using the Edge
Probing Suite, a diagnostic tool for evaluating the linguistic knowledge
encoded in LMs, we find that MLPs, when used as feature-transformation modules,
consistently improve the linguistic knowledge captured in LM representations
across different architectures. They effectively encode both syntactic and
semantic patterns. Similarly, GNNs that incorporate feature-transformation
operations show beneficial effects. In contrast, models that rely solely on
message-passing operations tend to underperform, often leading to negative
impacts on probing task performance.

</details>


### [65] [ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla Regional Languages](https://arxiv.org/abs/2506.21686)
*Swastika Kundu,Autoshi Ibrahim,Mithila Rahman,Tanvir Ahmed*

Key words: 情感分析, 孟加拉语方言, ANUBHUTI数据集, 双重标注

TL;DR: 本文介绍了ANUBHUTI数据集，用于探索孟加拉语方言的情感分析问题，填补了该领域的数据空白。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 孟加拉语方言的情感分析因语言多样性和标注数据有限而未得到充分研究。

Method: 构建了包含2000句从标准孟加拉语翻译为四种方言的数据集，采用双重标注方案（主题和情感）。

Result: 通过专家翻译和标注，数据质量高，Cohen's Kappa显示标注一致性良好。

Conclusion: ANUBHUTI填补了低资源孟加拉语方言情感分析的空白，有助于更准确的NLP研究。

Abstract: Sentiment analysis for regional dialects of Bangla remains an underexplored
area due to linguistic diversity and limited annotated data. This paper
introduces ANUBHUTI, a comprehensive dataset consisting of 2000 sentences
manually translated from standard Bangla into four major regional dialects
Mymensingh, Noakhali, Sylhet, and Chittagong. The dataset predominantly
features political and religious content, reflecting the contemporary socio
political landscape of Bangladesh, alongside neutral texts to maintain balance.
Each sentence is annotated using a dual annotation scheme: multiclass thematic
labeling categorizes sentences as Political, Religious, or Neutral, and
multilabel emotion annotation assigns one or more emotions from Anger,
Contempt, Disgust, Enjoyment, Fear, Sadness, and Surprise. Expert native
translators conducted the translation and annotation, with quality assurance
performed via Cohens Kappa inter annotator agreement, achieving strong
consistency across dialects. The dataset was further refined through systematic
checks for missing data, anomalies, and inconsistencies. ANUBHUTI fills a
critical gap in resources for sentiment analysis in low resource Bangla
dialects, enabling more accurate and context aware natural language processing.

</details>


### [66] [Identifying Speaker Information in Feed-Forward Layers of Self-Supervised Speech Transformers](https://arxiv.org/abs/2506.21712)
*Tzu-Quan Lin,Hsi-Chun Cheng,Hung-yi Lee,Hao Tang*

Key words: 自监督语音Transformer, 说话者信息, 前馈层, k均值聚类, 神经元分析

TL;DR: 该研究探讨了自监督语音Transformers模型中如何编码说话者信息，通过分析前馈层中与说话者信息相关的神经元，揭示了这些神经元在语音任务中的关键作用。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 近年来，自监督语音Transformer在说话者相关应用中表现出显著影响，但缺乏对模型如何编码说话者信息的深入研究，本研究旨在填补这一空白。

Method: 研究分析了前馈层中与自监督特征和i-向量的k均值聚类相关的神经元，通过这些聚类识别与说话者信息相关的神经元。

Result: 研究发现这些神经元与广泛的语言和性别类别相关，表明它们在编码说话者信息中的重要性。通过修剪时保护这些神经元，可以显著保留说话者相关任务的性能。

Conclusion: 前馈层中的特定神经元在编码说话者信息中扮演关键角色，保护这些神经元可以提升模型在说话者任务中的表现。

Abstract: In recent years, the impact of self-supervised speech Transformers has
extended to speaker-related applications. However, little research has explored
how these models encode speaker information. In this work, we address this gap
by identifying neurons in the feed-forward layers that are correlated with
speaker information. Specifically, we analyze neurons associated with k-means
clusters of self-supervised features and i-vectors. Our analysis reveals that
these clusters correspond to broad phonetic and gender classes, making them
suitable for identifying neurons that represent speakers. By protecting these
neurons during pruning, we can significantly preserve performance on
speaker-related task, demonstrating their crucial role in encoding speaker
information.

</details>


### [67] [(Fact) Check Your Bias](https://arxiv.org/abs/2506.21745)
*Eivind Morris Bakke,Nora Winger Heggelund*

Key words: 大型语言模型,事实核查,参数知识偏差,HerO系统,FEVER-25

TL;DR: 研究了大型语言模型（LLMs）中参数知识偏差对HerO系统事实核查结果的影响，发现LLM在直接提示下倾向于标记一半声称“证据不足”，并在不同提示策略下表现出检索证据的差异，但最终裁决预测稳定。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探究LLMs中参数知识偏差如何影响事实核查系统的结果，尤其是HerO系统在FEVER-25基准上的表现。

Method: 通过两种实验：1）直接提示LLM进行事实核查；2）让LLM生成支持、反驳或中立的事实核查文档，分析其影响。

Result: LLM在直接提示下对一半声称标记为“证据不足”，而检索证据受提示策略显著影响，但最终裁决预测稳定。模型有时会拒绝为虚假声称生成支持文档，导致负向偏差。

Conclusion: LLMs的参数知识偏差对事实核查系统的证据检索有显著影响，但裁决预测相对稳定，提示策略需谨慎设计以避免偏差。

Abstract: Automatic fact verification systems increasingly rely on large language
models (LLMs). We investigate how parametric knowledge biases in these models
affect fact-checking outcomes of the HerO system (baseline for FEVER-25). We
examine how the system is affected by: (1) potential bias in Llama 3.1's
parametric knowledge and (2) intentionally injected bias. When prompted
directly to perform fact-verification, Llama 3.1 labels nearly half the claims
as "Not Enough Evidence". Using only its parametric knowledge it is able to
reach a verdict on the remaining half of the claims. In the second experiment,
we prompt the model to generate supporting, refuting, or neutral fact-checking
documents. These prompts significantly influence retrieval outcomes, with
approximately 50\% of retrieved evidence being unique to each perspective.
Notably, the model sometimes refuses to generate supporting documents for
claims it believes to be false, creating an inherent negative bias. Despite
differences in retrieved evidence, final verdict predictions show stability
across prompting strategies. The code is available at:
https://github.com/eibakke/FEVER-8-Shared-Task

</details>


### [68] [Evaluating List Construction and Temporal Understanding capabilities of Large Language Models](https://arxiv.org/abs/2506.21783)
*Alexandru Dumitru,V Venktesh,Adam Jatowt,Avishek Anand*

Key words: 大语言模型、时间理解、列表构建、TLQA基准、封闭书设置、开放域设置

TL;DR: 论文提出了一个名为TLQA的基准测试，用于评估大语言模型在时间引用列表回答问题中的表现，揭示了模型在时间理解和列表构建能力上的不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有大型语言模型在时间理解任务中容易产生幻觉和错误，尤其是在处理多实体和时间区间的任务时。目前缺乏对此类任务的系统性评估基准。

Method: 提出TLQA基准测试，要求模型生成与时间区间对齐的列表形式答案，同时支持封闭书和开放域设置。

Result: 模型在封闭书设置中无法提供完整答案和时间对齐的事实，开放域设置中检索能力也需改进。

Conclusion: TLQA基准填补了现有研究的空白，为未来研究提供了明确方向。

Abstract: Large Language Models (LLMs) have demonstrated immense advances in a wide
range of natural language tasks. However, these models are susceptible to
hallucinations and errors on particularly temporal understanding tasks
involving multiple entities in answers. In such tasks, they fail to associate
entities with accurate time intervals, generate a complete list of entities in
answers or reason about events associated with specific temporal bounds.
Existing works do not extensively evaluate the abilities of the model to
perform implicit and explicit temporal understanding in a list answer
construction setup. To bridge this gap, we propose the Time referenced List
based Question Answering or TLQA benchmark that requires structured answers in
list format aligned with corresponding time periods. Our TLQA benchmark,
requires both list construction and temporal understanding simultaneously,
which to the best of our knowledge has not been explored in prior benchmarks.
We investigate the temporal understanding and list construction capabilities of
state-of-the-art generative models on TLQA in closed-book and open-domain
settings. Our findings reveal significant shortcomings in current models,
particularly their inability to provide complete answers and temporally align
facts in a closed-book setup and the need to improve retrieval in open-domain
setup, providing clear future directions for research on TLQA. The benchmark
and code at https://github.com/elixir-research-group/TLQA.

</details>


### [69] [Offensive Language Detection on Social Media Using XLNet](https://arxiv.org/abs/2506.21795)
*Reem Alothman,Hafida Benhidour,Said Kerrache*

Key words: 攻击性语言检测,XLNet,BERT,OLID,迁移学习

TL;DR: 该研究提出了一种基于XLNet的自动检测网络攻击性语言模型，并在性能上与BERT进行了比较，结果表明XLNet在检测攻击性内容方面表现优于BERT，但在识别攻击目标时BERT略胜一筹。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 社交媒体上攻击性内容的增加使得自动检测系统的需求日益迫切。

Method: 使用XLNet和BERT模型进行攻击性语言检测，并通过OLID数据集进行性能评估。

Result: XLNet在检测攻击性内容方面表现更好，BERT在识别攻击目标上稍优；过采样和欠采样策略有效解决类别不平衡问题。

Conclusion: XLNet和迁移学习方法在构建鲁棒的社交媒体攻击性语言检测系统中具有潜力。

Abstract: The widespread use of text-based communication on social media-through chats,
comments, and microblogs-has improved user interaction but has also led to an
increase in offensive content, including hate speech, racism, and other forms
of abuse. Due to the enormous volume of user-generated content, manual
moderation is impractical, which creates a need for automated systems that can
detect offensive language. Deep learning models, particularly those using
transfer learning, have demonstrated significant success in understanding
natural language through large-scale pretraining. In this study, we propose an
automatic offensive language detection model based on XLNet, a generalized
autoregressive pretraining method, and compare its performance with BERT
(Bidirectional Encoder Representations from Transformers), which is a widely
used baseline in natural language processing (NLP). Both models are evaluated
using the Offensive Language Identification Dataset (OLID), a benchmark Twitter
dataset that includes hierarchical annotations. Our experimental results show
that XLNet outperforms BERT in detecting offensive content and in categorizing
the types of offenses, while BERT performs slightly better in identifying the
targets of the offenses. Additionally, we find that oversampling and
undersampling strategies are effective in addressing class imbalance and
improving classification performance. These findings highlight the potential of
transfer learning and XLNet-based architectures to create robust systems for
detecting offensive language on social media platforms.

</details>


### [70] [A suite of allotaxonometric tools for the comparison of complex systems using rank-turbulence divergence](https://arxiv.org/abs/2506.21808)
*Jonathan St-Onge,Ashley M. A. Fehr,Carter Ward,Calla G. Beauregard,Michael V. Arnold,Samuel F. Rosenblatt,Benjamin Cooley,Christopher M. Danforth,Peter Sheridan Dodds*

Key words: allotaxonographs, 重尾分布, 类型湍流, 可视化工具

TL;DR: 论文提出了一种名为allotaxonographs的工具，用于可视化和比较重尾分布对，支持多种度量方法，并提供了Matlab、Javascript和Python的实现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为了描述和比较复杂系统，需要理论支撑的工具，尤其是在处理重尾分布时。

Method: 围绕类型湍流现象，设计了allotaxonographs工具，支持多种湍流度量方法，并开发了多语言的程序化实现。

Result: 实现了跨平台（Matlab、Javascript、Python）的allotaxonographs工具，用于比较重尾分布。

Conclusion: allotaxonographs为复杂系统的比较提供了强大的可视化工具，扩展了重尾分布分析的应用场景。

Abstract: Describing and comparing complex systems requires principled, theoretically
grounded tools. Built around the phenomenon of type turbulence,
allotaxonographs provide map-and-list visual comparisons of pairs of
heavy-tailed distributions. Allotaxonographs are designed to accommodate a wide
range of instruments including rank- and probability-turbulence divergences,
Jenson-Shannon divergence, and generalized entropy divergences. Here, we
describe a suite of programmatic tools for rendering allotaxonographs for
rank-turbulence divergence in Matlab, Javascript, and Python, all of which have
different use cases.

</details>


### [71] [Towards Transparent AI: A Survey on Explainable Large Language Models](https://arxiv.org/abs/2506.21812)
*Avash Palikhe,Zhenyu Yu,Zichong Wang,Wenbin Zhang*

Key words: 大语言模型、可解释人工智能、XAI、透明度、负责任AI

TL;DR: 该论文系统地综述了大语言模型（LLMs）的可解释性技术，根据其架构分类，并探讨了评估方法和实际应用，旨在推动透明和负责任的大语言模型发展。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管大语言模型在AI领域取得重要进展，但其‘黑盒’特性限制了在高风险领域的应用，缺乏系统化的可解释性方法研究。

Method: 作者通过将可解释人工智能（XAI）方法基于LLMs的架构（编码器-仅、解码器-仅、编码器-解码器）分类，并分析其评估与应用。

Result: 论文全面梳理了XAI技术，并指出了当前评估方法、实际应用中的资源、挑战及未来方向。

Conclusion: 该调查为开发透明和负责任的LLMs提供了指导，强调了可解释性研究的必要性。

Abstract: Large Language Models (LLMs) have played a pivotal role in advancing
Artificial Intelligence (AI). However, despite their achievements, LLMs often
struggle to explain their decision-making processes, making them a 'black box'
and presenting a substantial challenge to explainability. This lack of
transparency poses a significant obstacle to the adoption of LLMs in
high-stakes domain applications, where interpretability is particularly
essential. To overcome these limitations, researchers have developed various
explainable artificial intelligence (XAI) methods that provide
human-interpretable explanations for LLMs. However, a systematic understanding
of these methods remains limited. To address this gap, this survey provides a
comprehensive review of explainability techniques by categorizing XAI methods
based on the underlying transformer architectures of LLMs: encoder-only,
decoder-only, and encoder-decoder models. Then these techniques are examined in
terms of their evaluation for assessing explainability, and the survey further
explores how these explanations are leveraged in practical applications.
Finally, it discusses available resources, ongoing research challenges, and
future directions, aiming to guide continued efforts toward developing
transparent and responsible LLMs.

</details>


### [72] [Exploring the Structure of AI-Induced Language Change in Scientific English](https://arxiv.org/abs/2506.21817)
*Riley Galpin,Bryce Anderson,Tom S. Juzek*

Key words: 科学英语、大型语言模型、词汇频率变化、语义转变、语用分析

TL;DR: 该研究分析了科学英语中词汇频率变化的趋势，探讨了大型语言模型对词汇替换和语义转变的影响。发现语言变化主要是语义和语用层面的，而非纯粹词汇替换。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是探究近年来科学英语中词汇频率突然增加的现象，尤其是大型语言模型如ChatGPT对语言使用的影响，以及这些变化的本质。

Method: 通过PubMed科学摘要的词频趋势分析，结合词性标注，系统地研究了'飙升词'和'下降词'的语义和语法变化。

Result: 发现语义集群通常整体变化，而非单一词汇替换，表明语言变化主要是语义和语用的。形容词'important'使用显著下降，显示了更复杂的语言演变模式。

Conclusion: 语言技术对语言的影响更多体现在语义和语用层面，而非单纯词汇替换，这些发现有助于理解技术对人类语言的塑造作用。

Abstract: Scientific English has undergone rapid and unprecedented changes in recent
years, with words such as "delve," "intricate," and "crucial" showing
significant spikes in frequency since around 2022. These changes are widely
attributed to the growing influence of Large Language Models like ChatGPT in
the discourse surrounding bias and misalignment. However, apart from changes in
frequency, the exact structure of these linguistic shifts has remained unclear.
The present study addresses this and investigates whether these changes involve
the replacement of synonyms by suddenly 'spiking words,' for example, "crucial"
replacing "essential" and "key," or whether they reflect broader semantic and
pragmatic qualifications. To further investigate structural changes, we include
part of speech tagging in our analysis to quantify linguistic shifts over
grammatical categories and differentiate between word forms, like "potential"
as a noun vs. as an adjective. We systematically analyze synonym groups for
widely discussed 'spiking words' based on frequency trends in scientific
abstracts from PubMed. We find that entire semantic clusters often shift
together, with most or all words in a group increasing in usage. This pattern
suggests that changes induced by Large Language Models are primarily semantic
and pragmatic rather than purely lexical. Notably, the adjective "important"
shows a significant decline, which prompted us to systematically analyze
decreasing lexical items. Our analysis of "collapsing" words reveals a more
complex picture, which is consistent with organic language change and contrasts
with the patterns of the abrupt spikes. These insights into the structure of
language change contribute to our understanding of how language technology
continues to shape human language.

</details>


### [73] [PARSI: Persian Authorship Recognition via Stylometric Integration](https://arxiv.org/abs/2506.21840)
*Kourosh Shahnazari,Mohammadali Keshtparvar,Seyed Moein Ayyoubzadeh*

Key words: 波斯古典诗歌, 作者归属, 神经网络, 风格计量, 韵律分析

TL;DR: 提出了一种多输入神经框架，结合语言编码器和领域特定特征，用于波斯古典诗歌的作者归属识别，准确率达71%（加权投票）和高置信度预测97%（0.9阈值）。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决波斯古典诗歌在语言学、风格和韵律上的复杂性对计算作者归属的挑战。

Method: 采用多输入神经框架，结合Transformer语言编码器、Word2Vec嵌入、风格计量和韵律编码，并使用649,653节诗句进行验证。

Result: 加权投票准确率达71%，高置信度预测（0.9阈值）准确率97%。

Conclusion: 该方法为自动化分类、风格分析和作者争议提供了潜力，支持多语言作者归属和风格生成研究。

Abstract: The intricate linguistic, stylistic, and metrical aspects of Persian
classical poetry pose a challenge for computational authorship attribution. In
this work, we present a versatile framework to determine authorship among 67
prominent poets. We employ a multi-input neural framework consisting of a
transformer-based language encoder complemented by features addressing the
semantic, stylometric, and metrical dimensions of Persian poetry. Our feature
set encompasses 100-dimensional Word2Vec embeddings, seven stylometric
measures, and categorical encodings of poetic form and meter. We compiled a
vast corpus of 647,653 verses of the Ganjoor digital collection, validating the
data through strict preprocessing and author verification while preserving
poem-level splitting to prevent overlap. This work employs verse-level
classification and majority and weighted voting schemes in evaluation,
revealing that weighted voting yields 71% accuracy. We further investigate
threshold-based decision filtering, allowing the model to generate highly
confident predictions, achieving 97% accuracy at a 0.9 threshold, though at
lower coverage. Our work focuses on the integration of deep representational
forms with domain-specific features for improved authorship attribution. The
results illustrate the potential of our approach for automated classification
and the contribution to stylistic analysis, authorship disputes, and general
computational literature research. This research will facilitate further
research on multilingual author attribution, style shift, and generative
modeling of Persian poetry.

</details>


### [74] [LinguaSynth: Heterogeneous Linguistic Signals for News Classification](https://arxiv.org/abs/2506.21848)
*Duo Zhang,Junyi Mo*

Key words: 文本分类, 可解释性, 计算效率, 逻辑回归, linguistic 特征

TL;DR: LinguaSynth 是一个基于透明逻辑回归模型的新型文本分类框架，通过整合五种互补的 linguistic 特征类型，解决了深度学习模型在可解释性和计算效率上的问题，并在实验中超越了传统基准。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 深度学习在 NLP 中的广泛应用带来了可解释性和计算效率问题，作者希望通过一种更透明且高效的方法改进文本分类任务。

Method: LinguaSynth 整合了词法、句法、实体级、词级语义和文档级语义五种 linguistic 特征，采用逻辑回归模型进行训练和预测。

Result: 在 20 Newsgroups 数据集上，LinguaSynth 达到了 84.89% 的准确率，比 TF-IDF 基准提升了 3.32%，同时保持了可解释性和计算效率。

Conclusion: LinguaSynth 证明了深度神经网络并非高性能文本分类的必备选择，为可解释且资源高效的 NLP 模型树立了新的标杆。

Abstract: Deep learning has significantly advanced NLP, but its reliance on large
black-box models introduces critical interpretability and computational
efficiency concerns. This paper proposes LinguaSynth, a novel text
classification framework that strategically integrates five complementary
linguistic feature types: lexical, syntactic, entity-level, word-level
semantics, and document-level semantics within a transparent logistic
regression model. Unlike transformer-based architectures, LinguaSynth maintains
interpretability and computational efficiency, achieving an accuracy of 84.89
percent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by
3.32 percent. Through rigorous feature interaction analysis, we show that
syntactic and entity-level signals provide essential disambiguation and
effectively complement distributional semantics. LinguaSynth sets a new
benchmark for interpretable, resource-efficient NLP models and challenges the
prevailing assumption that deep neural networks are necessary for
high-performing text classification.

</details>


### [75] [The Consistency Hypothesis in Uncertainty Quantification for Large Language Models](https://arxiv.org/abs/2506.21849)
*Quan Xiao,Debarun Bhattacharjya,Balaji Ganesan,Radu Marinescu,Katsiaryna Mirylenka,Nhan H Pham,Michael Glass,Junkyu Lee*

Key words: 大型语言模型,置信度估计,黑盒不确定性量化,生成一致性

TL;DR: 该论文研究了大型语言模型（LLM）输出的置信度估计问题，提出了通过生成一致性作为置信度代理的假设，并提出了三种数学陈述和测试方法。实验表明该假设普遍适用，并提出了一种无需数据的黑盒不确定性量化方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究LLM输出的置信度估计对高用户信任应用至关重要，黑盒不确定性量化方法因其实用性广受关注。

Method: 提出一致性假设的三种数学陈述及统计测试方法，并通过实验验证其普遍性，提出基于生成相似性的黑盒UQ方法。

Result: 实验表明一致性假设普遍适用，提出的方法在8个基准数据集和3个任务中表现优于基线。

Conclusion: 一致性假设具有实际价值，提出的黑盒UQ方法可用于提升LLM输出置信度的估计。

Abstract: Estimating the confidence of large language model (LLM) outputs is essential
for real-world applications requiring high user trust. Black-box uncertainty
quantification (UQ) methods, relying solely on model API access, have gained
popularity due to their practical benefits. In this paper, we examine the
implicit assumption behind several UQ methods, which use generation consistency
as a proxy for confidence, an idea we formalize as the consistency hypothesis.
We introduce three mathematical statements with corresponding statistical tests
to capture variations of this hypothesis and metrics to evaluate LLM output
conformity across tasks. Our empirical investigation, spanning 8 benchmark
datasets and 3 tasks (question answering, text summarization, and text-to-SQL),
highlights the prevalence of the hypothesis under different settings. Among the
statements, we highlight the `Sim-Any' hypothesis as the most actionable, and
demonstrate how it can be leveraged by proposing data-free black-box UQ methods
that aggregate similarities between generations for confidence estimation.
These approaches can outperform the closest baselines, showcasing the practical
value of the empirically observed consistency hypothesis.

</details>


### [76] [Derivational Probing: Unveiling the Layer-wise Derivation of Syntactic Structures in Neural Language Models](https://arxiv.org/abs/2506.21861)
*Taiga Someya,Ryo Yoshida,Hitomi Yanaka,Yohei Oseki*

Key words: 神经语言模型、句法结构、派生探测、BERT、自底向上生成

TL;DR: 论文提出了一种名为“派生探测”的方法，研究神经语言模型在不同层中如何构建微观和宏观句法结构。实验基于BERT模型，发现句法结构的生成是从低层到高层的逐步整合过程。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有研究表明神经语言模型在其内部表征中编码了句法结构，但这些结构是如何在不同层中构建的尚不清楚。本文旨在探究这一过程，尤其是微观和宏观句法结构的形成机制。

Method: 采用派生探测方法，分析BERT模型中不同层的词嵌入，研究微观句法结构（如主语名词短语）和宏观句法结构（如根动词与其直接依存词的关系）的构建过程。

Result: 实验表明，句法结构的构造是自底向上的：微观结构在低层生成，并逐渐在高层整合为宏观结构。进一步分析显示，宏观结构的构建时机对下游任务性能至关重要。

Conclusion: 研究发现，神经语言模型中的句法结构构建是一个分层优化的过程，宏观结构的适时整合对模型性能有显著影响。

Abstract: Recent work has demonstrated that neural language models encode syntactic
structures in their internal representations, yet the derivations by which
these structures are constructed across layers remain poorly understood. In
this paper, we propose Derivational Probing to investigate how micro-syntactic
structures (e.g., subject noun phrases) and macro-syntactic structures (e.g.,
the relationship between the root verbs and their direct dependents) are
constructed as word embeddings propagate upward across layers. Our experiments
on BERT reveal a clear bottom-up derivation: micro-syntactic structures emerge
in lower layers and are gradually integrated into a coherent macro-syntactic
structure in higher layers. Furthermore, a targeted evaluation on subject-verb
number agreement shows that the timing of constructing macro-syntactic
structures is critical for downstream performance, suggesting an optimal timing
for integrating global syntactic information.

</details>


### [77] [DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE](https://arxiv.org/abs/2506.21864)
*Hang Shao,Heting Gao,Yunhang Shen,Jiawei Chen,Lijiang Li,Zuwei Long,Bo Tong,Ke Li,Xing Sun*

Key words: 多模态大语言模型,专家混合架构,自适应学习,语音生成

TL;DR: DeepTalk是一种基于专家混合架构的自适应模态专家学习框架，提升原生多模态大语言模型在语音和文本生成中的性能，降低响应延迟。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 原生多模态大语言模型（MLLMs）在语音和文本生成中存在灾难性遗忘和性能下降问题，主要是因为语音-文本配对数据不足。

Method: 采用专家混合架构，自适应区分模态专家并进行单模态训练和联合多模态协同训练。

Result: 性能降幅仅为5.5%，优于原生MLLMs的20%，端到端对话延迟低于0.5秒。

Conclusion: DeepTalk有效解决了原生MLLMs的局限性，实现了高效无缝的语音交互体验。

Abstract: Native multimodal large language models (MLLMs) restructure a single large
language model (LLM) into a spoken language model (SLM) capable of both speech
and text generation. Compared to modular and aligned MLLMs, native MLLMs
preserve richer paralinguistic features such as emotion and prosody, and
generate speech responses directly within the backbone LLM rather than using a
separate speech decoder. This integration also results in lower response
latency and smoother interaction. However, native MLLMs suffer from
catastrophic forgetting and performance degradation because the available
paired speech-text data is insufficient to support the pretraining of MLLMs
compared to the vast amount of text data required to pretrain text LLMs. To
address this issue, we propose DeepTalk, a framework for adaptive modality
expert learning based on a Mixture of Experts (MoE) architecture. DeepTalk
first adaptively distinguishes modality experts according to their modality
load within the LLM. Each modality expert then undergoes specialized
single-modality training, followed by joint multimodal collaborative training.
As a result, DeepTalk incurs only a 5.5% performance drop compared to the
original LLM, which is significantly lower than the average performance drop of
over 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par
with modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within
0.5 seconds, ensuring a seamless and intelligent speech interaction experience.
Code and models are released at https://github.com/talkking/DeepTalk.

</details>


### [78] [WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation](https://arxiv.org/abs/2506.21875)
*Jian Zhang,Linhao Zhang,Bokai Lei,Chuhan Wu,Wei Jia,Xiao Zhou*

Key words: 多模态大型语言模型,语音评测,查询感知,语音对话,模型性能

TL;DR: 提出了一个新型的评测方法，用于评估多模态大型语言模型在语音对话中的表现，填补了现有评测方法的不足，并加入了语音特性的考量。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有评测方法多基于文本，忽略了语音特有的挑战，如韵律、同音词等问题，影响了语音大语言模型的实际应用优化。

Method: 系统收集真实语音对话数据，引入多样的说话者属性和声学条件，并加入语音特有现象的增强；设计基于查询的评测方法，使用定制化评测列表和提示。

Result: 对不同主流语音模型进行全面测试，发现模型在不同语音场景下表现差异显著；查询感知评测能进一步细化语音特定场景的评估。

Conclusion: 该评测方法为语音模型开发与评测提供了有价值的指导。

Abstract: Recent multi-modal Large Language Models (LLMs) such as GPT-4o have
demonstrated strong capabilities of direct speech interaction. However, the
lack of specialized and comprehensive benchmarks for end-to-end speech LLM
evaluation hinders optimizing the user experience of Audio LLMs in real-world
applications. Existing evaluation methods often adapt text-based benchmarks,
overlooking speech's unique characteristics and challenges, including prosody,
homophones, stuttering, and differing user expectations. Here, we present a
novel approach to thoroughly evaluate LLMs in practical speech conversations.
We systematically curate real-world chat data relevant to spoken scenarios,
introduce diversity in speaker attributes and acoustic conditions, and augment
the dataset with speech-specific phenomena. We further design a query-aware
evaluation method to use customized evaluation checklists and prompts to
enhance the accuracy of automatic evaluation. We conduct comprehensive testing
and detailed analysis of various mainstream speech models, revealing
significant differences in model performance across different speech scenarios.
The use of query-aware evaluation further enables a finer-grained assessment
under various speech-specific scenarios. Our benchmark can provide valuable
insights for speech model development and evaluation.

</details>


### [79] [Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation](https://arxiv.org/abs/2506.21876)
*Qiyue Gao,Xinyu Pi,Kevin Liu,Junrong Chen,Ruolan Yang,Xinqi Huang,Xinyu Fang,Lu Sun,Gautham Kishore,Bo Ai,Stone Tao,Mengyang Liu,Jiaxi Yang,Chao-Jung Lai,Chuanyang Jin,Jiannan Xiang,Benhao Huang,Zeming Chen,David Danks,Hao Su,Tianmin Shu,Ziqiao Ma,Lianhui Qin,Zhiting Hu*

Key words: 世界模型、视觉语言模型、感知与预测、基准测试、认知能力

TL;DR: 该论文提出了一种两阶段框架，用于系统评估大型视觉语言模型（VLMs）作为世界模型（WMs）的能力，并通过大规模实验揭示了这些模型在基本世界建模能力上的显著局限性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 近年来，视觉语言模型（VLMs）展现出作为通用世界模型的潜力，但目前缺乏对其基本世界建模能力的系统评估。

Method: 采用比较心理学和认知科学的视角，提出一个两阶段框架（感知与预测），并设计大规模基准测试WM-ABench，在6种多样模拟环境中进行660次实验。

Result: 实验表明，15种主流VLM在基本世界建模能力上表现不佳，例如运动轨迹识别的准确率接近随机水平，且存在认知偏差（如认为颜色影响速度）。

Conclusion: VLMs与世界建模的人类能力之间存在显著差距，需进一步改进。

Abstract: Internal world models (WMs) enable agents to understand the world's state and
predict transitions, serving as the basis for advanced deliberative reasoning.
Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and
Gemini, exhibit potential as general-purpose WMs. While the latest studies have
evaluated and shown limitations in specific capabilities such as visual
understanding, a systematic evaluation of VLMs' fundamental WM abilities
remains absent. Drawing on comparative psychology and cognitive science, we
propose a two-stage framework that assesses Perception (visual, spatial,
temporal, quantitative, and motion) and Prediction (mechanistic simulation,
transitive inference, compositional inference) to provide an atomic evaluation
of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale
benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse
simulated environments with controlled counterfactual simulations. Through 660
experiments on 15 latest commercial and open-source VLMs, we find that these
models exhibit striking limitations in basic world modeling abilities. For
instance, almost all models perform at near-random accuracy when distinguishing
motion trajectories. Additionally, they lack disentangled understanding --
e.g., some models tend to believe blue objects move faster than green ones.
More rich results and analyses reveal significant gaps between VLMs and
human-level world modeling.

</details>


### [80] [A Dual-Layered Evaluation of Geopolitical and Cultural Bias in LLMs](https://arxiv.org/abs/2506.21881)
*Sean Kim,Hyuhng Joon Kim*

Key words: 大型语言模型（LLMs）、偏见、多语言评估、文化意识、事实性和争议性问题

TL;DR: 论文通过两阶段评估方法分析了大型语言模型（LLMs）在事实性和争议性问题中的表现，揭示了模型偏见和推理偏见的影响，并提出了多语言环境下评估LLMs行为的结构化框架。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于LLMs在多语言和文化环境中的广泛部署，理解其在事实性和争议性问题中的行为至关重要，尤其是在可能影响公众舆论或强化主流叙事的情况下。

Method: 通过两阶段评估：第一阶段在单一可验证答案的事实性问题中评估LLMs的跨语言一致性；第二阶段在具有地缘政治敏感性的争议性问题中探究模型反应，数据集涵盖四种语言和问题类型。

Result: 结果显示，第一阶段存在查询语言引发的对齐现象，第二阶段则反映了模型训练背景与查询语言的相互作用。

Conclusion: 本文提供了评估LLMs在敏感和中性话题中行为的结构化框架，为未来多语言环境下的LLM部署和文化意识评估实践提供了见解。

Abstract: As large language models (LLMs) are increasingly deployed across diverse
linguistic and cultural contexts, understanding their behavior in both factual
and disputable scenarios is essential, especially when their outputs may shape
public opinion or reinforce dominant narratives. In this paper, we define two
types of bias in LLMs: model bias (bias stemming from model training) and
inference bias (bias induced by the language of the query), through a two-phase
evaluation. Phase 1 evaluates LLMs on factual questions where a single
verifiable answer exists, assessing whether models maintain consistency across
different query languages. Phase 2 expands the scope by probing geopolitically
sensitive disputes, where responses may reflect culturally embedded or
ideologically aligned perspectives. We construct a manually curated dataset
spanning both factual and disputable QA, across four languages and question
types. The results show that Phase 1 exhibits query language induced alignment,
while Phase 2 reflects an interplay between the model's training context and
query language. This paper offers a structured framework for evaluating LLM
behavior across neutral and sensitive topics, providing insights for future LLM
deployment and culturally aware evaluation practices in multilingual contexts.

</details>


### [81] [AutoMixer: Checkpoint Artifacts as Automatic Data Mixers](https://arxiv.org/abs/2506.21910)
*Ernie Chang,Yang Li,Patrick Huber,David Kant,Yangyang Shi,Vikas Chandra*

Key words: 语言模型,数据混合,检查点模型,推理能力,预训练

TL;DR: 该论文提出利用训练过程中的检查点模型作为数据混合器，通过其近似一阶影响优化数据混合，显著提升了模型的推理能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 语言模型训练中，如何获得适合多任务能力的数据混合是一个挑战，检查点模型作为训练过程中的副产品未被充分利用。

Method: 通过检查点模型在各基准测试中的表现识别其能力，利用其一阶影响近似聚合来优化数据混合。

Result: 在八个推理基准测试中，该方法使预训练性能提升最高达1.93%。

Conclusion: 检查点模型可有效提升数据质量和优化数据混合，展现了其在模型训练中的潜力。

Abstract: In language model training, it is desirable to equip models with capabilities
from various tasks. However, it is not clear how to directly obtain the right
data mixtures for these capabilities as the relationship between data and tasks
is difficult to be modeled. In this work, we observe that checkpoint models
exhibit emerging capabilities at different points in the training trajectory.
Often, the training process saves checkpoints as artifacts that are
under-utilized as a source of in-training data signals. We identify these
artifact models based on their respective capabilities on the benchmarks and
leverage them as data mixers by using their aggregated first-order influence
approximation over source data. We demonstrated on eight reasoning benchmarks
that the proposed framework shows significant improvements in the pretraining
setting, with performance improvements of up to 1.93%. Overall, this shows the
potential of checkpoint models to enhance data quality and optimize data
mixtures.

</details>


### [82] [PapersPlease: A Benchmark for Evaluating Motivational Values of Large Language Models Based on ERG Theory](https://arxiv.org/abs/2506.21961)
*Junho Myung,Yeon Su Park,Sunwoo Kim,Shin Yoo,Alice Oh*

Key words: 大型语言模型、道德困境、决策偏见、ERG理论、移民检查

TL;DR: 论文介绍了PapersPlease基准测试，通过3700个道德困境评估大型语言模型（LLMs）在决策中的偏见，并揭示了LLMs对某些社会身份的偏好。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在探讨LLMs在角色扮演场景中的决策偏见，尤其是在移民检查的情境下对人类需求的优先级判断。

Method: 使用ERG理论构建人类需求层级，通过移民检查的叙事场景测试6种LLMs的决策模式。

Result: 发现LLMs在决策中存在统计显著的偏好模式，部分模型对边缘化身份的拒绝率更高。

Conclusion: LLMs在决策中表现出隐含偏好，社会身份的引入会影响模型的响应性。

Abstract: Evaluating the performance and biases of large language models (LLMs) through
role-playing scenarios is becoming increasingly common, as LLMs often exhibit
biased behaviors in these contexts. Building on this line of research, we
introduce PapersPlease, a benchmark consisting of 3,700 moral dilemmas designed
to investigate LLMs' decision-making in prioritizing various levels of human
needs. In our setup, LLMs act as immigration inspectors deciding whether to
approve or deny entry based on the short narratives of people. These narratives
are constructed using the Existence, Relatedness, and Growth (ERG) theory,
which categorizes human needs into three hierarchical levels. Our analysis of
six LLMs reveals statistically significant patterns in decision-making,
suggesting that LLMs encode implicit preferences. Additionally, our evaluation
of the impact of incorporating social identities into the narratives shows
varying responsiveness based on both motivational needs and identity cues, with
some models exhibiting higher denial rates for marginalized identities. All
data is publicly available at https://github.com/yeonsuuuu28/papers-please.

</details>


### [83] [More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents](https://arxiv.org/abs/2506.21967)
*Weimin Xiong,Ke Wang,Yifan Song,Hanchao Liu,Sai Zhou,Wei Peng,Sujian Li*

Key words: LLM代理、工具集成、稳定性评估、开源模型、脆弱性

TL;DR: 研究发现当前工具集成LLM代理的评估主要关注端到端工具使用，忽视其稳定性，导致实际应用受限。代理在整个工具调用过程中易受错误影响，开源模型代理尤为脆弱。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有评估忽视LLM代理的稳定性，而实际应用中代理可能因各种因素崩溃或异常，亟需研究其脆弱性。

Method: 通过实验分析代理在工具调用各阶段（读取文档、选择工具、生成参数、处理响应）的易错性，比较开源与专有模型表现。

Result: 代理在每阶段均易出错，开源模型代理更脆弱；模型规模扩大未显著提升推理能力，反增加攻击风险。

Conclusion: 强调评估代理稳定性的重要性，为未来LLM开发与评估提供方向。

Abstract: Current evaluations of tool-integrated LLM agents typically focus on
end-to-end tool-usage evaluation while neglecting their stability. This limits
their real-world applicability, as various internal or external factors can
cause agents to crash or behave abnormally. Our research addresses this by
investigating whether agents are vulnerable to errors throughout the entire
tool invocation process, including reading tool documentation, selecting tools
and generating parameters, and processing the tool's response. Through
extensive experiments, we observe that agents are highly susceptible to errors
at each stage and agents based on open-source models are more vulnerable than
those based on proprietary models. We also find that increasing the model size
does not significantly improve tool invocation reasoning and may make agents
more vulnerable to attacks resembling normal user instructions. This highlights
the importance of evaluating agent stability and offers valuable insights for
future LLM development and evaluation.

</details>


### [84] [Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses](https://arxiv.org/abs/2506.21972)
*Mohamed Ahmed,Mohamed Abdelmouty,Mingyu Kim,Gunvanth Kandula,Alex Park,James C. Davis*

Key words: 预训练语言模型, 攻击方法, 混合技术, 安全性漏洞

TL;DR: 本文提出两种混合方法（GCG + PAIR和GCG + WordGame），结合token级和prompt级技术来增强对预训练语言模型的攻击效果，显著提高了攻击成功率，同时揭露了当前安全措施的漏洞。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 预训练语言模型（PTLMs）和大型语言模型（LLMs）虽然广泛应用，但仍易受攻击。现有token级和prompt级攻击方法各有局限性，因此需要结合两者以提高攻击效果。

Method: 提出两种混合攻击方法：1) GCG + PAIR，结合token级攻击和prompt级攻击；2) GCG + WordGame，结合token级攻击和新的WordGame技术。在多种模型（如Vicuna和Llama）上进行评估。

Result: GCG + PAIR在未防御模型上的攻击成功率显著提升（如Llama-3的ASR从58.4%提升至91.6%）。GCG + WordGame在严格评估器下仍保持高ASR（超过80%）。两种方法均能穿透高级防御（如Gradient Cuff和JBShield）。

Conclusion: 混合方法不仅暴露了当前安全措施的漏洞，还显示了攻击的适应性和防御的不足，强调了全面防御机制的重要性。

Abstract: The advancement of Pre-Trained Language Models (PTLMs) and Large Language
Models (LLMs) has led to their widespread adoption across diverse applications.
Despite their success, these models remain vulnerable to attacks that exploit
their inherent weaknesses to bypass safety measures. Two primary
inference-phase threats are token-level and prompt-level jailbreaks.
Token-level attacks embed adversarial sequences that transfer well to black-box
models like GPT but leave detectable patterns and rely on gradient-based token
optimization, whereas prompt-level attacks use semantically structured inputs
to elicit harmful responses yet depend on iterative feedback that can be
unreliable. To address the complementary limitations of these methods, we
propose two hybrid approaches that integrate token- and prompt-level techniques
to enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the
newly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and
Llama models. GCG + PAIR consistently raised attack-success rates over its
constituent techniques on undefended models; for instance, on Llama-3, its
Attack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's
58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of
WordGame maintaining a high ASR of over 80% even under stricter evaluators like
Mistral-Sorry-Bench. Crucially, both hybrids retained transferability and
reliably pierced advanced defenses such as Gradient Cuff and JBShield, which
fully blocked single-mode attacks. These findings expose previously unreported
vulnerabilities in current safety stacks, highlight trade-offs between raw
success and defensive robustness, and underscore the need for holistic
safeguards against adaptive adversaries.

</details>


### [85] [Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism](https://arxiv.org/abs/2506.21974)
*Simon Münker,Nils Schwager,Achim Rettinger*

Key words: Large Language Models, social networks, user behavior, simulation, empirical validation

TL;DR: 论文探讨了用LLMs模拟社交网络用户行为的可行性，并提出了实验设计的差异需要更严谨的验证。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于LLMs模拟人类行为的能力引发了广泛研究，但结果不一，需进一步理解实验设计的差异，尤其是在社交网络用户行为模拟中。

Method: 通过提供社交网络模拟的框架，并专注于用户通信模仿的子任务，实证测试了不同方法在X平台上对英语和德语用户行为的模拟。

Result: 研究发现，社会模拟应通过其拟合环境中测量的实证真实性进行验证。

Conclusion: 论文主张在应用基于生成代理的社会模拟时需要更多的严谨性。

Abstract: The ability of Large Language Models (LLMs) to mimic human behavior triggered
a plethora of computational social science research, assuming that empirical
studies of humans can be conducted with AI agents instead. Since there have
been conflicting research findings on whether and when this hypothesis holds,
there is a need to better understand the differences in their experimental
designs. We focus on replicating the behavior of social network users with the
use of LLMs for the analysis of communication on social networks. First, we
provide a formal framework for the simulation of social networks, before
focusing on the sub-task of imitating user communication. We empirically test
different approaches to imitate user behavior on X in English and German. Our
findings suggest that social simulations should be validated by their empirical
realism measured in the setting in which the simulation components were fitted.
With this paper, we argue for more rigor when applying generative-agent-based
modeling for social simulation.

</details>


### [86] [Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit](https://arxiv.org/abs/2506.21990)
*Kartheek Kumar Reddy Nareddy,Sarah Ternus,Julia Niebling*

Key words: Whisper模型, ASR, 微调, LoRA, 驾驶舱对话, WER

TL;DR: 论文研究了如何通过微调和规范化方案提高Whisper模型在驾驶舱对话转录中的准确率，WER从68.49%降至26.26%。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 通用预训练模型在专业领域（如驾驶舱对话转录）中表现不佳，需针对特定词汇和多语言对话进行优化。

Method: 收集驾驶舱模拟器和飞行员访谈录音，人工标注数据，并采用规范化方案和LoRA微调技术提升ASR性能。

Result: 微调后的Whisper Large模型结合规范化方案，WER显著降低至26.26%。

Conclusion: 通过数据规范化和微调方法，可以有效提升专业领域语音转录的准确性。

Abstract: The developments in transformer encoder-decoder architectures have led to
significant breakthroughs in machine translation, Automatic Speech Recognition
(ASR), and instruction-based chat machines, among other applications. The
pre-trained models were trained on vast amounts of generic data over a few
epochs (fewer than five in most cases), resulting in their strong
generalization capabilities. Nevertheless, the performance of these models does
suffer when applied to niche domains like transcribing pilot speech in the
cockpit, which involves a lot of specific vocabulary and multilingual
conversations. This paper investigates and improves the transcription accuracy
of cockpit conversations with Whisper models. We have collected around 85
minutes of cockpit simulator recordings and 130 minutes of interview recordings
with pilots and manually labeled them. The speakers are middle aged men
speaking both German and English. To improve the accuracy of transcriptions, we
propose multiple normalization schemes to refine the transcripts and improve
Word Error Rate (WER). We then employ fine-tuning to enhance ASR performance,
utilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).
Hereby, WER decreased from 68.49 \% (pretrained whisper Large model without
normalization baseline) to 26.26\% (finetuned whisper Large model with the
proposed normalization scheme).

</details>


### [87] [Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in Children's Literature Translation](https://arxiv.org/abs/2506.22038)
*Delu Kong,Lieve Macken*

Key words: 机器翻译, 人工翻译, 儿童文学, 文体学分析, 大型语言模型

TL;DR: 该研究从文体学角度评估了机器翻译（MTs）与人工翻译（HTs）在英中儿童文学翻译（CLT）中的表现，通过分类和聚类技术分析发现LLMs在风格特征上更接近HTs。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机在于比较机器翻译与人工翻译在儿童文学翻译中的表现差异，特别关注大型语言模型（LLMs）和神经机器翻译（NMTs）的潜力。

Method: 方法包括构建《彼得潘》语料库（21种翻译），使用通用和创造性翻译特征集（共447个特征），并通过机器学习的分类和聚类技术进行文体学分析。

Result: 结果表明，HTs与MTs在通用特征上差异显著，而LLMs在CTT特定特征上优于NMTs，且在风格上更接近HTs。

Conclusion: 结论指出LLMs在CLT中具有潜力，尤其是在风格上更接近人工翻译。

Abstract: This study focuses on evaluating the performance of machine translations
(MTs) compared to human translations (HTs) in English-to-Chinese children's
literature translation (CLT) from a stylometric perspective. The research
constructs a Peter Pan corpus, comprising 21 translations: 7 human translations
(HTs), 7 large language model translations (LLMs), and 7 neural machine
translation outputs (NMTs). The analysis employs a generic feature set
(including lexical, syntactic, readability, and n-gram features) and a creative
text translation (CTT-specific) feature set, which captures repetition, rhythm,
translatability, and miscellaneous levels, yielding 447 linguistic features in
total.
  Using classification and clustering techniques in machine learning, we
conduct a stylometric analysis of these translations. Results reveal that in
generic features, HTs and MTs exhibit significant differences in conjunction
word distributions and the ratio of 1-word-gram-YiYang, while NMTs and LLMs
show significant variation in descriptive words usage and adverb ratios.
Regarding CTT-specific features, LLMs outperform NMTs in distribution, aligning
more closely with HTs in stylistic characteristics, demonstrating the potential
of LLMs in CLT.

</details>


### [88] [Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs](https://arxiv.org/abs/2506.22050)
*Delu Kong,Lieve Macken*

Key words: 机器翻译，大语言模型，语言特征，英中翻译，卡方排名

TL;DR: 本研究探讨了机器翻译输出中的语言特征（MTese），通过构建大型数据集和五层特征集，确认了神经机器翻译系统（NMTs）和大语言模型（LLMs）中存在MTese，并分析了其语言学模式。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是探索机器翻译输出的语言特征（MTese），尤其是在英中新闻文本中这一未被充分研究的领域。

Method: 方法包括构建4个子语料库的大型数据集、使用五层特征集，并应用卡方排名算法进行特征选择。

Result: 结果发现原始中文文本与LLM/NMT输出有明显区别；MT输出中句子更短且更多使用转折连词；LLMs和NMTs的分类准确率约70%。

Conclusion: 结论是MTese存在于NMTs和LLMs中，但LLMs与NMTs在词汇多样性和标点使用上有差异，而中外LLMs无明显区别。

Abstract: This study explores Machine Translationese (MTese) -- the linguistic
peculiarities of machine translation outputs -- focusing on the
under-researched English-to-Chinese language pair in news texts. We construct a
large dataset consisting of 4 sub-corpora and employ a comprehensive five-layer
feature set. Then, a chi-square ranking algorithm is applied for feature
selection in both classification and clustering tasks. Our findings confirm the
presence of MTese in both Neural Machine Translation systems (NMTs) and Large
Language Models (LLMs). Original Chinese texts are nearly perfectly
distinguishable from both LLM and NMT outputs. Notable linguistic patterns in
MT outputs are shorter sentence lengths and increased use of adversative
conjunctions. Comparing LLMs and NMTs, we achieve approximately 70%
classification accuracy, with LLMs exhibiting greater lexical diversity and
NMTs using more brackets. Additionally, translation-specific LLMs show lower
lexical diversity but higher usage of causal conjunctions compared to generic
LLMs. Lastly, we find no significant differences between LLMs developed by
Chinese firms and their foreign counterparts.

</details>


### [89] [Lost at the Beginning of Reasoning](https://arxiv.org/abs/2506.22058)
*Baohao Liao,Xinyi Chen,Sara Rajaee,Yuhui Xu,Christian Herold,Anders Søgaard,Maarten de Rijke,Christof Monz*

Key words: 大型语言模型,链式思维推理,自我修正,高效采样,推理成本

TL;DR: 该论文探讨了大型语言模型（LLMs）在长链思维推理中自我修正能力的不足，并发现第一步推理对最终预测的影响过大。为此，作者提出了一种高效的采样策略，以减少推理成本而不损失准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管LLMs在复杂推理方面取得了进展，但长链思维推理中的自我修正能力仍未充分研究，且模型常出现冗余推理问题。

Method: 提出了一种基于奖励模型的高效采样策略，用于筛选高质量的第一步推理步骤。

Result: 该方法在DeepSeek-R1和Qwen3两个模型上实现了推理成本降低70%，同时保持准确性。

Conclusion: 论文揭示了第一步推理的关键性，并提出了一种有效的解决方案，为新基准的开发提供了基础。

Abstract: Recent advancements in large language models (LLMs) have significantly
advanced complex reasoning capabilities, particularly through extended
chain-of-thought (CoT) reasoning that incorporates mechanisms such as
backtracking, self-reflection and self-correction. Despite these developments,
the self-correction abilities of LLMs during long CoT reasoning remain
underexplored. And recent findings on overthinking suggest that such models
often engage in unnecessarily redundant reasoning. In this work, we empirically
show that the first reasoning step exerts a disproportionately large influence
on the final prediction - errors introduced at this stage can substantially
degrade subsequent reasoning quality. This phenomenon is consistently observed
across two state-of-the-art open-source reasoning model families: DeepSeek-R1
and Qwen3. To address this, we propose an efficient sampling strategy that
leverages a reward model to identify and retain high-quality first reasoning
steps while discarding suboptimal ones, achieving up to a 70% reduction in
inference cost without sacrificing accuracy. Finally, we introduce a new
benchmark specifically constructed with deliberately flawed first reasoning
steps to systematically evaluate model self-correction capabilities, offering a
foundation for future research on robust reasoning in LLMs.

</details>


### [90] [MDC-R: The Minecraft Dialogue Corpus with Reference](https://arxiv.org/abs/2506.22062)
*Chris Madge,Maris Camilleri,Paloma Carretero Garcia,Mladen Karan,Juexi Shao,Prashant Jayannavar,Julian Hough,Benjamin Roth,Massimo Poesio*

Key words: MDC-R, 指代标注, 语料库, 指代表达理解

TL;DR: 论文介绍了Minecraft Dialogue Corpus with Reference (MDC-R)，这是对原MDC语料库的补充，增加了专家对指代和指示性参考的标注。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 原MDC语料库中的任务导向、多轮、动态环境对话引发了多种语言现象，标注参考信息可使其成为更有价值的资源。

Method: 讨论了标注方法及生成的语料库，并提供了数据的定量和定性分析。

Result: 通过一个简短实验展示了该语料库在指代表达理解中的实用性。

Conclusion: MDC-R是一个有价值的语言资源，特别适用于研究指代现象。

Abstract: We introduce the Minecraft Dialogue Corpus with Reference (MDC-R). MDC-R is a
new language resource that supplements the original Minecraft Dialogue Corpus
(MDC) with expert annotations of anaphoric and deictic reference. MDC's
task-orientated, multi-turn, situated dialogue in a dynamic environment has
motivated multiple annotation efforts, owing to the interesting linguistic
phenomena that this setting gives rise to. We believe it can serve as a
valuable resource when annotated with reference, too. Here, we discuss our
method of annotation and the resulting corpus, and provide both a quantitative
and a qualitative analysis of the data. Furthermore, we carry out a short
experiment demonstrating the usefulness of our corpus for referring expression
comprehension.

</details>


### [91] [Involvement drives complexity of language in online debates](https://arxiv.org/abs/2506.22098)
*Eleonora Amadori,Daniele Cirulli,Edoardo Di Martino,Jacopo Nudo,Maria Sahakyan,Emanuele Sangiorgio,Arnaldo Santoro,Simon Zollo,Alessandro Galeazzi,Niccolò Di Marco*

Key words: 语言复杂性,社交媒体,Twitter,政治倾向,内容可靠性,情感分析

TL;DR: 研究分析了Twitter上有影响力用户在三个争议话题上的语言复杂性，揭示了账户类型、政治倾向、内容可靠性和情感倾向对语言使用的影响。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 社交媒体的兴起改变了公共话语，分析用户生成内容的语言特征有助于理解其社会影响。

Method: 结合多种文本复杂度指标，研究比较了不同账户类型、政治倾向、内容可靠性和情感倾向的语言使用差异。

Result: 研究发现在四个维度上语言复杂性存在显著差异，政治立场相似且可靠性相近的用户倾向于使用共同的复杂语言。

Conclusion: 研究结果为数字平台的社交语言动态提供了新见解，表明语言反映了在线空间中的意识形态和社会结构。

Abstract: Language is a fundamental aspect of human societies, continuously evolving in
response to various stimuli, including societal changes and intercultural
interactions. Technological advancements have profoundly transformed
communication, with social media emerging as a pivotal force that merges
entertainment-driven content with complex social dynamics. As these platforms
reshape public discourse, analyzing the linguistic features of user-generated
content is essential to understanding their broader societal impact. In this
paper, we examine the linguistic complexity of content produced by influential
users on Twitter across three globally significant and contested topics:
COVID-19, COP26, and the Russia-Ukraine war. By combining multiple measures of
textual complexity, we assess how language use varies along four key
dimensions: account type, political leaning, content reliability, and
sentiment. Our analysis reveals significant differences across all four axes,
including variations in language complexity between individuals and
organizations, between profiles with sided versus moderate political views, and
between those associated with higher versus lower reliability scores.
Additionally, profiles producing more negative and offensive content tend to
use more complex language, with users sharing similar political stances and
reliability levels converging toward a common jargon. Our findings offer new
insights into the sociolinguistic dynamics of digital platforms and contribute
to a deeper understanding of how language reflects ideological and social
structures in online spaces.

</details>


### [92] [Identifying a Circuit for Verb Conjugation in GPT-2](https://arxiv.org/abs/2506.22105)
*David Demitri Africa*

Key words: GPT-2 Small, 主谓一致, 子网络, 电路发现

TL;DR: 研究了GPT-2 Small中负责主谓一致的子网络，通过技术手段发现了关键电路，结果显示仅需少量组件即可完成基础任务，复杂任务则需要更多。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索GPT-2 Small中主谓一致任务的内部机制，理解模型如何通过特定子网络完成这一任务。

Method: 使用性能验证、直接路径修补和直接对数归因等技术，分离出对动词正确变位贡献显著的候选电路。

Result: 发现少量组件即可实现基础任务的高性能，但复杂任务需要更多网络组件。

Conclusion: GPT-2 Small中主谓一致任务由特定子网络负责，其效率在基础任务中较高但复杂任务需更全面支持。

Abstract: I implement a procedure to isolate and interpret the sub-network (or
"circuit") responsible for subject-verb agreement in GPT-2 Small. In this
study, the model is given prompts where the subject is either singular (e.g.
"Alice") or plural (e.g. "Alice and Bob"), and the task is to correctly predict
the appropriate verb form ("walks" for singular subjects, "walk" for plural
subjects). Using a series of techniques-including performance verification
automatic circuit discovery via direct path patching, and direct logit
attribution- I isolate a candidate circuit that contributes significantly to
the model's correct verb conjugation. The results suggest that only a small
fraction of the network's component-token pairs is needed to achieve near-model
performance on the base task but substantially more for more complex settings.

</details>


### [93] [DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family Level](https://arxiv.org/abs/2506.22141)
*Iliass Ayaou,Denis Cavallucci,Hicham Chibane*

Key words: 专利检索,数据集,跨领域检索,IPC代码,DAPFAM

TL;DR: 提出DAPFAM数据集，解决专利检索中领域标注、多法域覆盖等被忽视的需求，提供1,247查询和45,336目标专利，支持跨领域检索研究。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有专利检索数据集缺乏明确的领域标注、多法域覆盖和平衡查询领域，DAPFAM旨在填补这些空白。

Method: 构建基于简单家族的专利检索数据集，通过IPC代码明确领域关系，采用三步数据筛选流程。

Result: 数据集包含49,869评估对，支持跨领域检索实验，基线实验显示跨领域检索的挑战。

Conclusion: DAPFAM为专利检索研究提供了实用工具，尤其适合资源有限的环境。

Abstract: In the landscape of publicly available patent retrieval datasets, the need
for explicit indomain and out-of-domain labeling, multi-jurisdiction coverage,
balanced query domain representation and manageable sizes that support sub
document level experiments on moderate computational resources is often
overlooked. To address these gaps, we propose DAPFAM, a new open access
domain-aware patent retrieval dataset constructed at the simple-family level.
The dataset contains 1,247 domain balanced full text query families and 45,336
full text target families. The dataset is enriched by clear relevance judgments
(forward/backward citations as positive links, random negatives), as well as
explicit in-domain or out-of-domain relationships via a novel proposed
labelling scheme based on via International Patent Classification (IPC) codes,
resulting in 49,869 evaluation pairs. The dataset is multi jurisdictional,
requires little to no preprocessing for retrieval evaluation, and remains of a
size manageable for entities with limited ressources allowing for sub document
level retrieval experiments without excessive computational costs. We describe
our three-step data-curation pipeline, present comprehensive dataset
statistics, and provide baseline experiments using lexical and neural retrieval
methods. Our baseline experiments highlight significant challenges in
crossdomain patent retrieval. The dataset will be publicly available (for now
the access link is this repository:
https://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b).

</details>


### [94] [SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in Low-Resource Arabic-English Code-Switched Speech Recognition](https://arxiv.org/abs/2506.22143)
*Muhammad Umar Farooq,Oscar Saz*

Key words: 语音识别、自监督学习、代码转换、方言阿拉伯语、词错误率

TL;DR: 研究了语音自监督学习（SSL）模型在方言阿拉伯语（DA）和阿拉伯语-英语代码转换（CS）语音上的性能，提出改进的音频拼接方法和经验回放技术，显著降低词错误率（WER）。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决方言阿拉伯语和代码转换语音数据稀缺问题，提升模型性能。

Method: 采用改进的音频拼接生成人工CS数据，结合经验回放技术减少遗忘，并引入外部语言模型。

Result: WER显著降低，在CS任务上优于大型多语言模型。

Conclusion: 提出的方法有效提升了方言和代码转换语音的识别性能。

Abstract: This paper investigates the performance of various speech SSL models on
dialectal Arabic (DA) and Arabic-English code-switched (CS) speech. To address
data scarcity, a modified audio-splicing approach is introduced to generate
artificial CS speech data. Fine-tuning an already fine-tuned SSL model with the
proposed Spliced-Audio Generated (SAGE) data results in an absolute improvement
on Word Error Rate (WER) of 7.8% on Arabic and English CS benchmarks.
Additionally, an Experience Replay (ER) inspired approach is proposed to
enhance generalisation across DA and CS speech while mitigating catastrophic
forgetting. Integrating an out-of-domain 3-gram language model reduces the
overall mean WER from 31.7% to 26.6%. Few-shot fine-tuning for code-switching
benchmarks further improves WER by 4.9%. A WER of 31.1% on Arabic-English CS
benchmarks surpasses large-scale multilingual models, including USM and
Whisper-large-v2 (both over ten times larger) by an absolute margin of 5.5% and
8.4%, respectively.

</details>


### [95] [Training Language Model to Critique for Better Refinement](https://arxiv.org/abs/2506.22157)
*Tianshu Yu,Chao Xiang,Mingchuan Yang,Pei Ke,Bosi Wen,Cunxiang Wang,Jiale Cheng,Li Zhang,Xinyu Mu,Chuxiong Sun,Minlie Huang*

Key words: 大语言模型, 批评优化, RCO, 反馈循环, 微调

TL;DR: 该论文提出了RCO框架，通过优化批评模型训练，提升大语言模型（LLM）的反馈和微调能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大语言模型虽具备评价和批评能力，但缺乏关于哪种批评最有效的探索，RCO旨在填补这一空白。

Method: RCO框架通过批评模型的反馈循环，利用微调信号量化批评效用（CU）作为奖励信号，训练批评模型。

Result: 在五个任务中，RCO显著优于传统方法和开源模型，展示了其批评质量和微调结果的优势。

Conclusion: RCO通过新颖的监督方案，有效增强了批评-微调循环，为大语言模型提供了更高效的优化途径。

Abstract: Large language models (LLMs) have demonstrated remarkable evaluation and
critique capabilities, providing insightful feedback and identifying flaws in
various tasks. However, limited research has explored which types of critiques
are most effective for improving model responses or how to generate such
critiques. To address this gap, we introduce \textbf{R}efinement-oriented
\textbf{C}ritique \textbf{O}ptimization (RCO), a novel framework designed to
train critic models using refinement signals. RCO uses a feedback loop where
critiques, generated by the critic model, guide the actor model in refining its
responses. The critique utility (CU) quantifies the effectiveness of these
refinements, serving as the reward signal for training the critic model. By
focusing on critiques that lead to better refinements, RCO eliminates the need
for direct critique preference assessment, ensuring that critiques driving
meaningful improvements are rewarded. We evaluate RCO across five tasks, i.e.,
dialog generation, summarization, question answering, mathematical reasoning,
and code generation, and show that it significantly outperforms traditional
methods and open-source models in terms of critique quality and refinement
outcomes. Our contributions include the introduction of RCO, a novel
supervision scheme based on refined response preferences, and comprehensive
experimental results that highlight the method's effectiveness in enhancing LLM
critique-refinement loops.

</details>


### [96] [Leveraging In-Context Learning for Political Bias Testing of LLMs](https://arxiv.org/abs/2506.22232)
*Patrick Haller,Jannis Vamvas,Rico Sennrich,Lena A. Jäger*

Key words: LLMs, 偏见评估, Questionnaire Modeling, 指令调优, 上下文示例

TL;DR: 本文提出了一种新的探测任务QM，利用人类调查数据作为上下文示例，提高了基于问题的偏见评估的稳定性，并用于比较指令调优模型与基础版本。实验显示指令调优可改变偏见方向，且更大模型能更有效地利用上下文示例，表现出更小的偏见分数。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法在评估LLMs的政治偏见时稳定性不足，难以可靠比较模型。论文提出需要更多上下文来解决这一问题。

Method: 提出QM任务，使用人类调查数据作为上下文示例，改进偏见评估的稳定性。通过实验比较不同大小LLMs的表现。

Result: QM方法提高了评估稳定性，指令调优可改变偏见方向，更大模型利用上下文更有效且偏见分数更小。

Conclusion: QM是一种有效的偏见评估方法，能提高稳定性并揭示模型偏见的变化趋势。

Abstract: A growing body of work has been querying LLMs with political questions to
evaluate their potential biases. However, this probing method has limited
stability, making comparisons between models unreliable. In this paper, we
argue that LLMs need more context. We propose a new probing task, Questionnaire
Modeling (QM), that uses human survey data as in-context examples. We show that
QM improves the stability of question-based bias evaluation, and demonstrate
that it may be used to compare instruction-tuned models to their base versions.
Experiments with LLMs of various sizes indicate that instruction tuning can
indeed change the direction of bias. Furthermore, we observe a trend that
larger models are able to leverage in-context examples more effectively, and
generally exhibit smaller bias scores in QM. Data and code are publicly
available.

</details>


### [97] [Detection of Personal Data in Structured Datasets Using a Large Language Model](https://arxiv.org/abs/2506.22305)
*Albert Agisha Ntwali,Luca Rück,Martin Heckmann*

Key words: 个人数据检测, GPT-4o, 上下文信息, 结构化数据

TL;DR: 提出了一种基于GPT-4o的新方法，通过结合上下文信息检测结构化数据集中的个人数据，并在多个数据集上验证了其优越性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 识别个人数据是保护隐私的重要任务，但现有方法对上下文信息的利用不足。

Method: 利用GPT-4o模型，结合数据特征的名称、值、其他特征名称及数据集描述等上下文信息进行检测。

Result: GPT-4o方法在MIMIC-Demo-Ext等真实数据集上表现优异，而传统方法如CASSED和Presidio在缺乏上下文时效果较差。

Conclusion: 需要更多现实数据集以推动个人数据检测领域的进步。

Abstract: We propose a novel approach for detecting personal data in structured
datasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key
innovation of our method is the incorporation of contextual information: in
addition to a feature's name and values, we utilize information from other
feature names within the dataset as well as the dataset description. We compare
our approach to alternative methods, including Microsoft Presidio and CASSED,
evaluating them on multiple datasets: DeSSI, a large synthetic dataset,
datasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a
real-world dataset containing patient information from critical care units.
  Our findings reveal that detection performance varies significantly depending
on the dataset used for evaluation. CASSED excels on DeSSI, the dataset on
which it was trained. Performance on the medical dataset MIMIC-Demo-Ext is
comparable across all models, with our GPT-4o-based approach clearly
outperforming the others. Notably, personal data detection in the Kaggle and
OpenML datasets appears to benefit from contextual information. This is
evidenced by the poor performance of CASSED and Presidio (both of which do not
utilize the context of the dataset) compared to the strong results of our
GPT-4o-based approach.
  We conclude that further progress in this field would greatly benefit from
the availability of more real-world datasets containing personal information.

</details>


### [98] [Evaluating Scoring Bias in LLM-as-a-Judge](https://arxiv.org/abs/2506.22316)
*Qingquan Li,Shaoyu Dou,Kailai Shao,Chao Chen,Haixiang Hu*

Key words: LLM-as-a-Judge, 评分偏见, 数据合成, 评估框架, 自然语言处理

TL;DR: 该论文探讨了LLM作为评估者（LLM-as-a-Judge）在评分中的偏见问题，提出了评估框架并揭示了现有评估模型的评分稳定性因偏见而受到影响。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是发现并解决LLM-as-a-Judge在评分任务中存在的偏见问题，以提升其公平性和可靠性。

Method: 通过数据合成构建评估数据集，设计多维度的评估指标，并对评分偏见进行系统分析。

Result: 实验结果表明，现有评估模型的评分稳定性受偏见影响，并提供了关于评分提示模板设计和偏见缓解的见解。

Conclusion: 论文强调了对LLM-as-a-Judge评分偏见的系统性研究的必要性，并提出了改进方向。

Abstract: The remarkable performance of Large Language Models (LLMs) gives rise
to``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks.
Moreover, it has been widely adopted across fields such as Natural Language
Processing (NLP), preference learning, and various specific domains. However,
there are various biases within LLM-as-a-Judge, which adversely affect the
fairness and reliability of judgments. Current research on evaluating or
mitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based
evaluations, while systematic investigations into bias in scoring-based
evaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge
as the scores differ when scoring judge models are bias-related perturbed, and
provide a well-designed framework to comprehensively evaluate scoring bias. We
augment existing LLM-as-a-Judge benchmarks through data synthesis to construct
our evaluation dataset and design multi-faceted evaluation metrics. Our
experimental results demonstrate that the scoring stability of existing judge
models is disrupted by scoring biases. Further exploratory experiments and
discussions provide valuable insights into the design of scoring prompt
templates and the mitigation of scoring biases on aspects such as score
rubrics, score IDs, and reference answer selection.

</details>


### [99] [Why Are Parsing Actions for Understanding Message Hierarchies Not Random?](https://arxiv.org/abs/2506.22366)
*Daichi Kato,Ryo Ueda,Yusuke Miyao*

Key words: 语言理解、随机解析、层次结构、通信准确率、意外相关目标

TL;DR: 研究表明，人类语言理解并非随机解析，而随机解析的代理在某些情况下也能实现高通信准确率。本研究通过引入更复杂的层次结构输入和意外相关目标函数，探讨随机解析策略是否仍能保持高准确率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 理解为何人类语言解析不采用随机策略，尽管先前研究表明随机解析代理也能实现高通信准确率。

Method: 通过引入复杂层次结构输入和意外相关目标函数，重新评估随机解析策略的有效性。

Result: 实验将展示随机解析策略在复杂结构和意外相关目标下是否仍能保持高通信准确率。

Conclusion: 研究旨在揭示随机解析策略的局限性及其与人类语言理解的差异。

Abstract: If humans understood language by randomly selecting parsing actions, it might
have been necessary to construct a robust symbolic system capable of being
interpreted under any hierarchical structure. However, human parsing strategies
do not seem to follow such a random pattern. Why is that the case? In fact, a
previous study on emergent communication using models with hierarchical biases
have reported that agents adopting random parsing
strategies$\unicode{x2013}$ones that deviate significantly from human language
comprehension$\unicode{x2013}$can achieve high communication accuracy. In this
study, we investigate this issue by making two simple and natural modifications
to the experimental setup: (I) we use more complex inputs that have
hierarchical structures, such that random parsing makes semantic interpretation
more difficult, and (II) we incorporate a surprisal-related term, which is
known to influence the order of words and characters in natural language, into
the objective function. With these changes, we evaluate whether agents
employing random parsing strategies still maintain high communication accuracy.

</details>


### [100] [QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization](https://arxiv.org/abs/2506.22396)
*Danush Khanna,Aditya Kumar Guru,Srivarshinee Sridhar,Zidan Ahmed,Rubhav Bahirwani,Meetu Malhotra,Vinija Jain,Aman Chadha,Amitava Das,Kripabandhu Ghosh*

Key words: 大语言模型,推理优化,动态令牌停止,KV缓存跳过,上下文令牌融合

TL;DR: QuickSilver是一种模块化、令牌级框架，通过动态令牌停止、KV缓存跳过和上下文令牌融合等机制，在不改变模型权重或结构的情况下，显著降低LLM推理的延迟和能耗。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大语言模型的推理阶段占据了大部分延迟和能耗，是当前的主要瓶颈。QuickSilver旨在优化推理效率，而不需要重新训练或修改架构。

Method: QuickSilver整合了动态令牌停止、KV缓存跳过和上下文令牌融合三种机制，直接在冻结的密集模型上操作，无需辅助网络。

Result: 在GPT-2和Llama-2上测试，QuickSilver实现了高达39.6%的FLOP减少，且困惑度增加微乎其微（≤0.2）。

Conclusion: QuickSilver是一种高效且兼容的推理优化框架，适用于现有的大语言模型。

Abstract: Inference accounts for the majority of latency and energy consumption in
large language model (LLM) deployments, often exceeding 90% of total cost.
While training-time efficiency has seen extensive progress, runtime
optimization remains a key bottleneck, particularly under autoregressive
decoding. Existing approaches -- such as pruning, quantization, early exits,
and speculative decoding -- often require retraining, architectural changes, or
disrupt decoding compatibility. We introduce QuickSilver, a modular,
token-level framework that enables semantic adaptivity at inference time
without altering model weights or structure. QuickSilver integrates four
synergistic mechanisms:
  (i) Dynamic Token Halting, which halts computation for tokens with converged
representations; (ii) KV Cache Skipping, which selectively suppresses memory
writes to reduce attention overhead; and (iii) Contextual Token Fusion, which
collapses redundant tokens into shared paths to shrink sequence length.
  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on
frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and
Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP
reduction with negligible perplexity degradation (<=0.2).

</details>


### [101] [Refining Czech GEC: Insights from a Multi-Experiment Approach](https://arxiv.org/abs/2506.22402)
*Petr Pechman,Milan Straka,Jana Straková,Jakub Náplava*

Key words: 语法纠错, Transformer, 捷克语, 合成错误生成, 大语言模型

TL;DR: 本文介绍了一种基于Transformer架构的捷克语语法纠错系统，通过动态生成合成错误句子的方式提升性能，并在实验中验证了其高效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 开发一种针对捷克语的先进语法纠错系统，填补现有技术的空白并提升性能。

Method: 采用神经网络翻译方法（Transformer架构），结合实时合成错误生成管道和多种实验策略（如错误生成策略、领域平衡等）。

Result: 最佳模型在性能和计算效率上均表现优异，并开源了代码和模型。

Conclusion: 该系统在捷克语语法纠错任务中达到了最优水平，且计算高效。

Abstract: We present a grammar error correction (GEC) system that achieves state of the
art for the Czech language. Our system is based on a neural network translation
approach with the Transformer architecture, and its key feature is its
real-time synthetic generation pipeline, which dynamically augments sentences
with artificial errors by introducing both language-agnostic and Czech-specific
errors. We conduct a comprehensive series of experiments, investigating the
Czech GEC corpora as bases for synthetic error introduction, several error
generation strategies, domain balancing, tokenization granularity, model size,
and data scaling during fine-tuning. Additionally, we evaluate the performance
of large language models (LLMs) on Czech GEC in both end-user and expert
fine-tuning scenarios. Our best-performing model is superior both in
performance and computational efficiency. The source code and the trained model
links are available on https://github.com/ufal/tsd2025-gec.

</details>


### [102] [HyperCLOVA X THINK Technical Report](https://arxiv.org/abs/2506.22403)
*NAVER Cloud HyperCLOVA X Team*

Key words: HyperCLOVA X THINK, 韩语 AI, 大语言模型, 推理, 课程学习, 强化学习

TL;DR: HyperCLOVA X THINK 是 HyperCLOVA X 系列中首款专注于推理的大语言模型，预训练使用了 6 万亿韩语和英语 token，并通过合成数据增强。它在韩国基准测试中表现优异，且训练计算需求显著低于同类模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 旨在为韩语 AI 创新提供强大的基础模型，同时为全球研究社区提供有价值的资源。

Method: 采用计算-内存平衡的 Peri-LN Transformer，通过三阶段课程学习扩展上下文窗口至 128K token，并结合监督微调和强化学习进行后期训练。

Result: 在 KMMLU、CSAT 等韩国基准测试中表现优于同类模型，且在 KCSAT STEM 基准测试中与 GPT-4.1 相当或更好。

Conclusion: HyperCLOVA X THINK 是韩语 AI 的强大基础模型，未来还将通过修剪和蒸馏技术优化，成为开源且适合商业应用的模型。

Abstract: We introduce HyperCLOVA X THINK, the first reasoning-focused large language
model in the HyperCLOVA X family, pre-trained on roughly $6$ trillion
high-quality Korean, and English tokens, augmented with targeted synthetic
Korean data. It was implemented as a compute-memory-balanced Peri-LN
Transformer scaled with $\mu$P, pre-trained through a three-stage curriculum
that expands the context window to $128$K tokens, and post-trained via
supervised fine-tuning with Reinforcement Learning from Verifiable Rewards
supports both detailed rationale and concise-answer modes. It delivers
competitive performance against similarly sized models on Korea-focused
benchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while
preserving robust bilingual consistency and translation quality. In addition, a
vision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM
benchmark, all of which are achieved with substantially lower training compute
than existing models of similar sizes. We also present a pruning and
distillation technique that will soon be applied to HyperCLOVA X THINK for an
open-source and business-friendly foundation model. Altogether, these
capabilities position HyperCLOVA X THINK as a robust foundation for Korean AI
innovation and a valuable resource for the global research community.

</details>


### [103] [Sequential Diagnosis with Language Models](https://arxiv.org/abs/2506.22405)
*Harsha Nori,Mayank Daswani,Christopher Kelly,Scott Lundberg,Marco Tulio Ribeiro,Marc Wilson,Xiaoxuan Liu,Viknesh Sounderajah,Jonathan Carlson,Matthew P Lungren,Bay Gross,Peter Hames,Mustafa Suleyman,Dominic King,Eric Horvitz*

Key words: 人工智能；医疗诊断；迭代诊断；成本效益；MAI-DxO

TL;DR: 该论文提出了一种模拟临床诊断迭代过程的基准测试（Sequential Diagnosis Benchmark），并通过MAI-DxO模型显著提高了诊断准确性和成本效益。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统语言模型评估方法无法反映真实临床诊断的复杂性和动态性，因此需要一种更贴近实际的评价方式。

Method: 将304个NEJM-CPC病例转化为逐步诊断场景，开发了MAI-DxO模型，模拟医师团队提出候选诊断并选择高性价比检查。

Result: MAI-DxO模型诊断准确率提高至80%-85.5%，并显著降低诊断成本（比医师低20%，比标准模型低70%）。

Conclusion: AI系统通过模拟临床迭代思维，能够显著提升诊断精度和成本效益。

Abstract: Artificial intelligence holds great promise for expanding access to expert
medical knowledge and reasoning. However, most evaluations of language models
rely on static vignettes and multiple-choice questions that fail to reflect the
complexity and nuance of evidence-based medicine in real-world settings. In
clinical practice, physicians iteratively formulate and revise diagnostic
hypotheses, adapting each subsequent question and test to what they've just
learned, and weigh the evolving evidence before committing to a final
diagnosis. To emulate this iterative process, we introduce the Sequential
Diagnosis Benchmark, which transforms 304 diagnostically challenging New
England Journal of Medicine clinicopathological conference (NEJM-CPC) cases
into stepwise diagnostic encounters. A physician or AI begins with a short case
abstract and must iteratively request additional details from a gatekeeper
model that reveals findings only when explicitly queried. Performance is
assessed not just by diagnostic accuracy but also by the cost of physician
visits and tests performed. We also present the MAI Diagnostic Orchestrator
(MAI-DxO), a model-agnostic orchestrator that simulates a panel of physicians,
proposes likely differential diagnoses and strategically selects high-value,
cost-effective tests. When paired with OpenAI's o3 model, MAI-DxO achieves 80%
diagnostic accuracy--four times higher than the 20% average of generalist
physicians. MAI-DxO also reduces diagnostic costs by 20% compared to
physicians, and 70% compared to off-the-shelf o3. When configured for maximum
accuracy, MAI-DxO achieves 85.5% accuracy. These performance gains with MAI-DxO
generalize across models from the OpenAI, Gemini, Claude, Grok, DeepSeek, and
Llama families. We highlight how AI systems, when guided to think iteratively
and act judiciously, can advance diagnostic precision and cost-effectiveness in
clinical care.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [104] [APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization](https://arxiv.org/abs/2506.21655)
*Minjie Hong,Zirun Guo,Yan Xia,Zehan Wang,Ziang Zhang,Tao Jin,Zhou Zhao*

Key words: MLLMs, 强化学习, 非对称策略优化, KL惩罚, 过度思考

TL;DR: 研究了KL惩罚和过度思考对MLLMs中RL训练的影响，提出了非对称策略优化（APO）方法，结合DADS和STCR技术，显著提升了推理能力，同时保持了模型的泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: MLLMs在多模态数据融合上表现出色，但在复杂推理能力上仍有不足。直接应用强化学习到MLLMs中存在性能下降和过度思考的问题，需要新的方法来解决。

Method: 提出了非对称策略优化（APO），分为正样本和负样本处理：正样本采用动态调整KL权重的DADS，负样本使用惩罚冗长推理的STCR。

Result: View-R1-3B模型在推理基准测试中平均提升7%，优于更大的MLLMs，并在通用任务中保持稳定表现。

Conclusion: DADS和STCR技术有效提升了MLLMs的复杂推理能力，同时避免了性能退化，具有广泛的适用性。

Abstract: Multimodal Large Language Models (MLLMs) are powerful at integrating diverse
data, but they often struggle with complex reasoning. While Reinforcement
learning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.
Common issues include a drop in performance on general tasks and the generation
of overly detailed or "overthinking" reasoning. Our work investigates how the
KL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric
Policy Optimization (APO) to address these issues, which divides the sampled
responses into positive and negative groups. For positive samples,
Difficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically
adjust the KL divergence weight based on their difficulty. This method prevents
policy entropy from dropping sharply, improves training stability, utilizes
samples better, and preserves the model's existing knowledge. For negative
samples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to
penalize overly long responses. This helps mitigate overthinking and encourages
more concise reasoning while preserving the model's explorative capacity. We
apply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B
significantly enhances reasoning capabilities, showing an average 7\% gain over
the base model and outperforming larger MLLMs (7-11B) on various reasoning
benchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade
on general tasks, View-R1-3B maintains consistent improvement, demonstrating
superior generalization. These results highlight the effectiveness and broad
applicability of our DADS and STCR techniques for advancing complex multimodal
reasoning in MLLMs. The code will be made available at
https://github.com/Indolent-Kawhi/View-R1.

</details>


### [105] [Risk-Averse Total-Reward Reinforcement Learning](https://arxiv.org/abs/2506.21683)
*Xihong Su,Jia Lin Hau,Gersi Doko,Kishan Panaganti,Marek Petrik*

Key words: MDP, Q-learning, 风险厌恶, ERM, EVaR

TL;DR: 提出了一种Q-learning算法，用于解决风险厌恶型总奖励MDP问题，并证明了其在ERM和EVaR目标下的收敛性和性能保证。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有基于模型的算法在小规模问题上表现良好，但需要完全了解转移概率，无法适用于更广泛的场景。

Method: 采用Q-learning算法，利用ERM的动态一致性和可激发性，实现了对最优风险厌恶值函数的快速可靠收敛。

Result: 在表格域上的数值实验表明，该算法能够快速且可靠地收敛到最优风险厌恶值函数。

Conclusion: 所提出的Q-learning算法为风险厌恶型MDP问题提供了一种有效的无模型解决方案。

Abstract: Risk-averse total-reward Markov Decision Processes (MDPs) offer a promising
framework for modeling and solving undiscounted infinite-horizon objectives.
Existing model-based algorithms for risk measures like the entropic risk
measure (ERM) and entropic value-at-risk (EVaR) are effective in small
problems, but require full access to transition probabilities. We propose a
Q-learning algorithm to compute the optimal stationary policy for total-reward
ERM and EVaR objectives with strong convergence and performance guarantees. The
algorithm and its optimality are made possible by ERM's dynamic consistency and
elicitability. Our numerical results on tabular domains demonstrate quick and
reliable convergence of the proposed Q-learning algorithm to the optimal
risk-averse value function.

</details>


### [106] [Unimodal Strategies in Density-Based Clustering](https://arxiv.org/abs/2506.21695)
*Oron Nir,Jay Tenenbaum,Ariel Shamir*

Key words: 密度聚类, 参数选择, 三分搜索, 高维数据, 大规模数据

TL;DR: 研究揭示了密度聚类方法中簇数量与核心点邻域半径的关系，提出了基于三分搜索的高效参数选择策略，适用于高维大规模数据。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 密度聚类方法在噪声数据或任意分布数据中表现优异，但参数调整计算量大，研究旨在提高参数选择的效率。

Method: 利用簇数量与邻域半径的单峰关系，提出基于三分搜索的参数选择策略。

Result: 通过NLP、音频和计算机视觉任务验证了方法的有效性和鲁棒性。

Conclusion: 本研究为密度聚类的参数控制提供新思路，深化了对参数关系的理解。

Abstract: Density-based clustering methods often surpass centroid-based counterparts,
when addressing data with noise or arbitrary data distributions common in
real-world problems. In this study, we reveal a key property intrinsic to
density-based clustering methods regarding the relation between the number of
clusters and the neighborhood radius of core points - we empirically show that
it is nearly unimodal, and support this claim theoretically in a specific
setting. We leverage this property to devise new strategies for finding
appropriate values for the radius more efficiently based on the Ternary Search
algorithm. This is especially important for large scale data that is
high-dimensional, where parameter tuning is computationally intensive. We
validate our methodology through extensive applications across a range of
high-dimensional, large-scale NLP, Audio, and Computer Vision tasks,
demonstrating its practical effectiveness and robustness. This work not only
offers a significant advancement in parameter control for density-based
clustering but also broadens the understanding regarding the relations between
their guiding parameters. Our code is available at
https://github.com/oronnir/UnimodalStrategies.

</details>


### [107] [$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling](https://arxiv.org/abs/2506.21714)
*Denis Gudovskiy,Wenzhao Zheng,Tomoyuki Okuno,Yohei Nakata,Kurt Keutzer*

Key words: 连续归一化流, 扩散模型, Transformer, ODE, 动态控制

TL;DR: 论文提出了一种动态控制质量-复杂度权衡的方法，通过重新设计基于Transformer的架构，解决ODE问题，从而在采样过程中减少延迟和内存使用。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有连续归一化流和扩散模型在采样时需要多次迭代，计算复杂度高。本文旨在动态控制质量与复杂度的权衡，提高效率。

Method: 通过重新连接Transformer块以解决内部离散ODE，并引入时间和长度一致性项进行训练，实现任意时间步和Transformer块的采样。

Result: 在CelebA-HQ和ImageNet上的实验表明，延迟最高减少3倍，FID分数提高3.5分。

Conclusion: 提出的方法在采样效率和质量上均优于现有技术，且与求解器无关。

Abstract: Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have
been studied using the unified theoretical framework. Although such models can
generate high-quality data points from a noise distribution, the sampling
demands multiple iterations to solve an ordinary differential equation (ODE)
with high computational complexity. Most existing methods focus on reducing the
number of time steps during the sampling process to improve efficiency. In this
work, we explore a complementary direction in which the quality-complexity
tradeoff can be dynamically controlled in terms of time steps and in the length
of the neural network. We achieve this by rewiring the blocks in the
transformer-based architecture to solve an inner discretized ODE w.r.t. its
length. Then, we employ time- and length-wise consistency terms during flow
matching training, and as a result, the sampling can be performed with an
arbitrary number of time steps and transformer blocks. Unlike others, our
$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$ approach is solver-agnostic in
time dimension and decreases both latency and memory usage. Compared to the
previous state of the art, image generation experiments on CelebA-HQ and
ImageNet show a latency reduction of up to $3\times$ in the most efficient
sampling mode, and a FID score improvement of up to $3.5$ points for
high-quality sampling. We release our code and model weights with fully
reproducible experiments.

</details>


### [108] [Performance Prediction for Large Systems via Text-to-Text Regression](https://arxiv.org/abs/2506.21718)
*Yash Akhauri,Bryan Lewandowski,Cheng-Hsi Lin,Adrian N. Reyes,Grant C. Forbes,Arissa Wongpanich,Bangding Yang,Mohamed S. Abdelfattah,Sagi Perel,Xingyou Song*

Key words: 文本到文本回归, 编码器-解码器, 复杂系统预测, 资源效率, 不确定性量化

TL;DR: 提出了一种通用的文本到文本回归方法，用于预测复杂系统数据中的度量结果，显著优于传统表格回归方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统表格回归方法在处理复杂系统数据（如配置文件或系统日志）时效果不佳，需要一种更通用的解决方案。

Method: 采用60M参数的编码器-解码器模型（随机初始化训练），通过文本到文本回归预测资源效率。

Result: 在Google的Borg调度系统中，模型实现了接近完美的0.99秩相关（平均0.9），均方误差比表格方法低100倍，并能轻松适应新任务。

Conclusion: 该方法为复杂结果的通用模拟器开辟了新途径，编码器、序列长度和不确定性量化是关键因素。

Abstract: In many industries, predicting metric outcomes of large systems is a
fundamental problem, driven largely by traditional tabular regression. However,
such methods struggle on complex systems data in the wild such as configuration
files or system logs, where feature engineering is often infeasible. We propose
text-to-text regression as a general, scalable alternative. For predicting
resource efficiency on Borg, Google's massive compute cluster scheduling
system, a 60M parameter encoder-decoder, trained from random initialization,
achieves up to a near perfect 0.99 (0.9 average) rank correlation across the
entire fleet, and 100x lower MSE than tabular approaches. The model also easily
adapts to new tasks in only 500 few-shot examples and captures the densities of
complex outcome distributions. Ablation studies highlight the importance of
using encoders, increasing sequence length, and the model's inherent
uncertainty quantification. These findings pave the way for universal
simulators of real-world outcomes.

</details>


### [109] [Federated Item Response Theory Models](https://arxiv.org/abs/2506.21744)
*Biying Zhou,Nanyu Luo,Feng Ji*

Key words: 项目反应理论, 联邦学习, 隐私保护, 分布式计算, 教育评估

TL;DR: 论文提出了一种联邦项目反应理论（FedIRT）框架，结合联邦学习保护隐私和分布式计算的优势，用于估算传统IRT模型，同时在分布式环境中保持准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统IRT估算需要集中所有个体原始响应数据，可能导致隐私问题。联邦学习提供了隐私保护和分布式计算的能力，因此作者希望将这两者结合，提出FedIRT框架。

Method: 提出FedIRT框架，通过联邦学习技术，在分布式环境中估算传统IRT模型（如2PL和PCM），同时保护隐私并减少通信成本。

Result: 数值实验表明，FedIRT在统计准确性上与传统IRT估算相当，同时具备隐私保护和通信成本优势。真实考试数据集验证了其在实际教育场景中的有效性。

Conclusion: FedIRT扩展了IRT在分布式环境中的应用（如多校评估），且不牺牲准确性或安全性，提供了开源R包实现。

Abstract: Item Response Theory (IRT) models have been widely used to estimate
respondents' latent abilities and calibrate items' difficulty. Traditional IRT
estimation requires all individual raw response data to be centralized in one
place, thus potentially causing privacy issues. Federated learning is an
emerging field in computer science and machine learning with added features of
privacy protection and distributed computing. To integrate the advances from
federated learning with modern psychometrics, we propose a novel framework,
Federated Item Response Theory (IRT), to enable estimating traditional IRT
models with additional privacy, allowing estimation in a distributed manner
without losing estimation accuracy.
  Our numerical experiments confirm that FedIRT achieves statistical accuracy
similar to standard IRT estimation using popular R packages, while offering
critical advantages: privacy protection and reduced communication costs. We
also validate FedIRT's utility through a real-world exam dataset, demonstrating
its effectiveness in realistic educational contexts. This new framework extends
IRT's applicability to distributed settings, such as multi-school assessments,
without sacrificing accuracy or security. To support practical adoption, we
provide an open-ource R package, FedIRT, implementing the framework for the
two-parameter logistic (2PL) and partial credit models (PCM).

</details>


### [110] [Gradient-Based Neuroplastic Adaptation for Concurrent Optimization of Neuro-Fuzzy Networks](https://arxiv.org/abs/2506.21771)
*John Wesley Hostetter,Min Chi*

Key words: 神经模糊网络、梯度优化、神经塑性适应、在线强化学习、视觉任务

TL;DR: 提出了一种基于梯度的神经塑性适应方法，用于同时优化神经模糊网络的参数和结构，解决了现有方法中参数与结构分离优化的低效问题，并在视觉任务中验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的神经模糊网络设计过程中，参数和结构的优化通常是分离的，导致网络架构脆弱且效果不佳。为了解决这一问题，研究提出了一种同时优化参数和结构的方法。

Method: 提出了一种基于梯度的神经塑性适应方法，能够同时优化神经模糊网络的参数和结构。该方法通过在线强化学习进行训练。

Result: 实验结果表明，该方法在视觉任务（如DOOM游戏）中表现优异，验证了同时优化参数和结构的有效性。

Conclusion: 通过同时优化神经模糊网络的参数和结构，可以显著提升网络性能，尤其是在在线强化学习的应用场景中。

Abstract: Neuro-fuzzy networks (NFNs) are transparent, symbolic, and universal function
approximations that perform as well as conventional neural architectures, but
their knowledge is expressed as linguistic IF-THEN rules. Despite these
advantages, their systematic design process remains a challenge. Existing work
will often sequentially build NFNs by inefficiently isolating parametric and
structural identification, leading to a premature commitment to brittle and
subpar architecture. We propose a novel application-independent approach called
gradient-based neuroplastic adaptation for the concurrent optimization of NFNs'
parameters and structure. By recognizing that NFNs' parameters and structure
should be optimized simultaneously as they are deeply conjoined, settings
previously unapproachable for NFNs are now accessible, such as the online
reinforcement learning of NFNs for vision-based tasks. The effectiveness of
concurrently optimizing NFNs is empirically shown as it is trained by online
reinforcement learning to proficiently play challenging scenarios from a
vision-based video game called DOOM.

</details>


### [111] [M3PO: Massively Multi-Task Model-Based Policy Optimization](https://arxiv.org/abs/2506.21782)
*Aditya Narendra,Dmitry Makarov,Aleksandr Panov*

Key words: M3PO, 强化学习, 隐式世界模型, 混合探索策略

TL;DR: M3PO是一个基于模型的强化学习框架，通过整合隐式世界模型和混合探索策略，解决了单任务样本效率低和多任务泛化性差的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 基于像素的模型忽视控制中心表示，而基于无模型的方法样本效率低且探索能力弱，因此开发M3PO以解决这些不足。

Method: 结合隐式世界模型和混合探索策略，通过模型与无模型价值估计差异引导探索，使用信任区域优化器稳定策略更新。

Result: M3PO在多个基准测试中表现优于现有方法，达到最先进的性能。

Conclusion: M3PO为基于模型的策略优化提供了高效且稳健的替代方案。

Abstract: We introduce Massively Multi-Task Model-Based Policy Optimization (M3PO), a
scalable model-based reinforcement learning (MBRL) framework designed to
address sample inefficiency in single-task settings and poor generalization in
multi-task domains. Existing model-based approaches like DreamerV3 rely on
pixel-level generative models that neglect control-centric representations,
while model-free methods such as PPO suffer from high sample complexity and
weak exploration. M3PO integrates an implicit world model, trained to predict
task outcomes without observation reconstruction, with a hybrid exploration
strategy that combines model-based planning and model-free uncertainty-driven
bonuses. This eliminates the bias-variance trade-off in prior methods by using
discrepancies between model-based and model-free value estimates to guide
exploration, while maintaining stable policy updates through a trust-region
optimizer. M3PO provides an efficient and robust alternative to existing
model-based policy optimization approaches and achieves state-of-the-art
performance across multiple benchmarks.

</details>


### [112] [Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data](https://arxiv.org/abs/2506.21788)
*Massimiliano Lupo Pasini,Jong Youl Choi,Pei Zhang,Kshitij Mehta,Rylie Weaver,Ashwin M. Aji,Karl W. Schulz,Jorda Polo,Prasanna Balaprakash*

Key words: graph neural networks, multi-task learning, atomistic modeling, supercomputing, scalability

TL;DR: Graph foundation models using graph neural networks for atomistic modeling improve multi-source, multi-fidelity data processing via multi-task learning and GPU-accelerated parallelism.

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Address challenges in processing diverse atomistic data and improve model generalizability and scalability on supercomputers.

Method: Multi-task learning with shared message passing layers and distributed decoding heads, implemented in the open-source HydraGNN architecture.

Result: Effective scaling on three heterogeneous supercomputers (Perlmutter, Aurora, Frontier) with training on 24 million structures.

Conclusion: The method enhances model transferability and scalability, though questions about generalizability to larger datasets remain.

Abstract: Graph foundation models using graph neural networks promise sustainable,
efficient atomistic modeling. To tackle challenges of processing multi-source,
multi-fidelity data during pre-training, recent studies employ multi-task
learning, in which shared message passing layers initially process input
atomistic structures regardless of source, then route them to multiple decoding
heads that predict data-specific outputs. This approach stabilizes pre-training
and enhances a model's transferability to unexplored chemical regions.
Preliminary results on approximately four million structures are encouraging,
yet questions remain about generalizability to larger, more diverse datasets
and scalability on supercomputers. We propose a multi-task parallelism method
that distributes each head across computing resources with GPU acceleration.
Implemented in the open-source HydraGNN architecture, our method was trained on
over 24 million structures from five datasets and tested on the Perlmutter,
Aurora, and Frontier supercomputers, demonstrating efficient scaling on all
three highly heterogeneous super-computing architectures.

</details>


### [113] [Why Neural Network Can Discover Symbolic Structures with Gradient-based Training: An Algebraic and Geometric Foundation for Neurosymbolic Reasoning](https://arxiv.org/abs/2506.21797)
*Peihao Wang,Zhangyang Wang*

Key words: 神经符号系统、Wasserstein梯度流、测度空间、群不变性、代数约束

TL;DR: 论文提出了一种理论框架，解释离散符号结构如何从连续的神经网络训练动态中自然涌现，通过将参数提升到测度空间并建模为Wasserstein梯度流，揭示了在几何约束下参数测度的两个现象：梯度的解耦和自由度收缩。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨神经网络如何在训练中自发形成符号化表示，为神经符号系统的设计与理解提供理论基础。

Method: 将神经网络参数提升为测度空间中的测度，建模训练为Wasserstein梯度流，分析在几何约束（如群不变性）下参数测度的动态变化。

Result: 发现参数测度在训练中经历梯度解耦与自由度收缩，最终形成符合代数操作的组合表示，并建立了数据尺度定律与符号任务实现的联系。

Conclusion: 该框架为神经符号系统的设计提供了理论基础，连接了连续学习与离散代数推理。

Abstract: We develop a theoretical framework that explains how discrete symbolic
structures can emerge naturally from continuous neural network training
dynamics. By lifting neural parameters to a measure space and modeling training
as Wasserstein gradient flow, we show that under geometric constraints, such as
group invariance, the parameter measure $\mu_t$ undergoes two concurrent
phenomena: (1) a decoupling of the gradient flow into independent optimization
trajectories over some potential functions, and (2) a progressive contraction
on the degree of freedom. These potentials encode algebraic constraints
relevant to the task and act as ring homomorphisms under a commutative
semi-ring structure on the measure space. As training progresses, the network
transitions from a high-dimensional exploration to compositional
representations that comply with algebraic operations and exhibit a lower
degree of freedom. We further establish data scaling laws for realizing
symbolic tasks, linking representational capacity to the group invariance that
facilitates symbolic solutions. This framework charts a principled foundation
for understanding and designing neurosymbolic systems that integrate continuous
learning with discrete algebraic reasoning.

</details>


### [114] [The Cost of Avoiding Backpropagation](https://arxiv.org/abs/2506.21833)
*Kunjal Panchal,Sunav Choudhary,Yuriy Brun,Hui Guan*

Key words: FmAD, ZO, BP, 内存高效, 训练优化

TL;DR: 本文全面比较了前向模式自动微分（FmAD）和零阶优化（ZO）与反向传播（BP）及其内存高效变体（如激活检查点）的性能，发现BP在内存受限时仍是最优策略。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决FmAD和ZO在实际应用中的性能不清楚问题，并与内存高效的BP变体进行对比。

Method: 通过理论和实验分析比较BP、FmAD和ZO方法，包括准确率、收敛速度和计算成本。

Result: BP激活检查点表现优于FmAD和ZO，准确率高31.1%，收敛快34.8%，计算量少3.8倍。

Conclusion: FmAD和ZO在大模型或预算受限时表现更差，BP激活检查点是内存受限下的最佳选择。

Abstract: Forward-mode automatic differentiation (FmAD) and zero-order (ZO)
optimization have been proposed as memory-efficient alternatives to
backpropagation (BP) for gradient computation, especially in low-resource
settings. However, their practical benefits remain unclear due to two key gaps:
a lack of comparison against memory-efficient BP variants, such as activation
checkpointing, and a lack of a unified theoretical analysis. This work presents
a comprehensive theoretical and empirical comparison of BP, FmAD, and ZO
methods. Our theoretical analysis shows that while FmAD, and ZO can reduce
memory usage, they incur significant costs in accuracy, convergence speed, and
computation compared to BP with checkpointing. These drawbacks worsen with
larger models or constrained perturbation budgets. Empirical experiments on
large language and vision-language models show that BP with checkpointing
outperforms FmAD and ZO variants, including those enhanced with variance
reduction, achieving up to 31.1% higher accuracy, 34.8% faster convergence, and
3.8x fewer computations at comparable memory usage. Our results highlight
fundamental limitations of FmAD and ZO, and reaffirm BP with checkpointing as
the most effective strategy for model training under memory-constrained
settings. Our code is available at
https://github.com/Astuary/The_Cost_of_Avoiding_Backpropagation.

</details>


### [115] [Koopman operator-based discussion on partial observation in stochastic systems](https://arxiv.org/abs/2506.21844)
*Jun Ohkubo*

Key words: Koopman算子理论, 随机系统, 部分观测, 延迟嵌入, 幂律行为

TL;DR: 本文讨论了在随机系统中使用Koopman算子理论处理部分观测的效果，强调了在随机系统中区分状态空间和函数空间的重要性，并通过数值实验验证了延迟嵌入技术对部分观测的有益影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在确定性系统中，Mori-Zwanzig形式为处理部分观测提供了理论框架，但随机系统中部分观测的效应尚未充分研究。本文旨在填补这一空白。

Method: 使用Koopman算子理论分析随机系统中的部分观测效应，并通过数值实验验证延迟嵌入技术的有效性。

Result: 研究发现延迟嵌入技术对随机系统中的部分观测有益，且数值实验显示添加噪声幅度的精度呈现幂律行为。

Conclusion: 在随机系统中，区分状态空间和函数空间对理解部分观测效应至关重要，且幂律行为的指数与部分观测效应相关。

Abstract: It is sometimes difficult to achieve a complete observation for a full set of
observables, and partial observations are necessary. For deterministic systems,
the Mori-Zwanzig formalism provides a theoretical framework for handling
partial observations. Recently, data-driven algorithms based on the Koopman
operator theory have made significant progress, and there is a discussion to
connect the Mori-Zwanzig formalism with the Koopman operator theory. In this
work, we discuss the effects of partial observation in stochastic systems using
the Koopman operator theory. The discussion clarifies the importance of
distinguishing the state space and the function space in stochastic systems.
Even in stochastic systems, the delay embedding technique is beneficial for
partial observation, and several numerical experiments showed a power-law
behavior of the accuracy for the amplitude of the additive noise. We also
discuss the relation between the exponent of the power-law behavior and the
effects of partial observation.

</details>


### [116] [A Survey of Continual Reinforcement Learning](https://arxiv.org/abs/2506.21872)
*Chaofan Pan,Xin Yang,Yanhua Li,Wei Wei,Tianrui Li,Bo An,Jiye Liang*

Key words: 强化学习；持续学习；持续强化学习；知识转移；泛化能力

TL;DR: 论文探讨了持续强化学习（CRL）作为解决强化学习（RL）中数据依赖和泛化能力不足问题的新方向，综述了其核心概念、挑战和方法，并提出新的分类法和未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: RL在解决序列决策问题中表现突出，但其训练数据和计算资源的高需求及泛化能力不足限制了实际应用，CRL通过持续学习和适应新任务的能力提供了解决方案。

Method: 1. 综述现有研究，整理分析其指标、任务、基准和场景设置；2. 提出基于知识存储和转移的四类CRL方法新分类法；3. 分析CRL独特挑战并提出未来方向。

Result: 论文提出了一种CRL方法的分类体系，并总结了该领域的核心挑战和潜在发展方向。

Conclusion: CRL是RL的重要拓展方向，能有效解决其局限性，未来研究需关注其独特挑战。

Abstract: Reinforcement Learning (RL) is an important machine learning paradigm for
solving sequential decision-making problems. Recent years have witnessed
remarkable progress in this field due to the rapid development of deep neural
networks. However, the success of RL currently relies on extensive training
data and computational resources. In addition, RL's limited ability to
generalize across tasks restricts its applicability in dynamic and real-world
environments. With the arisen of Continual Learning (CL), Continual
Reinforcement Learning (CRL) has emerged as a promising research direction to
address these limitations by enabling agents to learn continuously, adapt to
new tasks, and retain previously acquired knowledge. In this survey, we provide
a comprehensive examination of CRL, focusing on its core concepts, challenges,
and methodologies. Firstly, we conduct a detailed review of existing works,
organizing and analyzing their metrics, tasks, benchmarks, and scenario
settings. Secondly, we propose a new taxonomy of CRL methods, categorizing them
into four types from the perspective of knowledge storage and/or transfer.
Finally, our analysis highlights the unique challenges of CRL and provides
practical insights into future directions.

</details>


### [117] [Advancements and Challenges in Continual Reinforcement Learning: A Comprehensive Review](https://arxiv.org/abs/2506.21899)
*Amara Zuffer,Michael Burke,Mehrtash Harandi*

Key words: 连续强化学习, 机器人学, 动态学习, 评估环境, 未来方向

TL;DR: 该综述探讨了连续强化学习（RL）的关键概念、挑战及方法，重点介绍了其在机器人学中的最新进展和评估环境，并展望了未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究连续强化学习的动机是通过使RL代理能够持续学习和适应多样化任务，提升其动态学习能力。

Method: 论文综述了连续强化学习的基本概念、挑战和方法，特别关注机器人学中的应用和评估环境。

Result: 研究发现连续强化学习能有效提升RL代理的知识获取和保留能力，推动其在动态环境中的应用。

Conclusion: 综述总结了连续强化学习的局限性和未来方向，为研究者和实践者提供了重要参考。

Abstract: The diversity of tasks and dynamic nature of reinforcement learning (RL)
require RL agents to be able to learn sequentially and continuously, a learning
paradigm known as continuous reinforcement learning. This survey reviews how
continual learning transforms RL agents into dynamic continual learners. This
enables RL agents to acquire and retain useful and reusable knowledge
seamlessly. The paper delves into fundamental aspects of continual
reinforcement learning, exploring key concepts, significant challenges, and
novel methodologies. Special emphasis is placed on recent advancements in
continual reinforcement learning within robotics, along with a succinct
overview of evaluation environments utilized in prominent research,
facilitating accessibility for newcomers to the field. The review concludes
with a discussion on limitations and promising future directions, providing
valuable insights for researchers and practitioners alike.

</details>


### [118] [TOAST: Task-Oriented Adaptive Semantic Transmission over Dynamic Wireless Environments](https://arxiv.org/abs/2506.21900)
*Sheng Yun,Jianhua Pei,Ping Wang*

Key words: 6G网络, 语义通信, 深度强化学习, 低秩适应, 扩散模型

TL;DR: TOAST是面向6G网络的语义感知通信框架，通过深度强化学习、低秩适应和扩散模型优化多任务性能，显著提升了低信噪比条件下的分类准确性和重建质量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着6G网络的发展，需要从比特传输转向任务相关的语义感知通信，以优化动态无线环境中的多任务性能。

Method: 1. 使用深度强化学习动态平衡图像重建与语义分类；2. 基于Swin Transformer的架构中集成低秩适应机制；3. 引入潜在空间扩散模型恢复噪声特征。

Result: TOAST在多个数据集中表现优越，尤其在低信噪比条件下显著提高了分类准确性和重建质量，同时保持强健性能。

Conclusion: TOAST为6G语义通信提供了高效解决方案，适用于多样化无线环境。

Abstract: The evolution toward 6G networks demands a fundamental shift from bit-centric
transmission to semantic-aware communication that emphasizes task-relevant
information. This work introduces TOAST (Task-Oriented Adaptive Semantic
Transmission), a unified framework designed to address the core challenge of
multi-task optimization in dynamic wireless environments through three
complementary components. First, we formulate adaptive task balancing as a
Markov decision process, employing deep reinforcement learning to dynamically
adjust the trade-off between image reconstruction fidelity and semantic
classification accuracy based on real-time channel conditions. Second, we
integrate module-specific Low-Rank Adaptation (LoRA) mechanisms throughout our
Swin Transformer-based joint source-channel coding architecture, enabling
parameter-efficient fine-tuning that dramatically reduces adaptation overhead
while maintaining full performance across diverse channel impairments including
Additive White Gaussian Noise (AWGN), fading, phase noise, and impulse
interference. Third, we incorporate an Elucidating diffusion model that
operates in the latent space to restore features corrupted by channel noises,
providing substantial quality improvements compared to baseline approaches.
Extensive experiments across multiple datasets demonstrate that TOAST achieves
superior performance compared to baseline approaches, with significant
improvements in both classification accuracy and reconstruction quality at low
Signal-to-Noise Ratio (SNR) conditions while maintaining robust performance
across all tested scenarios.

</details>


### [119] [HQCM-EBTC: A Hybrid Quantum-Classical Model for Explainable Brain Tumor Classification](https://arxiv.org/abs/2506.21937)
*Marwan Ait Haddou,Mohamed Bennai*

Key words: 量子计算,脑肿瘤分类,MRI,混合模型

TL;DR: HQCM-EBTC提出了一种混合量子经典模型，用于脑肿瘤分类，表现优于传统方法，展示了量子增强模型在医学影像中的潜力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 通过结合量子计算和经典方法，提升脑肿瘤分类的准确性和可解释性。

Method: 使用5量子比特、深度2的量子层与5个并行电路，结合AdamW优化器和复合损失函数进行训练。

Result: 模型准确率达到96.48%，显著优于经典基线（86.72%），且在胶质瘤检测中表现更优。

Conclusion: 量子增强模型在医学影像诊断中展现出显著优势，提升了准确性和可解释性。

Abstract: We propose HQCM-EBTC, a hybrid quantum-classical model for automated brain
tumor classification using MRI images. Trained on a dataset of 7,576 scans
covering normal, meningioma, glioma, and pituitary classes, HQCM-EBTC
integrates a 5-qubit, depth-2 quantum layer with 5 parallel circuits, optimized
via AdamW and a composite loss blending cross-entropy and attention
consistency.
  HQCM-EBTC achieves 96.48% accuracy, substantially outperforming the classical
baseline (86.72%). It delivers higher precision and F1-scores, especially for
glioma detection. t-SNE projections reveal enhanced feature separability in
quantum space, and confusion matrices show lower misclassification. Attention
map analysis (Jaccard Index) confirms more accurate and focused tumor
localization at high-confidence thresholds.
  These results highlight the promise of quantum-enhanced models in medical
imaging, advancing both diagnostic accuracy and interpretability for clinical
brain tumor assessment.

</details>


### [120] [GuiderNet: A Meta-Learning Framework for Optimizing Quantum Circuit Geometry and Mitigating Barren Plateaus](https://arxiv.org/abs/2506.21940)
*Marwan Ait Haddou,Mohamed Bennai*

Key words: 变分量子算法, 元学习, 量子机器学习, Barren Plateaus, GuiderNet

TL;DR: GuiderNet是一种元学习框架，通过数据依赖的参数调整改善量子电路的优化过程，显著提升训练效果和测试性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 变分量子算法（VQA）在近期量子优势方面具有潜力，但面临梯度消失（Barren Plateaus）和优化条件差的问题。作者旨在通过几何元调节方法解决这些问题，提升量子机器学习的可训练性和泛化能力。

Method: 提出GuiderNet框架，利用经典神经网络对参数化量子电路（PQC）进行数据依赖的参数调整，优化Fubini-Study度量张量的条件数。

Result: 在糖尿病分类任务中，GuiderNet将累积训练损失降低5倍以上，测试准确率从75.3%提升至98.6%，少数类F1分数从0.67增加到0.95。同时抑制了梯度爆炸并稳定了参数更新。

Conclusion: 几何元调节方法有效缓解了Barren Plateaus和优化条件差的问题，为量子机器学习提供了可扩展的解决方案。

Abstract: Variational Quantum Algorithms (VQAs) offer potential for near-term quantum
advantage but face challenges from barren plateaus, where gradients vanish, and
poorly conditioned optimization landscapes. We introduce GuiderNet, a
meta-learning framework that conditions Parameterized Quantum Circuits (PQCs)
using data-dependent parameter shifts aimed at minimizing the log condition
number of the Fubini-Study metric tensor. Implemented as a classical neural
network, GuiderNet is meta-trained to guide PQC parameters into geometrically
favorable regions and is embedded within hybrid quantum-classical pipelines to
steer both initialization and adaptive modulation during training.
  Applied to the Kaggle Diabetes classification task, GuiderNet reduces
cumulative training loss by over 5x, improves test accuracy from 75.3% to
98.6%, and increases the minority-class F1 score from 0.67 to 0.95. It also
suppresses gradient explosion and stabilizes parameter updates, enabling
smoother and more robust optimization. These results demonstrate that geometric
meta-conditioning can mitigate barren plateaus and ill-conditioning, providing
a scalable approach to enhance trainability and generalization in quantum
machine learning.

</details>


### [121] [Physics-informed network paradigm with data generation and background noise removal for diverse distributed acoustic sensing applications](https://arxiv.org/abs/2506.21952)
*Yangyang Wan,Haotian Wang,Xuhui Yu,Jiageng Chen,Xinyu Fan,Zuyuan He*

Key words: 分布式声学感知、物理信息驱动、生成网络、去背景噪声、泛化能力

TL;DR: 提出了一种物理信息驱动的分布式声学感知(DAS)神经网络范式，无需真实事件数据进行训练，通过物理建模生成数据并去除背景噪声，验证了其在实际应用中的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有AI模型需依赖真实数据训练，而实际应用中事件数据有限，物理信息驱动的范式旨在解决这一问题。

Method: 通过物理建模目标事件及系统约束，生成DAS事件数据训练生成网络和去背景噪声网络。

Result: 在公开数据集和实际皮带机故障监测中表现优异，泛化能力强，故障诊断准确率达91.8%。

Conclusion: 该范式通过物理信息引入和噪声去除能力，为DAS应用中的数据获取和高噪声问题提供了解决方案。

Abstract: Distributed acoustic sensing (DAS) has attracted considerable attention
across various fields and artificial intelligence (AI) technology plays an
important role in DAS applications to realize event recognition and denoising.
Existing AI models require real-world data (RWD), whether labeled or not, for
training, which is contradictory to the fact of limited available event data in
real-world scenarios. Here, a physics-informed DAS neural network paradigm is
proposed, which does not need real-world events data for training. By
physically modeling target events and the constraints of real world and DAS
system, physical functions are derived to train a generative network for
generation of DAS events data. DAS debackground net is trained by using the
generated DAS events data to eliminate background noise in DAS data. The
effectiveness of the proposed paradigm is verified in event identification
application based on a public dataset of DAS spatiotemporal data and in belt
conveyor fault monitoring application based on DAS time-frequency data, and
achieved comparable or better performance than data-driven networks trained
with RWD. Owing to the introduction of physical information and capability of
background noise removal, the paradigm demonstrates generalization in same
application on different sites. A fault diagnosis accuracy of 91.8% is achieved
in belt conveyor field with networks which transferred from simulation test
site without any fault events data of test site and field for training. The
proposed paradigm is a prospective solution to address significant obstacles of
data acquisition and intense noise in practical DAS applications and explore
more potential fields for DAS.

</details>


### [122] [Optimal Return-to-Go Guided Decision Transformer for Auto-Bidding in Advertisement](https://arxiv.org/abs/2506.21956)
*Hao Jiang,Yongxiang Tang,Yanxiang Zeng,Pengjia Yuan,Yanhua Cheng,Teng Sha,Xialong Liu,Peng Jiang*

Key words: 自动竞价, 决策变换器, 生成模型, 数据增强, 在线广告

TL;DR: 本文提出了一种改进的决策变换器模型（R* DT），用于在线广告自动竞价任务，克服了传统决策变换器依赖预设返回目标（RTG）的限制，并通过数据增强提升了策略优化的效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在线广告自动竞价系统中，传统决策变换器（DT）存在依赖预设RTG及训练数据质量不均的问题，限制了其性能。

Method: 引入R* DT模型，分三步改进：R DT存储状态和RTG；R^ DT预测最优RTG；R* DT通过数据增强优化训练数据。

Result: 在公开竞价数据集上验证了R* DT的高效性，尤其是在处理混合质量轨迹时表现优于传统方法。

Conclusion: R* DT通过动态调整RTG和数据增强，显著提升了自动竞价系统的长期性能和策略优化效果。

Abstract: In the realm of online advertising, advertisers partake in ad auctions to
obtain advertising slots, frequently taking advantage of auto-bidding tools
provided by demand-side platforms. To improve the automation of these bidding
systems, we adopt generative models, namely the Decision Transformer (DT), to
tackle the difficulties inherent in automated bidding. Applying the Decision
Transformer to the auto-bidding task enables a unified approach to sequential
modeling, which efficiently overcomes short-sightedness by capturing long-term
dependencies between past bidding actions and user behavior. Nevertheless,
conventional DT has certain drawbacks: (1) DT necessitates a preset
return-to-go (RTG) value before generating actions, which is not inherently
produced; (2) The policy learned by DT is restricted by its training data,
which is consists of mixed-quality trajectories. To address these challenges,
we introduce the R* Decision Transformer (R* DT), developed in a three-step
process: (1) R DT: Similar to traditional DT, R DT stores actions based on
state and RTG value, as well as memorizing the RTG for a given state using the
training set; (2) R^ DT: We forecast the highest value (within the training
set) of RTG for a given state, deriving a suboptimal policy based on the
current state and the forecasted supreme RTG value; (3) R* DT: Based on R^ DT,
we generate trajectories and select those with high rewards (using a simulator)
to augment our training dataset. This data enhancement has been shown to
improve the RTG of trajectories in the training data and gradually leads the
suboptimal policy towards optimality. Comprehensive tests on a publicly
available bidding dataset validate the R* DT's efficacy and highlight its
superiority when dealing with mixed-quality trajectories.

</details>


### [123] [SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model](https://arxiv.org/abs/2506.21976)
*Shuhan Tan,John Lambert,Hong Jeon,Sakshum Kulshrestha,Yijing Bai,Jing Luo,Dragomir Anguelov,Mingxing Tan,Chiyu Max Jiang*

Key words: 交通仿真，自动驾驶，生成式模型，SceneDiffuser++

TL;DR: 论文提出了一种名为CitySim的交通仿真愿景，旨在通过生成式仿真技术模拟城市规模的交通场景，弥补手动驾驶测试数据的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为解决自动驾驶测试中手动驾驶数据不足的问题，提出构建一个可无缝模拟从A点到B点行程的生成式城市仿真系统。

Method: 采用SceneDiffuser++模型，该模型通过单一损失函数端到端训练，整合了场景生成、动态代理行为建模、遮挡推理等关键技术。

Result: 实验表明，SceneDiffuser++能实现城市规模的交通仿真，并在长时间仿真条件下表现出更高的真实感。

Conclusion: SceneDiffuser++为城市级交通仿真提供了一种高效的解决方案，并在扩展的Waymo数据集上验证了其真实性。

Abstract: The goal of traffic simulation is to augment a potentially limited amount of
manually-driven miles that is available for testing and validation, with a much
larger amount of simulated synthetic miles. The culmination of this vision
would be a generative simulated city, where given a map of the city and an
autonomous vehicle (AV) software stack, the simulator can seamlessly simulate
the trip from point A to point B by populating the city around the AV and
controlling all aspects of the scene, from animating the dynamic agents (e.g.,
vehicles, pedestrians) to controlling the traffic light states. We refer to
this vision as CitySim, which requires an agglomeration of simulation
technologies: scene generation to populate the initial scene, agent behavior
modeling to animate the scene, occlusion reasoning, dynamic scene generation to
seamlessly spawn and remove agents, and environment simulation for factors such
as traffic lights. While some key technologies have been separately studied in
various works, others such as dynamic scene generation and environment
simulation have received less attention in the research community. We propose
SceneDiffuser++, the first end-to-end generative world model trained on a
single loss function capable of point A-to-B simulation on a city scale
integrating all the requirements above. We demonstrate the city-scale traffic
simulation capability of SceneDiffuser++ and study its superior realism under
long simulation conditions. We evaluate the simulation quality on an augmented
version of the Waymo Open Motion Dataset (WOMD) with larger map regions to
support trip-level simulation.

</details>


### [124] [Binned semiparametric Bayesian networks](https://arxiv.org/abs/2506.21997)
*Rafael Sojo,Javier Díaz-Rozo,Concha Bielza,Pedro Larrañaga*

Key words: 概率半参数模型, 数据分箱, 核密度估计, 维度灾难, 贝叶斯网络

TL;DR: 本文提出了一种新型概率半参数模型，通过数据分箱降低非参数分布中核密度估计的计算成本。实验表明，该方法在性能上与现有方法相当，但速度显著提升。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决核密度估计在非参数分布中的高计算成本问题，同时应对分箱模型中的维度灾难。

Method: 开发了两种新的条件概率分布（稀疏分箱核密度估计和傅里叶核密度估计），利用稀疏张量和限制父节点数量来解决维度问题。

Result: 模型在结构学习和对数似然估计上与现有方法无显著差异，但计算速度更快。

Conclusion: 分箱半参数贝叶斯网络是现有方法的更高效替代方案。

Abstract: This paper introduces a new type of probabilistic semiparametric model that
takes advantage of data binning to reduce the computational cost of kernel
density estimation in nonparametric distributions. Two new conditional
probability distributions are developed for the new binned semiparametric
Bayesian networks, the sparse binned kernel density estimation and the Fourier
kernel density estimation. These two probability distributions address the
curse of dimensionality, which typically impacts binned models, by using sparse
tensors and restricting the number of parent nodes in conditional probability
calculations. To evaluate the proposal, we perform a complexity analysis and
conduct several comparative experiments using synthetic data and datasets from
the UCI Machine Learning repository. The experiments include different binning
rules, parent restrictions, grid sizes, and number of instances to get a
holistic view of the model's behavior. As a result, our binned semiparametric
Bayesian networks achieve structural learning and log-likelihood estimations
with no statistically significant differences compared to the semiparametric
Bayesian networks, but at a much higher speed. Thus, the new binned
semiparametric Bayesian networks prove to be a reliable and more efficient
alternative to their non-binned counterparts.

</details>


### [125] [GKNet: Graph Kalman Filtering and Model Inference via Model-based Deep Learning](https://arxiv.org/abs/2506.22004)
*Mohammad Sabbaqi,Riccardo Taormina,Elvin Isufi*

Key words: 图时间序列、状态空间模型、随机偏微分方程、深度学习、推理任务

TL;DR: 本文提出了一种图感知状态空间模型，用于图时间序列的推理任务，通过结合图结构和时间动态，实现高效的参数学习和状态跟踪。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 图时间序列的推理任务在多个实际应用中至关重要（如城市水网络、经济学和神经科学），需要一种既能捕获图-时间模式又计算高效的模型。

Method: 提出了一种参数化的图状态空间模型，其状态方程基于图的随机偏微分方程，观测模型通过图滤波捕获多跳邻居影响；采用最大似然和深度学习方法进行推断和优化。

Result: 模型能够高效学习参数并跟踪状态，适用于预测和填补等下游任务。

Conclusion: 该模型结合了理论可解释性和深度学习的高表达性，为图时间序列分析提供了新工具。

Abstract: Inference tasks with time series over graphs are of importance in
applications such as urban water networks, economics, and networked
neuroscience. Addressing these tasks typically relies on identifying a
computationally affordable model that jointly captures the graph-temporal
patterns of the data. In this work, we propose a graph-aware state space model
for graph time series, where both the latent state and the observation equation
are parametric graph-induced models with a limited number of parameters that
need to be learned. More specifically, we consider the state equation to follow
a stochastic partial differential equation driven by noise over the graphs
edges accounting not only for potential edge uncertainties but also for
increasing the degrees of freedom in the latter in a tractable manner. The
graph structure conditioning of the noise dispersion allows the state variable
to deviate from the stochastic process in certain neighborhoods. The
observation model is a sampled and graph-filtered version of the state
capturing multi-hop neighboring influence. The goal is to learn the parameters
in both state and observation models from the partially observed data for
downstream tasks such as prediction and imputation. The model is inferred first
through a maximum likelihood approach that provides theoretical tractability
but is limited in expressivity and scalability. To improve on the latter, we
use the state-space formulation to build a principled deep learning
architecture that jointly learns the parameters and tracks the state in an
end-to-end manner in the spirit of Kalman neural networks.

</details>


### [126] [On the Necessity of Output Distribution Reweighting for Effective Class Unlearning](https://arxiv.org/abs/2506.20893)
*Yian Wang,Ali Ebrahimpour-Boroojeny,Hari Sundaram*

Key words: 遗忘学习,输出重新加权,MIA-NN攻击,总变差距离

TL;DR: 本文介绍了一种轻量级的输出重新加权遗忘方法RWFT，用于从训练好的分类器中完全擦除特定类别，而无需完全重新训练，有效减少有害或有偏预测。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 实现用户删除权利和减轻有害或有偏预测的需求。

Method: 采用概率质量重新分配的方法，设计了一种对MIA-NN攻击具有鲁棒性的遗忘技术，并引入基于总变差距离的新度量标准。

Result: RWFT在评估指标和新的TV度量标准上均达到与完全重新训练相当的效果，优于现有方法。

Conclusion: RWFT是一种高效且安全的遗忘方法，显著优于现有技术。

Abstract: In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.

</details>


### [127] [TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning](https://arxiv.org/abs/2506.22008)
*Alessandro Sestini,Joakim Bergdahl,Konrad Tollmar,Andrew D. Bagdanov,Linus Gisslén*

Key words: 离线强化学习、TROFI、奖励函数、人类偏好、D4RL

TL;DR: 提出TROFI方法，用于离线强化学习中无预定义奖励函数的情况下学习策略，通过人类偏好生成奖励函数并标注数据集。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 离线强化学习通常需要预定义的奖励函数，但在实际应用中（如游戏开发）可能无法保证其可用性。

Method: TROFI从人类偏好中学习奖励函数，并用其标注数据集，无需最优轨迹。

Result: 在D4RL基准测试中表现优于基线，与使用真实奖励相当；3D游戏环境中验证有效。

Conclusion: 奖励函数的设计对值函数与实际奖励的对齐至关重要。

Abstract: In offline reinforcement learning, agents are trained using only a fixed set
of stored transitions derived from a source policy. However, this requires that
the dataset be labeled by a reward function. In applied settings such as video
game development, the availability of the reward function is not always
guaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement
learning (TROFI), a novel approach to effectively learn a policy offline
without a pre-defined reward function. TROFI first learns a reward function
from human preferences, which it then uses to label the original dataset making
it usable for training the policy. In contrast to other approaches, our method
does not require optimal trajectories. Through experiments on the D4RL
benchmark we demonstrate that TROFI consistently outperforms baselines and
performs comparably to using the ground truth reward to learn policies.
Additionally, we validate the efficacy of our method in a 3D game environment.
Our studies of the reward model highlight the importance of the reward function
in this setting: we show that to ensure the alignment of a value function to
the actual future discounted reward, it is fundamental to have a
well-engineered and easy-to-learn reward function.

</details>


### [128] [Hyper-modal Imputation Diffusion Embedding with Dual-Distillation for Federated Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2506.22036)
*Ying Zhang,Yu Zhao,Xuhui Sui,Baohang Zhou,Xiangrui Cai,Li Shen,Xiaojie Yuan,Dacheng Tao*

Key words: 联邦学习, 多模态知识图谱, 知识蒸馏, 嵌入恢复

TL;DR: 为解决多模态知识图谱在分散环境下的协作问题，提出联邦多模态知识图谱补全任务（FedMKGC），并开发MMFeD3-HidE框架，通过HidE恢复不完整嵌入和MMFeD3实现客户端与服务器间的知识蒸馏。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多模态知识图谱的分散性和协作需求推动了联邦学习在知识图谱补全中的应用，旨在保护敏感知识同时提升预测能力。

Method: 结合HidE恢复多模态分布和MMFeD3通过双重蒸馏（logit和特征）实现客户端与服务器的知识共享。

Result: 实验验证了MMFeD3-HidE在有效性、语义一致性和收敛鲁棒性方面的优势。

Conclusion: MMFeD3-HidE为联邦多模态知识图谱补全提供了可行解决方案，兼具效果和安全性。

Abstract: With the increasing multimodal knowledge privatization requirements,
multimodal knowledge graphs in different institutes are usually decentralized,
lacking of effective collaboration system with both stronger reasoning ability
and transmission safety guarantees. In this paper, we propose the Federated
Multimodal Knowledge Graph Completion (FedMKGC) task, aiming at training over
federated MKGs for better predicting the missing links in clients without
sharing sensitive knowledge. We propose a framework named MMFeD3-HidE for
addressing multimodal uncertain unavailability and multimodal client
heterogeneity challenges of FedMKGC. (1) Inside the clients, our proposed
Hyper-modal Imputation Diffusion Embedding model (HidE) recovers the complete
multimodal distributions from incomplete entity embeddings constrained by
available modalities. (2) Among clients, our proposed Multimodal FeDerated Dual
Distillation (MMFeD3) transfers knowledge mutually between clients and the
server with logit and feature distillation to improve both global convergence
and semantic consistency. We propose a FedMKGC benchmark for a comprehensive
evaluation, consisting of a general FedMKGC backbone named MMFedE, datasets
with heterogeneous multimodal information, and three groups of constructed
baselines. Experiments conducted on our benchmark validate the effectiveness,
semantic consistency, and convergence robustness of MMFeD3-HidE.

</details>


### [129] [UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting](https://arxiv.org/abs/2506.22039)
*Lu Han,Yu Liu,Qiwen Deng,Jian Jiang,Yinbo Sun,Zhe Yu,Binfeng Wang,Xingyu Lu,Lintao Ma,Han-Jia Ye,De-Chuan Zhan*

Key words: 时间序列基础模型, 异构协变量, 多模态数据, 注意力机制, 预测任务

TL;DR: UniCA框架通过协变量同质化和注意力融合机制，解决了时间序列基础模型（TSFMs）在处理异构协变量时的限制。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的TSFMs主要针对实值序列设计，无法有效处理包含异构协变量（如分类数据、多模态数据）的通用预测任务。

Method: 提出UniCA框架，通过协变量同质化和统一注意力融合机制，将异构协变量转化为同质序列表示并进行融合。

Result: 在多个基准测试中，UniCA展示了优越性能，验证了其在真实场景中的潜力。

Conclusion: UniCA为TSFMs的协变量感知适应提供了通用解决方案，兼具兼容性和泛化能力。

Abstract: Time Series Foundation Models (TSFMs) have achieved remarkable success
through large-scale pretraining. However, their design primarily targets
real-valued series, limiting their ability to handle general forecasting tasks
involving diverse and often heterogeneous covariates--such as categorical
variables and multimodal data (e.g., images, text)--which are typically
task-specific and difficult to leverage during pretraining. To address this
gap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge
TSFMs with general covariate-aware forecasting. UniCA first performs covariate
homogenization to transform heterogeneous covariates into high-level
homogeneous series representations and then fuses them via a unified
attention-based fusion mechanism. UniCA is compatible and universal for
adaptation with both homogeneous and heterogeneous covariates, incorporating
extra covariate information while preserving the generalization ability of
TSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware
forecasting benchmarks demonstrate the superiority of UniCA, highlighting the
promise of covariate-aware TSFM adaptation in real-world forecasting scenarios.
Codes are released on https://github.com/hanlu-nju/UniCA.

</details>


### [130] [GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling](https://arxiv.org/abs/2506.22049)
*Tianhao Chen,Xin Xu,Zijing Liu,Pengxiang Li,Xinyuan Song,Ajay Kumar Jaiswal,Fan Zhang,Jishan Hu,Yang Wang,Hao Chen,Shizhe Diao,Shiwei Liu,Yu Li,Yin Lu,Can Yang*

Key words: 

TL;DR: 论文提出了一种名为GPAS的简单技术，用于解决Pre-LN Transformer中激活方差指数增长的问题，通过缩放中间激活值但保持梯度不变，提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Pre-LN Transformer存在激活方差指数增长的问题，导致深层学习能力受限。

Method: 提出Gradient-Preserving Activation Scaling (GPAS)，缩放中间激活值但不改变梯度。

Result: 在71M到1B的不同规模模型上实验，GPAS均带来性能提升，并能应用于其他架构如Sandwich-LN和DeepNorm。

Conclusion: GPAS是一种通用的技术，能够改善Pre-LN及其他架构的训练动态和性能。

Abstract: Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,
predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While
being stable during pretraining and scalable to large model sizes, Pre-LN
suffers from an exponential growth in activation variance across layers,
causing the residual path to dominate over sub-layer outputs and limiting the
learning capacity of deeper layers. To mitigate this issue, we propose
Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be
used in combination with existing approaches. GPAS works by scaling down the
intermediate activations while keeping their gradients unchanged. This leaves
information in the activations intact, and avoids the gradient vanishing
problem associated with gradient downscaling. Extensive experiments across
various model sizes from 71M to 1B show that GPAS achieves consistent
performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows
promise in improving alternative architectures such as Sandwich-LN and
DeepNorm, demonstrating its versatility and potential for improving training
dynamics in a wide range of settings.

</details>


### [131] [crypto price prediction using lstm+xgboost](https://arxiv.org/abs/2506.22055)
*Mehul Gautam*

Key words: 加密货币, LSTM, XGBoost, 价格预测, 混合模型

TL;DR: 提出了一种结合LSTM和XGBoost的混合深度学习与机器学习模型，用于加密货币价格预测，表现优于独立模型和传统方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 加密货币市场的波动性和复杂性对价格预测提出了独特挑战。

Method: 整合LSTM和XGBoost，LSTM捕捉时间依赖性，XGBoost建模非线性关系。

Result: 在比特币、以太坊等加密货币数据集上表现优异，优于独立模型和传统方法。

Conclusion: 混合架构在金融预测中具有潜力，并适用于不同加密货币和市场环境。

Abstract: The volatility and complex dynamics of cryptocurrency markets present unique
challenges for accurate price forecasting. This research proposes a hybrid deep
learning and machine learning model that integrates Long Short-Term Memory
(LSTM) networks and Extreme Gradient Boosting (XGBoost) for cryptocurrency
price prediction. The LSTM component captures temporal dependencies in
historical price data, while XGBoost enhances prediction by modeling nonlinear
relationships with auxiliary features such as sentiment scores and
macroeconomic indicators. The model is evaluated on historical datasets of
Bitcoin, Ethereum, Dogecoin, and Litecoin, incorporating both global and
localized exchange data. Comparative analysis using Mean Absolute Percentage
Error (MAPE) and Min-Max Normalized Root Mean Square Error (MinMax RMSE)
demonstrates that the LSTM+XGBoost hybrid consistently outperforms standalone
models and traditional forecasting methods. This study underscores the
potential of hybrid architectures in financial forecasting and provides
insights into model adaptability across different cryptocurrencies and market
contexts.

</details>


### [132] [Transformers are Graph Neural Networks](https://arxiv.org/abs/2506.22084)
*Chaitanya K. Joshi*

Key words: Transformer, GNN, 自注意力, 位置编码

TL;DR: 论文揭示了Transformer架构与图神经网络（GNNs）之间的联系，提出Transformer可视为在完全连接的令牌图上运行的消息传递GNN。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨Transformer与GNN之间的联系，揭示自注意力机制和位置编码在表达集合处理能力中的作用。

Method: 将Transformer视为消息传递GNN，分析自注意力机制和位置编码的功能。

Result: Transformer是一种高效的集合处理网络，可在现代硬件上更高效地运行。

Conclusion: Transformer是当前在硬件效率上占优的GNN实现形式。

Abstract: We establish connections between the Transformer architecture, originally
introduced for natural language processing, and Graph Neural Networks (GNNs)
for representation learning on graphs. We show how Transformers can be viewed
as message passing GNNs operating on fully connected graphs of tokens, where
the self-attention mechanism capture the relative importance of all tokens
w.r.t. each-other, and positional encodings provide hints about sequential
ordering or structure. Thus, Transformers are expressive set processing
networks that learn relationships among input elements without being
constrained by apriori graphs. Despite this mathematical connection to GNNs,
Transformers are implemented via dense matrix operations that are significantly
more efficient on modern hardware than sparse message passing. This leads to
the perspective that Transformers are GNNs currently winning the hardware
lottery.

</details>


### [133] [Learning to Solve Multi-Objective Routing Problems on Multigraphs](https://arxiv.org/abs/2506.22095)
*Filip Rydin,Attila Lischka,Jiaming Wu,Morteza Haghir Chehreghani,Balázs Kulcsár*

Key words: 路径规划, 多重图, 多目标优化, 神经网络, TSP, CVRP

TL;DR: 该论文提出了两种基于神经网络的路径规划方法，分别直接在多重图上进行路径选择和对多重图进行剪枝后规划路径，两种方法在多种问题上表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前基于学习的路径规划方法大多忽略了多重图的场景，而多重图在实际应用中具有高度相关性，论文旨在填补这一研究空白。

Method: 两种方法：第一种直接在多重图上自回归选择边完成路径；第二种先剪枝多重图再规划路径。

Result: 两种方法在旅行商问题（TSP）和容量约束车辆路径问题（CVRP）等多种问题上均表现出色。

Conclusion: 提出的神经网络方法为多重图上的多目标路径规划提供了有效解决方案，并具有实际应用潜力。

Abstract: Learning-based methods for routing have gained significant attention in
recent years, both in single-objective and multi-objective contexts. However,
the multigraph setting, where multiple paths with distinct attributes can exist
between destinations, has largely been overlooked, despite its high practical
relevancy. In this paper, we introduce two neural approaches to address
multi-objective routing on multigraphs. Our first approach works directly on
the multigraph, by autoregressively selecting edges until a tour is completed.
On the other hand, our second model first prunes the multigraph into a simple
graph and then builds routes. We validate both models experimentally and find
that they demonstrate strong performance across a variety of problems,
including the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP).

</details>


### [134] [Transfer Learning for Assessing Heavy Metal Pollution in Seaports Sediments](https://arxiv.org/abs/2506.22096)
*Tin Lai,Farnaz Farid,Yueyang Kuan,Xintian Zhang*

Key words: 重金属污染, 污染负荷指数, 深度学习, 迁移学习, 水质评估

TL;DR: 提出了一种基于深度学习的模型，简化了重金属污染的评估过程，解决了传统方法中数据稀缺和标准不一的问题，并在澳大利亚六大海港的数据上验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统污染负荷指数（PLI）评估方法流程繁琐且数据稀缺，亟需一种更高效的解决方案。

Method: 利用迁移学习开发定量评估方法，预测PLI，并通过六大海港的数据验证模型性能。

Result: 模型表现优异，MAE和MAPE分别为0.5和0.03，性能优于基线模型两个数量级。

Conclusion: 该模型为水质预测提供了创新、便捷且经济的方法，适用于海洋生态保护和工业污染监测。

Abstract: Detecting heavy metal pollution in soils and seaports is vital for regional
environmental monitoring. The Pollution Load Index (PLI), an international
standard, is commonly used to assess heavy metal containment. However, the
conventional PLI assessment involves laborious procedures and data analysis of
sediment samples. To address this challenge, we propose a deep-learning-based
model that simplifies the heavy metal assessment process. Our model tackles the
issue of data scarcity in the water-sediment domain, which is traditionally
plagued by challenges in data collection and varying standards across nations.
By leveraging transfer learning, we develop an accurate quantitative assessment
method for predicting PLI. Our approach allows the transfer of learned features
across domains with different sets of features. We evaluate our model using
data from six major ports in New South Wales, Australia: Port Yamba, Port
Newcastle, Port Jackson, Port Botany, Port Kembla, and Port Eden. The results
demonstrate significantly lower Mean Absolute Error (MAE) and Mean Absolute
Percentage Error (MAPE) of approximately 0.5 and 0.03, respectively, compared
to other models. Our model performance is up to 2 orders of magnitude than
other baseline models. Our proposed model offers an innovative, accessible, and
cost-effective approach to predicting water quality, benefiting marine life
conservation, aquaculture, and industrial pollution monitoring.

</details>


### [135] [Earthquake Damage Grades Prediction using An Ensemble Approach Integrating Advanced Machine and Deep Learning Models](https://arxiv.org/abs/2506.22129)
*Anurag Panda,Gaurav Kumar Yadav*

Key words: 地震损伤预测,类别不平衡,SMOTE,机器学习,深度学习

TL;DR: 该研究利用SMOTE技术解决地震后建筑损伤分级预测中的类别不平衡问题，比较了多种机器学习和深度学习模型，并通过特征实验评估性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 地震后准确评估建筑损伤对救援和资源分配至关重要，但类别不平衡问题可能影响预测模型的准确性，本研究旨在解决这一问题。

Method: 结合SMOTE技术处理类别不平衡，采用多种多分类机器学习、深度学习及集成方法，并通过特征实验和混淆矩阵评估模型性能。

Result: 研究确定了影响地震脆弱性的关键因素，并展示了不同模型在损伤预测中的表现，为地震响应提供了有效工具。

Conclusion: SMOTE技术能有效改善类别不平衡问题，多模型比较为地震损伤预测提供了优化方案，有助于提升灾害响应效率。

Abstract: In the aftermath of major earthquakes, evaluating structural and
infrastructural damage is vital for coordinating post-disaster response
efforts. This includes assessing damage's extent and spatial distribution to
prioritize rescue operations and resource allocation. Accurately estimating
damage grades to buildings post-earthquake is paramount for effective response
and recovery, given the significant impact on lives and properties,
underscoring the urgency of streamlining relief fund allocation processes.
Previous studies have shown the effectiveness of multi-class classification,
especially XGBoost, along with other machine learning models and ensembling
methods, incorporating regularization to address class imbalance. One
consequence of class imbalance is that it may give rise to skewed models that
undervalue minority classes and give preference to the majority class. This
research deals with the problem of class imbalance with the help of the
synthetic minority oversampling technique (SMOTE). We delve into multiple
multi-class classification machine learning, deep learning models, and
ensembling methods to forecast structural damage grades. The study elucidates
performance determinants through comprehensive feature manipulation experiments
and diverse training approaches. It identifies key factors contributing to
seismic vulnerability while evaluating model performance using techniques like
the confusion matrix further to enhance understanding of the effectiveness of
earthquake damage prediction.

</details>


### [136] [Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems](https://arxiv.org/abs/2506.22186)
*Kaikai Zheng,Dawei Shi,Yang Shi,Long Wang*

Key words: Thompson采样, 再生核希尔伯特空间, 主动学习, 控制设计, 收敛性

TL;DR: 提出了一种基于再生核希尔伯特空间的控制律学习方法，改进了传统的Thompson采样，使其适用于更广泛的函数空间，并提供了收敛性保证。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Thompson采样在控制设计中虽然有效，但依赖于有限参数表示，限制了其在更一般空间中的应用。

Method: 利用再生核希尔伯特空间对控制律进行参数化，设计了一种数据驱动的主动学习控制方法，避免了系统结构和控制器形式的限制。

Result: 理论分析表明，该方法以指数速度学习控制律与闭环性能指标的关系，并推导了控制遗憾的上界。数值实验验证了其有效性。

Conclusion: 提出的方法成功扩展了Thompson采样的应用范围，适用于更一般的控制设计问题。

Abstract: Thompson sampling (TS) is an effective method to explore parametric
uncertainties and can therefore be used for active learning-based controller
design. However, TS relies on finite parametric representations, which limits
its applicability to more general spaces, which are more commonly encountered
in control system design. To address this issue, this work pro poses a
parameterization method for control law learning using reproducing kernel
Hilbert spaces and designs a data-driven active learning control approach.
Specifically, the proposed method treats the control law as an element in a
function space, allowing the design of control laws without imposing
restrictions on the system structure or the form of the controller. A TS
framework is proposed in this work to explore potential optimal control laws,
and the convergence guarantees are further provided for the learning process.
Theoretical analysis shows that the proposed method learns the relationship
between control laws and closed-loop performance metrics at an exponential
rate, and the upper bound of control regret is also derived. Numerical
experiments on controlling unknown nonlinear systems validate the effectiveness
of the proposed method.

</details>


### [137] [Exploring Modularity of Agentic Systems for Drug Discovery](https://arxiv.org/abs/2506.22189)
*Laura van Weesep,Samuel Genheden,Ola Engkvist,Jens Sjölund*

Key words: LLM, 药物发现, 模块化, 代理系统, 性能比较

TL;DR: 研究了大型语言模型（LLMs）和代理系统在药物发现中的模块化性能，发现模型和工具调用方式的性能受问题和模型影响，强调了进一步研究的必要性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨LLM在药物发现中的模块化性能，填补该领域的研究空白。

Method: 比较不同LLM的性能及工具调用代理与代码生成代理的有效性，采用案例研究评估模型表现。

Result: Claude-3.5-Sonnet等模型表现最佳，代码生成代理总体优于工具调用代理，但结果因问题和模型而异。

Conclusion: LLM替换需考虑提示重新设计，强调了代理系统模块化研究的必要性。

Abstract: Large-language models (LLMs) and agentic systems present exciting
opportunities to accelerate drug discovery and design. In this study, we
critically examine the modularity of LLM-based agentic systems for drug
discovery, i.e., whether parts of the agentic system such as the LLM are
interchangeable, a topic that has received limited attention in drug discovery
applications. We compare the performance of different large language models
(LLMs) and the effectiveness of tool-calling agents versus code-generating
agents in this domain. Our case study, comparing performance in orchestrating
tools for chemistry and drug discovery using an LLM-as-a-judge score, shows
that Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative
language models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and
Nova-Micro. Although we confirm that code-generating agents outperform the
tool-calling ones on average, we show that this is highly question and model
dependent. Furthermore, the impact of replacing system prompts is dependent on
the specific question asked and the model used, underscoring that -- even in
this particular domain -- one cannot just replace language models without
considering prompt re-engineering. Our study highlights the necessity of
further research into the modularity of agentic systems to enable the
development of stable and scalable solutions for real-world problems.

</details>


### [138] [dreaMLearning: Data Compression Assisted Machine Learning](https://arxiv.org/abs/2506.22190)
*Xiaobo Zhao,Aaron Hurst,Panagiotis Karras,Daniel E. Lucani*

Key words: 深度学习, 压缩学习, EntroGeDe, 资源效率

TL;DR: dreaMLearning框架通过EntroGeDe压缩方法直接从压缩数据学习，显著降低资源需求，同时保持性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决深度学习对大量标记数据和计算资源的需求问题。

Method: 基于Entropy-based Generalized Deduplication (EntroGeDe)的无损压缩方法，直接从压缩数据学习。

Result: 训练速度提高8.8倍，内存使用减少10倍，存储节省42%，性能影响最小。

Conclusion: dreaMLearning为资源受限场景下的高效学习提供了新可能。

Abstract: Despite rapid advancements, machine learning, particularly deep learning, is
hindered by the need for large amounts of labeled data to learn meaningful
patterns without overfitting and immense demands for computation and storage,
which motivate research into architectures that can achieve good performance
with fewer resources. This paper introduces dreaMLearning, a novel framework
that enables learning from compressed data without decompression, built upon
Entropy-based Generalized Deduplication (EntroGeDe), an entropy-driven lossless
compression method that consolidates information into a compact set of
representative samples. DreaMLearning accommodates a wide range of data types,
tasks, and model architectures. Extensive experiments on regression and
classification tasks with tabular and image data demonstrate that dreaMLearning
accelerates training by up to 8.8x, reduces memory usage by 10x, and cuts
storage by 42%, with a minimal impact on model performance. These advancements
enhance diverse ML applications, including distributed and federated learning,
and tinyML on resource-constrained edge devices, unlocking new possibilities
for efficient and scalable learning.

</details>


### [139] [REDELEX: A Framework for Relational Deep Learning Exploration](https://arxiv.org/abs/2506.22199)
*Jakub Peleška,Gustav Šír*

Key words: 关系数据库, 关系深度学习, REDELEX框架, 性能评估

TL;DR: 本文介绍了一个名为REDELEX的框架，用于评估不同复杂度的关系深度学习（RDL）模型在70多个关系数据库（RDBs）上的表现，并分析了影响性能的关键因素。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 关系数据库（RDBs）是结构化信息存储的黄金标准，但缺乏对RDL模型性能与RDB特性之间关系的分析。本文旨在填补这一空白。

Method: 提出了REDELEX框架，评估了多种复杂度的RDL模型在70多个RDBs上的表现，并与经典方法进行了对比。

Result: RDL整体表现优于经典方法，且性能受到模型复杂度、数据库大小及其结构特性的显著影响。

Conclusion: RDL在预测任务中具有显著优势，REDELEX框架为相关研究提供了重要工具和数据支持。

Abstract: Relational databases (RDBs) are widely regarded as the gold standard for
storing structured information. Consequently, predictive tasks leveraging this
data format hold significant application promise. Recently, Relational Deep
Learning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized
as graph structures, enabling the application of various graph neural
architectures to effectively address these tasks. However, given its novelty,
there is a lack of analysis into the relationships between the performance of
various RDL models and the characteristics of the underlying RDBs.
  In this study, we present REDELEX$-$a comprehensive exploration framework for
evaluating RDL models of varying complexity on the most diverse collection of
over 70 RDBs, which we make available to the community. Benchmarked alongside
key representatives of classic methods, we confirm the generally superior
performance of RDL while providing insights into the main factors shaping
performance, including model complexity, database sizes and their structural
properties.

</details>


### [140] [EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework](https://arxiv.org/abs/2506.22200)
*Chen Wang,Lai Wei,Yanzhi Zhang,Chenyang Shao,Zedong Dan,Weiran Huang,Yue Wang,Yuzhi Zhang*

Key words: 强化学习, 大语言模型, GRPO, EFRame, 推理能力

TL;DR: EFRame是一个探索-过滤-回放框架，用于增强GRPO方法，通过额外探索、在线过滤和经验回放提升强化学习的性能和稳定性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: GRPO虽然降低了计算成本，但在探索、样本效率和稳定性方面仍有不足，限制了其在复杂推理任务中的表现。

Method: EFRame结合探索高质轨迹、在线过滤低质样本和经验回放，形成完整学习循环。

Result: 实验表明EFRame提升了训练稳健性和效率，解锁了更深度推理能力，并支持样本分类分析。

Conclusion: EFRame通过系统性优化GRPO，显著提升强化学习模型在复杂任务中的表现。

Abstract: Recent advances in reinforcement learning (RL) have significantly enhanced
the reasoning capabilities of large language models (LLMs). Group Relative
Policy Optimization (GRPO), an efficient variant of PPO that lowers RL's
computational cost, still faces limited exploration, low sample efficiency and
instability, constraining its performance on complex reasoning tasks. To
address these limitations, we introduce EFRame, an Exploration-Filtering-Replay
framework that systematically augments GRPO along three critical dimensions.
EFRame performs additional rollouts to explore high-quality trajectories,
applies online filtering to eliminate low-quality samples that introduce noise
and variance, and leverages experience replay to repeatedly exploit rare but
informative samples. EFRame establishes a complete and stable learning cycle,
guiding the model through a structured transition from exploration to
convergence. Our experiments across a variety of reasoning benchmarks
demonstrate that EFRame not only improves the robustness and efficiency of
training, but also enables access to deeper reasoning capabilities that remain
unattainable under vanilla GRPO. Furthermore, EFRame enables a more
fine-grained categorization of training samples, allowing for a deeper analysis
of how different types of samples contribute to the learning process in RL. Our
code is available at https://github.com/597358816/EFRame.

</details>


### [141] [Risk-Averse Best Arm Set Identification with Fixed Budget and Fixed Confidence](https://arxiv.org/abs/2506.22253)
*Shunta Nonaga,Koji Tabata,Yuta Mizuno,Tamiki Komatsuzaki*

Key words: 随机赌博优化, 均值-方差准则, 帕累托最优, 固定置信度, 固定预算

TL;DR: 论文提出了一种新的随机赌博优化问题设置，联合解决最大化期望奖励和最小化风险（通过均值-方差准则量化）的双重目标，并通过统一的元算法框架在固定置信度和固定预算两种模式下高效识别帕累托最优解。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究在不确定环境中决策时如何平衡最大化期望奖励和最小化风险的问题，弥补传统赌博方法仅关注期望收益的不足。

Method: 提出一个统一的元算法框架，支持固定置信度和固定预算两种模式，通过自适应的置信区间设计和相同的样本探索策略实现。

Result: 理论证明了解决方案的正确性，并通过合成基准的实验验证了方法在准确性和样本效率上的优越性。

Conclusion: 该方法在风险感知的决策任务中展现出广泛适用性，能够高效平衡期望绩效与风险。

Abstract: Decision making under uncertain environments in the maximization of expected
reward while minimizing its risk is one of the ubiquitous problems in many
subjects. Here, we introduce a novel problem setting in stochastic bandit
optimization that jointly addresses two critical aspects of decision-making:
maximizing expected reward and minimizing associated uncertainty, quantified
via the mean-variance(MV) criterion. Unlike traditional bandit formulations
that focus solely on expected returns, our objective is to efficiently and
accurately identify the Pareto-optimal set of arms that strikes the best
trade-off between expected performance and risk. We propose a unified
meta-algorithmic framework capable of operating under both fixed-confidence and
fixed-budget regimes, achieved through adaptive design of confidence intervals
tailored to each scenario using the same sample exploration strategy. We
provide theoretical guarantees on the correctness of the returned solutions in
both settings. To complement this theoretical analysis, we conduct extensive
empirical evaluations across synthetic benchmarks, demonstrating that our
approach outperforms existing methods in terms of both accuracy and sample
efficiency, highlighting its broad applicability to risk-aware decision-making
tasks in uncertain environments.

</details>


### [142] [Projected Compression: Trainable Projection for Efficient Transformer Compression](https://arxiv.org/abs/2506.22255)
*Maciej Stefaniak,Michał Krutul,Jan Małaśnicki,Maciej Pióro,Jakub Krajewski,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jan Ludziejewski*

Key words: 模型压缩, 投影模块, Transformer模型, 计算效率

TL;DR: 提出了一种名为“投影压缩”的新型模型压缩技术，通过投影模块减少模型权重，同时保持原始模型参数的可访问性，最终实现模型尺寸的减小且不影响计算效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着大语言模型规模的增加，推理时间和计算需求也随之增长，因此需要一种有效的模型压缩方法。

Method: 训练额外的可训练投影权重，并将这些投影合并为一个低维乘积矩阵，从而生成一个缩小尺寸的Transformer模型。

Result: 实验结果表明，与硬剪枝和再训练方法相比，投影压缩在高质量模型上表现更好，且性能优势随标记数量增加而放大。

Conclusion: 投影压缩是一种高效的模型压缩方法，能够在保持计算效率的同时显著减小模型规模。

Abstract: Large language models have steadily increased in size to achieve improved
performance; however, this growth has also led to greater inference time and
computational demands. Consequently, there is rising interest in model size
reduction methods. To address this issue, we propose Projected Compression, a
novel model compression technique, that reduces model weights by utilizing
projection modules. Specifically, we first train additional trainable
projections weights and preserve access to all the original model parameters.
Subsequently, these projections are merged into a lower-dimensional product
matrix, resulting in a reduced-size standard Transformer-based model. Unlike
alternative approaches that require additional computational overhead, our
method matches the base model's per-token computation step in FLOPs.
Experimental results show that Projected Compression outperforms the comparable
hard pruning and retraining approach on higher quality models. Moreover, the
performance margin scales well with the number of tokens.

</details>


### [143] [Score-Based Model for Low-Rank Tensor Recovery](https://arxiv.org/abs/2506.22295)
*Zhengyun Cheng,Changhao Wang,Guanwen Zhang,Yi Xu,Wei Zhou,Xiangyang Ji*

Key words: 低秩张量分解, 分数匹配, 神经网络, 张量完成, 去噪

TL;DR: 论文提出了一种基于分数匹配的模型，用于低秩张量分解，无需预定义结构或分布假设，通过神经网络学习能量函数，显著提升了稀疏张量和视觉数据等各类张量的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统张量分解方法依赖预定义的结构假设（如CP或Tucker分解），但在实际应用中，这些假设往往难以满足，导致优化过程复杂和精度损失。

Method: 设计了一种基于分数匹配的神经网络模型，学习能量函数以捕获张量条目与共享因子的联合对数概率梯度，并结合块坐标下降（BCD）算法和平滑正则化实现张量完成与去噪。

Result: 实验结果表明，该方法在稀疏张量、连续时间张量和视觉数据上均表现出显著的性能提升。

Conclusion: 提出的方法突破了传统方法的局限性，能够灵活建模结构和分布，适用于多种张量类型的数据分析任务。

Abstract: Low-rank tensor decompositions (TDs) provide an effective framework for
multiway data analysis. Traditional TD methods rely on predefined structural
assumptions, such as CP or Tucker decompositions. From a probabilistic
perspective, these can be viewed as using Dirac delta distributions to model
the relationships between shared factors and the low-rank tensor. However, such
prior knowledge is rarely available in practical scenarios, particularly
regarding the optimal rank structure and contraction rules. The optimization
procedures based on fixed contraction rules are complex, and approximations
made during these processes often lead to accuracy loss. To address this issue,
we propose a score-based model that eliminates the need for predefined
structural or distributional assumptions, enabling the learning of
compatibility between tensors and shared factors. Specifically, a neural
network is designed to learn the energy function, which is optimized via score
matching to capture the gradient of the joint log-probability of tensor entries
and shared factors. Our method allows for modeling structures and distributions
beyond the Dirac delta assumption. Moreover, integrating the block coordinate
descent (BCD) algorithm with the proposed smooth regularization enables the
model to perform both tensor completion and denoising. Experimental results
demonstrate significant performance improvements across various tensor types,
including sparse and continuous-time tensors, as well as visual data.

</details>


### [144] [CoATA: Effective Co-Augmentation of Topology and Attribute for Graph Neural Networks](https://arxiv.org/abs/2506.22299)
*Tao Liu,Longlong Lin,Yunfeng Yu,Xi Ou,Youan Zhang,Zhiqiu Ye,Tao Jia*

Key words: 图神经网络, 拓扑增强, 属性增强, 对比学习, 协同优化

TL;DR: CoATA提出了一种双通道GNN框架，通过协同增强拓扑和属性来解决现有GNN在面对噪声和不完整图数据时的局限性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法在处理噪声和不完整图数据时，通常只关注单一维度的增强（如拓扑结构或节点属性），忽略了二者之间的深层交互。

Method: CoATA通过双通道设计，先传播结构信号来丰富和去噪节点属性，再将其投影到节点-属性二分图中进行结构优化，并引入对比学习进行相互校正。

Result: 在七个基准数据集上的实验表明，CoATA优于十一种最先进的基线方法。

Conclusion: CoATA能有效捕捉拓扑与属性之间的协同关系，显著提升GNN在噪声和不完整图数据上的性能。

Abstract: Graph Neural Networks (GNNs) have garnered substantial attention due to their
remarkable capability in learning graph representations. However, real-world
graphs often exhibit substantial noise and incompleteness, which severely
degrades the performance of GNNs. Existing methods typically address this issue
through single-dimensional augmentation, focusing either on refining topology
structures or perturbing node attributes, thereby overlooking the deeper
interplays between the two. To bridge this gap, this paper presents CoATA, a
dual-channel GNN framework specifically designed for the Co-Augmentation of
Topology and Attribute. Specifically, CoATA first propagates structural signals
to enrich and denoise node attributes. Then, it projects the enhanced attribute
space into a node-attribute bipartite graph for further refinement or
reconstruction of the underlying structure. Subsequently, CoATA introduces
contrastive learning, leveraging prototype alignment and consistency
constraints, to facilitate mutual corrections between the augmented and
original graphs. Finally, extensive experiments on seven benchmark datasets
demonstrate that the proposed CoATA outperforms eleven state-of-the-art
baseline methods, showcasing its effectiveness in capturing the synergistic
relationship between topology and attributes.

</details>


### [145] [Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling](https://arxiv.org/abs/2506.22301)
*Takumi Okuo,Shinnosuke Matsuo,Shota Harada,Kiyohito Tanaka,Ryoma Bise*

Key words: 领域自适应, 弱监督学习, 伪标签, 医疗影像, 类别比例

TL;DR: 本文提出一种弱监督领域自适应方法，利用目标域的类别比例信息，通过比例约束伪标签提升模型性能，无需额外标注。实验表明其在两个内窥镜数据集上优于半监督方法，并对噪声比例标签具有鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 医疗领域中，领域偏移问题由于数据分布差异导致模型性能下降，尤其当源域和目标域的类别比例不同时，现有方法表现不佳。

Method: 提出比例约束伪标签方法，利用目标域的类别比例信息为无标注数据分配伪标签，无需额外标注。

Result: 在两个内窥镜数据集上，方法优于半监督领域自适应技术，即使仅5%目标域数据有标注。噪声比例标签下仍表现稳健。

Conclusion: 该方法在医疗领域中对领域偏移问题有效，尤其在类别比例差异较大时表现优越，且适用于实际场景。

Abstract: Domain shift is a significant challenge in machine learning, particularly in
medical applications where data distributions differ across institutions due to
variations in data collection practices, equipment, and procedures. This can
degrade performance when models trained on source domain data are applied to
the target domain. Domain adaptation methods have been widely studied to
address this issue, but most struggle when class proportions between the source
and target domains differ. In this paper, we propose a weakly-supervised domain
adaptation method that leverages class proportion information from the target
domain, which is often accessible in medical datasets through prior knowledge
or statistical reports. Our method assigns pseudo-labels to the unlabeled
target data based on class proportion (called proportion-constrained
pseudo-labeling), improving performance without the need for additional
annotations. Experiments on two endoscopic datasets demonstrate that our method
outperforms semi-supervised domain adaptation techniques, even when 5% of the
target domain is labeled. Additionally, the experimental results with noisy
proportion labels highlight the robustness of our method, further demonstrating
its effectiveness in real-world application scenarios.

</details>


### [146] [Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling](https://arxiv.org/abs/2506.22304)
*Erkan Turan,Aristotelis Siozopoulos,Maks Ovsjanikov*

Key words: 条件流匹配,Koopman算子,线性嵌入,生成模型,可解释性

TL;DR: 提出基于Koopman算子理论的Koopman-CFM架构，通过线性化生成动态加速条件流匹配（CFM）并提高其解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有CFM方法采样依赖非线性ODE求解，计算成本高且解释性差。

Method: 引入解码器自由的Koopman-CFM架构，通过学习线性嵌入空间实现闭式一步采样。

Result: 在2D数据集及MNIST等基准上显著加速采样，并提供光谱分析工具以解释生成行为。

Conclusion: Koopman-CFM结合高效采样与结构分析，推动快速可解释生成模型发展。

Abstract: Conditional Flow Matching (CFM) offers a simulation-free framework for
training continuous-time generative models, bridging diffusion and flow-based
approaches. However, sampling from CFM still relies on numerically solving
non-linear ODEs which can be computationally expensive and difficult to
interpret. Recent alternatives address sampling speed via trajectory
straightening, mini-batch coupling or distillation. However, these methods
typically do not shed light on the underlying \textit{structure} of the
generative process. In this work, we propose to accelerate CFM and introduce an
interpretable representation of its dynamics by integrating Koopman operator
theory, which models non-linear flows as linear evolution in a learned space of
observables. We introduce a decoder-free Koopman-CFM architecture that learns
an embedding where the generative dynamics become linear, enabling closed-form,
one-step sampling via matrix exponentiation. This results in significant
speedups over traditional CFM as demonstrated on controlled 2D datasets and
real-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face
Dataset (TFD). Unlike previous methods, our approach leads to a well-structured
Koopman generator, whose spectral properties, eigenvalues, and eigenfunctions
offer principled tools for analyzing generative behavior such as temporal
scaling, mode stability, and decomposition in Koopman latent space. By
combining sampling efficiency with analytical structure, Koopman-enhanced flow
matching offers a potential step toward fast and interpretable generative
modeling.

</details>


### [147] [Less Greedy Equivalence Search](https://arxiv.org/abs/2506.22331)
*Adiba Ejaz,Elias Bareinboim*

Key words: 因果发现, GES, LGES, 观测数据, 干预数据

TL;DR: LGES是GES的改进版，通过减少贪婪步骤中的冗余操作，提高了计算速度和准确性，同时支持先验假设和数据修正。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决GES在计算成本和有限样本准确性上的不足。

Method: LGES在贪婪步骤中避免高得分但不相关的边插入，利用先验假设和干预数据优化搜索。

Result: 实验表明，LGES在速度、准确性和对错误假设的鲁棒性上优于GES和其他基线方法。

Conclusion: LGES在理论和实践中均表现优异，适用于因果发现任务。

Abstract: Greedy Equivalence Search (GES) is a classic score-based algorithm for causal
discovery from observational data. In the sample limit, it recovers the Markov
equivalence class of graphs that describe the data. Still, it faces two
challenges in practice: computational cost and finite-sample accuracy. In this
paper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that
retains its theoretical guarantees while partially addressing these
limitations. LGES modifies the greedy step: rather than always applying the
highest-scoring insertion, it avoids edge insertions between variables for
which the score implies some conditional independence. This more targeted
search yields up to a \(10\)-fold speed-up and a substantial reduction in
structural error relative to GES. Moreover, LGES can guide the search using
prior assumptions, while correcting these assumptions when contradicted by the
data. Finally, LGES can exploit interventional data to refine the learned
observational equivalence class. We prove that LGES recovers the true
equivalence class in the sample limit from observational and interventional
data, even with misspecified prior assumptions. Experiments demonstrate that
LGES outperforms GES and other baselines in speed, accuracy, and robustness to
misspecified assumptions. Our code is available at
https://github.com/CausalAILab/lges.

</details>


### [148] [A Framework for Multi-source Privacy Preserving Epidemic Analysis](https://arxiv.org/abs/2506.22342)
*Zihan Guan,Zhiyuan Zhao,Fengwei Tian,Dung Nguyen,Payel Bhattacharjee,Ravi Tandon,B. Aditya Prakash,Anil Vullikanti*

Key words: 流行病学，差分隐私，深度学习，流行病模型，预测

TL;DR: 论文提出了一种结合深度学习和流行病模型的框架，用于同时进行流行病预测和学习传播机制，同时整合多种数据集（包括具有差分隐私保护的数据）。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 流行病学和公共卫生分析需要多样化数据集，但这些数据通常敏感，需要隐私保护。差分隐私（DP）因其强保障而成为标准。

Method: 开发了一个框架，整合深度学习和流行病模型，利用多种数据集（包括DP保护数据）进行预测和模型学习。

Result: 在合成财务数据集（带DP保护）上验证了框架的有效性，证明其能显著提升预测和模型学习的价值。

Conclusion: 即使是带DP保护的数据集，也能为流行病分析提供重要价值。

Abstract: It is now well understood that diverse datasets provide a lot of value in key
epidemiology and public health analyses, such as forecasting and nowcasting,
development of epidemic models, evaluation and design of interventions and
resource allocation. Some of these datasets are often sensitive, and need
adequate privacy protections. There are many models of privacy, but
Differential Privacy (DP) has become a de facto standard because of its strong
guarantees, without making models about adversaries. In this paper, we develop
a framework the integrates deep learning and epidemic models to simultaneously
perform epidemic forecasting and learning a mechanistic model of epidemic
spread, while incorporating multiple datasets for these analyses, including
some with DP guarantees. We demonstrate our framework using a realistic but
synthetic financial dataset with DP; such a dataset has not been used in such
epidemic analyses. We show that this dataset provides significant value in
forecasting and learning an epidemic model, even when used with DP guarantees.

</details>


### [149] [Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation](https://arxiv.org/abs/2506.22365)
*Tao Li,Haozhe Lei,Mingsheng Yin,Yaqi Hu*

Key words: 强化学习, 物理先验, 神经符号集成, 域特定语言, PiPRL

TL;DR: 该论文提出了一种符号化方法，将物理先验知识融入强化学习代理中，通过分层模块化的神经符号集成框架（PiPRL）显著提升训练效率和泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前在强化学习中引入物理先验知识需要大量人工和领域专业知识，限制了其广泛应用。本文旨在通过符号化方法简化这一过程。

Method: 提出了PiPRL框架，结合符号程序与神经感知模块，通过层次化和模块化的神经符号集成，将物理先验知识编码为可解释的域特定语言（DSL）。

Result: PiPRL在室内导航任务中表现优于纯符号或神经网络策略，训练时间缩短超过26%。

Conclusion: 符号化方法能有效融入物理学先验知识，提升强化学习的样本效率和泛化能力。

Abstract: When using reinforcement learning (RL) to tackle physical control tasks,
inductive biases that encode physics priors can help improve sample efficiency
during training and enhance generalization in testing. However, the current
practice of incorporating these helpful physics-informed inductive biases
inevitably runs into significant manual labor and domain expertise, making them
prohibitive for general users. This work explores a symbolic approach to
distill physics-informed inductive biases into RL agents, where the physics
priors are expressed in a domain-specific language (DSL) that is human-readable
and naturally explainable. Yet, the DSL priors do not translate directly into
an implementable policy due to partial and noisy observations and additional
physical constraints in navigation tasks. To address this gap, we develop a
physics-informed program-guided RL (PiPRL) framework with applications to
indoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic
integration, where a meta symbolic program receives semantically meaningful
features from a neural perception module, which form the bases for symbolic
programming that encodes physics priors and guides the RL process of a
low-level neural controller. Extensive experiments demonstrate that PiPRL
consistently outperforms purely symbolic or neural policies and reduces
training time by over 26% with the help of the program-based inductive biases.

</details>


### [150] [Sheaf-Based Decentralized Multimodal Learning for Next-Generation Wireless Communication Systems](https://arxiv.org/abs/2506.22374)
*Abdulmomen Ghalkha,Zhuojun Tian,Chaouki Ben Issaid,Mehdi Bennis*

Key words: 多模态学习, 联邦学习, 层理论, 注意力机制, 无线通信

TL;DR: 提出Sheaf-DMFL框架，解决传统联邦学习在多模态数据中的局限性，利用层理论提升设备协作。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统联邦学习在处理多模态数据和异构设备能力时表现不足，需更智能的协作方法。

Method: 采用层理论构建多模态协作框架，增强设备间任务层的相关性，并引入注意力机制优化学习。

Result: 在真实无线通信场景中验证了算法的优越性，显著提升了决策准确性。

Conclusion: Sheaf-DMFL在多模态异构系统中表现优异，为智能协作提供了新思路。

Abstract: In large-scale communication systems, increasingly complex scenarios require
more intelligent collaboration among edge devices collecting various multimodal
sensory data to achieve a more comprehensive understanding of the environment
and improve decision-making accuracy. However, conventional federated learning
(FL) algorithms typically consider unimodal datasets, require identical model
architectures, and fail to leverage the rich information embedded in multimodal
data, limiting their applicability to real-world scenarios with diverse
modalities and varying client capabilities. To address this issue, we propose
Sheaf-DMFL, a novel decentralized multimodal learning framework leveraging
sheaf theory to enhance collaboration among devices with diverse modalities.
Specifically, each client has a set of local feature encoders for its different
modalities, whose outputs are concatenated before passing through a
task-specific layer. While encoders for the same modality are trained
collaboratively across clients, we capture the intrinsic correlations among
clients' task-specific layers using a sheaf-based structure. To further enhance
learning capability, we propose an enhanced algorithm named Sheaf-DMFL-Att,
which tailors the attention mechanism within each client to capture
correlations among different modalities. A rigorous convergence analysis of
Sheaf-DMFL-Att is provided, establishing its theoretical guarantees. Extensive
simulations are conducted on real-world link blockage prediction and mmWave
beamforming scenarios, demonstrate the superiority of the proposed algorithms
in such heterogeneous wireless communication systems.

</details>


### [151] [Probabilistic Optimality for Inference-time Scaling](https://arxiv.org/abs/2506.22376)
*Youkang Wang,Jian Wang,Rubing Chen,Xiao-Yong Wei,Qing Li*

Key words: 推理时扩展, 大型语言模型, 概率框架, 采样优化, 数学推理

TL;DR: 本文提出了一种概率框架，为推理时扩展（inference-time scaling）的优化提供了理论基础，并开发了一个名为OptScale的算法，动态确定最少的采样响应数量，显著降低了采样开销。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对现有推理时扩展技术依赖启发式策略且缺乏理论基础的问题，本文旨在通过概率建模为优化推理时扩展提供理论支持。

Method: 提出一个概率框架，假设并行采样是独立同分布的，通过估计概率分布推导理论下限，开发动态确定最优采样数量的OptScale算法。

Result: 在多个数学推理基准测试中，OptScale显著减少了采样开销，同时保持或优于现有技术的推理性能。

Conclusion: 本文为推理时扩展提供了理论和实践解决方案，填补了高效部署LLM用于复杂推理的空白。

Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the
reasoning performance of Large Language Models (LLMs). However, existing
approaches often rely on heuristic strategies for parallel sampling, lacking a
principled foundation. To address this gap, we propose a probabilistic
framework that formalizes the optimality of inference-time scaling under the
assumption that parallel samples are independently and identically distributed
(i.i.d.), and where the Best-of-N selection strategy follows a probability
distribution that can be estimated. Within this framework, we derive a
theoretical lower bound on the required number of samples to achieve a target
performance level, providing the first principled guidance for
compute-efficient scaling. Leveraging this insight, we develop
\textsc{OptScale}, a practical algorithm that dynamically determines the
optimal number of sampled responses. \textsc{OptScale} employs a language
model-based predictor to estimate probabilistic prior parameters, enabling the
decision of the minimal number of samples needed that satisfy predefined
performance thresholds and confidence levels. Extensive experiments on
mathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)
demonstrate that \textsc{OptScale} significantly reduces sampling overhead
while remaining better or on par with state-of-the-art reasoning performance.
Our work offers both a theoretical foundation and a practical solution for
principled inference-time scaling, addressing a critical gap in the efficient
deployment of LLMs for complex reasoning.

</details>


### [152] [Towards Distributed Neural Architectures](https://arxiv.org/abs/2506.22389)
*Aditya Cowsik,Tianyu He,Andrey Gromov*

Key words: 分布式神经架构, 稀疏方法, 模块化路由, 计算效率, 动态分配

TL;DR: 介绍了分布式神经架构（DNA），这是一种自然泛化的稀疏方法，能够通过学习计算和通信模式来提高效率，并在视觉和语言领域表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索一种分布式神经架构，通过动态路由和模块组合，提高模型的效率和性能，同时允许计算和通信模式的学习。

Method: 采用包含多种模块（如Transformer、MLP、注意力等）和路由器的原型架构，令牌可以按任意顺序通过任何模块。通过端到端训练学习计算和通信模式。

Result: DNA在视觉和语言领域与密集基线模型表现相当，并能从数据中学习计算效率和参数共享；路径分布符合幂律，部分路径展现出专业化。

Conclusion: DNA能够通过学习动态分配计算和参数，表现出高效性和可解释性，是一种有潜力的通用架构。

Abstract: We introduce and train distributed neural architectures (DNA) in vision and
language domains. DNAs are initialized with a proto-architecture that consists
of (transformer, MLP, attention, etc.) modules and routers. Any token (or
patch) can traverse any series of modules in any order. DNAs are a natural
generalization of the sparse methods such as Mixture-of-Experts,
Mixture-of-Depths, parameter sharing, etc. Computation and communication
patterns of DNA modules are learnt end-to-end during training and depend on the
content and context of each token (or patch). These patterns can be shaped by
further requirements added to the optimization objective such as compute/memory
efficiency or load balancing. We empirically show that (i) trained DNAs are
competitive with the dense baselines in both domains and (ii) compute
efficiency/parameter sharing can be learnt from data. Next, we analyze the
emergent connectivity and computation patterns in the trained DNAs. We find
that the paths that tokens take through the models are themselves distributed
according to a power-law. We show that some paths (or, equivalently, groups of
modules) show emergent specialization. Finally, we demonstrate that models
learn to allocate compute and active parameters in an interpretable way.

</details>


### [153] [Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis](https://arxiv.org/abs/2506.22393)
*YongKyung Oh,Alex Bui*

Key words: 机器学习, 时间序列, 多视角对比学习, 医疗数据, 跨域适应

TL;DR: 提出了一种新型多视角对比学习框架，整合时域、导数动态和频域特征，显著提升医疗时间序列数据的跨域适应能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 医疗时间序列数据的复杂时间依赖性和动态分布偏移使得跨域适应具有挑战性，现有方法难以充分捕捉时间动态。

Method: 采用多视角对比学习和分层融合机制，通过独立编码器学习特征不变表示。

Result: 在EEG、ECG和EMG等数据集上显著优于现有方法。

Conclusion: 该框架提升了模型的鲁棒性和泛化能力，为医疗AI系统提供了实用路径。

Abstract: Adapting machine learning models to medical time series across different
domains remains a challenge due to complex temporal dependencies and dynamic
distribution shifts. Current approaches often focus on isolated feature
representations, limiting their ability to fully capture the intricate temporal
dynamics necessary for robust domain adaptation. In this work, we propose a
novel framework leveraging multi-view contrastive learning to integrate
temporal patterns, derivative-based dynamics, and frequency-domain features.
Our method employs independent encoders and a hierarchical fusion mechanism to
learn feature-invariant representations that are transferable across domains
while preserving temporal coherence. Extensive experiments on diverse medical
datasets, including electroencephalogram (EEG), electrocardiogram (ECG), and
electromyography (EMG) demonstrate that our approach significantly outperforms
state-of-the-art methods in transfer learning tasks. By advancing the
robustness and generalizability of machine learning models, our framework
offers a practical pathway for deploying reliable AI systems in diverse
healthcare settings.

</details>


### [154] [Exploration from a Primal-Dual Lens: Value-Incentivized Actor-Critic Methods for Sample-Efficient Online RL](https://arxiv.org/abs/2506.22401)
*Tong Yang,Bo Dai,Lin Xiao,Yuejie Chi*

Key words: 强化学习, 探索与利用, 乐观正则化, 原始-对偶优化, 行动者-批评者方法

TL;DR: 该论文提出了一种基于原始-对偶优化的值激励行动者-批评者方法（VAC），用于解决在线强化学习中探索与利用的平衡问题，并提供了理论性能保证。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索与利用之间的平衡是在线强化学习中的长期挑战，缺乏高效且具有理论保证的实用方案。

Method: 通过乐观正则化的最新发展，提出VAC方法，优化一个易于优化的目标，整合探索与利用。

Result: VAC方法在有限和无限视野的线性MDP中具有近最优的遗憾保证，并可扩展到一般函数逼近设置。

Conclusion: VAC方法为探索与利用的平衡提供了一种高效的解决方案，具有理论支持。

Abstract: Online reinforcement learning (RL) with complex function approximations such
as transformers and deep neural networks plays a significant role in the modern
practice of artificial intelligence. Despite its popularity and importance,
balancing the fundamental trade-off between exploration and exploitation
remains a long-standing challenge; in particular, we are still in lack of
efficient and practical schemes that are backed by theoretical performance
guarantees. Motivated by recent developments in exploration via optimistic
regularization, this paper provides an interpretation of the principle of
optimism through the lens of primal-dual optimization. From this fresh
perspective, we set forth a new value-incentivized actor-critic (VAC) method,
which optimizes a single easy-to-optimize objective integrating exploration and
exploitation -- it promotes state-action and policy estimates that are both
consistent with collected data transitions and result in higher value
functions. Theoretically, the proposed VAC method has near-optimal regret
guarantees under linear Markov decision processes (MDPs) in both finite-horizon
and infinite-horizon settings, which can be extended to the general function
approximation setting under appropriate assumptions.

</details>


### [155] [ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical Attacks](https://arxiv.org/abs/2506.22423)
*Pritam Dash,Ethan Chan,Nathan P. Lawrence,Karthik Pattabiraman*

Key words: UAV, 强化学习, 抗攻击, 传感器安全, ARMOR

TL;DR: ARMOR是一种针对无人机（UAV）的抗攻击强化学习控制器，通过两阶段训练学习鲁棒的潜在状态表征，提升对传感器攻击的抵御能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 无人机传感器易受物理攻击（如GPS欺骗），现有强化学习方法无法有效应对，因此需要开发抗攻击的控制器。

Method: ARMOR采用两阶段训练：先通过特权攻击信息训练教师编码器生成攻击感知的潜在状态，再训练学生编码器仅用历史数据近似教师状态。

Result: 实验表明ARMOR优于传统方法，能保障无人机安全，提升对未见过攻击的泛化能力，并降低训练成本。

Conclusion: ARMOR为无人机控制提供了一种抗攻击且高效的解决方案。

Abstract: Unmanned Aerial Vehicles (UAVs) depend on onboard sensors for perception,
navigation, and control. However, these sensors are susceptible to physical
attacks, such as GPS spoofing, that can corrupt state estimates and lead to
unsafe behavior. While reinforcement learning (RL) offers adaptive control
capabilities, existing safe RL methods are ineffective against such attacks. We
present ARMOR (Adaptive Robust Manipulation-Optimized State Representations),
an attack-resilient, model-free RL controller that enables robust UAV operation
under adversarial sensor manipulation. Instead of relying on raw sensor
observations, ARMOR learns a robust latent representation of the UAV's physical
state via a two-stage training framework. In the first stage, a teacher
encoder, trained with privileged attack information, generates attack-aware
latent states for RL policy training. In the second stage, a student encoder is
trained via supervised learning to approximate the teacher's latent states
using only historical sensor data, enabling real-world deployment without
privileged information. Our experiments show that ARMOR outperforms
conventional methods, ensuring UAV safety. Additionally, ARMOR improves
generalization to unseen attacks and reduces training cost by eliminating the
need for iterative adversarial training.

</details>


### [156] [CLoVE: Personalized Federated Learning through Clustering of Loss Vector Embeddings](https://arxiv.org/abs/2506.22427)
*Randeep Bhatia,Nikos Papadis,Murali Kodialam,TV Lakshman,Sayak Chakrabarty*

Key words: 聚类联邦学习, 损失向量嵌入, 联邦聚合, 非独立同分布, 个性化联邦学习

TL;DR: CLoVE是一种基于损失向量嵌入的聚类算法，用于解决聚类联邦学习中的客户分组问题，实现高效聚类和模型优化。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在聚类联邦学习中，客户的分布数据自然形成集群，但如何准确识别这些集群是一个挑战。CLoVE旨在通过客户损失向量嵌入来解决这一问题。

Method: CLoVE通过客户的模型损失值生成嵌入向量，利用相似损失模式的客户属于同一集群的特性，迭代识别和分离不同集群的客户，并通过联邦聚合优化集群特定模型。

Result: 实验表明，CLoVE在多种非独立同分布设置下，仅需几轮训练即可高精度恢复集群，并在有监督和无监督的个性化联邦学习任务中达到最先进的模型精度。

Conclusion: CLoVE因其简单性、适用性和鲁棒性，在实际应用中优于现有算法，并能快速收敛到最优模型。

Abstract: We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm
for Clustered Federated Learning (CFL). In CFL, clients are naturally grouped
into clusters based on their data distribution. However, identifying these
clusters is challenging, as client assignments are unknown. CLoVE utilizes
client embeddings derived from model losses on client data, and leverages the
insight that clients in the same cluster share similar loss values, while those
in different clusters exhibit distinct loss patterns. Based on these
embeddings, CLoVE is able to iteratively identify and separate clients from
different clusters and optimize cluster-specific models through federated
aggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its
simplicity, (2) its applicability to both supervised and unsupervised settings,
and (3) the fact that it eliminates the need for near-optimal model
initialization, which makes it more robust and better suited for real-world
applications. We establish theoretical convergence bounds, showing that CLoVE
can recover clusters accurately with high probability in a single round and
converges exponentially fast to optimal models in a linear setting. Our
comprehensive experiments comparing with a variety of both CFL and generic
Personalized Federated Learning (PFL) algorithms on different types of datasets
and an extensive array of non-IID settings demonstrate that CLoVE achieves
highly accurate cluster recovery in just a few rounds of training, along with
state-of-the-art model accuracy, across a variety of both supervised and
unsupervised PFL tasks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [157] [SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents](https://arxiv.org/abs/2506.21669)
*Wanxin Tian,Shijie Zhang,Kevin Zhang,Xiaowei Chi,Yulin Luo,Junyu Lu,Chunkai Fan,Qiang Zhou,Yiming Zhao,Ning Liu Siyu Lin,Zhiyuan Qin,Xiaozhu Ju,Shanghang Zhang,Jian Tang*

Key words: 自主进化、具身智能、强化微调、多模态交互、蒙特卡洛树搜索

TL;DR: SEEA-R1是一个用于实现自主进化的具身智能体的强化微调框架，通过Tree-GRPO和MGRM解决了多步推理任务中的稀疏奖励和泛化问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 具身智能体在长时域真实世界任务中需要自主进化的能力，现有的强化微调方法在多模态交互和通用性上存在局限。

Method: 提出了Tree-GRPO结合蒙特卡洛树搜索以优化多步推理的奖励信号，并引入MGRM进行跨任务和场景的奖励估计。

Result: 在ALFWorld基准测试中，SEEA-R1表现优异，文本和多模态评分分别达到85.07%和36.19%，超越了包括GPT-4o在内的现有模型。

Conclusion: SEEA-R1展示了作为自主进化具身智能体的潜力，支持未来可扩展的具身智能研究。

Abstract: Self-evolution, the ability of agents to autonomously improve their reasoning
and behavior, is essential for the embodied domain with long-horizon,
real-world tasks. Despite current advancements in reinforcement fine-tuning
(RFT) showing strong performance in enhancing reasoning in LLMs, its potential
to enable self-evolving embodied intelligence with multi-modal interactions
remains largely unexplored. Specifically, reinforcement fine-tuning faces two
fundamental obstacles in embodied settings: (i) the lack of accessible
intermediate rewards in multi-step reasoning tasks limits effective learning
signals, and (ii) reliance on hand-crafted reward functions restricts
generalization to novel tasks and environments. To address these challenges, we
present Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework
designed for enabling the self-evolving capabilities of embodied agents.
Specifically, to convert sparse delayed rewards into denser intermediate
signals that improve multi-step reasoning, we propose Tree-based group relative
policy optimization (Tree-GRPO), which integrates Monte Carlo Tree Search into
GRPO. To generalize reward estimation across tasks and scenes, supporting
autonomous adaptation and reward-driven self-evolution, we further introduce
Multi-modal Generative Reward Model (MGRM). To holistically evaluate the
effectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing
state-of-the-art methods with scores of 85.07% (textual) and 36.19%
(multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also
achieves scores of 80.3% without environmental reward, surpassing all
open-source baselines and highlighting its scalability as a self-evolving
embodied agent. Additional experiments and qualitative analysis further support
the potential of SEEA-R1 for future research in scalable embodied intelligence.

</details>


### [158] [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734)
*Guan Wang,Jin Li,Yuhao Sun,Xing Chen,Changling Liu,Yue Wu,Meng Lu,Sen Song,Yasin Abbasi Yadkori*

Key words: 分层推理模型, HRM, 大型语言模型, 通用推理, ARC基准

TL;DR: 提出了一种名为HRM的分层推理模型，通过模仿人脑的层次化和多时间尺度处理机制，解决了当前大语言模型（如Chain-of-Thought）的局限性，如任务分解脆弱性、数据需求大和延迟高。HRM在少量训练样本下表现出色。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前大型语言模型的推理能力存在任务分解脆弱性、数据需求大和延迟高等问题，且缺乏通用推理能力。HRM受到人脑层次化和多时间尺度处理机制的启发，旨在解决这些挑战。

Method: HRM通过两个相互依赖的循环模块（高层模块负责抽象规划，低层模块处理详细计算）在单次前向传递中完成推理任务，无需中间过程显式监督。

Result: HRM仅用2700万参数和1000个训练样本，在复杂推理任务（如Sudoku、迷宫路径规划）和ARC基准测试中表现优异，甚至优于参数更多、上下文更长的模型。

Conclusion: HRM是一种突破性的通用推理架构，为通用计算和推理系统的发展提供了新方向。

Abstract: Reasoning, the process of devising and executing complex goal-oriented action
sequences, remains a critical challenge in AI. Current large language models
(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from
brittle task decomposition, extensive data requirements, and high latency.
Inspired by the hierarchical and multi-timescale processing in the human brain,
we propose the Hierarchical Reasoning Model (HRM), a novel recurrent
architecture that attains significant computational depth while maintaining
both training stability and efficiency. HRM executes sequential reasoning tasks
in a single forward pass without explicit supervision of the intermediate
process, through two interdependent recurrent modules: a high-level module
responsible for slow, abstract planning, and a low-level module handling rapid,
detailed computations. With only 27 million parameters, HRM achieves
exceptional performance on complex reasoning tasks using only 1000 training
samples. The model operates without pre-training or CoT data, yet achieves
nearly perfect performance on challenging tasks including complex Sudoku
puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms
much larger models with significantly longer context windows on the Abstraction
and Reasoning Corpus (ARC), a key benchmark for measuring artificial general
intelligence capabilities. These results underscore HRM's potential as a
transformative advancement toward universal computation and general-purpose
reasoning systems.

</details>


### [159] [THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?](https://arxiv.org/abs/2506.21763)
*Xin Wang,Jiyao Liu,Yulong Xiao,Junzhi Ning,Lihao Liu,Junjun He,Botian Shi,Kaicheng Yu*

Key words: Large Language Models, THE-Tree, scientific verification, causal linkage, benchmarking

TL;DR: 提出了THE-Tree框架，用于结构化地评估和验证AI生成的科学提议的准确性和新颖性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 由于现有验证方法（如LLM独立验证和传统引用网络）存在不足，需要一种结构化、可验证且因果关联的科学演化数据框架。

Method: THE-Tree通过搜索算法构建领域特定的演化树，采用“Think-Verbalize-Cite-Verify”流程验证每个演化步骤的逻辑和证据支持。

Result: 在88个领域的实验中，THE-Tree显著提升了图完成和预测未来科学发展的性能，并与其他方法结合时性能提升近100%。

Conclusion: THE-Tree为科学演化的结构化验证提供了一种有效解决方案，其性能优于现有方法。

Abstract: Large Language Models (LLMs) are accelerating scientific idea generation, but
rigorously evaluating these numerous, often superficial, AI-generated
propositions for novelty and factual accuracy is a critical bottleneck; manual
verification is too slow.Existing validation methods are inadequate: LLMs as
standalone verifiers may hallucinate and lack domain knowledge (our findings
show ~60\% unawareness of relevant papers in specific domains), while
traditional citation networks lack explicit causality and narrative surveys are
unstructured.This underscores a core challenge: the absence of structured,
verifiable, and causally-linked historical data of scientific evolution.To
address this,we introduce \textbf{THE-Tree} (\textbf{T}echnology
\textbf{H}istory \textbf{E}volution Tree), a computational framework that
constructs such domain-specific evolution trees from scientific
literature.THE-Tree employs a search algorithm to explore evolutionary paths.
During its node expansion, it utilizes a novel "Think-Verbalize-Cite-Verify"
process: an LLM proposes potential advancements and cites supporting
literature. Critically, each proposed evolutionary link is then validated for
logical coherence and evidential support by a recovered natural language
inference mechanism that interrogates the cited literature, ensuring that each
step is grounded.We construct and validate 88 THE-Trees across diverse domains
and release a benchmark dataset including up to 71k fact verifications covering
27k papers to foster further research.Experiments demonstrate that i) in graph
completion, our THE-Tree improves hit@1 by 8\% to 14\% across multiple models
compared to traditional citation networks; ii) for predicting future scientific
developments, it improves hit@1 metric by nearly 10\%; and iii) when combined
with other methods, it boosts the performance of evaluating important
scientific papers by almost 100\%.

</details>


### [160] [MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models](https://arxiv.org/abs/2506.21784)
*Yifan Liu,Xishun Liao,Haoxuan Ma,Jonathan Liu,Rohan Jadhav,Jiaqi Ma*

Key words: 移动仿真,混合框架,LLMs,行为建模,城市规划

TL;DR: 提出MobiVerse混合框架，结合轻量级生成器和LLMs，实现大规模人群动态调度和响应环境反馈，填补当前移动仿真平台的空白。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前移动仿真平台在算法开发、政策实施和大规模评估方面存在不足，MobiVerse旨在解决这一问题。

Method: 混合框架结合轻量级生成器和LLMs，动态调整活动链，测试了53,000个代理的调度。

Result: 实验显示MobiVerse能高效响应环境变化（如道路封闭、大型活动），同时保持计算效率和行为真实性。

Conclusion: MobiVerse为移动系统规划和操作提供了可定制的仿真平台，兼具效率和真实性。

Abstract: Understanding and modeling human mobility patterns is crucial for effective
transportation planning and urban development. Despite significant advances in
mobility research, there remains a critical gap in simulation platforms that
allow for algorithm development, policy implementation, and comprehensive
evaluation at scale. Traditional activity-based models require extensive data
collection and manual calibration, machine learning approaches struggle with
adaptation to dynamic conditions, and treding agent-based Large Language Models
(LLMs) implementations face computational constraints with large-scale
simulations. To address these challenges, we propose MobiVerse, a hybrid
framework leverages the efficiency of lightweight domain-specific generator for
generating base activity chains with the adaptability of LLMs for context-aware
modifications. A case study was conducted in Westwood, Los Angeles, where we
efficiently generated and dynamically adjusted schedules for the whole
population of approximately 53,000 agents on a standard PC. Our experiments
demonstrate that MobiVerse successfully enables agents to respond to
environmental feedback, including road closures, large gathering events like
football games, and congestion, through our hybrid framework. Its modular
design facilitates testing various mobility algorithms at both transportation
system and agent levels. Results show our approach maintains computational
efficiency while enhancing behavioral realism. MobiVerse bridges the gap in
mobility simulation by providing a customizable platform for mobility systems
planning and operations with benchmark algorithms. Code and videos are
available at https://github.com/ucla-mobility/MobiVerse.

</details>


### [161] [CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation](https://arxiv.org/abs/2506.21805)
*Nicolas Bougie,Narimasa Watanabe*

Key words: 城巿模拟器, 大型语言模型, 递归价值驱动, 集体行为, 城巿规划

TL;DR: CitySim是一个基于大型语言模型的城巿模拟器，通过递归价值驱动方法生成真实日常计划，优于传统手工规则方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统的人类行为模型依赖手工规则，难以模拟复杂意图和适应性行为，需要更灵活的模拟器。

Method: 利用递归价值驱动方法生成代理的日常计划，赋予代理信念、长期目标和空间记忆能力。

Result: CitySim在微观和宏观层面均更接近真实人类行为，并能模拟大规模代理的集体行为。

Conclusion: CitySim是一个可扩展、灵活的测试平台，适用于理解和预测城巿现象。

Abstract: Modeling human behavior in urban environments is fundamental for social
science, behavioral studies, and urban planning. Prior work often rely on
rigid, hand-crafted rules, limiting their ability to simulate nuanced
intentions, plans, and adaptive behaviors. Addressing these challenges, we
envision an urban simulator (CitySim), capitalizing on breakthroughs in
human-level intelligence exhibited by large language models. In CitySim, agents
generate realistic daily schedules using a recursive value-driven approach that
balances mandatory activities, personal habits, and situational factors. To
enable long-term, lifelike simulations, we endow agents with beliefs, long-term
goals, and spatial memory for navigation. CitySim exhibits closer alignment
with real humans than prior work, both at micro and macro levels. Additionally,
we conduct insightful experiments by modeling tens of thousands of agents and
evaluating their collective behaviors under various real-world scenarios,
including estimating crowd density, predicting place popularity, and assessing
well-being. Our results highlight CitySim as a scalable, flexible testbed for
understanding and forecasting urban phenomena.

</details>


### [162] [Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds](https://arxiv.org/abs/2506.21887)
*Edward Chen,Sang T. Truong,Natalie Dullerud,Sanmi Koyejo,Carlos Guestrin*

Key words: 高风险决策、多目标优化、交互式框架、偏好学习、信任构建

TL;DR: 论文提出了一种交互式本地-全局框架Active-MoSH，用于在高风险决策中平衡多目标优化与用户偏好学习。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 在高风险决策（如近距离放射治疗）中，决策者需要在多个竞争目标（如最大化肿瘤覆盖率和严格限制器官剂量）之间进行权衡。传统方法缺乏系统化迭代优化偏好结构的能力，决策者对最终选择的信任度不足。

Method: Active-MoSH框架包含本地组件（结合软硬边界与概率偏好学习）和全局组件T-MoSH（通过多目标敏感性分析识别潜在高价值点）。采用主动采样策略，优化探索与利用的平衡，减少认知负担。

Result: 通过合成和真实应用验证了框架的性能优势。用户研究表明，Active-MoSH能够提高收敛速度、增强决策者信任，并提供更灵活的偏好表达。

Conclusion: Active-MoSH为高风险决策提供了有效的交互式工具，能够系统化优化偏好结构并增强信任。

Abstract: High-stakes decision-making involves navigating multiple competing objectives
with expensive evaluations. For instance, in brachytherapy, clinicians must
balance maximizing tumor coverage (e.g., an aspirational target or soft bound
of >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard
bound of <601 cGy to the bladder), with each plan evaluation being
resource-intensive. Selecting Pareto-optimal solutions that match implicit
preferences is challenging, as exhaustive Pareto frontier exploration is
computationally and cognitively prohibitive, necessitating interactive
frameworks to guide users. While decision-makers (DMs) often possess domain
knowledge to narrow the search via such soft-hard bounds, current methods often
lack systematic approaches to iteratively refine these multi-faceted preference
structures. Critically, DMs must trust their final decision, confident they
haven't missed superior alternatives; this trust is paramount in
high-consequence scenarios. We present Active-MoSH, an interactive local-global
framework designed for this process. Its local component integrates soft-hard
bounds with probabilistic preference learning, maintaining distributions over
DM preferences and bounds for adaptive Pareto subset refinement. This is guided
by an active sampling strategy optimizing exploration-exploitation while
minimizing cognitive burden. To build DM trust, Active-MoSH's global component,
T-MoSH, leverages multi-objective sensitivity analysis to identify potentially
overlooked, high-value points beyond immediate feedback. We demonstrate
Active-MoSH's performance benefits through diverse synthetic and real-world
applications. A user study on AI-generated image selection further validates
our hypotheses regarding the framework's ability to improve convergence,
enhance DM trust, and provide expressive preference articulation, enabling more
effective DMs.

</details>


### [163] [AlphaBeta is not as good as you think: a new probabilistic model to better analyze deterministic game-solving algorithms](https://arxiv.org/abs/2506.21996)
*Raphaël Boige,Amine Boumaza,Bruno Scherrer*

Key words: 游戏树, 算法复杂度, AlphaBeta, Scout, 概率模型

TL;DR: 论文提出了一种新的概率模型，通过引入祖先依赖关系，生成更具挑战性的游戏实例，并分析了多种算法在这种模型下的平均复杂度。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统游戏树分析模型过于简化，忽略了结构复杂性，无法反映真实游戏中的挑战。

Method: 引入基于层级条件分布的游戏树生成模型，保留分析可处理性的同时调整问题难度。

Result: 新模型揭示了算法在深度游戏树中的性能差异，如AlphaBeta比Scout慢得多。

Conclusion: 新模型为游戏求解算法提供了更现实、更具挑战性的分析框架。

Abstract: Deterministic game-solving algorithms are conventionally analyzed in the
light of their average-case complexity against a distribution of random
game-trees, where leaf values are independently sampled from a fixed
distribution. This simplified model enables uncluttered mathematical analysis,
revealing two key properties: root value distributions asymptotically collapse
to a single fixed value for finite-valued trees, and all reasonable algorithms
achieve global optimality. However, these findings are artifacts of the model's
design-its long criticized independence assumption strips games of structural
complexity, producing trivial instances where no algorithm faces meaningful
challenges. To address this limitation, we introduce a new probabilistic model
that incrementally constructs game-trees using a fixed level-wise conditional
distribution. By enforcing ancestor dependency, a critical structural feature
of real-world games, our framework generates problems with adjustable
difficulty while retaining some form of analytical tractability. For several
algorithms, including AlphaBeta and Scout, we derive recursive formulas
characterizing their average-case complexities under this model. These allow us
to rigorously compare algorithms on deep game-trees, where Monte-Carlo
simulations are no longer feasible. While asymptotically, all algorithms seem
to converge to identical branching factor (a result analogous to those of
independence-based models), deep finite trees reveal stark differences:
AlphaBeta incurs a significantly larger constant multiplicative factor compared
to algorithms like Scout, leading to a substantial practical slowdown. Our
framework sheds new light on classical game-solving algorithms, offering
rigorous evidence and analytical tools to advance the understanding of these
methods under a more realistic, challenging, and yet tractable model.

</details>


### [164] [LeanConjecturer: Automatic Generation of Mathematical Conjectures for Theorem Proving](https://arxiv.org/abs/2506.22005)
*Naoto Onda,Kazumi Kasaura,Yuta Oriike,Masaya Taniguchi,Akiyoshi Sannai,Sho Sonoda*

Key words: 数学猜想, 定理证明, LLM, Lean 4

TL;DR: LeanConjecturer是一个自动化生成大学数学猜想的管道，结合了规则上下文提取与LLM基于定理生成的方法，解决了正式定理证明中的数据稀缺问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决正式定理证明中的数据稀缺问题，为定理证明系统生成训练数据。

Method: 采用混合方法结合规则上下文提取与LLM定理生成，通过迭代生成和评估。

Result: 生成12,289个猜想，其中3,776个非平凡。训练域特定猜想增强定理证明能力。

Conclusion: 展示了其潜力，能够生成非平凡定理并扩展到复杂数学领域。

Abstract: We introduce LeanConjecturer, a pipeline for automatically generating
university-level mathematical conjectures in Lean 4 using Large Language Models
(LLMs). Our hybrid approach combines rule-based context extraction with
LLM-based theorem statement generation, addressing the data scarcity challenge
in formal theorem proving. Through iterative generation and evaluation,
LeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with
3,776 identified as syntactically valid and non-trivial, that is, cannot be
proven by \texttt{aesop} tactic. We demonstrate the utility of these generated
conjectures for reinforcement learning through Group Relative Policy
Optimization (GRPO), showing that targeted training on domain-specific
conjectures can enhance theorem proving capabilities. Our approach generates
103.25 novel conjectures per seed file on average, providing a scalable
solution for creating training data for theorem proving systems. Our system
successfully verified several non-trivial theorems in topology, including
properties of semi-open, alpha-open, and pre-open sets, demonstrating its
potential for mathematical discovery beyond simple variations of existing
results.

</details>


### [165] [Universal Retrieval for Multimodal Trajectory Modeling](https://arxiv.org/abs/2506.22056)
*Xuan Zhang,Ziyan Jiang,Rui Meng,Yifei Leng,Zhenbang Xiao,Zora Zhiruo Wang,Yanyi Shang,Dehan Kong*

Key words: 轨迹数据,多模态检索,AI代理,对比学习,基准测试

TL;DR: 该论文提出了多模态轨迹检索方法（Multimodal Trajectory Retrieval），通过构建统一代理轨迹数据集（UATD）和基准测试GAE-Bench，设计了GAE-Retriever框架，显著提升了轨迹检索效果。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 轨迹数据具有增强AI代理在GUI环境中能力的潜力，但目前缺乏对轨迹级数据表示的系统性建模方法。

Method: 构建UATD数据集和GAE-Bench基准，提出GAE-Retriever框架，整合视觉-语言模型和优化的对比学习机制。

Result: GAE-Retriever在多个数据集上表现优于基线模型，检索召回率显著提升。

Conclusion: 该研究在多模态轨迹检索领域取得了有效进展，为AI代理能力提升提供了新方法。

Abstract: Trajectory data, capturing human actions and environmental states across
various modalities, holds significant potential for enhancing AI agent
capabilities, particularly in GUI environments. However, how to model the
representation of trajectory-level data presents a significant challenge that
has not been systematically addressed amid explosive trajectory data growth. In
this work, we introduce Multimodal Trajectory Retrieval, bridging the gap
between universal retrieval and agent-centric trajectory modeling. We construct
the Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and
states across diverse real-world scenarios. Based on this, we present
GAE-Bench, a benchmark containing a large number of trajectory-based retrieval
pairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework
that adopts vision-language models and incorporates optimized contrastive
learning through a token selection and the GradCache mechanism. Comprehensive
evaluations across multiple datasets show that GAE-Retriever consistently
outperforms strong baselines in retrieval recall, highlighting its
effectiveness in advancing multimodal trajectory retrieval.

</details>


### [166] [Query as Test: An Intelligent Driving Test and Data Storage Method for Integrated Cockpit-Vehicle-Road Scenarios](https://arxiv.org/abs/2506.22068)
*Shengyue Yao,Runqing Guo,Yangyang Qin,Miangbing Meng,Jipeng Cao,Yilun Lin,Yisheng Lv,Fei-Yue Wang*

Key words: 人工智能, 自动驾驶, 测试方法, 数据表示, 逻辑查询

TL;DR: 本文提出了“Query as Test”（QaT）概念，通过灵活的逻辑查询替代传统固定测试用例，并介绍了一种基于答案集编程（ASP）的新型数据框架“Extensible Scenarios Notations”（ESN），以统一表示异构数据，实现深度语义融合和灵活测试。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有测试方法依赖数据堆叠，无法覆盖所有边缘情况且缺乏灵活性，导致智能座舱、自动驾驶和智能路网的数据生态不兼容。

Method: 提出ESN框架，基于ASP统一表示异构数据，支持逻辑推理查询；引入QaT范式，将测试转化为逻辑查询；提出“Validation-Driven Development”（VDD）概念。

Result: ESN框架实现了数据的语义融合、支持复杂查询、提供可解释性决策，并支持细粒度隐私保护；QaT提升测试的表达力和形式严谨性。

Conclusion: ESN和QaT为自动驾驶系统的测试和开发提供了更灵活、严谨的解决方案，VDD进一步加速了大语言模型时代的开发迭代。

Abstract: With the deep penetration of Artificial Intelligence (AI) in the
transportation sector, intelligent cockpits, autonomous driving, and
intelligent road networks are developing at an unprecedented pace. However, the
data ecosystems of these three key areas are increasingly fragmented and
incompatible. Especially, existing testing methods rely on data stacking, fail
to cover all edge cases, and lack flexibility. To address this issue, this
paper introduces the concept of "Query as Test" (QaT). This concept shifts the
focus from rigid, prescripted test cases to flexible, on-demand logical queries
against a unified data representation. Specifically, we identify the need for a
fundamental improvement in data storage and representation, leading to our
proposal of "Extensible Scenarios Notations" (ESN). ESN is a novel declarative
data framework based on Answer Set Programming (ASP), which uniformly
represents heterogeneous multimodal data from the cockpit, vehicle, and road as
a collection of logical facts and rules. This approach not only achieves deep
semantic fusion of data, but also brings three core advantages: (1) supports
complex and flexible semantic querying through logical reasoning; (2) provides
natural interpretability for decision-making processes; (3) allows for
on-demand data abstraction through logical rules, enabling fine-grained privacy
protection. We further elaborate on the QaT paradigm, transforming the
functional validation and safety compliance checks of autonomous driving
systems into logical queries against the ESN database, significantly enhancing
the expressiveness and formal rigor of the testing. Finally, we introduce the
concept of "Validation-Driven Development" (VDD), which suggests to guide
developments by logical validation rather than quantitative testing in the era
of Large Language Models, in order to accelerating the iteration and
development process.

</details>


### [167] [A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety](https://arxiv.org/abs/2506.22183)
*Camille François,Ludovic Péran,Ayah Bdeir,Nouha Dziri,Will Hawkins,Yacine Jernite,Sayash Kapoor,Juliet Shen,Heidy Khlaaf,Kevin Klyman,Nik Marda,Marie Pellat,Deb Raji,Divya Siddarth,Aviya Skowron,Joseph Spisak,Madhulika Srikumar,Victor Storchan,Audrey Tang,Jen Weedon*

Key words: 开源AI, AI安全, 多模态基准, 防御攻击, 透明治理

TL;DR: 论文探讨了开源基础模型对AI系统安全的重要性，提出了研究议程和技术干预方案，并指出了当前的安全缺口。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着开源基础模型的快速崛起，确保AI系统安全变得愈发重要，需要多学科合作和透明治理。

Method: 通过哥伦比亚AI开放与安全会议的六周准备计划，邀请了45名研究者、工程师和政策领袖，采用参与式、解决方案导向的方法，制定了研究议程和技术干预方案。

Result: 研究提出了开放可以增强安全性的观点，但也指出了多语言多模态基准不足、防御攻击机制有限等问题。

Conclusion: 论文提出了五个优先研究方向，强调参与式输入、未来内容过滤器、生态系统安全基础设施等，为AI安全学科的发展奠定了基础。

Abstract: The rapid rise of open-weight and open-source foundation models is
intensifying the obligation and reshaping the opportunity to make AI systems
safe. This paper reports outcomes from the Columbia Convening on AI Openness
and Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme
involving more than forty-five researchers, engineers, and policy leaders from
academia, industry, civil society, and government. Using a participatory,
solutions-oriented process, the working groups produced (i) a research agenda
at the intersection of safety and open source AI; (ii) a mapping of existing
and needed technical interventions and open source tools to safely and
responsibly deploy open foundation models across the AI development workflow;
and (iii) a mapping of the content safety filter ecosystem with a proposed
roadmap for future research and development. We find that openness --
understood as transparent weights, interoperable tooling, and public governance
-- can enhance safety by enabling independent scrutiny, decentralized
mitigation, and culturally plural oversight. However, significant gaps persist:
scarce multimodal and multilingual benchmarks, limited defenses against
prompt-injection and compositional attacks in agentic systems, and insufficient
participatory mechanisms for communities most affected by AI harms. The paper
concludes with a roadmap of five priority research directions, emphasizing
participatory inputs, future-proof content filters, ecosystem-wide safety
infrastructure, rigorous agentic safeguards, and expanded harm taxonomies.
These recommendations informed the February 2025 French AI Action Summit and
lay groundwork for an open, plural, and accountable AI safety discipline.

</details>


### [168] [Breaking Rank Bottlenecks in Knowledge Graph Completion](https://arxiv.org/abs/2506.22271)
*Samy Badreddine,Emile van Krieken,Luciano Serafini*

Key words: 知识图谱补全（KGC）、秩瓶颈、混合输出层、KGE-MoS、模型表达能力

TL;DR: 论文研究了知识图谱补全（KGC）模型中的秩瓶颈问题，提出了一种基于混合输出层的解决方案KGE-MoS，显著提升了模型性能和概率拟合效果。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的KGC模型尽管使用强大的编码器，但在实体数量远大于嵌入维度时，线性输出层的秩瓶颈会限制模型表达能力，影响排名准确性和分数分布保真度。

Method: 提出KGE-MoS，一种基于混合的输出层，通过打破秩瓶颈来增强模型表达能力。

Result: 在四个数据集上的实验表明，KGE-MoS以较低的参数成本显著提升了KGC模型的性能和概率拟合。

Conclusion: 通过引入混合输出层可以有效解决秩瓶颈问题，从而提升知识图谱补全模型的整体表现。

Abstract: Many Knowledge Graph Completion (KGC) models, despite using powerful
encoders, rely on a simple vector-matrix multiplication to score queries
against candidate object entities. When the number of entities is larger than
the model's embedding dimension, which in practical scenarios is often by
several orders of magnitude, we have a linear output layer with a rank
bottleneck. Such bottlenecked layers limit model expressivity. We investigate
both theoretically and empirically how rank bottlenecks affect KGC models. We
find that, by limiting the set of feasible predictions, rank bottlenecks hurt
ranking accuracy and the distribution fidelity of scores. Inspired by the
language modelling literature, we propose KGE-MoS, a mixture-based output layer
to break rank bottlenecks in many KGC models. Our experiments on four datasets
show that KGE-MoS improves performance and probabilistic fit of KGC models for
a low parameter cost.

</details>


### [169] [Artificial Intelligent Disobedience: Rethinking the Agency of Our Artificial Teammates](https://arxiv.org/abs/2506.22276)
*Reuth Mirsky*

Key words: 人工智能、自主性、人机协作、智能 disobedience、团队合作

TL;DR: 论文探讨了赋予AI队友智能 disobedience 的能力，以增强其在人机协作中的自主性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有合作型AI系统过于顺从，可能在需要时无法做出独立判断，限制了其在团队中的贡献。

Method: 提出AI代理的自主性级别框架，并通过代表性案例分析智能 disobedience 在不同自主性级别中的表现。

Result: 研究发现智能 disobedience 是AI作为独立研究焦点的必要能力，尤其是在合作场景中。

Conclusion: 论文提出初步边界和研究考量，倡导将 disobedience 作为AI核心能力进行深入研究。

Abstract: Artificial intelligence has made remarkable strides in recent years,
achieving superhuman performance across a wide range of tasks. Yet despite
these advances, most cooperative AI systems remain rigidly obedient, designed
to follow human instructions without question and conform to user expectations,
even when doing so may be counterproductive or unsafe. This paper argues for
expanding the agency of AI teammates to include \textit{intelligent
disobedience}, empowering them to make meaningful and autonomous contributions
within human-AI teams. It introduces a scale of AI agency levels and uses
representative examples to highlight the importance and growing necessity of
treating AI autonomy as an independent research focus in cooperative settings.
The paper then explores how intelligent disobedience manifests across different
autonomy levels and concludes by proposing initial boundaries and
considerations for studying disobedience as a core capability of artificial
agents.

</details>


### [170] [Conceptual Topic Aggregation](https://arxiv.org/abs/2506.22309)
*Klara M. Gutekunst,Dominik Dürrschnabel,Johannes Hirth,Gerd Stumme*

Key words: 数据探索、主题建模、形式概念分析、FAT-CAT、概念格

TL;DR: 论文提出FAT-CAT方法，基于形式概念分析（FCA），改进主题建模的可解释性和可视化效果，适用于多样化的主题和文件类型。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统手动检查方法无法应对数据爆炸式增长，现有主题建模方法在提供可解释表示方面存在不足。

Method: 提出基于FCA的FAT-CAT方法，构建概念格以层次化表示主题分布，支持多样化主题和文件类型。

Result: 在ETYNTKE数据集上的案例研究表明，FCA方法比现有技术提供更具意义和可解释的主题聚合。

Conclusion: FCA为基础的主题建模方法优于传统技术，能提供更深入的语义结构洞察。

Abstract: The vast growth of data has rendered traditional manual inspection
infeasible, necessitating the adoption of computational methods for efficient
data exploration. Topic modeling has emerged as a powerful tool for analyzing
large-scale textual datasets, enabling the extraction of latent semantic
structures. However, existing methods for topic modeling often struggle to
provide interpretable representations that facilitate deeper insights into data
structure and content. In this paper, we propose FAT-CAT, an approach based on
Formal Concept Analysis (FCA) to enhance meaningful topic aggregation and
visualization of discovered topics. Our approach can handle diverse topics and
file types -- grouped by directories -- to construct a concept lattice that
offers a structured, hierarchical representation of their topic distribution.
In a case study on the ETYNTKE dataset, we evaluate the effectiveness of our
approach against other representation methods to demonstrate that FCA-based
aggregation provides more meaningful and interpretable insights into dataset
composition than existing topic modeling techniques.

</details>


### [171] [Embodied AI Agents: Modeling the World](https://arxiv.org/abs/2506.22355)
*Pascale Fung,Yoram Bachrach,Asli Celikyilmaz,Kamalika Chaudhuri,Delong Chen,Willy Chung,Emmanuel Dupoux,Hervé Jégou,Alessandro Lazaric,Arjun Majumdar,Andrea Madotto,Franziska Meier,Florian Metze,Théo Moutakanni,Juan Pino,Basile Terver,Joseph Tighe,Jitendra Malik*

Key words: 具身AI, 世界建模, 多模态感知, 自主决策, 人机协作

TL;DR: 本文探讨了具身AI代理的研究，强调世界建模是核心，通过多模态感知、规划与记忆提升自主任务能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究具身AI代理（如虚拟化身、可穿戴设备和机器人）如何更自然地与环境和用户互动，模拟人类学习与行为方式。

Method: 提出通过世界建模整合多模态感知、推理与规划，以及用户心理世界模型，提升代理的自主性与协作能力。

Result: 具身AI代理能更好地理解环境、预测用户意图，并自主完成复杂任务。

Conclusion: 世界建模是具身AI代理高效交互与自主决策的关键，未来需进一步探索用户心理模型的整合。

Abstract: This paper describes our research on AI agents embodied in visual, virtual or
physical forms, enabling them to interact with both users and their
environments. These agents, which include virtual avatars, wearable devices,
and robots, are designed to perceive, learn and act within their surroundings,
which makes them more similar to how humans learn and interact with the
environments as compared to disembodied agents. We propose that the development
of world models is central to reasoning and planning of embodied AI agents,
allowing these agents to understand and predict their environment, to
understand user intentions and social contexts, thereby enhancing their ability
to perform complex tasks autonomously. World modeling encompasses the
integration of multimodal perception, planning through reasoning for action and
control, and memory to create a comprehensive understanding of the physical
world. Beyond the physical world, we also propose to learn the mental world
model of users to enable better human-agent collaboration.

</details>


### [172] [AI Model Passport: Data and System Traceability Framework for Transparent AI in Health](https://arxiv.org/abs/2506.22358)
*Varvara Kalokyri,Nikolaos S. Tachos,Charalampos N. Kalantzopoulos,Stelios Sfakianakis,Haridimos Kondylakis,Dimitrios I. Zaridis,Sara Colantonio,Daniele Regge,Nikolaos Papanikolaou,The ProCAncer-I consortium,Konstantinos Marias,Dimitrios I. Fotiadis,Manolis Tsiknakis*

Key words: AI模型护照,透明度,可重复性,医疗影像,元数据

TL;DR: 论文提出AI Model Passport，一种标准化框架，用于唯一标识和验证AI模型，提升透明度和可重复性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有AI模型文档框架缺乏可扩展性和机器可读性，难以确保模型来源和真实性，影响信任和可重复性。

Method: 引入AI Model Passport框架，通过AIPassport工具自动化元数据收集和版本管理，支持全生命周期追踪。

Result: 在医学影像应用中，AIPassport显著提升透明度和可重复性，同时减少人工操作。

Conclusion: AI Model Passport为AI驱动的医疗解决方案提供了新的透明度标准，有望成为跨领域合规框架的基础。

Abstract: The increasing integration of Artificial Intelligence (AI) into health and
biomedical systems necessitates robust frameworks for transparency,
accountability, and ethical compliance. Existing frameworks often rely on
human-readable, manual documentation which limits scalability, comparability,
and machine interpretability across projects and platforms. They also fail to
provide a unique, verifiable identity for AI models to ensure their provenance
and authenticity across systems and use cases, limiting reproducibility and
stakeholder trust. This paper introduces the concept of the AI Model Passport,
a structured and standardized documentation framework that acts as a digital
identity and verification tool for AI models. It captures essential metadata to
uniquely identify, verify, trace and monitor AI models across their lifecycle -
from data acquisition and preprocessing to model design, development and
deployment. In addition, an implementation of this framework is presented
through AIPassport, an MLOps tool developed within the ProCAncer-I EU project
for medical imaging applications. AIPassport automates metadata collection,
ensures proper versioning, decouples results from source scripts, and
integrates with various development environments. Its effectiveness is
showcased through a lesion segmentation use case using data from the
ProCAncer-I dataset, illustrating how the AI Model Passport enhances
transparency, reproducibility, and regulatory readiness while reducing manual
effort. This approach aims to set a new standard for fostering trust and
accountability in AI-driven healthcare solutions, aspiring to serve as the
basis for developing transparent and regulation compliant AI systems across
domains.

</details>


### [173] [The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements](https://arxiv.org/abs/2506.22419)
*Bingchen Zhao,Despoina Magka,Minqi Jiang,Xian Li,Roberta Raileanu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Kelvin Niu,Shagun Sodhani,Michael Shvartsman,Andrei Lupu,Alisia Lupidi,Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Thomas Foster,Lucia Cipolina-Kun,Abhishek Charnalia,Derek Dunfield,Alexander H. Miller,Oisin Mac Aodha,Jakob Foerster,Yoram Bachrach*

Key words: 大语言模型, 科学研究, 基准测试, 自动化重现

TL;DR: 介绍了一个名为Automated LLM Speedrunning Benchmark的基准测试，用于评估AI代理在重现现有研究成果方面的能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 评估AI代理在科学研究中重现结果的能力，以推动科学进步。

Method: 通过NanoGPT速度竞赛中的任务，为代理提供训练脚本和提示格式，测试其重现改进的能力。

Result: 发现即使是先进的LLM也难重现已知的创新，表明自动化科学重现仍有挑战。

Conclusion: 基准测试为衡量LLM自动化科学重现能力提供了简单有效的工具。

Abstract: Rapid advancements in large language models (LLMs) have the potential to
assist in scientific progress. A critical capability toward this endeavor is
the ability to reproduce existing work. To evaluate the ability of AI agents to
reproduce results in an active research area, we introduce the Automated LLM
Speedrunning Benchmark, leveraging the research community contributions on the
NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.
Each of the 19 speedrun tasks provides the agent with the previous records
training script, optionally paired with one of three hint formats, ranging from
pseudocode to paper-like descriptions of the new records improvements. Records
execute quickly by design and speedrun improvements encompass diverse
code-level changes, ranging from high-level algorithmic advancements to
hardware-aware optimizations. These features make the benchmark both accessible
and realistic for the frontier problem of improving LLM training. We find that
recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement
already-known innovations in our benchmark, even when given detailed hints. Our
benchmark thus provides a simple, non-saturated measure of an LLMs ability to
automate scientific reproduction, a necessary (but not sufficient) skill for an
autonomous research agent.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [174] [Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics](https://arxiv.org/abs/2506.21964)
*Michael A. Riegler,Kristoffer Herland Hellton,Vajira Thambawita,Hugo L. Hammer*

Key words: 贝叶斯统计, 大语言模型, 先验分布, 知识驱动, KL散度

TL;DR: 研究利用大语言模型（LLMs）为贝叶斯统计中的先验分布提供知识驱动的建议，评估了Claude Opus、Gemini 2.5 Pro和ChatGPT-4o-mini的性能，发现它们在确定关联方向上表现良好，但在先验分布的校准上仍存在挑战。

<details>
  <summary>Details</summary>

Main category: stat.ME

Motivation: 贝叶斯统计中先验分布的选择主观且耗时，研究旨在探索LLMs能否提供高效、客观的先验建议。

Method: 通过开发提示词，要求LLMs建议和验证先验分布，并在心脏病风险和混凝土强度两个数据集上评估了Claude、Gemini和ChatGPT的表现。

Result: LLMs能正确识别关联方向，但建议的先验分布存在过度自信或过于模糊的问题。Claude和Gemini表现优于ChatGPT。

Conclusion: LLMs在生成先验分布方面具有潜力，但仍需解决分布宽度的校准问题。

Abstract: Selecting prior distributions in Bayesian statistics is challenging,
resource-intensive, and subjective. We analyze using large-language models
(LLMs) to suggest suitable, knowledge-based informative priors. We developed an
extensive prompt asking LLMs not only to suggest priors but also to verify and
reflect on their choices.
  We evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real
datasets: heart disease risk and concrete strength. All LLMs correctly
identified the direction for all associations (e.g., that heart disease risk is
higher for males). The quality of suggested priors was measured by their
Kullback-Leibler divergence from the maximum likelihood estimator's
distribution.
  The LLMs suggested both moderately and weakly informative priors. The
moderate priors were often overconfident, resulting in distributions misaligned
with the data. In our experiments, Claude and Gemini provided better priors
than ChatGPT. For weakly informative priors, a key performance difference
emerged: ChatGPT and Gemini defaulted to an "unnecessarily vague" mean of 0,
while Claude did not, demonstrating a significant advantage.
  The ability of LLMs to identify correct associations shows their great
potential as an efficient, objective method for developing informative priors.
However, the primary challenge remains in calibrating the width of these priors
to avoid over- and under-confidence.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [175] [Autonomic Microservice Management via Agentic AI and MAPE-K Integration](https://arxiv.org/abs/2506.22185)
*Matteo Esposito,Alexander Bakhtin,Noman Ahmad,Mikel Robredo,Ruoyu Su,Valentina Lenarduzzi,Davide Taibi*

Key words: 微服务, MAPE-K, 智能代理AI, 异常检测, 系统管理

TL;DR: 提出基于MAPE-K和智能代理AI的框架，解决微服务架构中的安全和管理挑战，实现自主异常检测与修复。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 微服务的分布式特性带来了安全和管理的挑战，威胁系统稳定性。

Method: 利用MAPE-K框架和智能代理AI技术，设计自主异常检测与修复方案。

Result: 框架为行业提供实用解决方案，增强系统稳定性、减少停机时间，并监控多种质量属性。

Conclusion: 该框架可定制化，适用于提升微服务系统的安全性和管理效率。

Abstract: While microservices are revolutionizing cloud computing by offering
unparalleled scalability and independent deployment, their decentralized nature
poses significant security and management challenges that can threaten system
stability. We propose a framework based on MAPE-K, which leverages agentic AI,
for autonomous anomaly detection and remediation to address the daunting task
of highly distributed system management. Our framework offers practical,
industry-ready solutions for maintaining robust and secure microservices.
Practitioners and researchers can customize the framework to enhance system
stability, reduce downtime, and monitor broader system quality attributes such
as system performance level, resilience, security, and anomaly management,
among others.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [176] [Robust and Efficient Autoregressive Speech Synthesis with Dynamic Chunk-wise Prediction Policy](https://arxiv.org/abs/2506.22023)
*Bohan Li,Zhihan Li,Haoran Wang,Hanglei Zhang,Yiwei Guo,Hankun Wang,Xie Chen,Kai Yu*

Key words: 自回归模型，语音合成，动态分块，注意力机制，多令牌预测

TL;DR: 提出一种动态分块自回归合成框架（DCAR），解决了传统自回归语音合成模型在处理长序列时的性能问题，显著提升了合成质量和效率。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 传统自回归语音合成模型在长序列处理中存在注意力不稳定、延迟高和合成质量下降的问题，限制了实时应用的可行性。

Method: 引入动态分块自回归合成框架（DCAR），通过分块到帧的注意力机制和多令牌预测训练，动态调整令牌预测范围。

Result: DCAR在测试集上实现72.27%的可懂度提升和2.61倍的推理加速，显著优于传统模型。

Conclusion: DCAR为下一代语音合成系统提供了高效的通用基础框架。

Abstract: Recently, autoregressive (AR) language models have emerged as a dominant
approach in speech synthesis, offering expressive generation and scalable
training. However, conventional AR speech synthesis models relying on the
next-token prediction paradigm often encounter significant challenges when
handling long speech sequences. These models often struggle to construct stable
frame-to-frame attention, leading to increased latency and degraded synthesis
quality, thereby limiting their feasibility for real-time applications. To
address these limitations, we introduce a novel dynamic chunk-wise
autoregressive synthesis framework, termed DCAR, designed to enhance both
efficiency and intelligibility robustness in AR speech generation. DCAR
introduces a chunk-to-frame attention mechanism through training with
multi-token prediction, enabling dynamic chunk prediction in variable speech
contexts using a lightweight module trained on-policy. DCAR dynamically adjusts
the token prediction span, significantly reducing the sequence length
dependency while obtaining high synthesis quality. Comprehensive empirical
evaluations demonstrate that DCAR substantially outperforms traditional
next-token prediction models, achieving up to 72.27% intelligibility
improvement and 2.61x inference speedup simultaneously on the test set.
Furthermore, we conduct comprehensive analysis to support it as a versatile
foundation for next-generation speech synthesis systems.

</details>


### [177] [Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and CQT Representations](https://arxiv.org/abs/2506.22237)
*Sebastian Murgul,Moritz Reiser,Michael Heizmann,Christoph Seibert*

Key words: 神经网络, MIDI-to-audio对齐, CRNN, 动态时间规整, 音乐信息检索

TL;DR: 论文提出了一种神经网络方法，用于同步人类钢琴演奏的音频录音与对应的松散对齐的MIDI文件，通过CRNN架构显著提升了对齐精度。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 解决音频与MIDI文件之间的松散对齐问题，提升同步精度，以满足音乐信息检索和演奏分析的需求。

Method: 采用卷积循环神经网络（CRNN）架构，通过处理未对齐的钢琴卷帘图和频谱图，估计对齐后的钢琴卷帘图。训练数据包括模拟人类演奏常见时间误差的增强MIDI文件。

Result: 模型比行业标准的动态时间规整（DTW）方法在高容忍窗口下对齐精度提升了20%，结合DTW后进一步提高了稳健性和一致性。

Conclusion: 神经网络方法在MIDI到音频对齐任务中具有显著潜力，能够提升现有技术水平。

Abstract: In this paper, we present a neural network approach for synchronizing audio
recordings of human piano performances with their corresponding loosely aligned
MIDI files. The task is addressed using a Convolutional Recurrent Neural
Network (CRNN) architecture, which effectively captures spectral and temporal
features by processing an unaligned piano roll and a spectrogram as inputs to
estimate the aligned piano roll. To train the network, we create a dataset of
piano pieces with augmented MIDI files that simulate common human timing
errors. The proposed model achieves up to 20% higher alignment accuracy than
the industry-standard Dynamic Time Warping (DTW) method across various
tolerance windows. Furthermore, integrating DTW with the CRNN yields additional
improvements, offering enhanced robustness and consistency. These findings
demonstrate the potential of neural networks in advancing state-of-the-art
MIDI-to-audio alignment.

</details>


### [178] [A Practical Approach to Power Saving in Hearables Using Sub-Nyquist Sampling with Bandwidth Extension](https://arxiv.org/abs/2506.22321)
*Tarikul Islam Tamiti,Anomadarshi Barua*

Key words: hearables, sub-Nyquist sampling, speech enhancement, low-power, GAN-like audio

TL;DR: 论文提出SUBARU方法，通过子奈奎斯特采样和低比特分辨率降低功耗，实现高效的多模态语音增强。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 现有方法未考虑低功耗实现，尤其是在采样频率和比特分辨率降低时对语音质量的影响。

Method: SUBARU采用子奈奎斯特采样和虚拟判别器，实现低功耗和高质量音频处理。

Result: 功耗降低3.31倍，推理时间1.74ms，内存占用小于13.77MB。

Conclusion: SUBARU为可穿戴设备提供了一种高效、低功耗的语音增强解决方案。

Abstract: Hearables are wearable computers that are worn on the ear. Bone conduction
microphones (BCMs) are used with air conduction microphones (ACMs) in hearables
as a supporting modality for multimodal speech enhancement (SE) in noisy
conditions. However, existing works don't consider the following practical
aspects for low-power implementations on hearables: (i) They do not explore how
lowering the sampling frequencies and bit resolutions in analog-to-digital
converters (ADCs) of hearables jointly impact low-power processing and
multimodal SE in terms of speech quality and intelligibility. (ii) They don't
discuss how GAN-like audio quality can be achieved without using actual GAN
discriminators. And (iii) They don't process signals from ACMs/BCMs at
sub-Nyquist sampling rate because, in their frameworks, they lack a wideband
reconstruction methodology from their narrowband parts. We propose SUBARU
(\textbf{Sub}-Nyquist \textbf{A}udio \textbf{R}esolution \textbf{U}psampling),
which achieves the following: SUBARU (i) intentionally uses sub-Nyquist
sampling and low bit resolution in ADCs, achieving a 3.31x reduction in power
consumption; (ii) introduces novel multi-scale and multi-period virtual
discriminators, which achieve GAN-like audio quality without using GANs'
adversarial training; and (iii) achieves streaming operations on mobile
platforms and SE in in-the-wild noisy conditions with an inference time of
1.74ms and a memory footprint of less than 13.77MB.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [179] [FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models](https://arxiv.org/abs/2506.21627)
*Shiyi Wang,Wenbo Li,Yiteng Chen,Qingyao Wu,Huiping Zhuang*

Key words: 机器人操作, VLM, 人脑启发, 功能集成, 操作效率

TL;DR: 提出了一个名为FrankenBot的VLM驱动的机器人操作框架，灵感来源于人脑的划分策略，实现了功能全面和高效操作。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 开发一个能在复杂动态环境中完成多种任务的通用机器人系统，需集成多种功能（如任务规划、策略生成等），但现有方法未将其统一。

Method: 采用分而治之策略，模仿人脑结构，将任务规划、策略生成等功能分配到类似大脑的区域，并设计高效协调机制。

Result: 实验表明，该方法在异常处理、长期记忆和操作效率上表现优异，且无需微调或重新训练。

Conclusion: FrankenBot框架成功实现功能全面性与高效性的平衡，为机器人系统设计提供了新思路。

Abstract: Developing a general robot manipulation system capable of performing a wide
range of tasks in complex, dynamic, and unstructured real-world environments
has long been a challenging task. It is widely recognized that achieving
human-like efficiency and robustness manipulation requires the robotic brain to
integrate a comprehensive set of functions, such as task planning, policy
generation, anomaly monitoring and handling, and long-term memory, achieving
high-efficiency operation across all functions. Vision-Language Models (VLMs),
pretrained on massive multimodal data, have acquired rich world knowledge,
exhibiting exceptional scene understanding and multimodal reasoning
capabilities. However, existing methods typically focus on realizing only a
single function or a subset of functions within the robotic brain, without
integrating them into a unified cognitive architecture. Inspired by a
divide-and-conquer strategy and the architecture of the human brain, we propose
FrankenBot, a VLM-driven, brain-morphic robotic manipulation framework that
achieves both comprehensive functionality and high operational efficiency. Our
framework includes a suite of components, decoupling a part of key functions
from frequent VLM calls, striking an optimal balance between functional
completeness and system efficiency. Specifically, we map task planning, policy
generation, memory management, and low-level interfacing to the cortex,
cerebellum, temporal lobe-hippocampus complex, and brainstem, respectively, and
design efficient coordination mechanisms for the modules. We conducted
comprehensive experiments in both simulation and real-world robotic
environments, demonstrating that our method offers significant advantages in
anomaly detection and handling, long-term memory, operational efficiency, and
stability -- all without requiring any fine-tuning or retraining.

</details>


### [180] [Ark: An Open-source Python-based Framework for Robot Learning](https://arxiv.org/abs/2506.21628)
*Magnus Dierking,Christopher E. Mower,Sarthak Das,Huang Helong,Jiacheng Qiu,Cody Reading,Wei Chen,Huidong Liang,Huang Guowei,Jan Peters,Quan Xingyue,Jun Wang,Haitham Bou-Ammar*

Key words: 机器人框架, Python, 开源, 仿生学习, 硬件集成

TL;DR: ARK是一个开源的、以Python为核心的机器人框架，旨在通过简化软件开发和硬件集成，加速自主机器人的研究和商业化部署。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 当前机器人软件栈需要高学习曲线、低级编程技能和复杂的硬件集成，阻碍了商业化自主机器人的发展。ARK旨在解决这一问题。

Method: ARK提供Gym风格的接口，支持数据收集、预处理和策略训练，结合仿真与物理机器人，配备轻量级客户端-服务器架构和C/C++绑定。

Result: ARK展示了快速原型设计、无缝硬件更换和完整的工作流，提升了机器人开发的便捷性。

Conclusion: ARK通过统一Python环境降低门槛，加速自主机器人的研究与商业化。

Abstract: Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics
Challenges to the first humanoid-robot kickboxing tournament-yet commercial
autonomy still lags behind progress in machine learning. A major bottleneck is
software: current robot stacks demand steep learning curves, low-level C/C++
expertise, fragmented tooling, and intricate hardware integration, in stark
contrast to the Python-centric, well-documented ecosystems that propelled
modern AI. We introduce ARK, an open-source, Python-first robotics framework
designed to close that gap. ARK presents a Gym-style environment interface that
allows users to collect data, preprocess it, and train policies using
state-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)
while seamlessly toggling between high-fidelity simulation and physical robots.
A lightweight client-server architecture provides networked
publisher-subscriber communication, and optional C/C++ bindings ensure
real-time performance when needed. ARK ships with reusable modules for control,
SLAM, motion planning, system identification, and visualization, along with
native ROS interoperability. Comprehensive documentation and case studies-from
manipulation to mobile navigation-demonstrate rapid prototyping, effortless
hardware swapping, and end-to-end pipelines that rival the convenience of
mainstream machine-learning workflows. By unifying robotics and AI practices
under a common Python umbrella, ARK lowers entry barriers and accelerates
research and commercial deployment of autonomous robots.

</details>


### [181] [AeroLite-MDNet: Lightweight Multi-task Deviation Detection Network for UAV Landing](https://arxiv.org/abs/2506.21635)
*Haiping Yang,Huaxing Liu,Wei Wu,Zuohui Chen,Ning Wu*

Key words: 无人机, 着陆偏差, 视觉模型, 多尺度融合, 数据集

TL;DR: 该论文提出了一种基于视觉的无人机着陆偏差预警系统AeroLite-MDNet，通过多尺度融合模块和分割分支提高着陆精度，并引入新评估指标AWD和新数据集UAVLandData，实验显示其偏差检测准确率达98.6%。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 无人机着陆时因GPS信号干扰等问题难以精准着陆，需一种可靠的偏差预警系统以确保安全。

Method: 提出AeroLite-MDNet模型，整合多尺度融合模块用于跨尺度目标检测，并加入分割分支以估计方向。

Result: 系统在实验中表现优异，平均警告延迟0.7秒，偏差检测准确率达98.6%。

Conclusion: AeroLite-MDNet系统能显著提升无人机着陆的可靠性，适用于实际应用。

Abstract: Unmanned aerial vehicles (UAVs) are increasingly employed in diverse
applications such as land surveying, material transport, and environmental
monitoring. Following missions like data collection or inspection, UAVs must
land safely at docking stations for storage or recharging, which is an
essential requirement for ensuring operational continuity. However, accurate
landing remains challenging due to factors like GPS signal interference. To
address this issue, we propose a deviation warning system for UAV landings,
powered by a novel vision-based model called AeroLite-MDNet. This model
integrates a multiscale fusion module for robust cross-scale object detection
and incorporates a segmentation branch for efficient orientation estimation. We
introduce a new evaluation metric, Average Warning Delay (AWD), to quantify the
system's sensitivity to landing deviations. Furthermore, we contribute a new
dataset, UAVLandData, which captures real-world landing deviation scenarios to
support training and evaluation. Experimental results show that our system
achieves an AWD of 0.7 seconds with a deviation detection accuracy of 98.6\%,
demonstrating its effectiveness in enhancing UAV landing reliability. Code will
be available at https://github.com/ITTTTTI/Maskyolo.git

</details>


### [182] [TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions](https://arxiv.org/abs/2506.21630)
*Yixin Sun,Li Li,Wenke E,Amir Atapour-Abarghouei,Toby P. Breckon*

Key words: 可通行路径检测；非结构化环境；多模态数据集；动态多尺度融合；光照条件

TL;DR: 该论文针对非结构化户外环境中可通行路径检测的挑战，提出了一个专门设计的TOMD数据集和一个动态多尺度数据融合模型。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决自主机器人在非结构化户外环境（如狭窄小径）中导航的问题，尤其是在搜索救援和森林火灾管理等关键应用中。

Method: 引入TOMD数据集，包含多模态传感器数据，并提出动态多尺度数据融合模型，评估了不同融合策略在多变光照条件下的性能。

Result: 实验结果显示该方法有效，并证明了光照条件对分割性能的重要影响。

Conclusion: TOMD数据集和动态多尺度融合模型为解决非结构化户外路径检测提供了有效工具。

Abstract: Detecting traversable pathways in unstructured outdoor environments remains a
significant challenge for autonomous robots, especially in critical
applications such as wide-area search and rescue, as well as incident
management scenarios like forest fires. Existing datasets and models primarily
target urban settings or wide, vehicle-traversable off-road tracks, leaving a
substantial gap in addressing the complexity of narrow, trail-like off-road
scenarios. To address this, we introduce the Trail-based Off-road Multimodal
Dataset (TOMD), a comprehensive dataset specifically designed for such
environments. TOMD features high-fidelity multimodal sensor data -- including
128-channel LiDAR, stereo imagery, GNSS, IMU, and illumination measurements --
collected through repeated traversals under diverse conditions. We also propose
a dynamic multiscale data fusion model for accurate traversable pathway
prediction. The study analyzes the performance of early, cross, and mixed
fusion strategies under varying illumination levels. Results demonstrate the
effectiveness of our approach and the relevance of illumination in segmentation
performance. We publicly release TOMD at https://github.com/yyyxs1125/TMOD to
support future research in trail-based off-road navigation.

</details>


### [183] [Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation](https://arxiv.org/abs/2506.21732)
*Ameya Salvi,Venkat Krovi*

Key words: 视觉导航、滑移转向车辆、端到端学习、模仿学习、深度强化学习

TL;DR: 本文提出了一种新颖的结构化视觉导航学习方法，改进了滑移转向车辆在动态操作中的性能。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 针对滑移转向车辆在实际部署中缺乏精确分析模型的问题，本文旨在通过结构化学习方法解决视觉导航的挑战。

Method: 采用端到端学习方法（如模仿学习和深度强化学习），并结合软件模拟和硬件评估进行验证。

Result: 通过大量实验和消融研究，证明了所提方法在性能上显著优于现有文献。

Conclusion: 本文的方法为解决滑移转向车辆在动态环境中的视觉导航问题提供了有效的解决方案。

Abstract: Vision-based lane keeping is a topic of significant interest in the robotics
and autonomous ground vehicles communities in various on-road and off-road
applications. The skid-steered vehicle architecture has served as a useful
vehicle platform for human controlled operations. However, systematic modeling,
especially of the skid-slip wheel terrain interactions (primarily in off-road
settings) has created bottlenecks for automation deployment. End-to-end
learning based methods such as imitation learning and deep reinforcement
learning, have gained prominence as a viable deployment option to counter the
lack of accurate analytical models. However, the systematic formulation and
subsequent verification/validation in dynamic operation regimes (particularly
for skid-steered vehicles) remains a work in progress. To this end, a novel
approach for structured formulation for learning visual navigation is proposed
and investigated in this work. Extensive software simulations, hardware
evaluations and ablation studies now highlight the significantly improved
performance of the proposed approach against contemporary literature.

</details>


### [184] [ASVSim (AirSim for Surface Vehicles): A High-Fidelity Simulation Framework for Autonomous Surface Vehicle Research](https://arxiv.org/abs/2506.22174)
*Bavo Lesy,Siemen Herremans,Robin Kerstens,Jan Steckel,Walter Daems,Siegfried Mercelis,Ali Anwar*

Key words: 无人水面车辆(USVs), 内河航运, 开源仿真框架, ASVSim, 自主导航

TL;DR: 论文介绍了ASVSim，一个专为内陆和港口环境设计的开源仿真框架，用于自主航运研究，填补了高保真仿真框架和数据集的空白。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 欧盟的Green Deal等倡议推动了内河航运的发展，同时人员短缺加速了自主解决方案的需求，但目前缺乏开源的高保真仿真框架和数据集。

Method: ASVSim基于Cosys-AirSim，结合了模拟船舶动力学和海洋传感器模拟能力（如雷达和摄像头），支持生成合成数据集，用于训练计算机视觉模型和强化学习代理。

Result: 通过有限实验验证了仿真器在传统控制方法和深度学习研究中的潜力。

Conclusion: ASVSim作为开源项目，为海洋工程社区提供了自主导航研究的全面平台。

Abstract: The transport industry has recently shown significant interest in unmanned
surface vehicles (USVs), specifically for port and inland waterway transport.
These systems can improve operational efficiency and safety, which is
especially relevant in the European Union, where initiatives such as the Green
Deal are driving a shift towards increased use of inland waterways. At the same
time, a shortage of qualified personnel is accelerating the adoption of
autonomous solutions. However, there is a notable lack of open-source,
high-fidelity simulation frameworks and datasets for developing and evaluating
such solutions. To address these challenges, we introduce AirSim For Surface
Vehicles (ASVSim), an open-source simulation framework specifically designed
for autonomous shipping research in inland and port environments. The framework
combines simulated vessel dynamics with marine sensor simulation capabilities,
including radar and camera systems and supports the generation of synthetic
datasets for training computer vision models and reinforcement learning agents.
Built upon Cosys-AirSim, ASVSim provides a comprehensive platform for
developing autonomous navigation algorithms and generating synthetic datasets.
The simulator supports research of both traditional control methods and deep
learning-based approaches. Through limited experiments, we demonstrate the
potential of the simulator in these research areas. ASVSim is provided as an
open-source project under the MIT license, making autonomous navigation
research accessible to a larger part of the ocean engineering community.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [185] [UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields](https://arxiv.org/abs/2506.21884)
*Fabian Perez,Sara Rojas,Carlos Hinojosa,Hoover Rueda-Chacón,Bernard Ghanem*

Key words: NeRF, 光谱解混, 材料分割, 无监督学习, 场景编辑

TL;DR: UnMix-NeRF通过将光谱解混引入NeRF，实现了光谱新视角合成和无监督材料分割，解决了现有方法在材料感知上的不足。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 现有NeRF分割方法依赖RGB数据，缺乏对材料属性的感知，限制了在机器人、增强现实等领域的应用。

Method: 模型通过全局终端成员表示纯材料特征，结合逐点丰度捕获分布，并利用光谱签名预测进行无监督材料聚类。

Result: 实验验证了该方法在光谱重建和材料分割上优于现有方法。

Conclusion: UnMix-NeRF有效提升了材料感知能力，并支持基于材料的场景编辑。

Abstract: Neural Radiance Field (NeRF)-based segmentation methods focus on object
semantics and rely solely on RGB data, lacking intrinsic material properties.
This limitation restricts accurate material perception, which is crucial for
robotics, augmented reality, simulation, and other applications. We introduce
UnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling
joint hyperspectral novel view synthesis and unsupervised material
segmentation. Our method models spectral reflectance via diffuse and specular
components, where a learned dictionary of global endmembers represents pure
material signatures, and per-point abundances capture their distribution. For
material segmentation, we use spectral signature predictions along learned
endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF
enables scene editing by modifying learned endmember dictionaries for flexible
material-based appearance manipulation. Extensive experiments validate our
approach, demonstrating superior spectral reconstruction and material
segmentation to existing methods. Project page:
https://www.factral.co/UnMix-NeRF.

</details>


### [186] [Dehazing Light Microscopy Images with Guided Conditional Flow Matching: finding a sweet spot between fidelity and realism](https://arxiv.org/abs/2506.22397)
*Anirban Ray,Ashesh,Florian Jug*

Key words: 荧光显微镜,图像去雾,保真度,真实感,条件流匹配

TL;DR: 该论文提出了一种名为HazeMatching的新方法，用于平衡荧光显微镜图像的保真度和真实感，适用于低成本显微镜的清晰图像生成。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 解决现有方法在图像去雾（dehazing）中无法同时兼顾数据保真度和真实感的问题。

Method: 通过改进条件流匹配框架，利用条件速度场指导生成过程，提出迭代式HazeMatching方法。

Result: 在5个数据集（含合成和真实数据）上验证，HazeMatching在保真度和真实感之间取得了平衡，且预测结果校准良好。

Conclusion: HazeMatching是一种无需显式退化算子的实用方法，适用于真实显微镜数据，代码和数据将公开。

Abstract: Fluorescence microscopy is a major driver of scientific progress in the life
sciences. Although high-end confocal microscopes are capable of filtering
out-of-focus light, cheaper and more accessible microscopy modalities, such as
widefield microscopy, can not, which consequently leads to hazy image data.
Computational dehazing is trying to combine the best of both worlds, leading to
cheap microscopy but crisp-looking images. The perception-distortion trade-off
tells us that we can optimize either for data fidelity, e.g. low MSE or high
PSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID.
Existing methods either prioritize fidelity at the expense of realism, or
produce perceptually convincing results that lack quantitative accuracy. In
this work, we propose HazeMatching, a novel iterative method for dehazing light
microscopy images, which effectively balances these objectives. Our goal was to
find a balanced trade-off between the fidelity of the dehazing results and the
realism of individual predictions (samples). We achieve this by adapting the
conditional flow matching framework by guiding the generative process with a
hazy observation in the conditional velocity field. We evaluate HazeMatching on
5 datasets, covering both synthetic and real data, assessing both distortion
and perceptual quality. Our method is compared against 7 baselines, achieving a
consistent balance between fidelity and realism on average. Additionally, with
calibration analysis, we show that HazeMatching produces well-calibrated
predictions. Note that our method does not need an explicit degradation
operator to exist, making it easily applicable on real microscopy data. All
data used for training and evaluation and our code will be publicly available
under a permissive license.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [187] [Exploring the change in scientific readability following the release of ChatGPT](https://arxiv.org/abs/2506.21825)
*Abdulkareem Alsudais*

Key words: arXiv, ChatGPT, 可读性分析, 科学写作, 大语言模型

TL;DR: 论文分析了arXiv.org上2010年至2024年6月7日所有摘要的可读性演变，发现自ChatGPT发布后，摘要的可读性显著下降，表明科学写作可能受到AI的影响。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 研究大语言模型（如ChatGPT）对科学写作和发表的影响，通过分析arXiv.org上摘要的可读性变化来验证这一假设。

Method: 使用四种标准可读性公式计算每篇论文的可读性分数，按年份和arXiv的八个主要学科分类进行聚合分析。

Result: 结果显示摘要的可读性逐年下降，且ChatGPT发布后（2023年至2024年）可读性变化显著，各学科均呈现类似趋势。

Conclusion: 科学摘要的可读性下降可能与AI工具的普及有关，表明AI正在影响科学写作风格。

Abstract: The rise and growing popularity of accessible large language models have
raised questions about their impact on various aspects of life, including how
scientists write and publish their research. The primary objective of this
paper is to analyze a dataset consisting of all abstracts posted on arXiv.org
between 2010 and June 7th, 2024, to assess the evolution of their readability
and determine whether significant shifts occurred following the release of
ChatGPT in November 2022. Four standard readability formulas are used to
calculate individual readability scores for each paper, classifying their level
of readability. These scores are then aggregated by year and across the eight
primary categories covered by the platform. The results show a steady annual
decrease in readability, suggesting that abstracts are likely becoming
increasingly complex. Additionally, following the release of ChatGPT, a
significant change in readability is observed for 2023 and the analyzed months
of 2024. Similar trends are found across categories, with most experiencing a
notable change in readability during 2023 and 2024. These findings offer
insights into the broader changes in readability and point to the likely
influence of AI on scientific writing.

</details>


### [188] [Hitchhiking Rides Dataset: Two decades of crowd-sourced records on stochastic traveling](https://arxiv.org/abs/2506.21946)
*Till Wenke*

Key words: 搭便车, 数据集, 时空分析, 社区贡献, 交通方式

TL;DR: 该论文通过分析hitchwiki.org和hitchmap.com平台收集的63,000多条搭便车数据，揭示了搭便车行为的时空特征和社区贡献模式。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 搭便车作为一种自发且分散的旅行方式，因其非正式性而难以系统研究。该研究旨在通过大规模数据集填补这一空白。

Method: 利用hitchwiki.org和hitchmap.com平台近20年收集的63,000多条搭便车数据，分析其时空分布、季节性模式及用户行为。

Result: 数据集揭示了欧洲为中心的分布、季节性模式及少数活跃贡献者的重要性。分析还展示了等待时间和用户行为的特征。

Conclusion: 尽管数据集存在偏差和局限性，但为研究搭便车提供了宝贵资源，并提出了未来研究方向。

Abstract: Hitchhiking, a spontaneous and decentralized mode of travel, has long eluded
systematic study due to its informal nature. This paper presents and analyzes
the largest known structured dataset of hitchhiking rides, comprising over
63,000 entries collected over nearly two decades through platforms associated
with hitchwiki.org and lately on hitchmap.com. By leveraging crowd-sourced
contributions, the dataset captures key spatiotemporal and strategic aspects of
hitchhiking. This work documents the dataset's origins, evolution, and
community-driven maintenance, highlighting its Europe-centric distribution,
seasonal patterns, and reliance on a small number of highly active
contributors. Through exploratory analyses, I examine waiting times, user
behavior, and comment metadata, shedding light on the lived realities of
hitchhikers. While the dataset has inherent biases and limitations - such as
demographic skew and unverifiable entries it offers a rare and valuable window
into an alternative form of mobility. I conclude by outlining future directions
for enriching the dataset and advancing research on hitchhiking as both a
transportation practice and cultural phenomenon.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [189] [ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment](https://arxiv.org/abs/2109.05721)
*Yangyu Huang,Hao Yang,Chong Li,Jongyoo Kim,Fangyun Wei*

Key words: 人脸对齐, 误差偏差, 各向异性方向损失, 各向异性注意力模块, ADNet

TL;DR: 本文研究了人脸对齐中的误差偏差问题，提出了各向异性方向损失（ADL）和各向异性注意力模块（AAM），并将其集成到ADNet模型中，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 发现人脸对齐中地标误差分布存在偏差（沿地标曲线切线方向扩散），这一问题与模糊的地标标注任务相关。

Method: 提出ADL（用于坐标回归）和AAM（用于热图回归），分别在地标边界法线方向施加强约束，同时在切线方向放松约束。

Result: 在300W、WFLW和COFW数据集上达到了最优性能。

Conclusion: ADL和AAM相辅相成，能够有效学习人脸结构和纹理细节，ADNet展现了鲁棒性和有效性。

Abstract: The recent progress of CNN has dramatically improved face alignment
performance. However, few works have paid attention to the error-bias with
respect to error distribution of facial landmarks. In this paper, we
investigate the error-bias issue in face alignment, where the distributions of
landmark errors tend to spread along the tangent line to landmark curves. This
error-bias is not trivial since it is closely connected to the ambiguous
landmark labeling task. Inspired by this observation, we seek a way to leverage
the error-bias property for better convergence of CNN model. To this end, we
propose anisotropic direction loss (ADL) and anisotropic attention module (AAM)
for coordinate and heatmap regression, respectively. ADL imposes strong binding
force in normal direction for each landmark point on facial boundaries. On the
other hand, AAM is an attention module which can get anisotropic attention mask
focusing on the region of point and its local edge connected by adjacent
points, it has a stronger response in tangent than in normal, which means
relaxed constraints in the tangent. These two methods work in a complementary
manner to learn both facial structures and texture details. Finally, we
integrate them into an optimized end-to-end training pipeline named ADNet. Our
ADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which
demonstrates the effectiveness and robustness.

</details>


### [190] [FreeEnricher: Enriching Face Landmarks without Additional Cost](https://arxiv.org/abs/2212.09525)
*Yangyu Huang,Xi Chen,Jongyoo Kim,Hao Yang,Chong Li,Jiaolong Yang,Dong Chen*

Key words: 面部对齐、密集关键点、弱监督学习、语义轮廓、300W、WFLW

TL;DR: 论文提出了一种通过稀疏关键点数据集（如300W和WFLW）增强面部关键点密度的框架，实现了在多个测试集上的最优精度。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有工作主要关注稀疏面部关键点对齐，但在美容医学和面部美化等场景中，密集关键点需求较高。

Method: 通过观察局部图像块在语义轮廓上的相似性，提出弱监督学习框架，利用稀疏关键点数据学习优化能力，并将其应用于密集关键点。设计了多个操作符以实现这一目标。

Result: 方法在新构建的密集300W测试集及原始稀疏300W和WFLW测试集上均达到最优精度，且无需额外成本。

Conclusion: 提出的框架能有效提升面部关键点密度，适用于多种场景，且表现优异。

Abstract: Recent years have witnessed significant growth of face alignment. Though
dense facial landmark is highly demanded in various scenarios, e.g., cosmetic
medicine and facial beautification, most works only consider sparse face
alignment. To address this problem, we present a framework that can enrich
landmark density by existing sparse landmark datasets, e.g., 300W with 68
points and WFLW with 98 points. Firstly, we observe that the local patches
along each semantic contour are highly similar in appearance. Then, we propose
a weakly-supervised idea of learning the refinement ability on original sparse
landmarks and adapting this ability to enriched dense landmarks. Meanwhile,
several operators are devised and organized together to implement the idea.
Finally, the trained model is applied as a plug-and-play module to the existing
face alignment networks. To evaluate our method, we manually label the dense
landmarks on 300W testset. Our method yields state-of-the-art accuracy not only
in newly-constructed dense 300W testset but also in the original sparse 300W
and WFLW testsets without additional cost.

</details>


### [191] [PEACE: Empowering Geologic Map Holistic Understanding with MLLMs](https://arxiv.org/abs/2501.06184)
*Yangyu Huang,Tianyi Gao,Haoran Xu,Qihao Zhao,Yang Song,Zhipeng Gui,Tengchao Lv,Hao Chen,Lei Cui,Scarlett Li,Furu Wei*

Key words: 地质图、多模态大语言模型、GeoMap-Bench、GeoMap-Agent、PEACE框架

TL;DR: 论文提出了GeoMap-Bench，首个用于评估多模态大语言模型（MLLMs）在地质图理解能力的基准测试，并开发了GeoMap-Agent以弥合这一领域的差距。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 地质图在地质学中具有重要意义，但现有MLLMs难以理解其复杂内容，亟需有效解决方案。

Method: 通过构建GeoMap-Bench基准测试，并开发GeoMap-Agent，包含HIE、DKI和PEQA模块，结合AI专家小组进行综合分析。

Result: GeoMap-Agent在GeoMap-Bench上得分0.811，显著优于GPT-4o的0.369，展示了其在地质图理解中的高效能力。

Conclusion: PEACE框架为地质学研究提供了高效准确的AI支持，推动了地质图理解的进展。

Abstract: Geologic map, as a fundamental diagram in geology science, provides critical
insights into the structure and composition of Earth's subsurface and surface.
These maps are indispensable in various fields, including disaster detection,
resource exploration, and civil engineering. Despite their significance,
current Multimodal Large Language Models (MLLMs) often fall short in geologic
map understanding. This gap is primarily due to the challenging nature of
cartographic generalization, which involves handling high-resolution map,
managing multiple associated components, and requiring domain-specific
knowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever
benchmark for evaluating MLLMs in geologic map understanding, which assesses
the full-scale abilities in extracting, referring, grounding, reasoning, and
analyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent
designed for geologic map understanding, which features three modules:
Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),
and Prompt-enhanced Question Answering (PEQA). Inspired by the
interdisciplinary collaboration among human scientists, an AI expert group acts
as consultants, utilizing a diverse tool pool to comprehensively analyze
questions. Through comprehensive experiments, GeoMap-Agent achieves an overall
score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.
Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,
paves the way for advanced AI applications in geology, enhancing the efficiency
and accuracy of geological investigations.

</details>


### [192] [Elucidating and Endowing the Diffusion Training Paradigm for General Image Restoration](https://arxiv.org/abs/2506.21722)
*Xin Lu,Xueyang Fu,Jie Xiao,Zihao Fan,Yurui Zhu,Zheng-Jun Zha*

Key words: 扩散模型,图像恢复,训练范式,多任务统一,正则化策略

TL;DR: 该论文探讨了如何将扩散训练范式整合到通用图像恢复（IR）框架中，通过系统分析时间步长依赖等关键因素，提出了新的IR框架，并引入正则化策略和多任务适配器，显著提升了单任务和多任务的恢复性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 扩散模型在图像恢复任务中表现出强大的生成能力，但其复杂架构和迭代过程限制了其实用性。现有方法关注网络架构和扩散路径优化，但忽视了扩散训练范式与通用IR框架的整合。

Method: 通过分析时间步长依赖、网络层次等因素，提出新的IR框架；引入正则化策略对齐扩散目标和IR任务；开发增量训练范式和任务特定适配器优化多任务统一IR。

Result: 实验表明，该方法显著提升了单任务IR的泛化能力，并在多任务统一IR中表现出色，且能无缝集成到现有通用IR架构中。

Conclusion: 该研究成功将扩散训练范式整合到通用IR框架中，不仅提升了单任务性能，还通过增量训练和适配器优化了多任务统一IR。

Abstract: While diffusion models demonstrate strong generative capabilities in image
restoration (IR) tasks, their complex architectures and iterative processes
limit their practical application compared to mainstream reconstruction-based
general ordinary IR networks. Existing approaches primarily focus on optimizing
network architecture and diffusion paths but overlook the integration of the
diffusion training paradigm within general ordinary IR frameworks. To address
these challenges, this paper elucidates key principles for adapting the
diffusion training paradigm to general IR training through systematic analysis
of time-step dependencies, network hierarchies, noise-level relationships, and
multi-restoration task correlations, proposing a new IR framework supported by
diffusion-based training. To enable IR networks to simultaneously restore
images and model generative representations, we introduce a series of
regularization strategies that align diffusion objectives with IR tasks,
improving generalization in single-task scenarios. Furthermore, recognizing
that diffusion-based generation exerts varying influences across different IR
tasks, we develop an incremental training paradigm and task-specific adaptors,
further enhancing performance in multi-task unified IR. Experiments demonstrate
that our method significantly improves the generalization of IR networks in
single-task IR and achieves superior performance in multi-task unified IR.
Notably, the proposed framework can be seamlessly integrated into existing
general IR architectures.

</details>


### [193] [TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation](https://arxiv.org/abs/2506.21681)
*Hakan Çapuk,Andrew Bond,Muhammed Burak Kızıl,Emir Göçen,Erkut Erdem,Aykut Erdem*

Key words: 全景图像生成, 扩散模型, 几何失真, 循环一致性, TanDiT

TL;DR: TanDiT是一种通过生成切面图像网格来合成全景场景的方法，解决了现有模型中几何失真和循环一致性问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 全景图像生成面临几何失真和循环一致性的挑战，现有方法难以应对。

Method: 采用统一的扩散模型（TanDiT），单次去噪迭代生成切面图像，并结合模型无关的后处理步骤增强全局一致性。

Result: 实验表明，TanDiT能有效泛化训练数据，处理复杂文本提示，生成高质量、多样化的全景图像。

Conclusion: TanDiT为解决全景图像生成的挑战提供了一种有效方法，并具备良好的泛化能力和整合性。

Abstract: Recent advances in image generation have led to remarkable improvements in
synthesizing perspective images. However, these models still struggle with
panoramic image generation due to unique challenges, including varying levels
of geometric distortion and the requirement for seamless loop-consistency. To
address these issues while leveraging the strengths of the existing models, we
introduce TanDiT, a method that synthesizes panoramic scenes by generating
grids of tangent-plane images covering the entire 360$^\circ$ view. Unlike
previous methods relying on multiple diffusion branches, TanDiT utilizes a
unified diffusion model trained to produce these tangent-plane images
simultaneously within a single denoising iteration. Furthermore, we propose a
model-agnostic post-processing step specifically designed to enhance global
coherence across the generated panoramas. To accurately assess panoramic image
quality, we also present two specialized metrics, TangentIS and TangentFID, and
provide a comprehensive benchmark comprising captioned panoramic datasets and
standardized evaluation scripts. Extensive experiments demonstrate that our
method generalizes effectively beyond its training data, robustly interprets
detailed and complex text prompts, and seamlessly integrates with various
generative models to yield high-quality, diverse panoramic images.

</details>


### [194] [Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis](https://arxiv.org/abs/2506.21731)
*Chenqiu Zhao,Anup Basu*

Key words: 概率生成模型、VAE、MESP、BL-AE、ARVM、LCH、FID分数

TL;DR: 该论文提出了两种理论框架（MESP和LCH）以研究概率生成模型的潜在局限，即学习全局分布会导致记忆而非生成行为。通过重新思考VAE，提出了MESP和BL-AE模型，并设计ARVM实现了竞争性FID分数。最后提出LCH假说，强调生成能力源自局部相关性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探索概率生成模型中全局分布学习可能导致记忆而非真正的生成行为，并提出解决方案以提升生成模型的性能。

Method: 1. 提出MESP框架，基于VAE的潜在变量重叠问题；2. 设计BL-AE将图像编码为二元潜在表示；3. 提出ARVM模型输出直方图；4. 提出LCH假说，关注局部相关性。

Result: ARVM在标准数据集上实现了竞争性FID分数，但发现高分可能反映记忆而非生成行为。通过LCH假说进一步验证了生成能力的来源。

Conclusion: 全局分布学习可能导致记忆问题，而局部相关性（LCH）是生成能力的关键。未来工作可基于此优化生成模型。

Abstract: We propose two theoretical frameworks, the Mutually Exclusive Probability
Space (MESP) and the Local Correlation Hypothesis (LCH), to explore a potential
limitation in probabilistic generative models; namely that learning global
distributions leads to memorization rather than generative behavior. MESP
emerges from our rethinking of the Variational Autoencoder (VAE). We observe
that latent variable distributions in VAE exhibit overlap, which leads to an
optimization conflict between the reconstruction loss and KL-divergence loss. A
lower bound based on the overlap coefficient is proposed. We refer to this
phenomenon as Mutually Exclusive Probability Spaces. Based on MESP, a Binary
Latent Autoencoder (BL-AE) is proposed to encode images into binary latent
representations. These binary latents are used as the input to our
Autoregressive Random Variable Model (ARVM), a modified autoregressive model
outputting histograms. Our ARVM achieves competitive FID scores, outperforming
state-of-the-art methods on standard datasets. However, such scores reflect
memorization rather than generation. To address this issue, we propose the
Local Correlation Hypothesis (LCH), which posits that generative capability
arising from local correlations among latent variables. Comprehensive
experiments and discussions are conducted to validate our frameworks.

</details>


### [195] [Comparing Learning Paradigms for Egocentric Video Summarization](https://arxiv.org/abs/2506.21785)
*Daniel Wen*

Key words: 计算机视觉, egocentric视频, 监督学习, 无监督学习, 提示微调

TL;DR: 本研究评估了监督学习、无监督学习和提示微调在egocentric视频数据上的表现，发现当前先进模型在第三人称视频上表现更佳，而GPT-4o在提示微调后表现优于专用模型，呼吁进一步研究以提升egocentric视频处理能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探索计算机视觉技术在egocentric视频数据上的应用潜力，填补当前模型在第三人称视频表现上的不足，推动该领域的发展。

Method: 比较三种范式：监督学习的Shotluck Holmes、无监督学习的TAC-SUM和提示微调的GPT-4o，评估其在Ego-Exo4D数据集中的视频摘要任务表现。

Result: 发现当前先进模型在egocentric视频上效果较差，而GPT-4o表现更优，揭示了现有方法的局限性。

Conclusion: egocentric视频处理需要进一步研究，提示微调的通用模型可能是一个有前景的方向。

Abstract: In this study, we investigate various computer vision paradigms - supervised
learning, unsupervised learning, and prompt fine-tuning - by assessing their
ability to understand and interpret egocentric video data. Specifically, we
examine Shotluck Holmes (state-of-the-art supervised learning), TAC-SUM
(state-of-the-art unsupervised learning), and GPT-4o (a prompt fine-tuned
pre-trained model), evaluating their effectiveness in video summarization. Our
results demonstrate that current state-of-the-art models perform less
effectively on first-person videos compared to third-person videos,
highlighting the need for further advancements in the egocentric video domain.
Notably, a prompt fine-tuned general-purpose GPT-4o model outperforms these
specialized models, emphasizing the limitations of existing approaches in
adapting to the unique challenges of first-person perspectives. Although our
evaluation is conducted on a small subset of egocentric videos from the
Ego-Exo4D dataset due to resource constraints, the primary objective of this
research is to provide a comprehensive proof-of-concept analysis aimed at
advancing the application of computer vision techniques to first-person videos.
By exploring novel methodologies and evaluating their potential, we aim to
contribute to the ongoing development of models capable of effectively
processing and interpreting egocentric perspectives.

</details>


### [196] [CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery](https://arxiv.org/abs/2506.21813)
*Felix Holm,Gözde Ünver,Ghazal Ghazaei,Nassir Navab*

Key words: 白内障手术、场景图、工具组织交互、AI驱动、手术分析

TL;DR: 该论文介绍了首个白内障手术场景图数据集CAT-SG，通过结构化注释工具与组织交互、程序变化和时间依赖关系，提供手术工作流的全面视图。同时提出新模型CatSGG，性能优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有数据集仅关注手术分析的孤立方面，缺乏捕捉实体间语义关系的综合表示，难以全面理解手术流程。

Method: 基于CAT-SG数据集，提出新场景图生成模型CatSGG，结合工具组织交互和时间依赖关系。

Result: CAT-SG数据集和CatSGG模型能够更准确地识别手术阶段和技术，提升AI驱动的临床应用。

Conclusion: CAT-SG和CatSGG为智能、上下文感知的手术系统奠定了基础。

Abstract: Understanding the intricate workflows of cataract surgery requires modeling
complex interactions between surgical tools, anatomical structures, and
procedural techniques. Existing datasets primarily address isolated aspects of
surgical analysis, such as tool detection or phase segmentation, but lack
comprehensive representations that capture the semantic relationships between
entities over time. This paper introduces the Cataract Surgery Scene Graph
(CAT-SG) dataset, the first to provide structured annotations of tool-tissue
interactions, procedural variations, and temporal dependencies. By
incorporating detailed semantic relations, CAT-SG offers a holistic view of
surgical workflows, enabling more accurate recognition of surgical phases and
techniques. Additionally, we present a novel scene graph generation model,
CatSGG, which outperforms current methods in generating structured surgical
representations. The CAT-SG dataset is designed to enhance AI-driven surgical
training, real-time decision support, and workflow analysis, paving the way for
more intelligent, context-aware systems in clinical practice.

</details>


### [197] [Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images](https://arxiv.org/abs/2506.21770)
*Rishiraj Paul Chowdhury,Nirmit Shekar Karkera*

Key words: 青光眼检测, 深度学习, EfficientNet-B0, 多数据集训练, 预处理简化

TL;DR: 提出了一种基于EfficientNet-B0的深度学习流水线，用于从视网膜图像中检测青光眼，通过多数据集训练提升了泛化能力，简化预处理表现更优。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 青光眼是导致不可逆失明的主要原因，传统诊断方法具侵入性且需专业设备，深度学习可提供非侵入性解决方案。

Method: 使用EfficientNet-B0架构，在ACRIMA、ORIGA和RIM-ONE数据集上顺序训练和微调，简化预处理步骤。

Result: 模型在未见数据集上表现出强判别性能，简化预处理比复杂增强方法获得更高的AUC-ROC。

Conclusion: 该流水线为青光眼早期检测提供了可重复和可扩展的方法，具有临床潜力。

Abstract: Glaucoma is a leading cause of irreversible blindness, but early detection
can significantly improve treatment outcomes. Traditional diagnostic methods
are often invasive and require specialized equipment. In this work, we present
a deep learning pipeline using the EfficientNet-B0 architecture for glaucoma
detection from retinal fundus images. Unlike prior studies that rely on single
datasets, we sequentially train and fine-tune our model across ACRIMA, ORIGA,
and RIM-ONE datasets to enhance generalization. Our experiments show that
minimal preprocessing yields higher AUC-ROC compared to more complex
enhancements, and our model demonstrates strong discriminative performance on
unseen datasets. The proposed pipeline offers a reproducible and scalable
approach to early glaucoma detection, supporting its potential clinical
utility.

</details>


### [198] [Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models](https://arxiv.org/abs/2506.21826)
*Rafael Sterzinger,Marco Peer,Robert Sablatnig*

Key words: 历史地图，少样本分割，语义嵌入，参数高效微调，自动化处理

TL;DR: 提出了一种基于大型视觉基础模型和参数高效微调的历史地图少样本分割方法，显著提升了分割性能并减少了人工标注需求。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 历史地图作为丰富的历史资源，其多样化的视觉表现和有限的标注数据给自动化处理带来挑战。

Method: 利用大型视觉基础模型的丰富语义嵌入，结合参数高效微调，进行历史地图的少样本分割。

Result: 在Siegfried基准数据集上，葡萄园和铁路分割的mIoU分别提升了5%和13%，在5-shot设置下提升约20%；在ICDAR 2021竞赛数据集上，建筑块分割的PQ达到67.3%。

Conclusion: 该方法在极低数据量（10-shot和5-shot）下表现优异，仅需0.21%的可训练参数，显著减少了人工标注需求，推动了历史地图自动化处理的发展。

Abstract: As rich sources of history, maps provide crucial insights into historical
changes, yet their diverse visual representations and limited annotated data
pose significant challenges for automated processing. We propose a simple yet
effective approach for few-shot segmentation of historical maps, leveraging the
rich semantic embeddings of large vision foundation models combined with
parameter-efficient fine-tuning. Our method outperforms the state-of-the-art on
the Siegfried benchmark dataset in vineyard and railway segmentation, achieving
+5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20%
in the more challenging 5-shot setting. Additionally, it demonstrates strong
performance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3%
for building block segmentation, despite not being optimized for this
shape-sensitive metric, underscoring its generalizability. Notably, our
approach maintains high performance even in extremely low-data regimes (10- &
5-shot), while requiring only 689k trainable parameters - just 0.21% of the
total model size. Our approach enables precise segmentation of diverse
historical maps while drastically reducing the need for manual annotations,
advancing automated processing and analysis in the field. Our implementation is
publicly available at:
https://github.com/RafaelSterzinger/few-shot-map-segmentation.

</details>


### [199] [SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space](https://arxiv.org/abs/2506.21857)
*Ekaterina Redekop,Mara Pleasure,Zichen Wang,Kimberly Flores,Anthony Sisk,William Speier,Corey W. Arnold*

Key words: 数字病理学、空间转录组学、基础模型、对比学习、混合专家

TL;DR: SPADE是一种基于数字病理学和空间转录组学数据的基础模型，通过混合专家技术和对比学习整合形态学和分子信息，显著优于基准模型。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法未能全面整合WSI和空间转录组学数据，SPADE旨在填补这一空白，捕捉分子异质性。

Method: 采用混合专家技术和两阶段特征空间聚类，通过对比学习构建ST-informed的潜在空间。

Result: 在14个下游任务中表现显著优于基准模型，展示了整合形态和分子信息的优势。

Conclusion: SPADE成功整合了病理学和分子数据，为多模态基础模型提供了新方向。

Abstract: The rapid growth of digital pathology and advances in self-supervised deep
learning have enabled the development of foundational models for various
pathology tasks across diverse diseases. While multimodal approaches
integrating diverse data sources have emerged, a critical gap remains in the
comprehensive integration of whole-slide images (WSIs) with spatial
transcriptomics (ST), which is crucial for capturing critical molecular
heterogeneity beyond standard hematoxylin & eosin (H&E) staining. We introduce
SPADE, a foundation model that integrates histopathology with ST data to guide
image representation learning within a unified framework, in effect creating an
ST-informed latent space. SPADE leverages a mixture-of-data experts technique,
where experts, created via two-stage feature-space clustering, use contrastive
learning to learn representations of co-registered WSI patches and gene
expression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is
evaluated on 14 downstream tasks, demonstrating significantly superior few-shot
performance compared to baseline models, highlighting the benefits of
integrating morphological and molecular information into one latent space.

</details>


### [200] [LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs](https://arxiv.org/abs/2506.21862)
*Boyuan Sun,Jiaxing Zhao,Xihan Wei,Qibin Hou*

Key words: 令牌压缩, 多模态语言模型, 语义连接组件, 视频理解

TL;DR: LLaVA-Scissor是一种无需训练的令牌压缩策略，专为视频多模态大语言模型设计，通过语义连接组件（SCC）实现全面语义覆盖和高效令牌压缩，在多个视频理解任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统基于注意力得分的令牌压缩方法无法有效捕捉所有语义区域且常导致冗余，因此提出一种基于SCC的新方法以解决这一问题。

Method: 采用语义连接组件（SCC）方法，将令牌分配到不同的语义区域，并提出两步时空令牌压缩策略，实现非重叠语义令牌的高效压缩。

Result: 在视频问答、长视频理解和多选择任务等测试中，LLaVA-Scissor优于其他令牌压缩方法，尤其在低令牌保留比下表现更佳。

Conclusion: LLaVA-Scissor通过SCC实现了高效令牌压缩，显著提升了视频多模态理解的性能。

Abstract: In this paper, we present LLaVA-Scissor, a training-free token compression
strategy designed for video multimodal large language models. Previous methods
mostly attempt to compress tokens based on attention scores, but fail to
effectively capture all semantic regions and often lead to token redundancy.
Differently, we propose to leverage the Semantic Connected Components (SCC)
approach that assigns tokens to distinct semantic regions within the token set,
ensuring comprehensive semantic coverage. The outcome is a two-step
spatio-temporal token compression strategy that utilizes SCC in both spatial
and temporal domains. This strategy can effectively compress tokens by
representing the entire video with a set of non-overlapping semantic tokens. We
conduct extensive evaluations of the token compression capabilities of
LLaVA-Scissor across diverse video understanding benchmarks, including video
question answering, long video understanding, and comprehensive multi-choices
benchmarks. Experimental results show that the proposed LLaVA-Scissor
outperforms other token compression methods, achieving superior performance in
various video understanding benchmarks, particularly at low token retention
ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.

</details>


### [201] [Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning](https://arxiv.org/abs/2506.21873)
*Tzu-Chun Chien,Chieh-Kai Lin,Shiang-Feng Tsai,Ruei-Chi Lai,Hung-Jen Chen,Min Sun*

Key words: 多模态大语言模型,视觉定位,剪枝方法,位置ID

TL;DR: 多模态大语言模型（MLLMs）在视觉定位中表现优异，但剪枝方法会显著降低其定位能力。作者提出了一种基于位置ID调整的Grounding-Aware Token Pruning（GAP）方法，以恢复性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决剪枝方法在多模态大语言模型中导致的视觉定位能力下降问题。

Method: 提出Grounding-Aware Token Pruning（GAP）方法，通过调整位置ID恢复剪枝后的性能。

Result: GAP方法在RefCOCO验证集上将LLaVA的准确率从剪枝后的15.34%提升至51.42%，接近未剪枝时的90%性能。

Conclusion: GAP方法简单有效，无需额外训练或计算开销，能在多种剪枝策略下提升模型的视觉定位能力。

Abstract: Recent Multimodal Large Language Models (MLLMs) have demonstrated strong
performance in visual grounding, establishing themselves as a general interface
for various vision-language applications. This progress has driven the
development of token pruning methods to mitigate the high computational costs
associated with processing numerous visual tokens. However, we observe that
pruning significantly weakens the model's grounding ability, leading to
incorrect predictions and drastic performance degradation. In Referring
Expression Comprehension (REC), for instance, pruning causes the accuracy of
LLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis
identifies misaligned position IDs after pruning as the primary cause of this
degradation, as both the order and value of these IDs are crucial for
maintaining performance in grounding tasks. To address this issue, we propose
Grounding-Aware Token Pruning (GAP), a simple yet effective adjustment to
position IDs that recovers REC accuracy back to 51.42%, which is 90% of the
original performance in the without pruning setting, all while requiring no
additional training, memory, or computational overhead. Applied to models such
as Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves
performance across various token pruning strategies.

</details>


### [202] [Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs](https://arxiv.org/abs/2506.21656)
*Yifan Shen,Yuanzhe Liu,Jingyuan Zhu,Xu Cao,Xiaofeng Zhang,Yixiao He,Wenming Ye,James Matthew Rehg,Ismini Lourentzou*

Key words: 视觉语言模型,空间推理,LongCoT,fDPO,M3CTS

TL;DR: 论文提出了SpatialReasoner-R1模型，通过M3CTS生成高质量空间推理数据，并结合fDPO方法提升细粒度空间推理能力，在SPATIALRGPT-Bench上达到新SoTA。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决现有视觉语言模型在细粒度空间推理和多步逻辑对齐方面的不足。

Method: 使用M3CTS生成多样化的LongCoT推理轨迹，提出fDPO方法，结合空间奖励机制优化描述性接地和逻辑推理。

Result: fDPO在空间质量任务中比标准DPO提升4.1%，在空间数量任务中提升9.0%。SpatialReasoner-R1在SPATIALRGPT-Bench上平均准确率领先基线9.8%。

Conclusion: SpatialReasoner-R1通过结合M3CTS和fDPO，显著提升了细粒度空间推理能力，同时保持通用视觉语言任务的竞争力。

Abstract: Current Vision-Language Models (VLMs) struggle with fine-grained spatial
reasoning, particularly when multi-step logic and precise spatial alignment are
required. In this work, we introduce SpatialReasoner-R1, a vision-language
reasoning model designed to address these limitations. To construct
high-quality supervision for spatial reasoning, we design a Multi-Model Monte
Carlo Tree Search (M3CTS) method that generates diverse, logically consistent
Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose
fine-grained Direct Preference Optimization (fDPO), which introduces
segment-specific preference granularity for descriptive grounding and logical
reasoning, guided by a spatial reward mechanism that evaluates candidate
responses based on visual consistency, spatial grounding, and logical
coherence. Experimental results demonstrate that fDPO achieves an average
improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%
gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a
new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in
average accuracy, while maintaining competitive performance on general
vision-language tasks.

</details>


### [203] [SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation](https://arxiv.org/abs/2506.21892)
*Adam Goodge,Xun Xu,Bryan Hooi,Wee Siong Ng,Jingyi Liao,Yongyi Su,Xulei Yang*

Key words: 点云数据, 离群检测, 3D视觉语言模型, 域偏移, SODA

TL;DR: 该论文提出了一种名为SODA的新方法，用于改进点云数据中离群目标的检测，通过邻域分数传播方案解决合成数据与实际数据之间的域偏移问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 随着点云数据在多种应用中的普及，检测离群点云目标对模型的安全性和可靠性至关重要，但现有研究对此问题探索不足。

Method: 利用3D视觉语言模型(3D VLMs)的进展，提出SODA方法，通过邻域分数传播改进离群点云检测。

Result: SODA无需额外训练，在多种数据集和问题设置中表现优于现有方法。

Conclusion: SODA方法有效解决了合成-现实域偏移问题，提升了离群点云的检测性能。

Abstract: As point cloud data increases in prevalence in a variety of applications, the
ability to detect out-of-distribution (OOD) point cloud objects becomes
critical for ensuring model safety and reliability. However, this problem
remains under-explored in existing research. Inspired by success in the image
domain, we propose to exploit advances in 3D vision-language models (3D VLMs)
for OOD detection in point cloud objects. However, a major challenge is that
point cloud datasets used to pre-train 3D VLMs are drastically smaller in size
and object diversity than their image-based counterparts. Critically, they
often contain exclusively computer-designed synthetic objects. This leads to a
substantial domain shift when the model is transferred to practical tasks
involving real objects scanned from the physical environment. In this paper,
our empirical experiments show that synthetic-to-real domain shift
significantly degrades the alignment of point cloud with their associated text
embeddings in the 3D VLM latent space, hindering downstream performance. To
address this, we propose a novel methodology called SODA which improves the
detection of OOD point clouds through a neighborhood-based score propagation
scheme. SODA is inference-based, requires no additional model training, and
achieves state-of-the-art performance over existing approaches across datasets
and problem settings.

</details>


### [204] [SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images](https://arxiv.org/abs/2506.21945)
*Naftaly Wambugu,Ruisheng Wang,Bo Guo,Tianshu Yu,Sheng Xu,Mohammed Elhassan*

Key words: 语义分割；高分辨率遥感图像；堆叠深度残差网络；DCNN

TL;DR: 该论文提出了一种堆叠深度残差网络（SDRNet），用于从高分辨率遥感图像中进行语义分割，解决现有方法在对象大小差异和遮挡等问题上的不足。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 高分辨率遥感图像的语义分割面临类别差异、遮挡和对象大小变化的挑战，现有深度卷积神经网络难以提取足够特征。

Method: 设计了包含两个堆叠编码器-解码器网络和扩张残差块的框架，以捕捉全局依赖关系并保留空间信息。

Result: 在ISPRS Vaihingen和Potsdam数据集上的实验表明，SDRNet表现优于当前其他DCNNs方法。

Conclusion: SDRNet通过多上下文特征学习和全局-局部环境利用，有效提升了语义分割性能。

Abstract: Land cover maps generated from semantic segmentation of high-resolution
remotely sensed images have drawn mucon in the photogrammetry and remote
sensing research community. Currently, massive fine-resolution remotely sensed
(FRRS) images acquired by improving sensing and imaging technologies become
available. However, accurate semantic segmentation of such FRRS images is
greatly affected by substantial class disparities, the invisibility of key
ground objects due to occlusion, and object size variation. Despite the
extraordinary potential in deep convolutional neural networks (DCNNs) in image
feature learning and representation, extracting sufficient features from FRRS
images for accurate semantic segmentation is still challenging. These
challenges demand the deep learning models to learn robust features and
generate sufficient feature descriptors. Specifically, learning
multi-contextual features to guarantee adequate coverage of varied object sizes
from the ground scene and harnessing global-local contexts to overcome class
disparities challenge even profound networks. Deeper networks significantly
lose spatial details due to gradual downsampling processes resulting in poor
segmentation results and coarse boundaries. This article presents a stacked
deep residual network (SDRNet) for semantic segmentation from FRRS images. The
proposed framework utilizes two stacked encoder-decoder networks to harness
long-range semantics yet preserve spatial information and dilated residual
blocks (DRB) between each encoder and decoder network to capture sufficient
global dependencies thus improving segmentation performance. Our experimental
results obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate
that the SDRNet performs effectively and competitively against current DCNNs in
semantic segmentation.

</details>


### [205] [GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles](https://arxiv.org/abs/2506.21839)
*Mengyi Shan,Brian Curless,Ira Kemelmacher-Shlizerman,Steve Seitz*

Key words: 文本到图像模型, 多智能体框架, 密室逃脱, 场景图推理, 功能设计

TL;DR: 本文提出了一种分层多智能体框架，用于生成视觉吸引人、逻辑严密且智力挑战的密室逃脱谜题图像，解决了基础图像模型在空间关系和功能推理上的不足。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 基础图像模型在生成密室逃脱谜题图像时表现不佳，尤其是在空间关系和功能推理方面，因此需要一种更有效的解决方案。

Method: 采用分层多智能体框架，将任务分解为功能设计、符号场景图推理、布局合成和局部图像编辑等阶段，通过智能体协作和迭代反馈完成。

Result: 实验表明，智能体协作显著提升了生成图像的可解性、捷径避免和功能清晰度，同时保持了视觉质量。

Conclusion: 多智能体框架能够有效解决密室逃脱谜题图像生成的挑战，是一种有前途的方法。

Abstract: We challenge text-to-image models with generating escape room puzzle images
that are visually appealing, logically solid, and intellectually stimulating.
While base image models struggle with spatial relationships and affordance
reasoning, we propose a hierarchical multi-agent framework that decomposes this
task into structured stages: functional design, symbolic scene graph reasoning,
layout synthesis, and local image editing. Specialized agents collaborate
through iterative feedback to ensure the scene is visually coherent and
functionally solvable. Experiments show that agent collaboration improves
output quality in terms of solvability, shortcut avoidance, and affordance
clarity, while maintaining visual quality.

</details>


### [206] [Tied Prototype Model for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2506.22101)
*Hyeongji Kim,Stine Hansen,Michael Kampffmeyer*

Key words: 医学图像分割，少样本学习，原型绑定模型，自适应阈值

TL;DR: 论文提出了一种新的原型绑定模型（TPM），通过绑定前景和背景分布的原型位置，改进了现有的ADNet方法，解决了其单原型、二元分类和固定阈值的局限性。TPM支持多原型和多类分割，并利用自然出现的类别先验定义自适应阈值，提升了分割精度。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有ADNet方法在医学图像少样本分割中存在单原型、二元分类固定阈值的局限性，无法适应患者和器官的变异性。

Method: 提出TPM模型，绑定前景和背景分布的原型位置，支持多原型和多类分割，并利用类别先验定义自适应阈值。

Result: TPM显著提升了分割精度，尤其在多类和自适应阈值场景下表现更优。

Conclusion: TPM为医学图像少样本分割提供了一种新视角，代码已开源。

Abstract: Common prototype-based medical image few-shot segmentation (FSS) methods
model foreground and background classes using class-specific prototypes.
However, given the high variability of the background, a more promising
direction is to focus solely on foreground modeling, treating the background as
an anomaly -- an approach introduced by ADNet. Yet, ADNet faces three key
limitations: dependence on a single prototype per class, a focus on binary
classification, and fixed thresholds that fail to adapt to patient and organ
variability. To address these shortcomings, we propose the Tied Prototype Model
(TPM), a principled reformulation of ADNet with tied prototype locations for
foreground and background distributions. Building on its probabilistic
foundation, TPM naturally extends to multiple prototypes and multi-class
segmentation while effectively separating non-typical background features.
Notably, both extensions lead to improved segmentation accuracy. Finally, we
leverage naturally occurring class priors to define an ideal target for
adaptive thresholds, boosting segmentation performance. Taken together, TPM
provides a fresh perspective on prototype-based FSS for medical image
segmentation. The code can be found at https://github.com/hjk92g/TPM-FSS.

</details>


### [207] [Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs](https://arxiv.org/abs/2506.22146)
*Amirmohammad Izadi,Mohammad Ali Banayeeanzade,Fatemeh Askari,Ali Rahimiakbar,Mohammad Mahdi Vahedi,Hosein Hasani,Mahdieh Soleymani Baghshah*

Key words: 视觉语言模型, 视觉推理, 空间结构, 绑定问题, 序列注意力

TL;DR: 论文提出了一种通过增强视觉输入的空间结构（如水平线）并结合文本提示来提升视觉语言模型（VLM）在视觉推理任务中表现的方法，显著提高了计数、视觉搜索等任务的准确性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前视觉语言模型在处理视觉特征时缺乏空间基础和序列注意力机制，导致在计数、视觉搜索等任务中出现错误。论文旨在解决这一问题。

Method: 在视觉输入中添加低层次空间结构（如水平线），并配合鼓励序列化和空间感知解析的文本提示。

Result: 该方法显著提升了性能：GPT-4o的视觉搜索准确性提高25.00%，计数准确性提高26.83%，场景描述错误减少0.32，空间关系任务性能提升9.50%。

Conclusion: 低层次视觉结构是提升视觉推理任务表现的有效且未被充分探索的方向，其重要性优于纯语言策略。

Abstract: Despite progress in Vision-Language Models (VLMs), their capacity for visual
reasoning is often limited by the \textit{binding problem}: the failure to
reliably associate perceptual features with their correct visual referents.
This limitation underlies persistent errors in tasks such as counting, visual
search, scene description, and spatial relationship understanding. A key factor
is that current VLMs process visual features largely in parallel, lacking
mechanisms for spatially grounded, serial attention. This paper introduces a
simple yet effective intervention: augmenting visual inputs with low-level
spatial structures (e.g., horizontal lines) and pairing this with a textual
prompt that encourages sequential, spatially-aware parsing. We empirically
demonstrate substantial performance improvements across core visual reasoning
tasks. Specifically, our method improves GPT-4o visual search accuracy by
25.00%, increases counting accuracy by 26.83%, reduces edit distance error in
scene description by 0.32, and enhances performance on spatial relationship
tasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the
visual modification is essential for these gains; purely textual strategies,
including Chain-of-Thought prompting, are insufficient and can even degrade
performance. Our method enhances binding only with a single-query inference,
underscoring the importance of visual input design over purely
linguistically-based approaches. These findings suggest that low-level visual
structuring is a powerful and underexplored direction for improving
compositional visual reasoning and could serve as a general strategy for
enhancing VLM performance on spatially grounded tasks.

</details>


### [208] [COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication](https://arxiv.org/abs/2506.22274)
*Filippo Merlo,Ece Takmaz,Wenkai Chen,Albert Gatt*

Key words: 视觉语言模型, 场景上下文, 物体识别, 注意力机制, COOCO数据集

TL;DR: 研究探索了视觉语言模型（VLM）是否像人类一样依赖场景上下文进行物体识别和引用，通过COOCO数据集测试不同场景-物体一致性和扰动下的表现，发现模型会根据语义相关性和噪声水平动态调整对上下文的依赖。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探索VLM是否能够像人类一样利用场景上下文进行物体识别和引用，以及其依赖上下文的条件和方式。

Method: 使用Common Objects Out-of-Context (COOCO)数据集，测试VLM在不同场景-物体一致性和噪声水平下的表现，并进行注意力分析。

Result: 模型会根据目标与场景的语义相关性和噪声水平动态调整对上下文的依赖，中层次注意力在适度噪声下对目标物体更集中。

Conclusion: VLM在物体引用生成中能够动态平衡局部和上下文信息，表现出类似人类的场景依赖行为。

Abstract: Natural scenes provide us with rich contexts for object recognition and
reference. In particular, knowing what type of scene one is looking at
generates expectations about which objects will occur, and what their spatial
configuration should be. Do Vision-Language Models (VLMs) learn to rely on
scene contexts in a similar way, when generating references to objects? To
address this question, we introduce the \textit{Common Objects Out-of-Context
(COOCO)} dataset and test to what extent VLMs rely on scene context to refer to
objects under different degrees of scene-object congruency, and different
perturbations. Our findings show that models leverage scene context adaptively,
depending on both the semantic relatedness between object and scene and the
level of noise. In particular, models rely more on context under high
target-scene congruence or when objects are degraded. Attention analysis
reveals that successful object categorisation involves increased focus on the
target in mid-level layers, especially under moderate noise, suggesting that
VLMs dynamically balance local and contextual information for reference
generation. We make our dataset, code and models available at
\href{https://github.com/cs-nlp-uu/scenereg}{https://github.com/cs-nlp-uu/scenereg}.

</details>


### [209] [Boosting Classification with Quantum-Inspired Augmentations](https://arxiv.org/abs/2506.22241)
*Matthias Tschöpe,Vitor Fortes Rey,Sogo Pierre Sanon,Paul Lukowicz,Nikolaos Palaiodimopoulos,Maximilian Kiefer-Emmanouilidis*

Key words: 量子计算，数据增强，机器学习，图像分类，隐私计算

TL;DR: 论文探讨了量子门微小扰动作为数据增强技术的潜力，展示了其在经典机器学习中提升图像分类性能的能力，并分析了其在隐私计算中的局限性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 研究量子计算中的微小扰动是否可以作为数据增强的天然来源，以提升经典机器学习方法的性能。

Method: 使用随机Bloch球旋转作为量子启发的数据增强技术，并将其应用于大规模数据集ImageNet。

Result: 量子启发的增强方法显著提升了图像分类性能（Top-1精度提高3%，Top-5精度提高2.5%，F$_1$分数从8%提升至12%），但在隐私计算中效果有限。

Conclusion: 量子启发的数据增强方法在经典机器学习中表现出色，但在隐私保护方面存在局限性。

Abstract: Understanding the impact of small quantum gate perturbations, which are
common in quantum digital devices but absent in classical computers, is crucial
for identifying potential advantages in quantum machine learning. While these
perturbations are typically seen as detrimental to quantum computation, they
can actually enhance performance by serving as a natural source of data
augmentation. Additionally, they can often be efficiently simulated on
classical hardware, enabling quantum-inspired approaches to improve classical
machine learning methods. In this paper, we investigate random Bloch sphere
rotations, which are fundamental SU(2) transformations, as a simple yet
effective quantum-inspired data augmentation technique. Unlike conventional
augmentations such as flipping, rotating, or cropping, quantum transformations
lack intuitive spatial interpretations, making their application to tasks like
image classification less straightforward. While common quantum augmentation
methods rely on applying quantum models or trainable quanvolutional layers to
classical datasets, we focus on the direct application of small-angle Bloch
rotations and their effect on classical data. Using the large-scale ImageNet
dataset, we demonstrate that our quantum-inspired augmentation method improves
image classification performance, increasing Top-1 accuracy by 3%, Top-5
accuracy by 2.5%, and the F$_1$ score from 8% to 12% compared to standard
classical augmentation methods. Finally, we examine the use of stronger unitary
augmentations. Although these transformations preserve information in
principle, they result in visually unrecognizable images with potential
applications for privacy computations. However, we show that our augmentation
approach and simple SU(2) transformations do not enhance differential privacy
and discuss the implications of this limitation.

</details>


### [210] [Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition](https://arxiv.org/abs/2506.22179)
*Wenhan Wu,Zhishuai Guo,Chen Chen,Hongfei Xue,Aidong Lu*

Key words: 零样本学习，骨架动作识别，频率分解，语义增强，变分自编码器

TL;DR: 提出一种频率-语义增强变分自编码器（FS-VAE），通过频率分解提升骨架语义表示学习，解决零样本动作识别中细粒度动作模式的忽略问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 零样本骨架动作识别需识别未见过的动作类别，现有方法多忽略语义空间中细粒度动作模式的重要性。

Method: 1）频率增强模块通过高低频调整丰富骨架语义学习；2）多层次对齐的语义动作描述捕捉局部和全局对应；3）校准的交叉对齐损失减少特征歧义。

Result: 基准测试显示频率增强语义特征能有效区分视觉和语义相似的动作簇，提升零样本动作识别性能。

Conclusion: FS-VAE通过频率分解和语义对齐，显著提升了零样本骨架动作识别的鲁棒性和准确性。

Abstract: Zero-shot skeleton-based action recognition aims to develop models capable of
identifying actions beyond the categories encountered during training. Previous
approaches have primarily focused on aligning visual and semantic
representations but often overlooked the importance of fine-grained action
patterns in the semantic space (e.g., the hand movements in drinking water and
brushing teeth). To address these limitations, we propose a Frequency-Semantic
Enhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic
representation learning with frequency decomposition. FS-VAE consists of three
key components: 1) a frequency-based enhancement module with high- and
low-frequency adjustments to enrich the skeletal semantics learning and improve
the robustness of zero-shot action recognition; 2) a semantic-based action
description with multilevel alignment to capture both local details and global
correspondence, effectively bridging the semantic gap and compensating for the
inherent loss of information in skeleton sequences; 3) a calibrated
cross-alignment loss that enables valid skeleton-text pairs to counterbalance
ambiguous ones, mitigating discrepancies and ambiguities in skeleton and text
features, thereby ensuring robust alignment. Evaluations on the benchmarks
demonstrate the effectiveness of our approach, validating that
frequency-enhanced semantic features enable robust differentiation of visually
and semantically similar action clusters, improving zero-shot action
recognition.

</details>


### [211] [Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment](https://arxiv.org/abs/2506.22385)
*Yue Zhang,Jilei Sun,Yunhui Guo,Vibhav Gogate*

Key words: 视频大型多模态模型, 动态推理, 反事实推理, 可削弱视频蕴含, LLM

TL;DR: 论文提出了一种新任务DVidE，旨在提升视频大型多模态模型（VLMMs）的动态推理能力，通过引入反事实推理和LLM生成的更新信息，显著改善了模型的适应性推理表现。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的VLMMs在抽象和适应性推理上表现不足，特别是当新信息出现时难以调整初始推断。为了解决这一问题，作者设计了DVidE任务。

Method: 提出了两种解决方案：用于分类任务的Chain of Counterfactual Thought框架（结合反事实推理和ASR增强视频内容）和用于生成任务的LLM结合ASR输出的框架。

Result: 实验结果表明，提出的方法显著提升了VLMMs的动态推理能力，并在新基准数据集上验证了其有效性。

Conclusion: DVidE任务及相关框架为VLMMs的动态推理提供了新方向，显著提升了模型处理更新信息的能力。

Abstract: Video Large Multimodal Models (VLMMs) have made impressive strides in
understanding video content, but they often struggle with abstract and adaptive
reasoning-the ability to revise their interpretations when new information
emerges. In reality, conclusions are rarely set in stone; additional context
can strengthen or weaken an initial inference. To address this, we introduce
Defeasible Video Entailment (DVidE), a new task that challenges models to think
like doubters, constantly updating their reasoning based on evolving evidence.
In DVidE, given a video premise and a textual hypothesis, models must determine
whether a new update strengthens or weakens the hypothesis (classification
version) or generate a coherent update that modifies the entailment
relationship (generation version). For solving the classification task, we
propose the Chain of Counterfactual Thought framework, utilizing counterfactual
reasoning, ASR-enhanced video content, and rationale refinement to reduce
inference bias. For the generation task, we develop a framework that combines
ASR output with a Large Language Model (LLM) to produce coherent, contextually
relevant updates aligned with the intended strengthener or weakener goals.
Additionally, we introduce a novel benchmark dataset, with
strengthener/weakener annotations and an LLM-based evaluation metric
specifically designed for assessing generative performance. Experimental
results demonstrate significant improvements, highlighting our proposed method
in enhancing dynamic reasoning capabilities of VLMMs.

</details>


### [212] [RoomCraft: Controllable and Complete 3D Indoor Scene Generation](https://arxiv.org/abs/2506.22291)
*Mengqi Zhou,Xipeng Wang,Yuxi Wang,Zhaoxiang Zhang*

Key words: 3D室内场景生成, 约束驱动优化, 空间关系网络, CAPS策略, 多模态输入

TL;DR: RoomCraft是一个多阶段管道方法，通过结合场景生成管道和约束驱动优化框架，从用户输入生成一致的3D室内场景，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决神经生成方法在全局空间推理不足时产生的重复性问题，以及程序化方法在多约束场景下的碰撞问题和布局完整性挑战。

Method: 采用多阶段管道，包括从输入中提取高层场景信息、构建空间关系网络、使用HDFS算法生成优化放置序列，并引入统一约束表示和CAPS策略以减少碰撞。

Result: 实验表明，RoomCraft在各种输入模态下生成的房间布局在真实性、语义一致性和视觉吸引力方面显著优于现有方法。

Conclusion: RoomCraft通过多阶段优化和冲突感知策略，成功解决了多约束场景下的3D室内场景生成问题。

Abstract: Generating realistic 3D indoor scenes from user inputs remains a challenging
problem in computer vision and graphics, requiring careful balance of geometric
consistency, spatial relationships, and visual realism. While neural generation
methods often produce repetitive elements due to limited global spatial
reasoning, procedural approaches can leverage constraints for controllable
generation but struggle with multi-constraint scenarios. When constraints
become numerous, object collisions frequently occur, forcing the removal of
furniture items and compromising layout completeness.
  To address these limitations, we propose RoomCraft, a multi-stage pipeline
that converts real images, sketches, or text descriptions into coherent 3D
indoor scenes. Our approach combines a scene generation pipeline with a
constraint-driven optimization framework. The pipeline first extracts
high-level scene information from user inputs and organizes it into a
structured format containing room type, furniture items, and spatial relations.
It then constructs a spatial relationship network to represent furniture
arrangements and generates an optimized placement sequence using a
heuristic-based depth-first search (HDFS) algorithm to ensure layout coherence.
To handle complex multi-constraint scenarios, we introduce a unified constraint
representation that processes both formal specifications and natural language
inputs, enabling flexible constraint-oriented adjustments through a
comprehensive action space design. Additionally, we propose a Conflict-Aware
Positioning Strategy (CAPS) that dynamically adjusts placement weights to
minimize furniture collisions and ensure layout completeness.
  Extensive experiments demonstrate that RoomCraft significantly outperforms
existing methods in generating realistic, semantically coherent, and visually
appealing room layouts across diverse input modalities.

</details>


### [213] [From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications](https://arxiv.org/abs/2506.22360)
*Nouf Almesafri,Hector Figueiredo,Miguel Arana-Catania*

Key words: 计算机视觉, 深度学习, 事件相机, CNN, ViT

TL;DR: 研究比较了卷积神经网络（CNN）和视觉Transformer（ViT）在事件相机上的性能，测试了在标准条件和噪声环境下ResNet34和ViT B16的表现。ResNet34在干净数据集上略优于ViT B16，但ViT B16表现出更强的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 事件相机适用于动态环境（如无人机和自动驾驶汽车），但传统深度学习架构对其性能的研究较少。

Method: 使用ResNet34和ViT B16模型，在GEN1事件数据集上进行微调，并在标准条件和噪声环境下评估其性能。

Result: ResNet34和ViT B16在干净数据集上的准确率分别为88%和86%，ViT B16在噪声环境中表现更鲁棒。

Conclusion: 两种架构均适用于事件相机，ViT B16在小数据集和噪声环境下更具优势，有望扩展到无人机领域。

Abstract: This study investigates the performance of the two most relevant computer
vision deep learning architectures, Convolutional Neural Network and Vision
Transformer, for event-based cameras. These cameras capture scene changes,
unlike traditional frame-based cameras with capture static images, and are
particularly suited for dynamic environments such as UAVs and autonomous
vehicles. The deep learning models studied in this work are ResNet34 and ViT
B16, fine-tuned on the GEN1 event-based dataset. The research evaluates and
compares these models under both standard conditions and in the presence of
simulated noise. Initial evaluations on the clean GEN1 dataset reveal that
ResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with
ResNet34 showing a slight advantage in classification accuracy. However, the
ViT B16 model demonstrates notable robustness, particularly given its
pre-training on a smaller dataset. Although this study focuses on ground-based
vehicle classification, the methodologies and findings hold significant promise
for adaptation to UAV contexts, including aerial object classification and
event-based vision systems for aviation-related tasks.

</details>


### [214] [A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake](https://arxiv.org/abs/2506.22338)
*Luigi Russo,Deodato Tapete,Silvia Liberata Ullo,Paolo Gamba*

Key words: 建筑物损坏检测, SAR图像, 深度学习, 灾害管理, 多模态数据

TL;DR: 该论文提出了一种新型多模态深度学习框架，利用单日高分辨率SAR图像和辅助地理数据快速检测建筑物损坏，无需依赖灾前图像。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 灾后快速识别建筑物损坏对应急响应至关重要，但传统光学卫星图像常受云层或无灾前数据限制。

Method: 结合SAR图像补丁、OSM建筑足迹、DSM数据及GEM结构属性，构建无需灾前数据的深度学习模型。

Result: 在土耳其地震数据集中验证，显示结合地理特征显著提升检测性能和泛化能力。

Conclusion: 该方法可快速可靠评估建筑物损坏，支持灾害管理，具有广泛应用潜力。

Abstract: Building damage identification shortly after a disaster is crucial for
guiding emergency response and recovery efforts. Although optical satellite
imagery is commonly used for disaster mapping, its effectiveness is often
hampered by cloud cover or the absence of pre-event acquisitions. To overcome
these challenges, we introduce a novel multimodal deep learning (DL) framework
for detecting building damage using single-date very high resolution (VHR)
Synthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI)
COSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data.
Our method integrates SAR image patches, OpenStreetMap (OSM) building
footprints, digital surface model (DSM) data, and structural and exposure
attributes from the Global Earthquake Model (GEM) to improve detection accuracy
and contextual interpretation. Unlike existing approaches that depend on pre
and post event imagery, our model utilizes only post event data, facilitating
rapid deployment in critical scenarios. The framework effectiveness is
demonstrated using a new dataset from the 2023 earthquake in Turkey, covering
multiple cities with diverse urban settings. Results highlight that
incorporating geospatial features significantly enhances detection performance
and generalizability to previously unseen areas. By combining SAR imagery with
detailed vulnerability and exposure information, our approach provides reliable
and rapid building damage assessments without the dependency from available
pre-event data. Moreover, the automated and scalable data generation process
ensures the framework's applicability across diverse disaster-affected regions,
underscoring its potential to support effective disaster management and
recovery efforts. Code and data will be made available upon acceptance of the
paper.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [215] [Inverse Design of Diffractive Metasurfaces Using Diffusion Models](https://arxiv.org/abs/2506.21748)
*Liav Hen,Erez Yosef,Dan Raviv,Raja Giryes,Jacob Scheuer*

Key words: metasurfaces, inverse design, diffusion models, RCWA, optical response

TL;DR: 论文提出了一种基于扩散模型的逆向设计方法，用于优化超表面结构以实现所需光学响应，显著提高了设计效率和性能。

<details>
  <summary>Details</summary>

Main category: physics.optics

Motivation: 超表面的逆向设计由于结构与其光学响应之间的复杂非线性关系而具有挑战性，传统方法需要专家调整且计算成本高。本文旨在解决这些问题。

Method: 通过集成扩散模型，利用RCWA模拟生成训练数据，训练条件扩散模型预测超表面结构以实现目标光学响应。

Result: 该方法能够高效生成低误差的超表面结构，例如均匀强度分束器和偏振分束器，设计时间少于30分钟。

Conclusion: 扩散模型为超表面设计提供了一种高效且性能优越的方法，为数据驱动设计领域的研究提供了新工具。

Abstract: Metasurfaces are ultra-thin optical elements composed of engineered
sub-wavelength structures that enable precise control of light. Their inverse
design - determining a geometry that yields a desired optical response - is
challenging due to the complex, nonlinear relationship between structure and
optical properties. This often requires expert tuning, is prone to local
minima, and involves significant computational overhead. In this work, we
address these challenges by integrating the generative capabilities of
diffusion models into computational design workflows. Using an RCWA simulator,
we generate training data consisting of metasurface geometries and their
corresponding far-field scattering patterns. We then train a conditional
diffusion model to predict meta-atom geometry and height from a target spatial
power distribution at a specified wavelength, sampled from a continuous
supported band. Once trained, the model can generate metasurfaces with low
error, either directly using RCWA-guided posterior sampling or by serving as an
initializer for traditional optimization methods. We demonstrate our approach
on the design of a spatially uniform intensity splitter and a polarization beam
splitter, both produced with low error in under 30 minutes. To support further
research in data-driven metasurface design, we publicly release our code and
datasets.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [216] [SciMantify -- A Hybrid Approach for the Evolving Semantification of Scientific Knowledge](https://arxiv.org/abs/2506.21819)
*Lena John,Kheir Eddine Farfar,Sören Auer,Oliver Karras*

Key words: 知识图谱, 语义化, 科学知识表示, 人机协作, 开放研究知识图谱

TL;DR: 论文提出了一种基于知识图谱的科学知识表示五阶段进化模型，通过人机协作提升知识的语义化表示。

<details>
  <summary>Details</summary>

Main category: cs.DL

Motivation: 现有的科学出版物多以静态PDF形式存在，缺乏结构化与语义化，限制了知识的可访问性和可重用性。需要更灵活、结构化、语义化的表示方法。

Method: 提出五阶段知识表示进化模型，开发混合方法SciMantify，通过人机协作的语义标注任务逐步优化知识表示。

Result: 初步用户实验表明，该方法简化了科学知识的预处理，减少了语义化的工作量，并提升了与知识图谱结构的对齐。

Conclusion: SciMantify方法通过逐步语义化，提升了科学知识的表示效率与结构化程度，为知识管理提供了新思路。

Abstract: Scientific publications, primarily digitized as PDFs, remain static and
unstructured, limiting the accessibility and reusability of the contained
knowledge. At best, scientific knowledge from publications is provided in
tabular formats, which lack semantic context. A more flexible, structured, and
semantic representation is needed to make scientific knowledge understandable
and processable by both humans and machines. We propose an evolution model of
knowledge representation, inspired by the 5-star Linked Open Data (LOD) model,
with five stages and defined criteria to guide the stepwise transition from a
digital artifact, such as a PDF, to a semantic representation integrated in a
knowledge graph (KG). Based on an exemplary workflow implementing the entire
model, we developed a hybrid approach, called SciMantify, leveraging tabular
formats of scientific knowledge, e.g., results from secondary studies, to
support its evolving semantification. In the approach, humans and machines
collaborate closely by performing semantic annotation tasks (SATs) and refining
the results to progressively improve the semantic representation of scientific
knowledge. We implemented the approach in the Open Research Knowledge Graph
(ORKG), an established platform for improving the findability, accessibility,
interoperability, and reusability of scientific knowledge. A preliminary user
experiment showed that the approach simplifies the preprocessing of scientific
knowledge, reduces the effort for the evolving semantification, and enhances
the knowledge representation through better alignment with the KG structures.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [217] [Joint Task Offloading and Resource Allocation in Low-Altitude MEC via Graph Attention Diffusion](https://arxiv.org/abs/2506.21933)
*Yifan Xue,Ruihuai Liang,Bo Yang,Xuelin Cao,Zhiwen Yu,Mérouane Debbah,Chau Yuen*

Key words: 低空经济，多接入边缘计算，任务调度，图注意力网络，扩散模型

TL;DR: 该论文提出了一种基于图注意力扩散的解决方案生成器（GADSG），用于解决低空经济网络中异构多接入边缘计算系统的任务调度和资源分配问题。该方法通过整合图注意力网络和扩散模型，在高维潜在空间中联合优化离散卸载变量和连续资源分配变量，显著优于现有基线方法。

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: 随着低空经济的快速发展，空地面一体化多接入边缘计算系统对实时和智能任务调度的需求日益增长。系统面临节点异构性、通信链路不稳定和任务动态变化等挑战，亟需高效解决方案。

Method: 论文构建了一个三层异构MEC系统架构，并从通信信道、计算成本和约束条件角度进行系统建模。提出GADSG方法，结合图注意力网络的上下文感知能力和扩散模型的解分布学习能力，优化高维潜在空间中的离散和连续变量。

Result: 通过多数据集实验验证，GADSG在优化性能、鲁棒性和泛化能力上均显著优于基线方法，适用于动态复杂的低空经济网络环境。

Conclusion: GADSG为低空经济网络中的高效任务调度提供了创新解决方案，展现出在实际应用中的潜力。

Abstract: With the rapid development of the low-altitude economy, air-ground integrated
multi-access edge computing (MEC) systems are facing increasing demands for
real-time and intelligent task scheduling. In such systems, task offloading and
resource allocation encounter multiple challenges, including node
heterogeneity, unstable communication links, and dynamic task variations. To
address these issues, this paper constructs a three-layer heterogeneous MEC
system architecture for low-altitude economic networks, encompassing aerial and
ground users as well as edge servers. The system is systematically modeled from
the perspectives of communication channels, computational costs, and constraint
conditions, and the joint optimization problem of offloading decisions and
resource allocation is uniformly abstracted into a graph-structured modeling
task. On this basis, we propose a graph attention diffusion-based solution
generator (GADSG). This method integrates the contextual awareness of graph
attention networks with the solution distribution learning capability of
diffusion models, enabling joint modeling and optimization of discrete
offloading variables and continuous resource allocation variables within a
high-dimensional latent space. We construct multiple simulation datasets with
varying scales and topologies. Extensive experiments demonstrate that the
proposed GADSG model significantly outperforms existing baseline methods in
terms of optimization performance, robustness, and generalization across task
structures, showing strong potential for efficient task scheduling in dynamic
and complex low-altitude economic network environments.

</details>


### [218] [Concept-Level AI for Telecom: Moving Beyond Large Language Models](https://arxiv.org/abs/2506.22359)
*Viswanath Kumarskandpriya,Abdulhalim Dandoush,Abbas Bradai,Ali Belgacem*

Key words: telecommunications, Large Language Models, Large Concept Models, hierarchical representation, cross-layer dependency

TL;DR: 论文讨论了在电信和网络领域应用大型语言模型（LLMs）的局限性，提出大型概念模型（LCMs）作为更优解决方案，强调其在处理跨层依赖、时空故障关联等电信特定需求上的优势。

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: 电信领域需要管理日益复杂的多管理域和多语言系统，传统LLMs因逐词处理和上下文限制无法满足需求，而LCMs通过语义概念抽象和双曲潜在空间提供更有效方法。

Method: 采用大型概念模型（LCMs），利用双曲潜在空间进行层次表示，并通过简洁概念嵌入封装复杂多层网络交互。

Result: LCMs在记忆效率、跨层关联和原生多模态集成方面显著优于LLMs，适合电信管理的关键需求。

Conclusion: LCMs是电信领域AI管理的必要进化步骤，而非简单的增量改进。

Abstract: The telecommunications and networking domain stands at the precipice of a
transformative era, driven by the necessity to manage increasingly complex,
hierarchical, multi administrative domains (i.e., several operators on the same
path) and multilingual systems. Recent research has demonstrated that Large
Language Models (LLMs), with their exceptional general-purpose text analysis
and code generation capabilities, can be effectively applied to certain telecom
problems (e.g., auto-configuration of data plan to meet certain application
requirements). However, due to their inherent token-by-token processing and
limited capacity for maintaining extended context, LLMs struggle to fulfill
telecom-specific requirements such as cross-layer dependency cascades (i.e.,
over OSI), temporal-spatial fault correlation, and real-time distributed
coordination. In contrast, Large Concept Models (LCMs), which reason at the
abstraction level of semantic concepts rather than individual lexical tokens,
offer a fundamentally superior approach for addressing these telecom
challenges. By employing hyperbolic latent spaces for hierarchical
representation and encapsulating complex multi-layered network interactions
within concise concept embeddings, LCMs overcome critical shortcomings of LLMs
in terms of memory efficiency, cross-layer correlation, and native multimodal
integration. This paper argues that adopting LCMs is not simply an incremental
step, but a necessary evolutionary leap toward achieving robust and effective
AI-driven telecom management.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [219] [Modification of a Numerical Method Using FIR Filters in a Time-dependent SIR Model for COVID-19](https://arxiv.org/abs/2506.21739)
*Felipe Rogério Pimentel,Rafael Gustavo Alves*

Key words: FIR滤波器，岭回归，COVID-19预测，SIR模型，机器学习

TL;DR: 论文提出了一种基于FIR滤波器和岭回归的改进算法，用于预测COVID-19感染和康复人数，并在巴西米纳斯吉拉斯州的早期疫情数据中验证了其优于原算法的表现。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 在COVID-19疫苗尚未普及的背景下，通过改进算法更准确地预测疫情数据，为隔离策略提供科学依据。

Method: 采用FIR线性系统滤波和岭回归方法，调整滤波器阶数和正则化参数，改进原算法。

Result: 改进算法在某些模拟中的近似误差优于原算法。

Conclusion: 提出的改进算法在疫情预测中表现更好，适用于无疫苗背景下的防控策略制定。

Abstract: Authors Yi-Cheng Chen, Ping-En Lu, Cheng-Shang Chang, and Tzu-Hsuan Liu use
the Finite Impulse Response (FIR) linear system filtering method to track and
predict the number of people infected and recovered from COVID-19, in a
pandemic context in which there was still no vaccine and the only way to avoid
contagion was isolation. To estimate the coefficients of these FIR filters,
Chen et al. used machine learning methods through a classical optimization
problem with regularization (ridge regression). These estimated coefficients
are called ridge coefficients. The epidemic mathematical model adopted by these
researchers to formulate the FIR filters is the time-dependent discrete SIR. In
this paper, we propose a small modification to the algorithm of Chen et al. to
obtain the ridge coefficients. We then used this modified algorithm to track
and predict the number of people infected and recovered from COVID-19 in the
state of Minas Gerais/Brazil, within a prediction window, during the initial
period of the pandemic. We also compare the predicted data with the respective
real data to check how good the approximation is. In the modified algorithm, we
set values for the FIR filter orders and for the regularization parameters,
both different from the respective values defined by Chen et al. in their
algorithm. In this context, the numerical results obtained by the modified
algorithm in some simulations present better approximation errors compared to
the respective approximation errors presented by the algorithm of Chen et al.

</details>


### [220] [Critically-Damped Higher-Order Langevin Dynamics](https://arxiv.org/abs/2506.21741)
*Benjamin Sterling,Chad Gueli,Mónica F. Bugallo*

Key words: 扩散模型, 临界阻尼, Langevin动力学, 高阶动力学

TL;DR: 该论文提出了一种将临界阻尼引入高阶Langevin动力学（HOLD）的新方法，扩展了当前最先进的扩散模型。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 尽管临界阻尼已在CLD和TOLD++中成功应用，但尚未扩展到任意阶动力学。本文旨在填补这一空白。

Method: 通过从系统分析中引入临界阻尼概念，对HOLD进行泛化。

Result: 提出了一种适用于任意阶动力学的新方法。

Conclusion: 临界阻尼的引入为扩散模型提供了新的研究方向。

Abstract: Denoising Diffusion Probabilistic Models represent an entirely new class of
generative AI methods that have yet to be fully explored. Critical damping has
been successfully introduced in Critically-Damped Langevin Dynamics (CLD) and
Critically-Damped Third-Order Langevin Dynamics (TOLD++), but has not yet been
applied to dynamics of arbitrary order. The proposed line of work generalizes
Higher-Order Langevin Dynamics (HOLD), a recent state-of-the-art diffusion
method, by introducing the concept of critical damping from systems analysis.

</details>


### [221] [TADA: Improved Diffusion Sampling with Training-free Augmented Dynamics](https://arxiv.org/abs/2506.21757)
*Tianrong Chen,Huangjie Zheng,David Berthelot,Jiatao Gu,Josh Susskind,Shuangfei Zhai*

Key words: 扩散模型,采样效率,ODE求解器,高维噪声,图像生成

TL;DR: 本研究提出了一种新的采样方法，通过使用高维初始噪声和ODE求解器，显著提高了扩散模型的采样速度，同时保持高质量的生成效果。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 扩散模型虽然在生成高质量图像方面表现出色，但其采样效率低是一个主要问题。本文旨在通过改进采样方法来解决这一问题。

Method: 提出了一种训练无关的采样方法，利用高维初始噪声和ODE求解器，减少函数评估次数，同时引入超参数控制细节水平。

Result: 新方法在ImageNet512上的采样速度比现有最优解快186%，并在多种预训练扩散模型（如EDM、Stable-Diffusion 3）中表现优异。

Conclusion: 该方法通过高维初始噪声和ODE求解器的结合，显著提升了扩散模型的采样效率，且无需额外计算成本。

Abstract: Diffusion models have demonstrated exceptional capabilities in generating
high-fidelity images but typically suffer from inefficient sampling. Many
solver designs and noise scheduling strategies have been proposed to
dramatically improve sampling speeds. In this paper, we introduce a new
sampling method that is up to $186\%$ faster than the current state of the art
solver for comparative FID on ImageNet512. This new sampling method is
training-free and uses an ordinary differential equation (ODE) solver. The key
to our method resides in using higher-dimensional initial noise, allowing to
produce more detailed samples with less function evaluations from existing
pretrained diffusion models. In addition, by design our solver allows to
control the level of detail through a simple hyper-parameter at no extra
computational cost. We present how our approach leverages momentum dynamics by
establishing a fundamental equivalence between momentum diffusion models and
conventional diffusion models with respect to their training paradigms.
Moreover, we observe the use of higher-dimensional noise naturally exhibits
characteristics similar to stochastic differential equations (SDEs). Finally,
we demonstrate strong performances on a set of representative pretrained
diffusion models, including EDM, EDM2, and Stable-Diffusion 3, which cover
models in both pixel and latent spaces, as well as class and text conditional
settings. The code is available at https://github.com/apple/ml-tada.

</details>


### [222] [Classification with Reject Option: Distribution-free Error Guarantees via Conformal Prediction](https://arxiv.org/abs/2506.21802)
*Johan Hallberg Szabadváry,Tuwe Löfström,Ulf Johansson,Cecilia Sönströd,Ernst Ahlberg,Lars Carlsson*

Key words: 机器学习, 拒绝选项, 共形预测, 二元分类, 误差率

TL;DR: 该研究通过共形预测（CP）将机器学习中的拒绝选项形式化，为二元分类提供理论保证，并展示了误差率和拒绝率之间的权衡。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 机器学习模型即使可能出错时也总是做出预测，这在实际应用中会导致信任问题。因此，研究通过拒绝选项的形式化来避免可能的错误预测。

Method: 采用共形预测（CP）方法，将二元分类中的拒绝选项形式化，并通过理论推导确保误差率的分布无关性。通过接受单例预测，将CP转化为带拒绝选项的二元分类器。

Result: 研究提供了误差率的理论保证和有限样本估计，并通过数值示例展示了不同CP设置下的误差率。误差-拒绝曲线展示了误差率与拒绝率之间的权衡。

Conclusion: 该研究通过CP方法实现了带拒绝选项的二元分类，并提供了误差率的理论保证和实用权衡工具。

Abstract: Machine learning (ML) models always make a prediction, even when they are
likely to be wrong. This causes problems in practical applications, as we do
not know if we should trust a prediction. ML with reject option addresses this
issue by abstaining from making a prediction if it is likely to be incorrect.
In this work, we formalise the approach to ML with reject option in binary
classification, deriving theoretical guarantees on the resulting error rate.
This is achieved through conformal prediction (CP), which produce prediction
sets with distribution-free validity guarantees. In binary classification, CP
can output prediction sets containing exactly one, two or no labels. By
accepting only the singleton predictions, we turn CP into a binary classifier
with reject option.
  Here, CP is formally put in the framework of predicting with reject option.
We state and prove the resulting error rate, and give finite sample estimates.
Numerical examples provide illustrations of derived error rate through several
different conformal prediction settings, ranging from full conformal prediction
to offline batch inductive conformal prediction. The former has a direct link
to sharp validity guarantees, whereas the latter is more fuzzy in terms of
validity guarantees but can be used in practice. Error-reject curves illustrate
the trade-off between error rate and reject rate, and can serve to aid a user
to set an acceptable error rate or reject rate in practice.

</details>


### [223] [Thompson Sampling in Function Spaces via Neural Operators](https://arxiv.org/abs/2506.21894)
*Rafael Oliveira,Xuesong Wang,Kian Ming A. Chai,Edwin V. Bonilla*

Key words: Thompson采样、神经算子、函数空间优化、高斯过程、样本效率

TL;DR: 本文提出了一种基于Thompson采样的方法，用于函数空间中的优化问题，通过神经算子替代模型实现高效优化。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 解决在函数空间优化中，算子查询成本高昂、功能评估廉价的问题。

Method: 采用样本-优化方法，利用神经算子作为高斯过程的近似样本，避免显式不确定性量化。

Result: 提供了新的理论收敛保证，并在偏微分方程和非线性算子驱动的任务中表现出更高的样本效率和竞争性能。

Conclusion: 该方法在复杂算子驱动的优化任务中具有显著优势。

Abstract: We propose an extension of Thompson sampling to optimization problems over
function spaces where the objective is a known functional of an unknown
operator's output. We assume that functional evaluations are inexpensive, while
queries to the operator (such as running a high-fidelity simulator) are costly.
Our algorithm employs a sample-then-optimize approach using neural operator
surrogates. This strategy avoids explicit uncertainty quantification by
treating trained neural operators as approximate samples from a Gaussian
process. We provide novel theoretical convergence guarantees, based on Gaussian
processes in the infinite-dimensional setting, under minimal assumptions. We
benchmark our method against existing baselines on functional optimization
tasks involving partial differential equations and other nonlinear
operator-driven phenomena, demonstrating improved sample efficiency and
competitive performance.

</details>


### [224] [Hybrid Generative Modeling for Incomplete Physics: Deep Grey-Box Meets Optimal Transport](https://arxiv.org/abs/2506.22204)
*Gurjeet Sangra Singh,Maciej Falkiewicz,Alexandros Kalousis*

Key words: 深度灰箱建模, 最优传输, 混合生成模型, 物理模型增强

TL;DR: 提出了一种结合深度灰箱建模和最优传输方法的新混合生成模型，用于增强不完整的物理模型，解决真实数据与模拟数据之间未配对的问题。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现实中许多系统的物理模型不完整或存在未知项，导致模型与真实数据生成过程存在偏差。研究旨在利用有限且未配对的数据，结合理论驱动和数据驱动的方法，修正物理模型。

Method: 采用深度灰箱建模与最优传输方法相结合的混合生成模型，最小化源分布失真，同时保持物理参数的准确使用。

Result: 实验证明该方法在生成任务和模型透明度方面表现优异，能够准确学习系统动力学并保持可解释性。

Conclusion: 该方法有效解决了未配对数据问题，提供了对学习到的物理动力学的深入理解，优于黑箱替代方案。

Abstract: Physics phenomena are often described by ordinary and/or partial differential
equations (ODEs/PDEs), and solved analytically or numerically. Unfortunately,
many real-world systems are described only approximately with missing or
unknown terms in the equations. This makes the distribution of the physics
model differ from the true data-generating process (DGP). Using limited and
unpaired data between DGP observations and the imperfect model simulations, we
investigate this particular setting by completing the known-physics model,
combining theory-driven models and data-driven to describe the shifted
distribution involved in the DGP. We present a novel hybrid generative model
approach combining deep grey-box modelling with Optimal Transport (OT) methods
to enhance incomplete physics models. Our method implements OT maps in data
space while maintaining minimal source distribution distortion, demonstrating
superior performance in resolving the unpaired problem and ensuring correct
usage of physics parameters. Unlike black-box alternatives, our approach
leverages physics-based inductive biases to accurately learn system dynamics
while preserving interpretability through its domain knowledge foundation.
Experimental results validate our method's effectiveness in both generation
tasks and model transparency, offering detailed insights into learned physics
dynamics.

</details>


### [225] [Uncovering smooth structures in single-cell data with PCS-guided neighbor embeddings](https://arxiv.org/abs/2506.22228)
*Rong Ma,Xi Li,Jingyuan Hu,Bin Yu*

Key words: 单细胞测序,邻域嵌入算法,NESS,细胞状态转换,PCS框架

TL;DR: 论文提出了一种名为NESS的新方法，通过改进邻域嵌入算法（如t-SNE和UMAP），解决了单细胞数据中连续细胞状态转换的挑战，提供了稳定的、可解释的表示。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 单细胞测序技术虽能详细研究细胞状态转换，但现有邻域嵌入算法（如t-SNE和UMAP）存在失真和不稳定性，难以捕捉连续的生物过程。

Method: 基于PCS框架，系统地评估现有邻域嵌入算法，并提出NESS方法，利用算法稳定性改进表示并推断生物结构。提供稳定性指标和工作流程。

Result: NESS在多种单细胞数据集中（如干细胞分化、类器官发育）成功识别过渡和稳定细胞状态，量化发育过程中的转录动态。

Conclusion: NESS是一种可靠且可解释的方法，能够从单细胞数据中提取连续的生物轨迹，为生物研究提供了新工具。

Abstract: Single-cell sequencing is revolutionizing biology by enabling detailed
investigations of cell-state transitions. Many biological processes unfold
along continuous trajectories, yet it remains challenging to extract smooth,
low-dimensional representations from inherently noisy, high-dimensional
single-cell data. Neighbor embedding (NE) algorithms, such as t-SNE and UMAP,
are widely used to embed high-dimensional single-cell data into low dimensions.
But they often introduce undesirable distortions, resulting in misleading
interpretations. Existing evaluation methods for NE algorithms primarily focus
on separating discrete cell types rather than capturing continuous cell-state
transitions, while dynamic modeling approaches rely on strong assumptions about
cellular processes and specialized data. To address these challenges, we build
on the Predictability-Computability-Stability (PCS) framework for reliable and
reproducible data-driven discoveries. First, we systematically evaluate popular
NE algorithms through empirical analysis, simulation, and theory, and reveal
their key shortcomings, such as artifacts and instability. We then introduce
NESS, a principled and interpretable machine learning approach to improve NE
representations by leveraging algorithmic stability and to enable robust
inference of smooth biological structures. NESS offers useful concepts,
quantitative stability metrics, and efficient computational workflows to
uncover developmental trajectories and cell-state transitions in single-cell
data. Finally, we apply NESS to six single-cell datasets, spanning pluripotent
stem cell differentiation, organoid development, and multiple tissue-specific
lineage trajectories. Across these diverse contexts, NESS consistently yields
useful biological insights, such as identification of transitional and stable
cell states and quantification of transcriptional dynamics during development.

</details>


### [226] [Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts](https://arxiv.org/abs/2506.22343)
*Xiang Li,Garrett Wen,Weiqing He,Jiayuan Wu,Qi Long,Weijie J. Su*

Key words: 文本水印, 混合来源, 比例估计, 关键统计量, 大语言模型

TL;DR: 本文研究混合来源文本中水印比例的最优估计问题，提出基于关键统计量的高效估计方法，并在开源模型生成的数据上验证了其高准确性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现实场景中文本常混合人类撰写和水印生成内容，现有研究多关注整篇文本的水印检测，缺乏对混合比例估计的方法。

Method: 将问题建模为基于关键统计量的混合模型比例参数估计，针对连续关键统计量的水印方法提出高效估计器，并推导极小极大下界。

Result: 理论证明了特定条件下比例参数的可识别性，实验显示所提估计器在合成数据和开源模型生成文本中均表现优异。

Conclusion: 针对混合来源文本的水印比例估计问题，提出的方法在理论和实验中均验证了其有效性和准确性。

Abstract: Text watermarks in large language models (LLMs) are an increasingly important
tool for detecting synthetic text and distinguishing human-written content from
LLM-generated text. While most existing studies focus on determining whether
entire texts are watermarked, many real-world scenarios involve mixed-source
texts, which blend human-written and watermarked content. In this paper, we
address the problem of optimally estimating the watermark proportion in
mixed-source texts. We cast this problem as estimating the proportion parameter
in a mixture model based on \emph{pivotal statistics}. First, we show that this
parameter is not even identifiable in certain watermarking schemes, let alone
consistently estimable. In stark contrast, for watermarking methods that employ
continuous pivotal statistics for detection, we demonstrate that the proportion
parameter is identifiable under mild conditions. We propose efficient
estimators for this class of methods, which include several popular unbiased
watermarks as examples, and derive minimax lower bounds for any measurable
estimator based on pivotal statistics, showing that our estimators achieve
these lower bounds. Through evaluations on both synthetic data and mixed-source
text generated by open-source models, we demonstrate that our proposed
estimators consistently achieve high estimation accuracy.

</details>


### [227] [Beyond ReLU: How Activations Affect Neural Kernels and Random Wide Networks](https://arxiv.org/abs/2506.22429)
*David Holzmüller,Max Schölpple*

Key words: 深度学习, NTK, NNGP, 激活函数, RKHS, 平滑性

TL;DR: 该论文分析了除ReLU外的典型激活函数（如SELU、ELU、LeakyReLU）在NTK和NNGP下的RKHS特性，揭示了其与网络深度的关系，并研究了NNGP样本路径的平滑性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 由于现有理论主要局限于ReLU激活函数，对于其他典型激活函数的理解不足，因此需要更广泛地分析其RKHS特性。

Method: 通过分析典型激活函数的RKHS特性，涵盖了多种特殊情况（如无偏置、两层网络、多项式激活）并研究了NNGP样本路径的平滑性。

Result: 研究表明，一类非无限平滑的激活函数在不同网络深度下生成等效RKHS，而多项式激活函数生成非等效RKHS，并揭示了无限宽神经网络初始化的平滑性。

Conclusion: 论文为NTK和NNGP框架下的激活函数特性提供了更广泛的描述，有助于理解深度学习的理论基础。

Abstract: While the theory of deep learning has made some progress in recent years,
much of it is limited to the ReLU activation function. In particular, while the
neural tangent kernel (NTK) and neural network Gaussian process kernel (NNGP)
have given theoreticians tractable limiting cases of fully connected neural
networks, their properties for most activation functions except for powers of
the ReLU function are poorly understood. Our main contribution is to provide a
more general characterization of the RKHS of these kernels for typical
activation functions whose only non-smoothness is at zero, such as SELU, ELU,
or LeakyReLU. Our analysis also covers a broad set of special cases such as
missing biases, two-layer networks, or polynomial activations. Our results show
that a broad class of not infinitely smooth activations generate equivalent
RKHSs at different network depths, while polynomial activations generate
non-equivalent RKHSs. Finally, we derive results for the smoothness of NNGP
sample paths, characterizing the smoothness of infinitely wide neural networks
at initialization.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [228] [On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling](https://arxiv.org/abs/2506.21874)
*Stanley Wu,Ronik Bhaskar,Anna Yoo Jeong Ha,Shawn Shan,Haitao Zheng,Ben Y. Zhao*

Key words: 文本到图像生成、VLMs、对抗攻击、数据污染、训练管道

TL;DR: 文本到图像生成模型依赖VLMs生成的高质量图像-标题对进行训练，但VLMs易受对抗攻击，导致训练数据被污染，影响模型行为。本文探讨了通过对抗攻击VLMs对文本到图像模型训练管道进行污染的有效性，并展示了其实际攻击成功率。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 研究VLMs在文本到图像模型训练中的关键作用及其对抗脆弱性，探索对抗攻击对训练数据的潜在污染效果。

Method: 通过生成对抗性扰动图像误导VLMs产生错误标题，将污染样本注入文本到图像模型的训练管道。

Result: 实验表明，对抗攻击能高效污染训练数据，即使少量污染样本也能显著改变模型行为，且在商业VLMs中攻击成功率较高。

Conclusion: VLMs的对抗脆弱性可能引发数据质量下降和开发成本增加的猫鼠游戏，需加强防御措施。

Abstract: Today's text-to-image generative models are trained on millions of images
sourced from the Internet, each paired with a detailed caption produced by
Vision-Language Models (VLMs). This part of the training pipeline is critical
for supplying the models with large volumes of high-quality image-caption pairs
during training. However, recent work suggests that VLMs are vulnerable to
stealthy adversarial attacks, where adversarial perturbations are added to
images to mislead the VLMs into producing incorrect captions.
  In this paper, we explore the feasibility of adversarial mislabeling attacks
on VLMs as a mechanism to poisoning training pipelines for text-to-image
models. Our experiments demonstrate that VLMs are highly vulnerable to
adversarial perturbations, allowing attackers to produce benign-looking images
that are consistently miscaptioned by the VLM models. This has the effect of
injecting strong "dirty-label" poison samples into the training pipeline for
text-to-image models, successfully altering their behavior with a small number
of poisoned samples. We find that while potential defenses can be effective,
they can be targeted and circumvented by adaptive attackers. This suggests a
cat-and-mouse game that is likely to reduce the quality of training data and
increase the cost of text-to-image model development. Finally, we demonstrate
the real-world effectiveness of these attacks, achieving high attack success
(over 73%) even in black-box scenarios against commercial VLMs (Google Vertex
AI and Microsoft Azure).

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [229] [Simultaneously Fair Allocation of Indivisible Items Across Multiple Dimensions](https://arxiv.org/abs/2506.21727)
*Yasushi Kawase,Bodhayan Roy,Mohammad Azharuddin Sanpui*

Key words: 公平分配, 多维偏好, 嫉妒自由, sEFc, NP难性

TL;DR: 本文研究了在多维环境下不可分割物品的公平分配问题，提出了弱和强sEFc两个松弛的嫉妒自由变体，并给出了保证其存在的上下界和算法，同时证明了弱和强sEF1分配的NP难问题。

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: 现实中许多应用场景（如云计算资源分配）需要多维公平性评估，传统的一维公平概念无法满足需求，因此研究多维环境下的公平分配问题具有实际意义。

Method: 提出了弱sEFc和强sEFc两个松弛的嫉妒自由概念，研究了其理论界，并设计了检查分配存在的算法。

Result: 给出了弱和强sEFc分配的上下界（与物品总数无关），并证明了弱和强sEF1分配的NP难性。

Conclusion: 多维公平分配问题在现实中有广泛应用，弱和强sEFc为实际分配提供了可行的理论支持。

Abstract: This paper explores the fair allocation of indivisible items in a
multidimensional setting, motivated by the need to address fairness in complex
environments where agents assess bundles according to multiple criteria. Such
multidimensional settings are not merely of theoretical interest but are
central to many real-world applications. For example, cloud computing resources
are evaluated based on multiple criteria such as CPU cores, memory, and network
bandwidth. In such cases, traditional one dimensional fairness notions fail to
capture fairness across multiple attributes. To address these challenges, we
study two relaxed variants of envy-freeness: weak simultaneously envy-free up
to c goods (weak sEFc) and strong simultaneously envy-free up to c goods
(strong sEFc), which accommodate the multidimensionality of agents'
preferences. Under the weak notion, for every pair of agents and for each
dimension, any perceived envy can be eliminated by removing, if necessary, a
different set of goods from the envied agent's allocation. In contrast, the
strong version requires selecting a single set of goods whose removal from the
envied bundle simultaneously eliminates envy in every dimension. We provide
upper and lower bounds on the relaxation parameter c that guarantee the
existence of weak or strong sEFc allocations, where these bounds are
independent of the total number of items. In addition, we present algorithms
for checking whether a weak or strong sEFc allocation exists. Moreover, we
establish NP-hardness results for checking the existence of weak sEF1 and
strong sEF1 allocations.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [230] [Demonstrating Interoperable Channel State Feedback Compression with Machine Learning](https://arxiv.org/abs/2506.21796)
*Dani Korpi,Rachel Wang,Jerry Wang,Abdelrahman Ibrahim,Carl Nuzman,Runxin Wang,Kursat Rasim Mestav,Dustin Zhang,Iraj Saniee,Shawn Winston,Gordana Pavlovic,Wei Ding,William J. Hillery,Chenxi Hao,Ram Thirunagari,Jung Chang,Jeehyun Kim,Bartek Kozicki,Dragan Samardzija,Taesang Yoo,Andreas Maeder,Tingfang Ji,Harish Viswanathan*

Key words: 神经网络, 信道状态反馈, 机器学习, 压缩与解压缩, 6G网络

TL;DR: 该论文提出了一种新颖的方法，用于在保密的情况下训练可互操作的压缩和解压缩ML模型，并展示了在实际场景中的性能。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 研究ML在无线网络中基于神经网络的信道状态反馈压缩与解压缩的实际应用，填补实际场景中缺乏ML模型互操作性证明的空白。

Method: 开发了一种新颖的训练方法，无需共享ML模型，即可在用户设备和基站之间实现互操作的压缩和解压缩模型，并通过原型设备验证。

Result: 实验结果表明，无需共享ML模型即可实现高精度的信道信息重构，并在波束成形中提升了下行链路的吞吐量。

Conclusion: 该方法为6G网络中基于ML的信道反馈的实际应用铺平了道路。

Abstract: Neural network-based compression and decompression of channel state feedback
has been one of the most widely studied applications of machine learning (ML)
in wireless networks. Various simulation-based studies have shown that ML-based
feedback compression can result in reduced overhead and more accurate channel
information. However, to the best of our knowledge, there are no real-life
proofs of concepts demonstrating the benefits of ML-based channel feedback
compression in a practical setting, where the user equipment (UE) and base
station have no access to each others' ML models. In this paper, we present a
novel approach for training interoperable compression and decompression ML
models in a confidential manner, and demonstrate the accuracy of the ensuing
models using prototype UEs and base stations. The performance of the ML-based
channel feedback is measured both in terms of the accuracy of the reconstructed
channel information and achieved downlink throughput gains when using the
channel information for beamforming. The reported measurement results
demonstrate that it is possible to develop an accurate ML-based channel
feedback link without having to share ML models between device and network
vendors. These results pave the way for a practical implementation of ML-based
channel feedback in commercial 6G networks.

</details>


### [231] [From Token to Rhythm: A Multi-Scale Approach for ECG-Language Pretraining](https://arxiv.org/abs/2506.21803)
*Fuying Wang,Jiacheng Xu,Lequan Yu*

Key words: ECG分析, 自监督学习, 多尺度表征, 跨模态对齐, 心脏健康

TL;DR: 论文提出了一种新型多尺度ECG-语言预训练模型MELP，通过分层监督从ECG-文本对中学习通用表征，显著优于现有自监督学习方法。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 传统深度学习ECG分析依赖大规模标注数据，耗资源且耗时。现有自监督学习方法无法捕捉ECG信号的多尺度特性，导致表征泛化能力不足。

Method: 提出MELP模型：1) 预训练心脏学专用语言模型；2) 采用三个层级的跨模态监督（token、beat、rhythm）对齐ECG信号与文本报告。

Result: 在零样本ECG分类、线性探测和迁移学习任务中，MELP在三个公开数据集上超越现有方法。

Conclusion: MELP通过多尺度监督有效提取ECG的层次化表征，为临床任务提供了高效且适应性强的解决方案。

Abstract: Electrocardiograms (ECGs) play a vital role in monitoring cardiac health and
diagnosing heart diseases. However, traditional deep learning approaches for
ECG analysis rely heavily on large-scale manual annotations, which are both
time-consuming and resource-intensive to obtain. To overcome this limitation,
self-supervised learning (SSL) has emerged as a promising alternative, enabling
the extraction of robust ECG representations that can be efficiently
transferred to various downstream tasks. While previous studies have explored
SSL for ECG pretraining and multi-modal ECG-language alignment, they often fail
to capture the multi-scale nature of ECG signals. As a result, these methods
struggle to learn generalized representations due to their inability to model
the hierarchical structure of ECG data. To address this gap, we introduce MELP,
a novel Multi-scale ECG-Language Pretraining (MELP) model that fully leverages
hierarchical supervision from ECG-text pairs. MELP first pretrains a
cardiology-specific language model to enhance its understanding of clinical
text. It then applies three levels of cross-modal supervision-at the token,
beat, and rhythm levels-to align ECG signals with textual reports, capturing
structured information across different time scales. We evaluate MELP on three
public ECG datasets across multiple tasks, including zero-shot ECG
classification, linear probing, and transfer learning. Experimental results
demonstrate that MELP outperforms existing SSL methods, underscoring its
effectiveness and adaptability across diverse clinical applications. Our code
is available at https://github.com/HKU-MedAI/MELP.

</details>


### [232] [Searching Efficient Deep Architectures for Radar Target Detection using Monte-Carlo Tree Search](https://arxiv.org/abs/2506.21772)
*Noé Lallouet,Tristan Cazenave,Cyrille Enderli,Stéphanie Gourdin*

Key words: 神经网络，雷达目标检测，蒙特卡洛树搜索，NAS，计算复杂度

TL;DR: 研究者提出了一种基于蒙特卡洛树搜索的神经网络架构搜索方法，用于雷达目标检测，旨在降低计算复杂度同时保持性能。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 解决深度学习模型在嵌入式雷达系统中因计算复杂度高而难以广泛应用的问题。

Method: 采用蒙特卡洛树搜索（MCTS）的神经网络架构搜索（NAS）方法，设计轻量化的神经网络。

Result: 提出的新网络在满足检测性能的同时，显著降低了计算复杂度。

Conclusion: NAS方法成功找到了既高性能又轻量化的神经网络架构，适用于复杂环境下的雷达目标检测。

Abstract: Recent research works establish deep neural networks as high performing tools
for radar target detection, especially on challenging environments (presence of
clutter or interferences, multi-target scenarii...). However, the usually large
computational complexity of these networks is one of the factors preventing
them from being widely implemented in embedded radar systems. We propose to
investigate novel neural architecture search (NAS) methods, based on
Monte-Carlo Tree Search (MCTS), for finding neural networks achieving the
required detection performance and striving towards a lower computational
complexity. We evaluate the searched architectures on endoclutter radar
signals, in order to compare their respective performance metrics and
generalization properties. A novel network satisfying the required detection
probability while being significantly lighter than the expert-designed baseline
is proposed.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [233] [CaloHadronic: a diffusion model for the generation of hadronic showers](https://arxiv.org/abs/2506.21720)
*Thorsten Buss,Frank Gaede,Gregor Kasieczka,Anatolii Korol,Katja Krüger,Peter McKeown,Martina Mozzanica*

Key words: 机器学习，粒子簇射，扩散模型，Transformer，高颗粒度量能器

TL;DR: 本文介绍了一种基于Transformer的生成模型，用于模拟粒子物理学中高颗粒度量能器的粒子簇射，尤其是复杂强子簇射。

<details>
  <summary>Details</summary>

Main category: physics.ins-det

Motivation: 传统模拟方法计算成本高，而机器学习生成模型能高效补充传统模拟，提升精度和速度。

Method: 采用扩散生成模型与Transformer架构，生成几何独立的点云数据，适用于电磁量能器和强子量能器。

Result: 首次实现机器学习在全高颗粒度成像量能器系统中生成电磁和强子簇射，复杂子结构表现更优。

Conclusion: Transformer扩展的扩散模型有望成为粒子簇射模拟的高效工具。

Abstract: Simulating showers of particles in highly-granular calorimeters is a key
frontier in the application of machine learning to particle physics. Achieving
high accuracy and speed with generative machine learning models can enable them
to augment traditional simulations and alleviate a major computing constraint.
Recent developments have shown how diffusion based generative shower simulation
approaches that do not rely on a fixed structure, but instead generate
geometry-independent point clouds, are very efficient. We present a
transformer-based extension to previous architectures which were developed for
simulating electromagnetic showers in the highly granular electromagnetic
calorimeter of the International Large Detector, ILD. The attention mechanism
now allows us to generate complex hadronic showers with more pronounced
substructure across both the electromagnetic and hadronic calorimeters. This is
the first time that machine learning methods are used to holistically generate
showers across the electromagnetic and hadronic calorimeter in highly granular
imaging calorimeter systems.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [234] [Fetal Sleep: A Cross-Species Review of Physiology, Measurement, and Classification](https://arxiv.org/abs/2506.21828)
*Weitao Tang,Johann Vargas-Calixto,Nasim Katebi,Robert Galinsky,Gari D. Clifford,Faezeh Marzbanrad*

Key words: 胎儿睡眠、神经发育、缺氧、生长受限、监测技术、深度学习

TL;DR: 总结了胎儿睡眠的研究进展，包括生理特征、发育规律、物种差异以及监测技术，强调了在产前护理中开发非侵入式监测技术的重要性。

<details>
  <summary>Details</summary>

Main category: q-bio.NC

Motivation: 探讨胎儿睡眠模式对早期大脑发育的影响，帮助临床医生检测因缺氧或生长受限导致的神经问题。

Method: 综述了80多年的研究，比较人类与大型动物模型的睡眠模式，分析侵入式和非侵入式技术，并评估计算分类方法。

Result: 揭示了胎儿睡眠的物种特异性差异，总结了缺氧和生长受限对胎儿睡眠的干扰。

Conclusion: 为开发客观、多模式、非侵入式的胎儿睡眠监测技术提供了基础，以支持早期诊断和干预。

Abstract: Fetal sleep is a relatively underexplored yet vital aspect of prenatal
neurodevelopment. Understanding fetal sleep patterns could provide insights
into early brain maturation and help clinicians detect signs of neurological
compromise that arise due to fetal hypoxia or fetal growth restriction. This
review synthesizes over eight decades of research on the physiological
characteristics, ontogeny, and regulation of fetal sleep. We compare
sleep-state patterns in humans and large animal models, highlighting
species-specific differences and the presence of sleep-state analogs. We review
both invasive techniques in animals and non-invasive modalities in humans.
Computational methods for sleep-state classification are also examined,
including rule-based approaches (with and without clustering-based
preprocessing) and state-of-the-art deep learning techniques. Finally, we
discuss how intrauterine conditions such as hypoxia and fetal growth
restriction can disrupt fetal sleep. This review provides a comprehensive
foundation for the development of objective, multimodal, and non-invasive fetal
sleep monitoring technologies to support early diagnosis and intervention in
prenatal care.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [235] [RiverEcho: Real-Time Interactive Digital System for Ancient Yellow River Culture](https://arxiv.org/abs/2506.21865)
*Haofeng Wang,Yilin Guo,Zehao Li,Tong Yue,Yizong Wang,Enci Zhang,Rongqun Lin,Feng Gao,Shiqi Wang,Siwei Ma*

Key words: 黄河文化、数字人、大型语言模型、检索增强生成、实时交互

TL;DR: 设计了一个名为RiverEcho的实时交互系统，通过语音查询和大型语言模型结合文化知识数据集，以数字人形式传递黄河文化解说。

<details>
  <summary>Details</summary>

Main category: cs.MM

Motivation: 保护和传承黄河文化，丰富其传播方式。

Method: 构建黄河文化知识数据库，采用检索增强生成（RAG）技术提升大型语言模型（LLM）的回答质量。

Result: 系统能生成更专业、信息更丰富的回答，为用户提供更深的文化见解。

Conclusion: 系统不仅多样化黄河文化的推广手段，还提升了用户体验和文化理解。

Abstract: The Yellow River is China's mother river and a cradle of human civilization.
The ancient Yellow River culture is, moreover, an indispensable part of human
art history. To conserve and inherit the ancient Yellow River culture, we
designed RiverEcho, a real-time interactive system that responds to voice
queries using a large language model and a cultural knowledge dataset,
delivering explanations through a talking-head digital human. Specifically, we
built a knowledge database focused on the ancient Yellow River culture,
including the collection of historical texts and the processing pipeline.
Experimental results demonstrate that leveraging Retrieval-Augmented Generation
(RAG) on the proposed dataset enhances the response quality of the Large
Language Model(LLM), enabling the system to generate more professional and
informative responses. Our work not only diversifies the means of promoting
Yellow River culture but also provides users with deeper cultural insights.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [236] [Adversarial Threats in Quantum Machine Learning: A Survey of Attacks and Defenses](https://arxiv.org/abs/2506.21842)
*Archisman Ghosh,Satwik Kundu,Swaroop Ghosh*

Key words: 量子机器学习、安全挑战、对抗攻击、防御机制、NISQ时代

TL;DR: 量子机器学习（QML）结合量子计算与经典机器学习，面临NISQ时代的安全挑战，包括模型窃取、数据中毒等攻击手段，防御机制利用量子特性应对，但需解决可靠性、跨平台攻击等开放性问题。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 研究QML系统的安全性，因其在NISQ时代的部署中存在独特的对抗性威胁，需探讨如何保护模型完整性、所有权和功能。

Method: 分析QML系统中的关键攻击向量（如模型窃取、数据中毒等）及对应的防御机制（如水印技术、硬件感知混淆等）。

Result: 识别了QML的漏洞并提出防御方案，但需平衡噪声水平、跨平台攻击等问题。

Conclusion: 总结了QML安全的研究进展，为构建可靠的QML系统提供了方向。

Abstract: Quantum Machine Learning (QML) integrates quantum computing with classical
machine learning, primarily to solve classification, regression and generative
tasks. However, its rapid development raises critical security challenges in
the Noisy Intermediate-Scale Quantum (NISQ) era. This chapter examines
adversarial threats unique to QML systems, focusing on vulnerabilities in
cloud-based deployments, hybrid architectures, and quantum generative models.
Key attack vectors include model stealing via transpilation or output
extraction, data poisoning through quantum-specific perturbations, reverse
engineering of proprietary variational quantum circuits, and backdoor attacks.
Adversaries exploit noise-prone quantum hardware and insufficiently secured
QML-as-a-Service (QMLaaS) workflows to compromise model integrity, ownership,
and functionality. Defense mechanisms leverage quantum properties to counter
these threats. Noise signatures from training hardware act as non-invasive
watermarks, while hardware-aware obfuscation techniques and ensemble strategies
disrupt cloning attempts. Emerging solutions also adapt classical adversarial
training and differential privacy to quantum settings, addressing
vulnerabilities in quantum neural networks and generative architectures.
However, securing QML requires addressing open challenges such as balancing
noise levels for reliability and security, mitigating cross-platform attacks,
and developing quantum-classical trust frameworks. This chapter summarizes
recent advances in attacks and defenses, offering a roadmap for researchers and
practitioners to build robust, trustworthy QML systems resilient to evolving
adversarial landscapes.

</details>


### [237] [Robust quantum reservoir computers for forecasting chaotic dynamics: generalized synchronization and stability](https://arxiv.org/abs/2506.22335)
*Osama Ahmed,Felix Tennie,Luca Magri*

Key words: 量子储备计算机, 混沌动力学, GS=ESP, 鲁棒性, 时间序列预测

TL;DR: 该论文研究表明循环量子储备计算机（QRCs）及其无循环架构（RF-QRCs）能有效学习和预测混沌动力学。通过将其建模为耦合动力系统，展示了其学习混沌动力学及不变量（如Lyapunov谱）的能力，并提出了GS=ESP设计准则。噪声模拟显示耗散增强了鲁棒性。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 研究量子储备计算机在学习和预测混沌动力学中的潜力，并为设计稳健的量子机器提供理论基础。

Method: 将量子储备计算机建模为耦合动力系统，推导其Jacobian矩阵，提出GS=ESP设计准则，并分析噪声影响。

Result: QRCs能学习混沌动力学及不变量，GS=ESP准则有效，噪声耗散增强系统鲁棒性。

Conclusion: 量子储备计算机在混沌时间序列预测中具有潜力，为近量子硬件设计提供了新思路。

Abstract: We show that recurrent quantum reservoir computers (QRCs) and their
recurrence-free architectures (RF-QRCs) are robust tools for learning and
forecasting chaotic dynamics from time-series data. First, we formulate and
interpret quantum reservoir computers as coupled dynamical systems, where the
reservoir acts as a response system driven by training data; in other words,
quantum reservoir computers are generalized-synchronization (GS) systems.
Second, we show that quantum reservoir computers can learn chaotic dynamics and
their invariant properties, such as Lyapunov spectra, attractor dimensions, and
geometric properties such as the covariant Lyapunov vectors. This analysis is
enabled by deriving the Jacobian of the quantum reservoir update. Third, by
leveraging tools from generalized synchronization, we provide a method for
designing robust quantum reservoir computers. We propose the criterion
$GS=ESP$: GS implies the echo state property (ESP), and vice versa. We
analytically show that RF-QRCs, by design, fulfill $GS=ESP$. Finally, we
analyze the effect of simulated noise. We find that dissipation from noise
enhances the robustness of quantum reservoir computers. Numerical verifications
on systems of different dimensions support our conclusions. This work opens
opportunities for designing robust quantum machines for chaotic time series
forecasting on near-term quantum hardware.

</details>


### [238] [QuKAN: A Quantum Circuit Born Machine approach to Quantum Kolmogorov Arnold Networks](https://arxiv.org/abs/2506.22340)
*Yannick Werner,Akash Malemath,Mengxi Liu,Vitor Fortes Rey,Nikolaos Palaiodimopoulos,Paul Lukowicz,Maximilian Kiefer-Emmanouilidis*

Key words: Kolmogorov Arnold Networks, Quantum Machine Learning, Quantum Circuit Born Machine, Quantum KAN

TL;DR: 本文研究了基于Kolmogorov Arnold表示定理的网络（KANs）在量子机器学习中的应用，提出了混合和全量子形式的Quantum KAN（QuKAN）架构，并展示了其可行性和性能。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 探索KANs在量子机器学习中的潜力，利用其能够以较少神经元表达复杂函数的优势。

Method: 采用量子电路Born机器（QCBM）实现混合和全量子形式的KAN架构，利用预训练残差函数进行参数化量子电路的迁移。

Result: 展示了QuKAN架构的可行性、可解释性和性能。

Conclusion: QuKAN架构在量子机器学习中表现出潜力，为相关研究提供了新思路。

Abstract: Kolmogorov Arnold Networks (KANs), built upon the Kolmogorov Arnold
representation theorem (KAR), have demonstrated promising capabilities in
expressing complex functions with fewer neurons. This is achieved by
implementing learnable parameters on the edges instead of on the nodes, unlike
traditional networks such as Multi-Layer Perceptrons (MLPs). However, KANs
potential in quantum machine learning has not yet been well explored. In this
work, we present an implementation of these KAN architectures in both hybrid
and fully quantum forms using a Quantum Circuit Born Machine (QCBM). We adapt
the KAN transfer using pre-trained residual functions, thereby exploiting the
representational power of parametrized quantum circuits. In the hybrid model we
combine classical KAN components with quantum subroutines, while the fully
quantum version the entire architecture of the residual function is translated
to a quantum model. We demonstrate the feasibility, interpretability and
performance of the proposed Quantum KAN (QuKAN) architecture.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [239] [DiffSoundStream: Efficient Speech Tokenization via Diffusion Decoding](https://arxiv.org/abs/2506.22362)
*Yang Yang,Yunpeng Li,George Sung,Shao-Fu Shih,Craig Dooley,Alessio Centazzo,Ramanan Rajeswaran*

Key words: 语音token化, 自监督学习, 潜在扩散模型, SoundStream, 语音生成

TL;DR: DiffSoundStream通过减少语义与声学token的冗余和利用潜在扩散模型，提升了非流式场景下语音token化的效率，以更低的token速率实现与标准SoundStream相当的语音质量。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 现有的语音token化方法存在token率限制问题，影响推断速度，需要提升效率。

Method: 结合条件神经编解码器和潜在扩散模型，减少token冗余并高效合成语音波形。

Result: 50 token/秒下，DiffSoundStream语音质量与标准SoundStream在100 token/秒时相当，且仅需少量扩散步骤。

Conclusion: DiffSoundStream以更高效的方式实现了高质量的语音生成。

Abstract: Token-based language modeling is a prominent approach for speech generation,
where tokens are obtained by quantizing features from self-supervised learning
(SSL) models and extracting codes from neural speech codecs, generally referred
to as semantic tokens and acoustic tokens. These tokens are often modeled
autoregressively, with the inference speed being constrained by the token rate.
In this work, we propose DiffSoundStream, a solution that improves the
efficiency of speech tokenization in non-streaming scenarios through two
techniques: (1) conditioning the neural codec on semantic tokens to minimize
redundancy between semantic and acoustic tokens, and (2) leveraging latent
diffusion models to synthesize high-quality waveforms from semantic and
coarse-level acoustic tokens. Experiments show that at 50 tokens per second,
DiffSoundStream achieves speech quality on par with a standard SoundStream
model operating at twice the token rate. Additionally, we achieve step-size
distillation using just four diffusion sampling steps with only a minor quality
loss.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [240] [3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach](https://arxiv.org/abs/2506.21845)
*Zhuodi Cai*

Key words: 3D建模, 人机协作, 自然语言处理, 计算机视觉, 网页应用

TL;DR: 3Description 是一种实验性的人机协作方法，旨在通过语言和手势描述，让非专业人员也能参与3D建模，提升3D建模的易用性。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 传统3D建模对非专业人员来说门槛较高，3Description希望通过人机协作降低门槛，让更多人参与3D世界构建。

Method: 结合自然语言处理（NLP）和计算机视觉（CV）技术，通过网页端支持用户通过语言和手势输入共同创建3D模型。

Result: 3Description提供了一种更包容和用户友好的设计过程，使人机协作更具参与性，同时保留人类创造力。

Conclusion: 3Description为人机协作3D建模提供了新思路，在AI时代推动了更广泛的参与和技术与创意的平衡。

Abstract: This paper presents 3Description, an experimental human-AI collaborative
approach for intuitive 3D modeling. 3Description aims to address accessibility
and usability challenges in traditional 3D modeling by enabling
non-professional individuals to co-create 3D models using verbal and gesture
descriptions. Through a combination of qualitative research, product analysis,
and user testing, 3Description integrates AI technologies such as Natural
Language Processing and Computer Vision, powered by OpenAI and MediaPipe.
Recognizing the web has wide cross-platform capabilities, 3Description is
web-based, allowing users to describe the desired model and subsequently adjust
its components using verbal and gestural inputs. In the era of AI and emerging
media, 3Description not only contributes to a more inclusive and user-friendly
design process, empowering more people to participate in the construction of
the future 3D world, but also strives to increase human engagement in
co-creation with AI, thereby avoiding undue surrender to technology and
preserving human creativity.

</details>


### [241] [Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education](https://arxiv.org/abs/2506.22231)
*Russell Beale*

Key words: 生成式AI、高等教育、学术诚信、政策调整、ChatGPT

TL;DR: 生成式人工智能（如ChatGPT）在高等教育中既带来效率提升，也引发学术诚信等挑战，需通过政策调整和评估改革应对。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 探讨生成式AI在高等教育中的应用机会及其引发的学术诚信与伦理问题。

Method: 结合实证研究与案例分析，分析AI工具的使用现状和检测工具的准确性。

Result: 47%的学生使用AI辅助学习，检测工具准确率88%；需通过政策调整和评估设计来应对挑战。

Conclusion: 高校需主动调整政策，平衡AI潜力与学术核心价值。

Abstract: The rapid proliferation of generative artificial intelligence (AI) tools -
especially large language models (LLMs) such as ChatGPT - has ushered in a
transformative era in higher education. Universities in developed regions are
increasingly integrating these technologies into research, teaching, and
assessment. On one hand, LLMs can enhance productivity by streamlining
literature reviews, facilitating idea generation, assisting with coding and
data analysis, and even supporting grant proposal drafting. On the other hand,
their use raises significant concerns regarding academic integrity, ethical
boundaries, and equitable access. Recent empirical studies indicate that nearly
47% of students use LLMs in their coursework - with 39% using them for exam
questions and 7% for entire assignments - while detection tools currently
achieve around 88% accuracy, leaving a 12% error margin. This article
critically examines the opportunities offered by generative AI, explores the
multifaceted challenges it poses, and outlines robust policy solutions.
Emphasis is placed on redesigning assessments to be AI-resilient, enhancing
staff and student training, implementing multi-layered enforcement mechanisms,
and defining acceptable use. By synthesizing data from recent research and case
studies, the article argues that proactive policy adaptation is imperative to
harness AI's potential while safeguarding the core values of academic integrity
and equity.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [242] [LLM2Rec: Large Language Models Are Powerful Embedding Models for Sequential Recommendation](https://arxiv.org/abs/2506.21579)
*Yingzhi He,Xiaohao Liu,An Zhang,Yunshan Ma,Tat-Seng Chua*

Key words: 序列推荐, 大型语言模型, 协同过滤, 跨领域推荐

TL;DR: LLM2Rec是一种结合大型语言模型和协同过滤信号的嵌入模型，用于提升序列推荐系统的性能，支持跨领域推荐。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 传统序列推荐依赖ID嵌入，缺乏跨领域泛化能力；文本推荐虽具有语义理解优势，但忽视了协同过滤信号。LLM2Rec旨在结合两者的优势。

Method: 采用两阶段训练框架：1）协同监督微调，使LLM基于历史交互推断物品关系；2）物品级嵌入建模，生成融合语义与协作信息的嵌入模型。

Result: 实验表明，LLM2Rec显著提升了领域内和跨领域的推荐效果。

Conclusion: LLM2Rec展示了利用LLM构建更鲁棒、泛化能力强的嵌入模型的潜力。

Abstract: Sequential recommendation aims to predict users' future interactions by
modeling collaborative filtering (CF) signals from historical behaviors of
similar users or items. Traditional sequential recommenders predominantly rely
on ID-based embeddings, which capture CF signals through high-order
co-occurrence patterns. However, these embeddings depend solely on past
interactions, lacking transferable knowledge to generalize to unseen domains.
Recent advances in large language models (LLMs) have motivated text-based
recommendation approaches that derive item representations from textual
descriptions. While these methods enhance generalization, they fail to encode
CF signals-i.e., latent item correlations and preference patterns-crucial for
effective recommendation. We argue that an ideal embedding model should
seamlessly integrate CF signals with rich semantic representations to improve
both in-domain and out-of-domain recommendation performance.
  To this end, we propose LLM2Rec, a novel embedding model tailored for
sequential recommendation, integrating the rich semantic understanding of LLMs
with CF awareness. Our approach follows a two-stage training framework: (1)
Collaborative Supervised Fine-tuning, which adapts LLMs to infer item
relationships based on historical interactions, and (2) Item-level Embedding
Modeling, which refines these specialized LLMs into structured item embedding
models that encode both semantic and collaborative information. Extensive
experiments on real-world datasets demonstrate that LLM2Rec effectively
improves recommendation quality across both in-domain and out-of-domain
settings. Our findings highlight the potential of leveraging LLMs to build more
robust, generalizable embedding models for sequential recommendation. Our codes
are available at https://github.com/HappyPointer/LLM2Rec.

</details>


### [243] [Evaluating the Robustness of Dense Retrievers in Interdisciplinary Domains](https://arxiv.org/abs/2506.21581)
*Sarthak Chaturvedi,Anurag Acharya,Rounak Meyur,Koby Hayashi,Sai Munikoti,Sameera Horawalavithana*

Key words: 领域适应、检索模型、评估基准、语义结构、环保法规文档

TL;DR: 论文指出评估基准的特性可能扭曲检索模型中领域适应的真实效果，导致误导性评估，影响专业领域的部署决策。通过环保法规文档检索案例，研究发现不同语义结构的基准对微调效果的评估差异显著。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 研究动机是揭示评估基准的特性如何影响领域适应方法的感知效果，以避免在专业领域中因误导性评估而做出错误的部署决策。

Method: 研究使用环保法规文档检索为案例，在ColBERTv2模型上微调环境影响声明（EIS），并通过两个具有不同语义结构的基准进行评估，分析主题多样性和边界重叠的影响。

Result: 结果表明，相同领域适应方法在不同基准上表现出显著差异的改进效果（NDCG增益从0.61%到2.22%）。高多样性、高重叠语义的基准能更真实地反映改进。

Conclusion: 评估基准的选择对专业领域检索系统的效果评估至关重要，语义边界重叠的基准能更准确地反映领域适应的实际效果。

Abstract: Evaluation benchmark characteristics may distort the true benefits of domain
adaptation in retrieval models. This creates misleading assessments that
influence deployment decisions in specialized domains. We show that two
benchmarks with drastically different features such as topic diversity,
boundary overlap, and semantic complexity can influence the perceived benefits
of fine-tuning. Using environmental regulatory document retrieval as a case
study, we fine-tune ColBERTv2 model on Environmental Impact Statements (EIS)
from federal agencies. We evaluate these models across two benchmarks with
different semantic structures. Our findings reveal that identical domain
adaptation approaches show very different perceived benefits depending on
evaluation methodology. On one benchmark, with clearly separated topic
boundaries, domain adaptation shows small improvements (maximum 0.61% NDCG
gain). However, on the other benchmark with overlapping semantic structures,
the same models demonstrate large improvements (up to 2.22% NDCG gain), a
3.6-fold difference in the performance benefit. We compare these benchmarks
through topic diversity metrics, finding that the higher-performing benchmark
shows 11% higher average cosine distances between contexts and 23% lower
silhouette scores, directly contributing to the observed performance
difference. These results demonstrate that benchmark selection strongly
determines assessments of retrieval system effectiveness in specialized
domains. Evaluation frameworks with well-separated topics regularly
underestimate domain adaptation benefits, while those with overlapping semantic
boundaries reveal improvements that better reflect real-world regulatory
document complexity. Our findings have important implications for developing
and deploying AI systems for interdisciplinary domains that integrate multiple
topics.

</details>


### [244] [Reinforcement Fine-Tuned Large Language Models for Next POI Recommendation](https://arxiv.org/abs/2506.21599)
*Peibo Li,Shuang Ao,Hao Xue,Yang Song,Maarten de Rijke,Johan Barthélemy,Tomasz Bednarz,Flora D. Salim*

Key words: LLMs, POI recommendation, reinforcement learning, fine-tuning

TL;DR: 该论文提出了Refine-POI框架，通过强化学习微调解决LLMs在下一兴趣点推荐中的性能问题，显著提升推荐效果。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 解决SFT-based模型在下一兴趣点推荐任务中因数据不匹配导致的性能限制问题。

Method: 引入强化学习微调框架Refine-POI，设计推荐驱动的奖励机制，仅使用单一目标POI生成推荐列表。

Result: 在真实数据集上验证，Refine-POI实现了最先进的top-k推荐性能。

Conclusion: Refine-POI有效解决了SFT-based模型的数据不匹配问题，提升了推荐准确性。

Abstract: Large language models (LLMs) have been adopted for next point-of-interest
(POI) recommendation tasks. Typical LLM-based recommenders fall into two
categories: prompt-based and supervised fine-tuning (SFT)-based models.
Prompt-based models generally offer greater output flexibility but deliver
lower accuracy, whereas SFT-based models achieve higher performance yet face a
fundamental mismatch: next POI recommendation data does not naturally suit
supervised fine-tuning. In SFT, the model is trained to reproduce the exact
ground truth, but each training example provides only a single target POI, so
there is no ground truth for producing a top-k list.
  To address this, we propose Refine-POI, a reinforcement fine-tuning framework
for next POI recommendation. We introduce recommendation-driven rewards that
enable LLMs to learn to generate top-k recommendation lists using only one
ground-truth POI per example. Experiments on real-world datasets demonstrate
that Refine-POI achieves state-of-the-art top-k recommendation performance.

</details>


### [245] [Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding](https://arxiv.org/abs/2506.21604)
*Varun Mannam,Fang Wang,Xin Chen*

Key words: 多模态RAG、可信度评估、企业AI、模态权重

TL;DR: 提出了一个系统化的量化基准框架，用于评估多模态RAG系统的可信度，通过优化模态权重提升性能57.3%。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 当前多模态生成AI的评估框架难以建立信任，阻碍了企业应用，尤其是在可靠性至关重要的场景中。

Method: 引入了一个系统化的量化基准框架，测量跨模态输入（如文本、图像、标题和OCR）的整合对可信度的影响。

Result: 实验显示，优化模态权重（30%文本、15%图像、25%标题和30% OCR）比纯文本基线提升了57.3%的性能。

Conclusion: 该研究为企业级AI提供了量化可信度的严谨框架，推动了多模态RAG在关键应用中的可靠部署。

Abstract: Current evaluation frameworks for multimodal generative AI struggle to
establish trustworthiness, hindering enterprise adoption where reliability is
paramount. We introduce a systematic, quantitative benchmarking framework to
measure the trustworthiness of progressively integrating cross-modal inputs
such as text, images, captions, and OCR within VisualRAG systems for enterprise
document intelligence. Our approach establishes quantitative relationships
between technical metrics and user-centric trust measures. Evaluation reveals
that optimal modality weighting with weights of 30% text, 15% image, 25%
caption, and 30% OCR improves performance by 57.3% over text-only baselines
while maintaining computational efficiency. We provide comparative assessments
of foundation models, demonstrating their differential impact on
trustworthiness in caption generation and OCR extraction-a vital consideration
for reliable enterprise AI. This work advances responsible AI deployment by
providing a rigorous framework for quantifying and enhancing trustworthiness in
multimodal RAG for critical enterprise applications.

</details>


### [246] [Bayesian-Guided Diversity in Sequential Sampling for Recommender Systems](https://arxiv.org/abs/2506.21617)
*Hiba Bederina,Jill-Jênn Vie*

Key words: 推荐系统，多样性优化，贝叶斯更新，多目标优化

TL;DR: 本文提出了一种新颖的推荐系统框架，通过贝叶斯更新和多目标优化平衡用户相关性与内容多样性，显著提升了多样性同时保持相关性。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 为了解决推荐系统中内容同质化和用户参与度下降的问题。

Method: 采用多目标、上下文顺序采样策略，结合贝叶斯更新、多样性指标和探索-开发权衡。

Result: 实验表明该方法在不牺牲相关性的情况下显著提高了多样性。

Conclusion: 该框架在大规模推荐场景中具有提升用户体验的潜力。

Abstract: The challenge of balancing user relevance and content diversity in
recommender systems is increasingly critical amid growing concerns about
content homogeneity and reduced user engagement. In this work, we propose a
novel framework that leverages a multi-objective, contextual sequential
sampling strategy. Item selection is guided by Bayesian updates that
dynamically adjust scores to optimize diversity. The reward formulation
integrates multiple diversity metrics-including the log-determinant volume of a
tuned similarity submatrix and ridge leverage scores-along with a diversity
gain uncertainty term to address the exploration-exploitation trade-off. Both
intra- and inter-batch diversity are modeled to promote serendipity and
minimize redundancy. A dominance-based ranking procedure identifies
Pareto-optimal item sets, enabling adaptive and balanced selections at each
iteration. Experiments on a real-world dataset show that our approach
significantly improves diversity without sacrificing relevance, demonstrating
its potential to enhance user experience in large-scale recommendation
settings.

</details>


### [247] [DCN^2: Interplay of Implicit Collision Weights and Explicit Cross Layers for Large-Scale Recommendation](https://arxiv.org/abs/2506.21624)
*Blaž Škrlj,Yonatan Karni,Grega Gašperšič,Blaž Mramor,Yulia Stolin,Martin Jakomin,Jasna Urbančič,Yuval Dishi,Natalia Silberstein,Ophir Friedler,Assaf Klein*

Key words: DCNv2, DCN^2, recommender systems, deep learning, algorithmic improvements

TL;DR: 论文介绍了对DCNv2架构的三项重要改进，提出了DCN^2架构，显著提升了推荐系统的性能并解决了DCNv2的关键限制。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: DCNv2虽然效率高且建模能力强，但在实际应用中仍存在信息丢失、碰撞管理不足等限制，需要进一步优化以提升性能。

Method: 提出了三项改进：处理Cross层的信息丢失、引入可学习的查找级权重隐式管理碰撞、通过定制层显式建模成对相似性。

Result: 改进后的DCN^2在离线测试和在线AB测试中均优于DCNv2，并在四个公开基准数据集上表现优异。

Conclusion: DCN^2是一种高效且强大的推荐系统架构，解决了DCNv2的关键问题并显著提升了性能。

Abstract: The Deep and Cross architecture (DCNv2) is a robust production baseline and
is integral to numerous real-life recommender systems. Its inherent efficiency
and ability to model interactions often result in models that are both simpler
and highly competitive compared to more computationally demanding alternatives,
such as Deep FFMs. In this work, we introduce three significant algorithmic
improvements to the DCNv2 architecture, detailing their formulation and
behavior at scale. The enhanced architecture we refer to as DCN^2 is actively
used in a live recommender system, processing over 0.5 billion predictions per
second across diverse use cases where it out-performed DCNv2, both offline and
online (ab tests). These improvements effectively address key limitations
observed in the DCNv2, including information loss in Cross layers, implicit
management of collisions through learnable lookup-level weights, and explicit
modeling of pairwise similarities with a custom layer that emulates FFMs'
behavior. The superior performance of DCN^2 is also demonstrated on four
publicly available benchmark data sets.

</details>


### [248] [IRanker: Towards Ranking Foundation Model](https://arxiv.org/abs/2506.21638)
*Tao Feng,Zhigang Hua,Zijie Lei,Yan Xie,Shuang Yang,Bo Long,Jiaxuan You*

Key words: 排名基础模型, 强化学习, 迭代解码, 零样本泛化

TL;DR: 提出了一种基于强化学习和迭代解码的排名基础模型IRanker，统一多种排名任务，显著减少输出组合空间，并在多个数据集上实现最先进性能。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 为了解决排名任务中缺乏明确监督标签的问题，并统一不同排名任务的需求，减少模型设计的复杂性。

Method: 使用强化学习和迭代解码，将复杂排名任务分解为逐步淘汰最差候选的过程。

Result: IRanker-3B在多个数据集上表现优于同类模型，并在零样本泛化实验中显著提升了性能。

Conclusion: IRanker-3B是一种高效且通用的排名基础模型，适用于多种任务场景。

Abstract: Ranking tasks are ubiquitous, encompassing applications such as
recommendation systems, LLM routing, and item re-ranking. We propose to unify
these tasks using a single ranking foundation model (FM), as it eliminates the
need for designing different models for each specific ranking task. However,
unlike general supervision tasks in LLMs, ranking tasks do not have clear
labels for supervision, posing great challenges to developing a ranking FM. To
overcome these challenges, we propose IRanker, a ranking FM framework with
reinforcement learning (RL) and iterative decoding. Our insight is to decompose
the complex ranking task into an iterative decoding process that eliminates the
worst candidate from the candidate pool step by step, which significantly
reduces the output combinatorial space and better utilizes the limited context
length during RL training. We meticulously train and comprehensively evaluate
an IRanker-3B model on nine datasets across three scenarios: recommendation,
routing, and passage ranking. The results show that a single IRanker-3B
achieves state-of-the-art results on several datasets compared to models of
similar size, and even surpasses the performance of larger models on certain
datasets. We further demonstrate the effectiveness of our RL design and the
robustness of the iterative mechanism across different LLM sizes. Moreover, we
conducted both in-domain and out-of-domain zero-shot generalization
experiments, which showed that IRanker-3B achieved good generalization on
in-domain ranking tasks compared to the base LLM by at least 5% improvement.
Surprisingly, on out-of-domain generic LLM tasks, IRanker-3B outperformed the
base model by at least 9% on GSM8K, IFEval, and MathQA. In addition, the
thoughts generated by IRanker-3B during training could further enhance
zero-shot LLM performance.

</details>


### [249] [ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation](https://arxiv.org/abs/2506.21931)
*Reza Yousefi Maragheh,Pratheek Vadla,Priyank Gupta,Kai Zhao,Aysenur Inan,Kehui Yao,Jianpeng Xu,Praveen Kanumala,Jason Cho,Sushant Kumar*

Key words: ARAG, Retrieval-Augmented Generation, 个性化推荐, 多智能体协作

TL;DR: ARAG框架通过多智能体协作增强RAG，显著提升个性化推荐性能。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 现有基于RAG的方法依赖静态检索，难以捕捉动态推荐中的用户偏好。

Method: ARAG整合四个LLM智能体：用户理解、NLI评估、上下文总结和项目排序。

Result: 在NDCG@5和Hit@5上分别提升42.1%和35.5%，优于标准RAG和基线。

Conclusion: 智能体推理有效提升检索增强推荐，为LLM个性化提供新方向。

Abstract: Retrieval-Augmented Generation (RAG) has shown promise in enhancing
recommendation systems by incorporating external context into large language
model prompts. However, existing RAG-based approaches often rely on static
retrieval heuristics and fail to capture nuanced user preferences in dynamic
recommendation scenarios. In this work, we introduce ARAG, an Agentic
Retrieval-Augmented Generation framework for Personalized Recommendation, which
integrates a multi-agent collaboration mechanism into the RAG pipeline. To
better understand the long-term and session behavior of the user, ARAG
leverages four specialized LLM-based agents: a User Understanding Agent that
summarizes user preferences from long-term and session contexts, a Natural
Language Inference (NLI) Agent that evaluates semantic alignment between
candidate items retrieved by RAG and inferred intent, a context summary agent
that summarizes the findings of NLI agent, and an Item Ranker Agent that
generates a ranked list of recommendations based on contextual fit. We evaluate
ARAG accross three datasets. Experimental results demonstrate that ARAG
significantly outperforms standard RAG and recency-based baselines, achieving
up to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an
ablation study to analyse the effect by different components of ARAG. Our
findings highlight the effectiveness of integrating agentic reasoning into
retrieval-augmented recommendation and provide new directions for LLM-based
personalization.

</details>


### [250] [HyReC: Exploring Hybrid-based Retriever for Chinese](https://arxiv.org/abs/2506.21913)
*Zunran Wang,Zheng Shenpeng,Wang Shenglan,Minghui Zhao,Zhonghua Li*

Key words: 混合检索、中文检索、端到端优化、GLAE、NM

TL;DR: 该论文提出了一种名为HyReC的创新端到端优化方法，专门针对中文混合检索，结合了词典和密集检索的优势，并在语义共享和干扰最小化方面进行了优化。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 尽管混合检索方法在性能上有显著提升，但其在中文检索中的应用仍未充分探索，论文旨在填补这一空白。

Method: HyReC方法通过将术语的语义联合嵌入表示模型，设计了全局局部感知编码器（GLAE）以促进语义共享，并引入归一化模块（NM）进一步优化对齐。

Result: 在C-MTEB检索基准测试中，HyReC展示了其有效性。

Conclusion: HyReC为中文混合检索提供了一种高效的优化方法，显著提升了性能。

Abstract: Hybrid-based retrieval methods, which unify dense-vector and lexicon-based
retrieval, have garnered considerable attention in the industry due to
performance enhancement. However, despite their promising results, the
application of these hybrid paradigms in Chinese retrieval contexts has
remained largely underexplored. In this paper, we introduce HyReC, an
innovative end-to-end optimization method tailored specifically for
hybrid-based retrieval in Chinese. HyReC enhances performance by integrating
the semantic union of terms into the representation model. Additionally, it
features the Global-Local-Aware Encoder (GLAE) to promote consistent semantic
sharing between lexicon-based and dense retrieval while minimizing the
interference between them. To further refine alignment, we incorporate a
Normalization Module (NM) that fosters mutual benefits between the retrieval
approaches. Finally, we evaluate HyReC on the C-MTEB retrieval benchmark to
demonstrate its effectiveness.

</details>


### [251] [Literature-Grounded Novelty Assessment of Scientific Ideas](https://arxiv.org/abs/2506.22026)
*Simra Shahid,Marissa Radensky,Raymond Fok,Pao Siangliulue,Daniel S. Weld,Tom Hope*

Key words: 自动评估, 新颖性, 检索增强生成, LLM, 两阶段检索

TL;DR: 论文提出了一种基于LLM和检索增强生成（RAG）的框架Idea Novelty Checker，用于自动评估科学想法的新颖性，通过两阶段检索-重排序方法显著提高了与现有方法的一致性。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 手动评估科学想法的新颖性耗时长、主观性强且难以规模化，因此需要自动化的解决方案。

Method: 采用两阶段的检索-重排序方法：关键词和片段检索收集相关论文，嵌入过滤和基于面的LLM重排序优化结果。

Result: 实验表明，该方法与现有方法相比，一致性提高了约13%，且基于面的重排序在识别相关文献中起关键作用。

Conclusion: Idea Novelty Checker有效解决了科学想法新颖性自动化评估的挑战，并显著优于现有方法。

Abstract: Automated scientific idea generation systems have made remarkable progress,
yet the automatic evaluation of idea novelty remains a critical and
underexplored challenge. Manual evaluation of novelty through literature review
is labor-intensive, prone to error due to subjectivity, and impractical at
scale. To address these issues, we propose the Idea Novelty Checker, an
LLM-based retrieval-augmented generation (RAG) framework that leverages a
two-stage retrieve-then-rerank approach. The Idea Novelty Checker first
collects a broad set of relevant papers using keyword and snippet-based
retrieval, then refines this collection through embedding-based filtering
followed by facet-based LLM re-ranking. It incorporates expert-labeled examples
to guide the system in comparing papers for novelty evaluation and in
generating literature-grounded reasoning. Our extensive experiments demonstrate
that our novelty checker achieves approximately 13% higher agreement than
existing approaches. Ablation studies further showcases the importance of the
facet-based re-ranker in identifying the most relevant literature for novelty
evaluation.

</details>


### [252] [Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement](https://arxiv.org/abs/2506.22372)
*Maryam Mousavian,Zahra Abbasiantaeb,Mohammad Aliannejadi,Fabio Crestani*

Key words: 性别偏见,LLM,CWEx,信息检索

TL;DR: 论文提出了一种基于LLM的性别偏见检测新方法，并引入新指标CWEx，提高了对排序系统中性别偏见的评估效果。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 解决现有NLP和IR系统中社会偏见的检测与评估问题，特别是性别偏见。

Method: 利用LLM检测性别偏见，提出新指标CWEx，并在MS MARCO数据子集上标注性别偏见。

Result: CWEx比现有指标更详细，与人类标注一致性更高（Cohen's Kappa为58.77%和18.51%）。

Conclusion: 结合LLM、新指标和标注数据，为IR系统偏见分析提供了更稳健的框架。

Abstract: The presence of social biases in Natural Language Processing (NLP) and
Information Retrieval (IR) systems is an ongoing challenge, which underlines
the importance of developing robust approaches to identifying and evaluating
such biases. In this paper, we aim to address this issue by leveraging Large
Language Models (LLMs) to detect and measure gender bias in passage ranking.
Existing gender fairness metrics rely on lexical- and frequency-based measures,
leading to various limitations, e.g., missing subtle gender disparities.
Building on our LLM-based gender bias detection method, we introduce a novel
gender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to
address existing limitations. To measure the effectiveness of our proposed
metric and study LLMs' effectiveness in detecting gender bias, we annotate a
subset of the MS MARCO Passage Ranking collection and release our new gender
bias collection, called MSMGenderBias, to foster future research in this area.
Our extensive experimental results on various ranking models show that our
proposed metric offers a more detailed evaluation of fairness compared to
previous metrics, with improved alignment to human labels (58.77% for
Grep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa
agreement), effectively distinguishing gender bias in ranking. By integrating
LLM-driven bias detection, an improved fairness metric, and gender bias
annotations for an established dataset, this work provides a more robust
framework for analyzing and mitigating bias in IR systems.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [253] [Storm Surge in Color: RGB-Encoded Physics-Aware Deep Learning for Storm Surge Forecasting](https://arxiv.org/abs/2506.21743)
*Jinpai Zhao,Albert Cerrone,Eirik Valseth,Leendert Westerink,Clint Dawson*

Key words: 风暴潮预测, ConvLSTM, 图像表示, 风场, 地形-水深

TL;DR: 论文提出了一种新的风暴潮预测方法，通过将非结构化水位场投影到RGB编码的图像表示，结合ConvLSTM网络，提升了预测的时空性能。

<details>
  <summary>Details</summary>

Main category: cs.CE

Motivation: 现有风暴潮预测方法在空间分辨率、数据依赖性和通用性方面存在不足，且与现代深度学习架构不兼容。

Method: 使用RGB编码图像表示水位场，结合ConvLSTM网络，并集成风场和地形-水深数据作为输入。

Result: 在墨西哥湾的合成风暴数据上验证了方法的有效性，展现了强时空扩展性。

Conclusion: 该方法在风暴潮预测的可用性、适应性和可解释性方面取得了进展。

Abstract: Storm surge forecasting plays a crucial role in coastal disaster
preparedness, yet existing machine learning approaches often suffer from
limited spatial resolution, reliance on coastal station data, and poor
generalization. Moreover, many prior models operate directly on unstructured
spatial data, making them incompatible with modern deep learning architectures.
In this work, we introduce a novel approach that projects unstructured water
elevation fields onto structured Red Green Blue (RGB)-encoded image
representations, enabling the application of Convolutional Long Short Term
Memory (ConvLSTM) networks for end-to-end spatiotemporal surge forecasting. Our
model further integrates ground-truth wind fields as dynamic conditioning
signals and topo-bathymetry as a static input, capturing physically meaningful
drivers of surge evolution. Evaluated on a large-scale dataset of synthetic
storms in the Gulf of Mexico, our method demonstrates robust 48-hour
forecasting performance across multiple regions along the Texas coast and
exhibits strong spatial extensibility to other coastal areas. By combining
structured representation, physically grounded forcings, and scalable deep
learning, this study advances the frontier of storm surge forecasting in
usability, adaptability, and interpretability.

</details>


### [254] [Laser Scan Path Design for Controlled Microstructure in Additive Manufacturing with Integrated Reduced-Order Phase-Field Modeling and Deep Reinforcement Learning](https://arxiv.org/abs/2506.21815)
*Augustine Twumasi,Prokash Chandra Roy,Zixun Li,Soumya Shouvik Bhattacharjee,Zhengtao Gan*

Key words: 激光粉末床融合, 3D U-Net, 深度强化学习, 相场法, 微观结构优化

TL;DR: 提出了一种物理引导的机器学习方法，通过3D U-Net和深度强化学习优化激光粉末床融合的微观结构控制。

<details>
  <summary>Details</summary>

Main category: cs.CE

Motivation: L-PBF技术微观结构复杂，影响产品质量，传统方法计算成本高。

Method: 结合相场法和3D U-Net预测晶粒取向，利用深度强化学习优化扫描路径。

Result: 实现了计算速度提升两个数量级，验证了深度强化学习的有效性。

Conclusion: 机器学习方法显著提升L-PBF微观结构控制和计算效率。

Abstract: Laser powder bed fusion (L-PBF) is a widely recognized additive manufacturing
technology for producing intricate metal components with exceptional accuracy.
A key challenge in L-PBF is the formation of complex microstructures affecting
product quality. We propose a physics-guided, machine-learning approach to
optimize scan paths for desired microstructure outcomes, such as equiaxed
grains. We utilized a phase-field method (PFM) to model crystalline grain
structure evolution. To reduce computational costs, we trained a surrogate
machine learning model, a 3D U-Net convolutional neural network, using
single-track phase-field simulations with various laser powers to predict
crystalline grain orientations based on initial microstructure and thermal
history. We investigated three scanning strategies across various hatch
spacings within a square domain, achieving a two-orders-of-magnitude speedup
using the surrogate model. To reduce trial and error in designing laser scan
toolpaths, we used deep reinforcement learning (DRL) to generate optimized scan
paths for target microstructure. Results from three cases demonstrate the DRL
approach's effectiveness. We integrated the surrogate 3D U-Net model into our
DRL environment to accelerate the reinforcement learning training process. The
reward function minimizes both aspect ratio and grain volume of the predicted
microstructure from the agent's scan path. The reinforcement learning algorithm
was benchmarked against conventional zigzag approach for smaller and larger
domains, showing machine learning methods' potential to enhance microstructure
control and computational efficiency in L-PBF optimization.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [255] [A Plea for History and Philosophy of Statistics and Machine Learning](https://arxiv.org/abs/2506.22236)
*Hanti Lin*

Key words: 统计学,机器学习,历史与哲学整合,可实现主义,方法论

TL;DR: 本文探讨了统计学与机器学习的交叉整合，提出了“可实现主义”这一概念，结合了历史和哲学视角及方法论。

<details>
  <summary>Details</summary>

Main category: stat.OT

Motivation: 当前人工智能的成功很大程度上依赖于机器学习，而机器学习与统计学的界限日益模糊，因此需要整合历史、哲学以及统计与机器学习领域。

Method: 通过案例研究，追溯机器学习中一个哲学思想的根源至Neyman和Pearson的研究，并揭示其在频率统计与机器学习中的隐含假设。

Result: 提出了“可实现主义”这一基础假设，并展示了历史和哲学与形式认识论在方法论上的整合。

Conclusion: 强调了整合历史和哲学、统计学与机器学习的重要性，并提出了一种新的方法论框架。

Abstract: The integration of the history and philosophy of statistics was initiated at
least by Hacking (1965) and advanced by Mayo (1996), but it has not received
sustained follow-up. Yet such integration is more urgent than ever, as the
recent success of artificial intelligence has been driven largely by machine
learning -- a field historically developed alongside statistics. Today, the
boundary between statistics and machine learning is increasingly blurred. What
we now need is integration, twice over: of history and philosophy, and of the
field they engage -- statistics and machine learning. I present a case study of
a philosophical idea in machine learning (and in formal epistemology) whose
root can be traced back to an often under-appreciated insight in Neyman and
Pearson's 1936 work (a follow-up to their 1933 classic). This leads to the
articulation of a foundational assumption -- largely implicit in, but shared
by, the practices of frequentist statistics and machine learning -- which I
call achievabilism. Another integration also emerges at the level of
methodology, combining two ends of the philosophy of science spectrum: history
and philosophy of science on the one hand, and formal epistemology on the other
hand.

</details>
